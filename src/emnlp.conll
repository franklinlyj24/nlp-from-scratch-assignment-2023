-DOCSTART- -X- O
2018 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics1When -X- _ O
does -X- _ O
deep -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
work -X- _ O
for -X- _ O
loosely -X- _ O
related -X- _ O
document -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
? -X- _ O

We -X- _ O
are -X- _ O
the -X- _ O
ﬁrst -X- _ O
to -X- _ O
study -X- _ O
this -X- _ O
in -X- _ O
a -X- _ O
text -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
setting -X- _ O
and -X- _ O
across -X- _ O
more -X- _ O
than -X- _ O
500 -X- _ O
different -X- _ O
task -X- _ O
pairs -X- _ O
. -X- _ O

In -X- _ O
text -X- _ O
classiﬁcation -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
performance -X- _ O
also -X- _ O
depends -X- _ O
crucially -X- _ O
on -X- _ O
the -X- _ O
divergence -X- _ O
between -X- _ O
the -X- _ O
marginal -X- _ O
distributions -X- _ O
of -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
and -X- _ O
auxiliary -X- _ O
task -X- _ O
. -X- _ O

Document -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
comes -X- _ O
in -X- _ O
many -X- _ O
different -X- _ O
ﬂavors -X- _ O
, -X- _ O
including -X- _ O
spam -X- _ O
detection -X- _ O
, -X- _ O
sentiment -X- _ O
analysis -X- _ O
, -X- _ O
customer -X- _ O
support -X- _ O
ticket -X- _ O
routing -X- _ O
, -X- _ O
and -X- _ O
diagnosis -X- _ O
support -X- _ O
based -X- _ O
on -X- _ O
patient -X- _ O
records -X- _ O
, -X- _ O
but -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
topic -X- _ O
- -X- _ O
level -X- _ O
multi -X- _ O
- -X- _ O
way -X- _ O
classiﬁcation -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
dataset -X- _ O
, -X- _ O
a -X- _ O
corpus -X- _ O
of -X- _ O
newsgroup -X- _ O
posts -X- _ O
that -X- _ O
are -X- _ O
labeled -X- _ O
by -X- _ O
the -X- _ O
topics -X- _ O
of -X- _ O
the -X- _ O
newsgroups -X- _ O
. -X- _ O

One -X- _ O
key -X- _ O
challenge -X- _ O
in -X- _ O
document -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
is -X- _ O
the -X- _ O
high -X- _ O
number -X- _ O
of -X- _ O
feature -X- _ O
dimensions -X- _ O
introduced -X- _ O
by -X- _ O
n -X- _ O
- -X- _ O
gram -X- _ O
features -X- _ O
, -X- _ O
often -X- _ O
outnumbering -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
document -X- _ O
instances -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
corpus -X- _ O
. -X- _ O

Previous -X- _ O
empirical -X- _ O
meta -X- _ O
- -X- _ O
studies -X- _ O
of -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
have -X- _ O
focused -X- _ O
on -X- _ O
sequence -X- _ O
tagging -X- _ O
problems -X- _ O
and -X- _ O
recurrent -X- _ O
neural -X- _ O
networks -X- _ O
, -X- _ O
but -X- _ O
there -X- _ O
is -X- _ O
no -X- _ O
guarantee -X- _ O
that -X- _ O
results -X- _ O
extend -X- _ O
to -X- _ O
document -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
. -X- _ O

2rent -X- _ O
neural -X- _ O
networks -X- _ O
, -X- _ O
is -X- _ O
thus -X- _ O
motivated -X- _ O
by -X- _ O
a -X- _ O
) -X- _ O
an -X- _ O
interest -X- _ O
in -X- _ O
whether -X- _ O
previous -X- _ O
ﬁndings -X- _ O
generalize -X- _ O
to -X- _ O
document -X- _ O
classiﬁcation -X- _ O
algorithms -X- _ O
– -X- _ O
in -X- _ O
our -X- _ O
case -X- _ O
, -X- _ O
multi -X- _ B-MethodName
- -X- _ I-MethodName
layered -X- _ I-MethodName
perceptrons -X- _ I-MethodName
, -X- _ O
b -X- _ O
) -X- _ O
a -X- _ O
practical -X- _ O
consideration -X- _ O
that -X- _ O
any -X- _ O
recommendations -X- _ O
coming -X- _ O
out -X- _ O
of -X- _ O
a -X- _ O
study -X- _ O
of -X- _ O
document -X- _ O
classiﬁcation -X- _ O
would -X- _ O
be -X- _ O
helpful -X- _ O
to -X- _ O
a -X- _ O
wider -X- _ O
audience -X- _ O
. -X- _ O

As -X- _ O
already -X- _ O
said -X- _ O
, -X- _ O
our -X- _ O
focus -X- _ O
on -X- _ O
topic -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
classiﬁcation -X- _ I-TaskName
is -X- _ O
motivated -X- _ O
by -X- _ O
the -X- _ O
observation -X- _ O
that -X- _ O
this -X- _ O
is -X- _ O
an -X- _ O
extremely -X- _ O
common -X- _ O
problem -X- _ O
, -X- _ O
and -X- _ O
key -X- _ O
to -X- _ O
structuring -X- _ O
content -X- _ O
on -X- _ O
websites -X- _ O
, -X- _ O
customer -X- _ O
support -X- _ O
ticket -X- _ O
routing -X- _ O
, -X- _ O
intelligent -X- _ O
email -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O

Also -X- _ O
, -X- _ O
the -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
corpus -X- _ O
uses -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
20 -X- _ O
labels -X- _ O
that -X- _ O
are -X- _ O
hierarchically -X- _ O
organized -X- _ O
( -X- _ O
see -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
we -X- _ O
can -X- _ O
exploit -X- _ O
to -X- _ O
extract -X- _ O
a -X- _ O
large -X- _ O
set -X- _ O
of -X- _ O
task -X- _ O
pairs -X- _ O
. -X- _ O

The -X- _ O
problem -X- _ O
that -X- _ O
we -X- _ O
consider -X- _ O
is -X- _ O
the -X- _ O
following -X- _ O
: -X- _ O
If -X- _ O
we -X- _ O
have -X- _ O
two -X- _ O
topic -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
classiﬁcation -X- _ I-TaskName
datasets -X- _ O
that -X- _ O
are -X- _ O
loosely -X- _ O
related -X- _ O
– -X- _ O
i.e -X- _ O
, -X- _ O
contrasts -X- _ O
the -X- _ O
same -X- _ O
upper -X- _ O
level -X- _ O
classes -X- _ O
in -X- _ O
the -X- _ O
hierarchy -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
– -X- _ O
and -X- _ O
we -X- _ O
have -X- _ O
run -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
experiments -X- _ O
for -X- _ O
each -X- _ O
of -X- _ O
these -X- _ O
, -X- _ O
when -X- _ O
does -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
help -X- _ O
, -X- _ O
keeping -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
ﬁxed -X- _ O
? -X- _ O

We -X- _ O
present -X- _ O
the -X- _ O
ﬁrst -X- _ O
study -X- _ O
of -X- _ O
when -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
works -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
document -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
. -X- _ O

This -X- _ O
is -X- _ O
, -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
also -X- _ O
the -X- _ O
ﬁrst -X- _ O
metastudy -X- _ O
that -X- _ O
focuses -X- _ O
on -X- _ O
hard -X- _ O
parameter -X- _ O
sharing -X- _ O
in -X- _ O
multilayered -X- _ B-MethodName
perceptrons -X- _ I-MethodName
, -X- _ O
although -X- _ O
this -X- _ O
approach -X- _ O
to -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
goes -X- _ O
all -X- _ O
the -X- _ O
way -X- _ O
back -X- _ O
to -X- _ O
( -X- _ O
Caruana -X- _ O
, -X- _ O
1993 -X- _ O
) -X- _ O
. -X- _ O

2 -X- _ O
Related -X- _ O
Work -X- _ O
Document -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
has -X- _ O
a -X- _ O
very -X- _ O
long -X- _ O
history -X- _ O
and -X- _ O
is -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
most -X- _ O
fundamental -X- _ O
applications -X- _ O
of -X- _ O
machine -X- _ O
learning -X- _ O
. -X- _ O

The -X- _ O
dataset -X- _ O
that -X- _ O
we -X- _ O
use -X- _ O
, -X- _ O
is -X- _ O
20 -X- _ O
Newsgroups.1It -X- _ O
has -X- _ O
been -X- _ O
used -X- _ O
in -X- _ O
several -X- _ O
comparisons -X- _ O
of -X- _ O
classiﬁcation -X- _ O
algorithms -X- _ O
( -X- _ O
Dredze -X- _ O
, -X- _ O
Crammer -X- _ O
, -X- _ O
and -X- _ O
Pereira -X- _ O
, -X- _ O
2008 -X- _ O
; -X- _ O
Crammer -X- _ O
and -X- _ O
Chechik -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
some -X- _ O
of -X- _ O
the -X- _ O
best -X- _ O
results -X- _ O
have -X- _ O
been -X- _ O
achieved -X- _ O
with -X- _ O
random -X- _ O
forests -X- _ O
and -X- _ O
multi -X- _ B-MethodName
- -X- _ I-MethodName
layered -X- _ I-MethodName
perceptrons -X- _ I-MethodName
( -X- _ O
deep -X- _ O
learning -X- _ O
models -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
with -X- _ O
multi -X- _ B-MethodName
- -X- _ I-MethodName
layered -X- _ I-MethodName
perceptrons -X- _ I-MethodName
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
hard -X- _ O
parameter -X- _ O
sharing -X- _ O
, -X- _ O
in -X- _ O
which -X- _ O
we -X- _ O
jointly -X- _ O
learn -X- _ O
mmulti -X- _ B-MethodName
- -X- _ I-MethodName
layered -X- _ I-MethodName
perceptrons -X- _ I-MethodName
that -X- _ O
share -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
their -X- _ O
hidden -X- _ O
layers -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
( -X- _ O
Baxter -X- _ O
and -X- _ O
others -X- _ O
, -X- _ O
2000 -X- _ O
) -X- _ O
requires -X- _ O
the -X- _ O
tasks -X- _ O
to -X- _ O
have -X- _ O
shared -X- _ O
optimal -X- _ O
hypothesis -X- _ O
classes -X- _ O
; -X- _ O
which -X- _ O
does -X- _ O
not -X- _ O
have -X- _ O
to -X- _ O
be -X- _ O
the -X- _ O
case -X- _ O
in -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
. -X- _ O

1http -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
qwone.com -X- _ O
/ -X- _ O
˜jason -X- _ O
/ -X- _ O
20Newsgroups -X- _ O
/ -X- _ O

3comp -X- _ O
sys -X- _ O
aothers -X- _ O
brec -X- _ O
sport -X- _ O
cvehicles -X- _ O
dsci -X- _ O
sci -X- _ O
etalk -X- _ O
politic -X- _ O
freligion -X- _ O
gother -X- _ O
other -X- _ O
h -X- _ O
Figure -X- _ O
1 -X- _ O
: -X- _ O
Hierarchical -X- _ O
structure -X- _ O
of -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
, -X- _ O
with -X- _ O
a= -X- _ O
ibm.pc.hardware -X- _ O
, -X- _ O
mac.hardware -X- _ O
; -X- _ O
b= -X- _ O
graphics -X- _ O
, -X- _ O
os.ms-windows.misc -X- _ O
, -X- _ O
windows.x -X- _ O
; -X- _ O
c= -X- _ O
baseball -X- _ O
, -X- _ O
hockey -X- _ O
; -X- _ O
d= -X- _ O
autos -X- _ O
, -X- _ O
motorcycles -X- _ O
; -X- _ O
e= -X- _ O
crypt -X- _ O
, -X- _ O
electronics -X- _ O
, -X- _ O
med -X- _ O
, -X- _ O
space -X- _ O
; -X- _ O
f= -X- _ O
misc -X- _ O
, -X- _ O
guns -X- _ O
, -X- _ O
mideast -X- _ O
; -X- _ O
g= -X- _ O
misc -X- _ O
, -X- _ O
atheism -X- _ O
, -X- _ O
christian -X- _ O
, -X- _ O
h= -X- _ O
forsale -X- _ O
. -X- _ O

3 -X- _ O
Methodology -X- _ O
We -X- _ O
begin -X- _ O
with -X- _ O
a -X- _ O
brief -X- _ O
summary -X- _ O
of -X- _ O
our -X- _ O
methodology -X- _ O
: -X- _ O
We -X- _ O
sample -X- _ O
pairs -X- _ O
of -X- _ O
tasks -X- _ O
from -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
. -X- _ O

The -X- _ O
documents -X- _ O
are -X- _ O
represented -X- _ O
as -X- _ O
TF -X- _ O
- -X- _ O
IDF -X- _ O
vectors -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
train -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
multilayered -X- _ B-MethodName
perceptrons -X- _ I-MethodName
to -X- _ O
predict -X- _ O
topics -X- _ O
from -X- _ O
such -X- _ O
vectors -X- _ O
. -X- _ O

3.1 -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
The -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
data -X- _ O
set -X- _ O
is -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
approximately -X- _ O
20,000 -X- _ O
newsgroup -X- _ O
documents -X- _ O
, -X- _ O
partitioned -X- _ O
across -X- _ O
20 -X- _ O
different -X- _ O
topics -X- _ O
. -X- _ O

Some -X- _ O
of -X- _ O
the -X- _ O
newsgroups -X- _ O
are -X- _ O
very -X- _ O
closely -X- _ O
related -X- _ O
and -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
subtopics -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
topic -X- _ O
, -X- _ O
while -X- _ O
others -X- _ O
are -X- _ O
highly -X- _ O
unrelated -X- _ O
. -X- _ O

Classiﬁcation -X- _ O
tasks -X- _ O
Based -X- _ O
on -X- _ O
the -X- _ O
20 -X- _ O
Newsgroups -X- _ O
’ -X- _ O
structure -X- _ O

This -X- _ O
representation -X- _ O
is -X- _ O
known -X- _ O
to -X- _ O
be -X- _ O
efﬁcient -X- _ O
( -X- _ O
Salton -X- _ O
and -X- _ O
Buckley -X- _ O
, -X- _ O
1988 -X- _ O
; -X- _ O
Aizawa -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
; -X- _ O
especially -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
text -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
( -X- _ O
Zhang -X- _ O
, -X- _ O
Yoshida -X- _ O
, -X- _ O
and -X- _ O
Tang -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
keep -X- _ O
the -X- _ O
10,000 -X- _ B-HyperparameterValue
most -X- _ B-HyperparameterName
frequent -X- _ I-HyperparameterName
features -X- _ I-HyperparameterName
, -X- _ O
the -X- _ O
frequency -X- _ O
being -X- _ O
computed -X- _ O
on -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
available -X- _ O
for -X- _ O
the -X- _ O
entire -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
corpus -X- _ O
. -X- _ O

3.4 -X- _ O
Models -X- _ O
Both -X- _ O
our -X- _ O
single -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
architectures -X- _ O
consist -X- _ O
of -X- _ O
a -X- _ O
multi -X- _ B-MethodName
- -X- _ I-MethodName
layered -X- _ I-MethodName
perceptron -X- _ I-MethodName
with -X- _ O
two -X- _ O
hidden -X- _ O
layers -X- _ O
. -X- _ O

We -X- _ O
tune -X- _ O
the -X- _ O
following -X- _ O
hyper -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
architectures -X- _ O
on -X- _ O
a -X- _ O
similar -X- _ O
document -X- _ O
classiﬁcation -X- _ O
problem -X- _ O
, -X- _ O
using -X- _ O
data -X- _ O
from -X- _ O
Amazon -X- _ O
reviews,2and -X- _ O
, -X- _ O
following -X- _ O
( -X- _ O
Bingel -X- _ O
and -X- _ O
Søgaard -X- _ O
, -X- _ O
2https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
www.cs.jhu.edu -X- _ O
/ -X- _ O
˜mdredze -X- _ O
/ -X- _ O
datasets -X- _ O
/ -X- _ O
sentiment -X- _ O
/ -X- _ O
index2.html2017 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
the -X- _ O
same -X- _ O
hyper -X- _ O
- -X- _ O
parameter -X- _ O
values -X- _ O
to -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
: -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
hidden -X- _ I-HyperparameterName
layers -X- _ I-HyperparameterName
( -X- _ O
2 -X- _ B-HyperparameterValue
) -X- _ O
and -X- _ O
layer -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
( -X- _ O
100 -X- _ B-HyperparameterValue
) -X- _ O
. -X- _ O

These -X- _ O
features -X- _ O
are -X- _ O
similar -X- _ O
to -X- _ O
those -X- _ O
used -X- _ O
in -X- _ O
( -X- _ O
Bingel -X- _ O
and -X- _ O
Søgaard -X- _ O
, -X- _ O
Jensen -X- _ O
- -X- _ O
Shannon -X- _ O
Divergence -X- _ O
between -X- _ O
the -X- _ O
( -X- _ O
unigram -X- _ O
) -X- _ O
word -X- _ O
distributions -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
and -X- _ O
auxiliary -X- _ O
task -X- _ O
training -X- _ O
sets -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
internally -X- _ O
( -X- _ O
between -X- _ O
target -X- _ O
and -X- _ O
test -X- _ O
data -X- _ O
) -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
, -X- _ O
Gradients -X- _ O
of -X- _ O
the -X- _ O
loss -X- _ O
curve -X- _ O
at -X- _ O
10 -X- _ O
, -X- _ O
25 -X- _ O
, -X- _ O
50 -X- _ O
and -X- _ O
75 -X- _ O
percent -X- _ O
of -X- _ O
a -X- _ O
training -X- _ O
of -X- _ O
150 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
, -X- _ O
for -X- _ O
each -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
relative -X- _ O
differences -X- _ O
in -X- _ O
the -X- _ O
learning -X- _ O
curve -X- _ O
gradients -X- _ O
, -X- _ O
Type -X- _ O
- -X- _ O
token -X- _ O
ratios -X- _ O
and -X- _ O
out -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
vocabulary -X- _ O
rates -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
and -X- _ O
auxiliary -X- _ O
task -X- _ O
training -X- _ O
sets -X- _ O
, -X- _ O
and -X- _ O
their -X- _ O
relative -X- _ O
difference -X- _ O
, -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
ﬁt -X- _ O
logarithmic -X- _ O
functions -X- _ O
to -X- _ O
the -X- _ O
( -X- _ O
log -X- _ O
- -X- _ O
like -X- _ O
) -X- _ O
loss -X- _ O
curves -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
function -X- _ O
is -X- _ O
of -X- _ O
the -X- _ O
form -X- _ O
: -X- _ O
aln -X- _ O
( -X- _ O
ci+d -X- _ O
) -X- _ O
+ -X- _ O
b -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
include -X- _ O
aandcas -X- _ O
features -X- _ O
. -X- _ O

( -X- _ O
a -X- _ O
) -X- _ O
R -X- _ O
ELATED -X- _ O
TOPICS -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
U -X- _ O
NRELATED -X- _ O
TOPICS -X- _ O
Figure -X- _ O
4 -X- _ O
: -X- _ O
Mean -X- _ O
F -X- _ B-MetricName
1over -X- _ I-MetricName
the -X- _ O
number -X- _ O
of -X- _ O
epochs -X- _ B-HyperparameterName
, -X- _ O
for -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
( -X- _ O
crosses -X- _ O
/ -X- _ O
blue -X- _ O
) -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
models -X- _ O
( -X- _ O
points -X- _ O
/ -X- _ O
orange -X- _ O
) -X- _ O
, -X- _ O
for -X- _ O
classiﬁcation -X- _ O
problems -X- _ O
1 -X- _ O
and -X- _ O
2 -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
for -X- _ O
both -X- _ O
R -X- _ O
ELATED -X- _ O
TOPICS -X- _ O
and -X- _ O
U -X- _ O
NRELATED -X- _ O
TOPICS -X- _ O
, -X- _ O
as -X- _ O
explained -X- _ O
above -X- _ O
. -X- _ O

Figures -X- _ O
4a -X- _ O
and -X- _ O
4b -X- _ O
plot -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
epochs -X- _ I-HyperparameterName
on -X- _ O
the -X- _ O
F -X- _ B-MetricName
1scores -X- _ I-MetricName
. -X- _ O

F1scores -X- _ B-MetricName
, -X- _ O
obtained -X- _ O
by -X- _ O
a -X- _ O
logistic -X- _ B-MethodName
regression -X- _ I-MethodName
model -X- _ O
over100runs -X- _ B-HyperparameterValue
using -X- _ O
a -X- _ O
5 -X- _ B-HyperparameterValue
- -X- _ O
fold -X- _ B-HyperparameterName
cross -X- _ I-HyperparameterName
- -X- _ I-HyperparameterName
validation -X- _ I-HyperparameterName
procedure -X- _ O
, -X- _ O
are -X- _ O
reported -X- _ O
at -X- _ O
the -X- _ O
end -X- _ O
of -X- _ O
the -X- _ O
next -X- _ O
section -X- _ O
. -X- _ O

The -X- _ O
mean -X- _ O
F1scores -X- _ B-HyperparameterName
across -X- _ O
all -X- _ O
the -X- _ O
problems -X- _ O
, -X- _ O
and -X- _ O
ﬁve -X- _ O
runs -X- _ O
, -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
. -X- _ O

The -X- _ O
number -X- _ O
of -X- _ O
epochs -X- _ O
needed -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
models -X- _ O
is -X- _ O
slightly -X- _ O
greater -X- _ O
than -X- _ O
the -X- _ O
one -X- _ O
for -X- _ O
the -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
ones -X- _ O
( -X- _ O
Figures -X- _ O
4a -X- _ O
and -X- _ O
4b -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
global -X- _ O
stabilization -X- _ O
occurs -X- _ O
after -X- _ O
approximatively -X- _ O
75 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
. -X- _ O

Table -X- _ O
1 -X- _ O
: -X- _ O
Mean -X- _ O
F1score -X- _ B-HyperparameterName
for -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
models -X- _ O
, -X- _ O
with -X- _ O
average -X- _ O
fraction -X- _ O
of -X- _ O
datasets -X- _ O
with -X- _ O
improvements -X- _ O
. -X- _ O

Of -X- _ O
course -X- _ O
, -X- _ O
JSD -X- _ O
over -X- _ O
unigram -X- _ O
occurrences -X- _ O
is -X- _ O
more -X- _ O
closely -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
bias -X- _ O
arising -X- _ O
when -X- _ O
training -X- _ O
document -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
models -X- _ O
on -X- _ O
loosely -X- _ O
related -X- _ O
tasks -X- _ O
, -X- _ O
than -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
bias -X- _ O
in -X- _ O
sequence -X- _ O
models -X- _ O
. -X- _ O

7 -X- _ O
Conclusion -X- _ O
We -X- _ O
have -X- _ O
investigated -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
singletask -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
multi -X- _ B-MethodName
layer -X- _ I-MethodName
perceptrons -X- _ I-MethodName
for -X- _ O
text -X- _ O
classiﬁcation -X- _ O
using -X- _ O
a -X- _ O
TF -X- _ O
- -X- _ O
IDF -X- _ O
representation -X- _ O
of -X- _ O
documents -X- _ O
. -X- _ O

We -X- _ O
ran -X- _ O
experiments -X- _ O
on -X- _ O
the -X- _ O
20 -X- _ B-DatasetName
Newsgroups -X- _ I-DatasetName
corpus -X- _ O
and -X- _ O
took -X- _ O
advantage -X- _ O
of -X- _ O
the -X- _ O
class -X- _ O
hierarchy -X- _ O
in -X- _ O
this -X- _ O
dataset -X- _ O
, -X- _ O
to -X- _ O
extract -X- _ O
hundreds -X- _ O
of -X- _ O
pairs -X- _ O
of -X- _ O
loosely -X- _ O
related -X- _ O
documents -X- _ O
, -X- _ O
for -X- _ O
which -X- _ O
no -X- _ O
theoretical -X- _ O
guarantees -X- _ O
exist -X- _ O
. -X- _ O

Our -X- _ O
experiments -X- _ O
show -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
loss -X- _ O
curve -X- _ O
gradients -X- _ O
and -X- _ O
out -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
vocabulary -X- _ O
rates -X- _ O
, -X- _ O
supporting -X- _ O
recent -X- _ O
ﬁndings -X- _ O
from -X- _ O
sequence -X- _ O
tagging -X- _ O
( -X- _ O
Bingel -X- _ O
and -X- _ O
Søgaard -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
but -X- _ O
we -X- _ O
also -X- _ O
see -X- _ O
that -X- _ O
biases -X- _ O
in -X- _ O
the -X- _ O
marginal -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
data -X- _ O
, -X- _ O
as -X- _ O
measured -X- _ O
by -X- _ O
JSD -X- _ O
, -X- _ O
are -X- _ O
predictive -X- _ O
of -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
gains -X- _ O
in -X- _ O
document -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
. -X- _ O

Conﬁdence -X- _ O
- -X- _ O
weighted -X- _ O
linear -X- _ O
classiﬁcation -X- _ O
. -X- _ O

A -X- _ O
comparative -X- _ O
study -X- _ O
of -X- _ O
tf*idf -X- _ O
, -X- _ O
lsi -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
words -X- _ O
for -X- _ O
text -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
. -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
2018 -X- _ O
EMNLP -X- _ O
Workshop -X- _ O
BlackboxNLP -X- _ O
: -X- _ O
Analyzing -X- _ O
and -X- _ O
Interpreting -X- _ O
Neural -X- _ O
Networks -X- _ O
for -X- _ O
NLP -X- _ O
, -X- _ O
pages -X- _ O
9–15 -X- _ O
Brussels -X- _ O
, -X- _ O
Belgium -X- _ O
, -X- _ O
November -X- _ O
1 -X- _ O
, -X- _ O
2018 -X- _ O
. -X- _ O

c -X- _ O

2018 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics9Analyzing -X- _ O
Learned -X- _ O
Representations -X- _ O
of -X- _ O
a -X- _ O
Deep -X- _ O
ASR -X- _ B-TaskName
Performance -X- _ O

Prediction -X- _ O
Model -X- _ O
Zied -X- _ O
Elloumi1 -X- _ O
; -X- _ O
2Laurent -X- _ O
Besacier2Olivier -X- _ O
Galibert1 -X- _ O
firstname.name -X- _ O
@ -X- _ O
lne.fr -X- _ O
firstname.name -X- _ O
@ -X- _ O
univ -X- _ O
- -X- _ O
grenoble -X- _ O
- -X- _ O
alpes.frBenjamin -X- _ O

Lecouteux2 -X- _ O
Abstract -X- _ O
This -X- _ O
paper -X- _ O
addresses -X- _ O
a -X- _ O
relatively -X- _ O
new -X- _ O
task -X- _ O
: -X- _ O
prediction -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
performance -X- _ O
on -X- _ O
unseen -X- _ O
broadcast -X- _ O
programs -X- _ O
. -X- _ O

In -X- _ O
a -X- _ O
previous -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
presented -X- _ O
an -X- _ O
ASR -X- _ B-TaskName
performance -X- _ O
prediction -X- _ O
system -X- _ O
using -X- _ O
CNNs -X- _ B-MethodName
that -X- _ O
encode -X- _ O
both -X- _ O
text -X- _ O
( -X- _ O
ASR -X- _ B-TaskName
transcript -X- _ O
) -X- _ O
and -X- _ O
speech -X- _ O
, -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
predict -X- _ O
word -X- _ O
error -X- _ O
rate -X- _ O
. -X- _ O

This -X- _ O
work -X- _ O
is -X- _ O
dedicated -X- _ O
to -X- _ O
the -X- _ O
analysis -X- _ O
of -X- _ O
speech -X- _ O
signal -X- _ O
embeddings -X- _ O
and -X- _ O
text -X- _ O
embeddings -X- _ O
learnt -X- _ O
by -X- _ O
the -X- _ O
CNN -X- _ B-MethodName
while -X- _ O
training -X- _ O
our -X- _ O
prediction -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
try -X- _ O
to -X- _ O
better -X- _ O
understand -X- _ O
which -X- _ O
information -X- _ O
is -X- _ O
captured -X- _ O
by -X- _ O
the -X- _ O
deep -X- _ O
model -X- _ O
and -X- _ O
its -X- _ O
relation -X- _ O
with -X- _ O
different -X- _ O
conditioning -X- _ O
factors -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
shown -X- _ O
that -X- _ O
hidden -X- _ B-HyperparameterName
layers -X- _ I-HyperparameterName
convey -X- _ O
a -X- _ O
clear -X- _ O
signal -X- _ O
about -X- _ O
speech -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
and -X- _ O
broadcast -X- _ O
type -X- _ O
. -X- _ O

We -X- _ O
then -X- _ O
try -X- _ O
to -X- _ O
leverage -X- _ O
these -X- _ O
3 -X- _ O
types -X- _ O
of -X- _ O
information -X- _ O
at -X- _ O
training -X- _ O
time -X- _ O
through -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
. -X- _ O

Our -X- _ O
experiments -X- _ O
show -X- _ O
that -X- _ O
this -X- _ O
allows -X- _ O
to -X- _ O
train -X- _ O
slightly -X- _ O
more -X- _ O
efﬁcient -X- _ O
ASR -X- _ B-TaskName
performance -X- _ O
prediction -X- _ O
systems -X- _ O
that -X- _ O
- -X- _ O
in -X- _ O
addition -X- _ O
- -X- _ O
simultaneously -X- _ O
tag -X- _ O
the -X- _ O
analyzed -X- _ O
utterances -X- _ O
according -X- _ O
to -X- _ O
their -X- _ O
speech -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
and -X- _ O
broadcast -X- _ O
program -X- _ O
origin -X- _ O
. -X- _ O

1 -X- _ O

Introduction -X- _ O
Predicting -X- _ B-TaskName
automatic -X- _ I-TaskName
speech -X- _ I-TaskName
recognition -X- _ I-TaskName
( -X- _ I-TaskName
ASR -X- _ I-TaskName
) -X- _ I-TaskName
performance -X- _ I-TaskName
on -X- _ O
unseen -X- _ O
speech -X- _ O
recordings -X- _ O
is -X- _ O
an -X- _ O
important -X- _ O
Grail -X- _ O
of -X- _ O
speech -X- _ O
research -X- _ O
. -X- _ O

In -X- _ O
a -X- _ O
previous -X- _ O
paper -X- _ O
( -X- _ O
Elloumi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
presented -X- _ O
a -X- _ O
framework -X- _ O
for -X- _ O
modeling -X- _ O
and -X- _ O
evaluating -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
on -X- _ O
unseen -X- _ O
broadcast -X- _ O
programs -X- _ O
. -X- _ O

CNNs -X- _ B-MethodName
were -X- _ O
very -X- _ O
efﬁcient -X- _ O
encoding -X- _ O
both -X- _ O
text -X- _ O
( -X- _ O
ASR -X- _ O
transcript -X- _ O
) -X- _ O
and -X- _ O
speech -X- _ O
to -X- _ O
predict -X- _ O
ASR -X- _ B-TaskName
word -X- _ B-MetricName
error -X- _ I-MetricName
rate -X- _ I-MetricName
( -X- _ O
WER -X- _ B-MetricName
) -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
while -X- _ O
achieving -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
performance -X- _ O
prediction -X- _ O
results -X- _ O
, -X- _ O
our -X- _ O
CNN -X- _ B-MethodName
approach -X- _ O
is -X- _ O
more -X- _ O
difﬁcult -X- _ O
to -X- _ O
understand -X- _ O
compared -X- _ O
to -X- _ O
conventional -X- _ O
approaches -X- _ O
based -X- _ O
on -X- _ O
engineered -X- _ O
features -X- _ O
such -X- _ O
as -X- _ O
TransRater1for -X- _ O
instance -X- _ O
. -X- _ O

This -X- _ O
lack -X- _ O
of -X- _ O
interpretability -X- _ O
of -X- _ O
the -X- _ O
representations -X- _ O
learned -X- _ O
by -X- _ O
deep -X- _ O
neural -X- _ O
networks -X- _ O
is -X- _ O
a -X- _ O
1https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
hlt-mt -X- _ O
/ -X- _ O
TranscRatergeneral -X- _ O
problem -X- _ O
in -X- _ O
AI -X- _ O
. -X- _ O

Recent -X- _ O
papers -X- _ O
started -X- _ O
to -X- _ O
address -X- _ O
this -X- _ O
issue -X- _ O
and -X- _ O
analyzed -X- _ O
hidden -X- _ O
representations -X- _ O
learned -X- _ O
during -X- _ O
training -X- _ O
of -X- _ O
different -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
models -X- _ O
( -X- _ O
Mohamed -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2012 -X- _ O
; -X- _ O
Wu -X- _ O
and -X- _ O
King -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Belinkov -X- _ O
and -X- _ O
Glass -X- _ O
, -X- _ O
Contribution -X- _ O
. -X- _ O

This -X- _ O
work -X- _ O
is -X- _ O
dedicated -X- _ O
to -X- _ O
the -X- _ O
analysis -X- _ O
of -X- _ O
speech -X- _ O
signal -X- _ O
embeddings -X- _ O
and -X- _ O
text -X- _ O
embeddings -X- _ O
learnt -X- _ O
by -X- _ O
the -X- _ O
CNN -X- _ B-MethodName
during -X- _ O
training -X- _ O
of -X- _ O
our -X- _ O
ASR -X- _ B-TaskName
performance -X- _ B-TaskName
prediction -X- _ I-TaskName
model -X- _ O
. -X- _ O

Our -X- _ O
goal -X- _ O
is -X- _ O
to -X- _ O
better -X- _ O
understand -X- _ O
which -X- _ O
information -X- _ O
is -X- _ O
captured -X- _ O
by -X- _ O
the -X- _ O
deep -X- _ O
model -X- _ O
and -X- _ O
its -X- _ O
relation -X- _ O
with -X- _ O
conditioning -X- _ O
factors -X- _ O
such -X- _ O
as -X- _ O
speech -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
or -X- _ O
broadcast -X- _ O
program -X- _ O
type -X- _ O
. -X- _ O

For -X- _ O
this -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
data -X- _ O
set -X- _ O
presented -X- _ O
in -X- _ O
( -X- _ O
Elloumi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
which -X- _ O
contains -X- _ O
a -X- _ O
large -X- _ O
amount -X- _ O
of -X- _ O
speech -X- _ O
utterances -X- _ O
taken -X- _ O
from -X- _ O
various -X- _ O
collections -X- _ O
of -X- _ O
French -X- _ O
broadcast -X- _ O
programs -X- _ O
. -X- _ O

Following -X- _ O
a -X- _ O
methodology -X- _ O
similar -X- _ O
to -X- _ O
( -X- _ O
Belinkov -X- _ O
and -X- _ O
Glass -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
our -X- _ O
deep -X- _ O
performance -X- _ O
prediction -X- _ O
model -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
generate -X- _ O
utterance -X- _ O
level -X- _ O
features -X- _ O
that -X- _ O
are -X- _ O
given -X- _ O
to -X- _ O
a -X- _ O
shallow -X- _ O
classiﬁer -X- _ O
trained -X- _ O
to -X- _ O
solve -X- _ O
secondary -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
shown -X- _ O
that -X- _ O
hidden -X- _ B-HyperparameterName
layers -X- _ I-HyperparameterName
convey -X- _ O
a -X- _ O
clear -X- _ O
signal -X- _ O
about -X- _ O
speech -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
and -X- _ O
show -X- _ O
. -X- _ O

We -X- _ O
then -X- _ O
try -X- _ O
to -X- _ O
leverage -X- _ O
these -X- _ O
3 -X- _ O
types -X- _ O
of -X- _ O
information -X- _ O
at -X- _ O
training -X- _ O
time -X- _ O
through -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
. -X- _ O

Our -X- _ O
experiments -X- _ O
show -X- _ O
that -X- _ O
this -X- _ O
allows -X- _ O
to -X- _ O
train -X- _ O
slightly -X- _ O
more -X- _ O
efﬁcient -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
systems -X- _ O
that -X- _ O
in -X- _ O
addition -X- _ O
- -X- _ O
simultaneously -X- _ O
tag -X- _ O
the -X- _ O
analyzed -X- _ O
utterances -X- _ O
according -X- _ O
to -X- _ O
their -X- _ O
speech -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
and -X- _ O
broadcast -X- _ O
program -X- _ O
origin -X- _ O
. -X- _ O

Outline -X- _ O
. -X- _ O

The -X- _ O
paper -X- _ O
is -X- _ O
organized -X- _ O
as -X- _ O
follows -X- _ O
. -X- _ O

In -X- _ O
section -X- _ O
2 -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
a -X- _ O
brief -X- _ O
overview -X- _ O
of -X- _ O
related -X- _ O
works -X- _ O
and -X- _ O
present -X- _ O
our -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
system -X- _ O
in -X- _ O
section -X- _ O
3 -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
we -X- _ O
detail -X- _ O
our -X- _ O
methodology -X- _ O
to -X- _ O
evaluate -X- _ O
learned -X- _ O
representations -X- _ O
in -X- _ O
section -X- _ O
4 -X- _ O
. -X- _ O

Our -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
experiments -X- _ O
for -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
are -X- _ O
presented -X- _ O
in -X- _ O
section -X- _ O
5 -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
section -X- _ O
6 -X- _ O
concludes -X- _ O
this -X- _ O
work -X- _ O
. -X- _ O

102 -X- _ O
Related -X- _ O
works -X- _ O
Several -X- _ O
works -X- _ O
tried -X- _ O
to -X- _ O
understand -X- _ O
learned -X- _ O
representations -X- _ O
for -X- _ O
NLP -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
Automatic -X- _ B-TaskName
Speech -X- _ I-TaskName
Recognition -X- _ I-TaskName
( -X- _ I-TaskName
ASR -X- _ I-TaskName
) -X- _ I-TaskName
and -X- _ O
Neural -X- _ O
Machine -X- _ O
Translation -X- _ O
( -X- _ O
NMT -X- _ O
) -X- _ O
. -X- _ O

tried -X- _ O
to -X- _ O
better -X- _ O
understand -X- _ O
the -X- _ O
hidden -X- _ O
representations -X- _ O
of -X- _ O
NMT -X- _ O
models -X- _ O
which -X- _ O
were -X- _ O
given -X- _ O
to -X- _ O
a -X- _ O
shallow -X- _ O
classiﬁer -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
predict -X- _ O
syntactic -X- _ O
labels -X- _ O
( -X- _ O
Shi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
part -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
speech -X- _ O
labels -X- _ O
or -X- _ O
semantic -X- _ O
ones -X- _ O
( -X- _ O
Belinkov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

It -X- _ O
was -X- _ O
shown -X- _ O
that -X- _ O
lower -X- _ O
layers -X- _ O
are -X- _ O
better -X- _ O
at -X- _ O
POS -X- _ O
tagging -X- _ O
, -X- _ O
while -X- _ O
higher -X- _ O
layers -X- _ O
are -X- _ O
better -X- _ O
at -X- _ O
learning -X- _ O
semantics -X- _ O
. -X- _ O

( -X- _ O
Mohamed -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
Belinkov -X- _ O
and -X- _ O
Glass -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
analyzed -X- _ O
the -X- _ O
feature -X- _ O
representations -X- _ O
from -X- _ O
a -X- _ O
deep -X- _ O
ASR -X- _ B-TaskName
model -X- _ O
using -X- _ O
tSNE -X- _ O
visualization -X- _ O
( -X- _ O
Maaten -X- _ O
and -X- _ O
Hinton -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
and -X- _ O
tried -X- _ O
to -X- _ O
understand -X- _ O
which -X- _ O
layers -X- _ O
better -X- _ O
capture -X- _ O
the -X- _ O
phonemic -X- _ O
information -X- _ O
by -X- _ O
training -X- _ O
a -X- _ O
shallow -X- _ O
phone -X- _ O
classiﬁer -X- _ O
. -X- _ O

Also -X- _ O
relevant -X- _ O
is -X- _ O
the -X- _ O
work -X- _ O
of -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
who -X- _ O
proposed -X- _ O
an -X- _ O
in -X- _ O
- -X- _ O
depth -X- _ O
investigation -X- _ O
on -X- _ O
three -X- _ O
kinds -X- _ O
of -X- _ O
speaker -X- _ O
embeddings -X- _ O
learned -X- _ O
for -X- _ O
a -X- _ O
speaker -X- _ O
recognition -X- _ O
task -X- _ O
, -X- _ O
i.e. -X- _ O
i -X- _ O
- -X- _ O
vector -X- _ O
, -X- _ O
d -X- _ O
- -X- _ O
vector -X- _ O
and -X- _ O
RNN -X- _ O
/ -X- _ O
LSTM -X- _ O
based -X- _ O
sequence -X- _ O
- -X- _ O
vector -X- _ O
( -X- _ O
s -X- _ O
- -X- _ O
vector -X- _ O
) -X- _ O
. -X- _ O

Classiﬁcation -X- _ O
tasks -X- _ O
were -X- _ O
designed -X- _ O
to -X- _ O
facilitate -X- _ O
better -X- _ O
understanding -X- _ O
of -X- _ O
the -X- _ O
encoded -X- _ O
speaker -X- _ O
representations -X- _ O
. -X- _ O

Multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
was -X- _ O
also -X- _ O
proposed -X- _ O
to -X- _ O
integrate -X- _ O
different -X- _ O
speaker -X- _ O
embeddings -X- _ O
and -X- _ O
improve -X- _ O
speaker -X- _ O
veriﬁcation -X- _ O
performance -X- _ O
. -X- _ O

3 -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
system -X- _ O
In -X- _ O
( -X- _ O
Elloumi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
proposed -X- _ O
a -X- _ O
new -X- _ O
approach -X- _ O
using -X- _ O
convolution -X- _ B-MethodName
neural -X- _ I-MethodName
networks -X- _ I-MethodName
( -X- _ O
CNNs -X- _ B-MethodName
) -X- _ O
to -X- _ O
predict -X- _ B-TaskName
ASR -X- _ I-TaskName
performance -X- _ I-TaskName
from -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
heterogeneous -X- _ O
broadcast -X- _ O
programs -X- _ O
( -X- _ O
both -X- _ O
radio -X- _ O
and -X- _ O
TV -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
particularly -X- _ O
focused -X- _ O
on -X- _ O
the -X- _ O
combination -X- _ O
of -X- _ O
text -X- _ O
( -X- _ O
ASR -X- _ B-TaskName
transcription -X- _ O
) -X- _ O
and -X- _ O
signal -X- _ O
( -X- _ O
raw -X- _ O
speech -X- _ O
) -X- _ O
inputs -X- _ O
which -X- _ O
both -X- _ O
proved -X- _ O
useful -X- _ O
for -X- _ O
CNN -X- _ B-MethodName
prediction -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
observed -X- _ O
that -X- _ O
our -X- _ O
system -X- _ O
remarkably -X- _ O
predicts -X- _ B-TaskName
WER -X- _ I-TaskName
distribution -X- _ I-TaskName
on -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
speech -X- _ O
recordings -X- _ O
. -X- _ O

To -X- _ O
obtain -X- _ O
speech -X- _ O
transcripts -X- _ O
( -X- _ O
ASR -X- _ O
outputs -X- _ O
) -X- _ O
for -X- _ O
the -X- _ O
prediction -X- _ O
model -X- _ O
, -X- _ O
we -X- _ O
built -X- _ O
our -X- _ O
own -X- _ O
French -X- _ O
ASR -X- _ B-TaskName
system -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
KALDI -X- _ O
toolkit -X- _ O
( -X- _ O
Povey -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
. -X- _ O

A -X- _ O
hybrid -X- _ O
HMM -X- _ B-MethodName
- -X- _ O
DNN -X- _ B-MethodName
system -X- _ O
was -X- _ O
trained -X- _ O
using -X- _ O
100 -X- _ O
hours -X- _ O
of -X- _ O
broadcast -X- _ O
news -X- _ O
from -X- _ O
Quaero2 -X- _ B-DatasetName
, -X- _ O
ETAPE -X- _ B-DatasetName
( -X- _ O
Gravier -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
, -X- _ O
ESTER -X- _ B-DatasetName
1 -X- _ I-DatasetName
& -X- _ O
ESTER -X- _ B-DatasetName
2 -X- _ I-DatasetName
( -X- _ O
Galliano -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2005 -X- _ O
) -X- _ O
and -X- _ O
REPERE -X- _ B-DatasetName
2http -X- _ I-DatasetName
: -X- _ O
/ -X- _ O
/ -X- _ O
www.quaero.org -X- _ O
( -X- _ O
Kahn -X- _ O

et -X- _ O

al -X- _ O
. -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
collections -X- _ O
. -X- _ O

ASR -X- _ B-TaskName
performance -X- _ O
was -X- _ O
evaluated -X- _ O
on -X- _ O
the -X- _ O
held -X- _ O
out -X- _ O
corpora -X- _ O
presented -X- _ O
in -X- _ O
table -X- _ O
2 -X- _ O
( -X- _ O
used -X- _ O
to -X- _ O
train -X- _ O
and -X- _ O
evaluate -X- _ O
ASR -X- _ O
prediction -X- _ O
) -X- _ O
and -X- _ O
its -X- _ O
averaged -X- _ O
value -X- _ O
was -X- _ O
22.29 -X- _ O
% -X- _ O
on -X- _ O
the -X- _ O
TRAIN -X- _ O
set -X- _ O
, -X- _ O
22.35 -X- _ O
% -X- _ O
on -X- _ O
the -X- _ O
DEV -X- _ O
set -X- _ O
and -X- _ O
31.20 -X- _ O
% -X- _ O
on -X- _ O
the -X- _ O
TEST -X- _ O
set -X- _ O
( -X- _ O
which -X- _ O
contains -X- _ O
more -X- _ O
challenging -X- _ O
broadcast -X- _ O
programs -X- _ O
) -X- _ O
. -X- _ O

Figure -X- _ O
1 -X- _ O
shows -X- _ O
our -X- _ O
network -X- _ O
architecture -X- _ O
. -X- _ O

The -X- _ O
network -X- _ O
input -X- _ O
can -X- _ O
be -X- _ O
either -X- _ O
a -X- _ O
pure -X- _ O
text -X- _ O
input -X- _ O
, -X- _ O
a -X- _ O
pure -X- _ O
signal -X- _ O
input -X- _ O
( -X- _ O
raw -X- _ O
signal -X- _ O
) -X- _ O
or -X- _ O
a -X- _ O
dual -X- _ O
( -X- _ O
text+speech -X- _ O
) -X- _ O
input -X- _ O
. -X- _ O

To -X- _ O
avoid -X- _ O
memory -X- _ O
issues -X- _ O
, -X- _ O
signals -X- _ O
are -X- _ O
downsampled -X- _ O
to -X- _ O
8khz -X- _ O
and -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
on -X- _ O
six -X- _ O
- -X- _ O
second -X- _ O
speech -X- _ O
turns -X- _ O
( -X- _ O
shorter -X- _ O
speech -X- _ O
turns -X- _ O
are -X- _ O
padded -X- _ O
with -X- _ O
zeros -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
text -X- _ O
input -X- _ O
, -X- _ O
the -X- _ O
architecture -X- _ O
is -X- _ O
inspired -X- _ O
from -X- _ O
( -X- _ O
Kim -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
( -X- _ O
green -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
: -X- _ O
the -X- _ O
input -X- _ O
is -X- _ O
a -X- _ O
matrix -X- _ O
of -X- _ O
dimensions -X- _ O
296x100 -X- _ O
( -X- _ O
296 -X- _ O
is -X- _ O
the -X- _ O
longest -X- _ O
ASR -X- _ B-TaskName
hypothesis -X- _ O
length -X- _ O
in -X- _ O
our -X- _ O
corpus -X- _ O
; -X- _ O
100 -X- _ O
is -X- _ O
the -X- _ O
dimension -X- _ O
of -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
word -X- _ O
embeddings -X- _ O
on -X- _ O
a -X- _ O
large -X- _ O
held -X- _ O
out -X- _ O
text -X- _ O
corpus -X- _ O
of -X- _ O
3.3 -X- _ O
G -X- _ O
words -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
speech -X- _ O
input -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
best -X- _ O
architecture -X- _ O
( -X- _ O
m18 -X- _ O
) -X- _ O
proposed -X- _ O
in -X- _ O
( -X- _ O
Dai -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
( -X- _ O
colored -X- _ O
in -X- _ O
red -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
of -X- _ O
dimensions -X- _ O
48000 -X- _ O
x -X- _ O
1 -X- _ O
( -X- _ O
48000 -X- _ O
samples -X- _ O
correspond -X- _ O
to -X- _ O
6s -X- _ O
of -X- _ O
speech -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
WER -X- _ B-TaskName
prediction -X- _ I-TaskName
, -X- _ O
our -X- _ O
best -X- _ O
approach -X- _ O
( -X- _ O
called -X- _ O
CNN -X- _ B-MethodName
Softmax -X- _ O
) -X- _ O
used -X- _ O
softmax -X- _ O
probabilities -X- _ O
and -X- _ O
an -X- _ O
external -X- _ O
ﬁxed -X- _ O
WER -X- _ B-TaskName
V -X- _ O
ector -X- _ O
which -X- _ O
corresponds -X- _ O
to -X- _ O
a -X- _ O
discretization -X- _ O
of -X- _ O
the -X- _ O
WER -X- _ B-TaskName
output -X- _ O
space -X- _ O
( -X- _ O
see -X- _ O
( -X- _ O
Elloumi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
for -X- _ O
more -X- _ O
details -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
best -X- _ O
performance -X- _ O
obtained -X- _ O
is -X- _ O
19.24 -X- _ B-MetricValue
% -X- _ I-MetricValue
MAE3using -X- _ B-MetricName
text+speech -X- _ O
input -X- _ O
. -X- _ O

Our -X- _ O
ASR -X- _ B-TaskName
prediction -X- _ I-TaskName
system -X- _ O
is -X- _ O
built -X- _ O
using -X- _ O
both -X- _ O
Keras -X- _ O
( -X- _ O
Chollet -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
and -X- _ O
Tensorﬂow4 -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
next -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
analyze -X- _ O
the -X- _ O
representations -X- _ O
learnt -X- _ O
in -X- _ O
the -X- _ O
higher -X- _ O
layers -X- _ O
( -X- _ O
3 -X- _ O
blocks -X- _ O
colored -X- _ O
in -X- _ O
yellow -X- _ O
and -X- _ O
dotted -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
for -X- _ O
pure -X- _ O
text -X- _ O
( -X- _ O
TXT -X- _ O
) -X- _ O
, -X- _ O
pure -X- _ O
speech -X- _ O
( -X- _ O
RAW -X- _ O
- -X- _ O
SIG -X- _ O
) -X- _ O
and -X- _ O
both -X- _ O
( -X- _ O
TXT+RAW -X- _ O
- -X- _ O
SIG -X- _ O
) -X- _ O
. -X- _ O

4 -X- _ O
Evaluating -X- _ O
learned -X- _ O
representations -X- _ O
4.1 -X- _ O
Methodology -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
attempt -X- _ O
to -X- _ O
understand -X- _ O
what -X- _ O
our -X- _ O
best -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
system -X- _ O
( -X- _ O
Elloumi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
learned -X- _ O
. -X- _ O

We -X- _ O
analyze -X- _ O
the -X- _ O
text -X- _ O
and -X- _ O
speech -X- _ O
representations -X- _ O
obtained -X- _ O
by -X- _ O
our -X- _ O
architecture -X- _ O
. -X- _ O

Alike -X- _ O
( -X- _ O
Belinkov -X- _ O
and -X- _ O
Glass -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
joint -X- _ O
text+speech -X- _ O
model -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
generate -X- _ O
utterance -X- _ O
uate -X- _ O
WER -X- _ O
prediction -X- _ O
; -X- _ O
it -X- _ O
computes -X- _ O
the -X- _ O
absolute -X- _ O
deviation -X- _ O
between -X- _ O
the -X- _ O
true -X- _ O
and -X- _ O
predicted -X- _ O
WERs -X- _ O
, -X- _ O
averaged -X- _ O
over -X- _ O
the -X- _ O
number -X- _ B-MetricName
of -X- _ I-MetricName
utterances -X- _ I-MetricName
in -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

4https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
www.tensorﬂow.org -X- _ O

Figure -X- _ O
1 -X- _ O
: -X- _ O
Architecture -X- _ O
of -X- _ O
our -X- _ O
CNN -X- _ O
with -X- _ O
text -X- _ O
( -X- _ O
green -X- _ O
) -X- _ O
and -X- _ O
signal -X- _ O
( -X- _ O
red -X- _ O
) -X- _ O
inputs -X- _ O
for -X- _ O
WER -X- _ O
prediction -X- _ O
level -X- _ O
features -X- _ O
( -X- _ O
hidden -X- _ O
representations -X- _ O
of -X- _ O
speech -X- _ O
turns -X- _ O
colored -X- _ O
in -X- _ O
yellow -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
that -X- _ O
are -X- _ O
given -X- _ O
to -X- _ O
a -X- _ O
shallow -X- _ O
classiﬁer -X- _ O
trained -X- _ O
to -X- _ O
solve -X- _ O
secondary -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
: -X- _ O
STYLE -X- _ O
: -X- _ O
classify -X- _ O
the -X- _ O
utterances -X- _ O
between -X- _ O
( -X- _ O
spontaneous -X- _ O
and -X- _ O
non -X- _ O
spontaneous -X- _ O
) -X- _ O
styles -X- _ O
( -X- _ O
see -X- _ O
table -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
ACCENT -X- _ O
: -X- _ O
classify -X- _ O
the -X- _ O
utterances -X- _ O
between -X- _ O
native -X- _ O
andnon -X- _ O
native -X- _ O
speech -X- _ O
( -X- _ O
see -X- _ O
also -X- _ O
table -X- _ O
1 -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
speaker -X- _ O
annotations -X- _ O
provided -X- _ O
with -X- _ O
our -X- _ O
datasets -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
label -X- _ O
our -X- _ O
utterances -X- _ O
in -X- _ O
native -X- _ O
/ -X- _ O
non -X- _ O
native -X- _ O
speech -X- _ O
) -X- _ O
, -X- _ O
SHOW -X- _ O
: -X- _ O
classify -X- _ O
the -X- _ O
utterances -X- _ O
in -X- _ O
different -X- _ O
broadcast -X- _ O
programs -X- _ O
( -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
table -X- _ O
2 -X- _ O
, -X- _ O
each -X- _ O
utterance -X- _ O
of -X- _ O
our -X- _ O
corpus -X- _ O
is -X- _ O
labeled -X- _ O
with -X- _ O
a -X- _ O
broadcast -X- _ O
program -X- _ O
name -X- _ O
) -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
more -X- _ O
visual -X- _ O
analysis -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
plot -X- _ O
an -X- _ O
example -X- _ O
of -X- _ O
hidden -X- _ O
representations -X- _ O
projected -X- _ O
to -X- _ O
a -X- _ O
2 -X- _ O
- -X- _ O
D -X- _ O
space -X- _ O
using -X- _ O
t -X- _ O
- -X- _ O
distributed -X- _ O
Stochastic -X- _ O
Neighbor -X- _ O
Embedding -X- _ O
( -X- _ O
t -X- _ O
- -X- _ O
SNE -X- _ O
) -X- _ O
( -X- _ O
Maaten -X- _ O
and -X- _ O
Hinton -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
.5 -X- _ O
4.2 -X- _ O
Shallow -X- _ O
classiﬁers -X- _ O
We -X- _ O
built -X- _ O
three -X- _ O
shallow -X- _ O
classiﬁers -X- _ O
( -X- _ O
SHOW -X- _ O
, -X- _ O
STYLE -X- _ O
, -X- _ O
ACCENT -X- _ O
) -X- _ O
with -X- _ O
a -X- _ O
similar -X- _ O
architecture -X- _ O
. -X- _ O

The -X- _ O
classiﬁer -X- _ O
is -X- _ O
a -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
neural -X- _ O
network -X- _ O
with -X- _ O
one -X- _ B-HyperparameterValue
hidden -X- _ B-HyperparameterName
layer -X- _ I-HyperparameterName
( -X- _ O
size -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
hidden -X- _ I-HyperparameterName
layer -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
128 -X- _ B-HyperparameterValue
) -X- _ O
followed -X- _ O
by -X- _ O
dropout -X- _ B-HyperparameterName
( -X- _ O
rate -X- _ B-HyperparameterName
of -X- _ O
0.5 -X- _ B-HyperparameterValue
) -X- _ O
and -X- _ O
a -X- _ O
ReLU -X- _ O
non -X- _ O
- -X- _ O
linearity -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
a -X- _ O
softmax -X- _ O
layer -X- _ O
is -X- _ O
used -X- _ O
for -X- _ O
mapping -X- _ O
onto -X- _ O
the -X- _ O
label -X- _ O
set -X- _ O
size -X- _ O
. -X- _ O

We -X- _ O
chose -X- _ O
this -X- _ O
simple -X- _ O
formulation -X- _ O
as -X- _ O
we -X- _ O
are -X- _ O
interested -X- _ O
in -X- _ O
evaluating -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
representations -X- _ O
learned -X- _ O
by -X- _ O
our -X- _ O
ASR -X- _ B-TaskName
prediction -X- _ I-TaskName
model -X- _ O
, -X- _ O
rather -X- _ O
than -X- _ O
optimizing -X- _ O
the -X- _ O
secondary -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
. -X- _ O

5https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
lvdmaaten.github.io -X- _ O
/ -X- _ O
tsne -X- _ O
/ -X- _ O
code -X- _ O
/ -X- _ O
tsne_python.zipThe -X- _ O
network -X- _ O
input -X- _ O
size -X- _ O
depends -X- _ O
on -X- _ O
which -X- _ O
layer -X- _ O
to -X- _ O
analyze -X- _ O
( -X- _ O
see -X- _ O
ﬁgure -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O

Training -X- _ O
is -X- _ O
performed -X- _ O
using -X- _ O
Adam -X- _ O
( -X- _ O
Kingma -X- _ O
and -X- _ O
Ba -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
( -X- _ O
using -X- _ O
default -X- _ O
parameters -X- _ O
) -X- _ O
over -X- _ O
shufﬂed -X- _ O
mini -X- _ O
- -X- _ O
batches -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
minimize -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
entropy -X- _ O
loss -X- _ O
. -X- _ O

The -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
for -X- _ O
30 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
with -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
16 -X- _ B-HyperparameterValue
speech -X- _ O
utterances -X- _ O
. -X- _ O

After -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
keep -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
on -X- _ O
DEV -X- _ O
set -X- _ O
and -X- _ O
report -X- _ O
its -X- _ O
performance -X- _ O
on -X- _ O
the -X- _ O
TEST -X- _ O
set -X- _ O
. -X- _ O

The -X- _ O
classiﬁer -X- _ O
outputs -X- _ O
are -X- _ O
evaluated -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
accuracy -X- _ B-MetricName
. -X- _ O

4.3 -X- _ O
Data -X- _ O
A -X- _ O
data -X- _ O
set -X- _ O
from -X- _ O
( -X- _ O
Elloumi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
was -X- _ O
employed -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
, -X- _ O
divided -X- _ O
into -X- _ O
three -X- _ O
subsets -X- _ O
: -X- _ O
training -X- _ O
( -X- _ O
TRAIN -X- _ O
) -X- _ O
, -X- _ O
development -X- _ O
( -X- _ O
DEV -X- _ O
) -X- _ O
and -X- _ O
test -X- _ O
( -X- _ O
TEST -X- _ O
) -X- _ O
. -X- _ O

Speech -X- _ O
utterances -X- _ O
come -X- _ O
from -X- _ O
various -X- _ O
French -X- _ O
broadcast -X- _ O
collections -X- _ O
gathered -X- _ O
during -X- _ O
projects -X- _ O
or -X- _ O
shared -X- _ O
tasks -X- _ O
: -X- _ O
Quaero -X- _ B-DatasetName
, -X- _ O
ETAPE -X- _ B-DatasetName
, -X- _ O
ESTER -X- _ B-DatasetName
1 -X- _ I-DatasetName
& -X- _ O
ESTER -X- _ B-DatasetName
2 -X- _ I-DatasetName
andREPERE -X- _ B-DatasetName
. -X- _ O

The -X- _ O
TEST -X- _ O
set -X- _ O
contains -X- _ O
unseen -X- _ O
broadcast -X- _ O
programs -X- _ O
that -X- _ O
are -X- _ O
different -X- _ O
from -X- _ O
those -X- _ O
present -X- _ O
in -X- _ O
TRAIN -X- _ O
and -X- _ O
DEV -X- _ O
( -X- _ O
Elloumi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Category -X- _ O
TRAIN -X- _ O
DEV -X- _ O
TEST -X- _ O
Table -X- _ O
1 -X- _ O
: -X- _ O

Distribution -X- _ O
of -X- _ O
our -X- _ O
utterances -X- _ O
between -X- _ O
non -X- _ O
spontaneous -X- _ O
and -X- _ O
spontaneous -X- _ O
styles -X- _ O
, -X- _ O
native -X- _ O
and -X- _ O
non -X- _ O
native -X- _ O
accents -X- _ O
Tables -X- _ O
1 -X- _ O
and -X- _ O
2 -X- _ O
show -X- _ O
the -X- _ O
whole -X- _ O
data -X- _ O
set -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
speech -X- _ O
turns -X- _ O
available -X- _ O
for -X- _ O
each -X- _ O
classiﬁcation -X- _ O
task -X- _ O
. -X- _ O

We -X- _ O
clearly -X- _ O
see -X- _ O
that -X- _ O
the -X- _ O
data -X- _ O
is -X- _ O
unbalanced -X- _ O
for -X- _ O
the -X- _ O
three -X- _ O
categories -X- _ O
( -X- _ O
STYLE -X- _ O
, -X- _ O
ACCENT -X- _ O
, -X- _ O
SHOW -X- _ O
) -X- _ O
. -X- _ O

Since -X- _ O
we -X- _ O
are -X- _ O
interested -X- _ O
in -X- _ O
evaluating -X- _ O
the -X- _ O
discriminative -X- _ O
power -X- _ O
of -X- _ O
our -X- _ O
learned -X- _ O
representations -X- _ O
for -X- _ O

12Show -X- _ O
TRAIN -X- _ O
DEV -X- _ O
FINTER -X- _ O
- -X- _ O
DEBATE -X- _ O
7632 -X- _ O
833 -X- _ O
FRANCE3 -X- _ O
- -X- _ O
DEBATE -X- _ O
928 -X- _ O
77 -X- _ O
LCP -X- _ O
- -X- _ O
PileEtFace -X- _ O
4487 -X- _ O
525 -X- _ O
Table -X- _ O
2 -X- _ O
: -X- _ O
Number -X- _ B-MetricValue
of -X- _ I-MetricValue
utterances -X- _ I-MetricValue
for -X- _ O
each -X- _ O
broadcast -X- _ O
program -X- _ O
these -X- _ O
3 -X- _ O
tasks -X- _ O
, -X- _ O
we -X- _ O
extracted -X- _ O
a -X- _ O
balanced -X- _ O
version -X- _ O
of -X- _ O
our -X- _ O
TRAIN -X- _ O
/ -X- _ O
DEV -X- _ O
/ -X- _ O
TEST -X- _ O
sets -X- _ O
by -X- _ O
ﬁltering -X- _ O
among -X- _ O
over -X- _ O
- -X- _ O
represented -X- _ O
labels -X- _ O
( -X- _ O
ﬁnal -X- _ O
number -X- _ O
of -X- _ O
kept -X- _ O
utterances -X- _ O
corresponds -X- _ O
to -X- _ O
bold -X- _ O
numbers -X- _ O
in -X- _ O
table -X- _ O
1 -X- _ O
and -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O

Table -X- _ O
3 -X- _ O
shows -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
our -X- _ O
ﬁnal -X- _ O
balanced -X- _ O
TRAIN -X- _ O
/ -X- _ O
DEV -X- _ O
/ -X- _ O
TEST -X- _ O
sets -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
categories -X- _ O
for -X- _ O
each -X- _ O
task.6 -X- _ O
# -X- _ O
CatgTurns -X- _ O
of -X- _ O
speech -X- _ O
per -X- _ O
category -X- _ O
TRAIN -X- _ O
DEV -X- _ O
TEST -X- _ O
Table -X- _ O
3 -X- _ O
: -X- _ O
Description -X- _ O
of -X- _ O
our -X- _ O
balanced -X- _ O
data -X- _ O
set -X- _ O
for -X- _ O
each -X- _ O
category -X- _ O
4.4 -X- _ O
Results -X- _ O
For -X- _ O
each -X- _ O
classiﬁcation -X- _ O
task -X- _ O
, -X- _ O
we -X- _ O
build -X- _ O
a -X- _ O
shallow -X- _ O
classiﬁer -X- _ O
using -X- _ O
the -X- _ O
hidden -X- _ O
representations -X- _ O
of -X- _ O
TXT -X- _ O
, -X- _ O
RAW -X- _ O
- -X- _ O
SIG -X- _ O
and -X- _ O
TXT+RAW -X- _ O
- -X- _ O
SIG -X- _ O
blocks -X- _ O
as -X- _ O
input -X- _ O
. -X- _ O

The -X- _ O
experimental -X- _ O
results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
table -X- _ O
4 -X- _ O
for -X- _ O
both -X- _ O
DEV -X- _ O
and -X- _ O
TEST -X- _ O
sets -X- _ O
separated -X- _ O
by -X- _ O
two -X- _ O
vertical -X- _ O
bars -X- _ O
( -X- _ O
jj -X- _ O
) -X- _ O
. -X- _ O

Classiﬁcation -X- _ O
performance -X- _ O
is -X- _ O
all -X- _ O
above -X- _ O
a -X- _ O
random -X- _ O
baseline -X- _ O
accuracy -X- _ B-MetricName
( -X- _ O
> -X- _ O
50 -X- _ B-MetricValue
% -X- _ I-MetricValue
for -X- _ O
STYLE -X- _ O
and -X- _ O
ACCENT -X- _ O
and -X- _ O
> -X- _ O
20 -X- _ B-MetricValue
% -X- _ I-MetricValue
for -X- _ O
SHOW -X- _ O
) -X- _ O
. -X- _ O

This -X- _ O
shows -X- _ O
that -X- _ O
training -X- _ O
a -X- _ O
deep -X- _ O
WER -X- _ B-TaskName
prediction -X- _ I-TaskName
system -X- _ O
gives -X- _ O
representation -X- _ O
layers -X- _ O
that -X- _ O
contain -X- _ O
a -X- _ O
meaningful -X- _ O
amount -X- _ O
of -X- _ O
information -X- _ O
about -X- _ O
speech -X- _ O
style -X- _ O
, -X- _ O
speech -X- _ O
accent -X- _ O
and -X- _ O
broadcast -X- _ O
program -X- _ O
label -X- _ O
. -X- _ O

Predicting -X- _ O
utterance -X- _ O
style -X- _ O
( -X- _ O
spontaneous -X- _ O
/ -X- _ O
non -X- _ O
spontaneous -X- _ O
) -X- _ O
is -X- _ O
slightly -X- _ O
easier -X- _ O
than -X- _ O
predicting -X- _ O
accent -X- _ O
( -X- _ O
native -X- _ O
/ -X- _ O
non -X- _ O
native -X- _ O
) -X- _ O
especially -X- _ O
from -X- _ O
text -X- _ O
input -X- _ O
. -X- _ O

One -X- _ O
explanation -X- _ O
might -X- _ O
be -X- _ O
that -X- _ O
speech -X- _ O
utterances -X- _ O
are -X- _ O
short -X- _ O
( -X- _ O
< -X- _ O
6s -X- _ O
) -X- _ O
while -X- _ O
accent -X- _ O
identiﬁcation -X- _ O
needs -X- _ O
probably -X- _ O
longer -X- _ O
sequences -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
observe -X- _ O
that -X- _ O
using -X- _ O
both -X- _ O
text -X- _ O
and -X- _ O
speech -X- _ O
improves -X- _ O
the -X- _ O
learned -X- _ O
representations -X- _ O
for -X- _ O
the -X- _ O
STYLE -X- _ O
task -X- _ O
while -X- _ O
it -X- _ O
is -X- _ O
DEBATE -X- _ O
shows -X- _ O
were -X- _ O
ﬁnally -X- _ O
removed -X- _ O
since -X- _ O
they -X- _ O
represent -X- _ O
a -X- _ O
too -X- _ O
small -X- _ O
amount -X- _ O
of -X- _ O
speech -X- _ O
turns.less -X- _ O
clear -X- _ O
for -X- _ O
the -X- _ O
ACCENT -X- _ O
task -X- _ O
( -X- _ O
for -X- _ O
which -X- _ O
improvement -X- _ O
seen -X- _ O
on -X- _ O
DEV -X- _ O
is -X- _ O
not -X- _ O
conﬁrmed -X- _ O
on -X- _ O
TEST -X- _ O
) -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
text -X- _ O
input -X- _ O
is -X- _ O
signiﬁcantly -X- _ O
better -X- _ O
than -X- _ O
speech -X- _ O
input -X- _ O
whereas -X- _ O
we -X- _ O
could -X- _ O
have -X- _ O
expected -X- _ O
better -X- _ O
performance -X- _ O
from -X- _ O
speech -X- _ O
for -X- _ O
the -X- _ O
SHOW -X- _ O
task -X- _ O
( -X- _ O
speech -X- _ O
signals -X- _ O
convey -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
audio -X- _ O
characteristics -X- _ O
of -X- _ O
a -X- _ O
broadcast -X- _ O
program -X- _ O
) -X- _ O
. -X- _ O

It -X- _ O
means -X- _ O
that -X- _ O
text -X- _ O
input -X- _ O
contains -X- _ O
correlated -X- _ O
information -X- _ O
with -X- _ O
broadcast -X- _ O
- -X- _ O
program -X- _ O
type -X- _ O
, -X- _ O
speech -X- _ O
style -X- _ O
and -X- _ O
speaker -X- _ O
’s -X- _ O
accent -X- _ O
. -X- _ O

In -X- _ O
case -X- _ O
of -X- _ O
SHOW -X- _ O
task -X- _ O
, -X- _ O
our -X- _ O
performance -X- _ O
prediction -X- _ O
system -X- _ O
is -X- _ O
able -X- _ O
to -X- _ O
capture -X- _ O
information -X- _ O
( -X- _ O
vocabulary -X- _ O
, -X- _ O
topic -X- _ O
, -X- _ O
syntax -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O
) -X- _ O
about -X- _ O
a -X- _ O
speciﬁc -X- _ O
broadcast -X- _ O
program -X- _ O
type -X- _ O
, -X- _ O
based -X- _ O
on -X- _ O
textual -X- _ O
features -X- _ O
and -X- _ O
to -X- _ O
differ -X- _ O
it -X- _ O
from -X- _ O
others -X- _ O
( -X- _ O
radio -X- _ O
programs -X- _ O
, -X- _ O
TV -X- _ O
debate -X- _ O
programs -X- _ O
, -X- _ O
phone -X- _ O
calls -X- _ O
, -X- _ O
broadcast -X- _ O
news -X- _ O
programs -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O
) -X- _ O
. -X- _ O

Likewise -X- _ O
, -X- _ O
the -X- _ O
textual -X- _ O
information -X- _ O
captured -X- _ O
is -X- _ O
very -X- _ O
different -X- _ O
between -X- _ O
spontaneous -X- _ O
/ -X- _ O
non -X- _ O
- -X- _ O
spontaneous -X- _ O
speech -X- _ O
styles -X- _ O
and -X- _ O
native -X- _ O
/ -X- _ O
non -X- _ O
- -X- _ O
native -X- _ O
speaker -X- _ O
’s -X- _ O
accents -X- _ O
. -X- _ O

Among -X- _ O
the -X- _ O
representations -X- _ O
analyzed -X- _ O
, -X- _ O
the -X- _ O
outputs -X- _ O
of -X- _ O
the -X- _ O
CNNs -X- _ B-MethodName
( -X- _ O
A1 -X- _ O
, -X- _ O
B1 -X- _ O
) -X- _ O
lead -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
classiﬁcation -X- _ O
results -X- _ O
, -X- _ O
in -X- _ O
line -X- _ O
with -X- _ O
previous -X- _ O
ﬁndings -X- _ O
about -X- _ O
convolutions -X- _ O
as -X- _ O
feature -X- _ O
extractors -X- _ O
. -X- _ O

Performance -X- _ O
then -X- _ O
drops -X- _ O
using -X- _ O
the -X- _ O
higher -X- _ O
( -X- _ O
fully -X- _ O
connected -X- _ O
) -X- _ O
layers -X- _ O
that -X- _ O
do -X- _ O
not -X- _ O
generate -X- _ O
better -X- _ O
representations -X- _ O
for -X- _ O
detecting -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
or -X- _ O
show -X- _ O
. -X- _ O

Layer -X- _ O
Dim -X- _ O
. -X- _ O
SHOW -X- _ O
STYLE -X- _ O
ACCENT -X- _ O
TXT -X- _ O
RAW -X- _ O
- -X- _ O
SIG -X- _ O
TXT -X- _ O
+ -X- _ O
RAW -X- _ O
- -X- _ O
SIG -X- _ O
Table -X- _ O
4 -X- _ O
: -X- _ O
Show -X- _ O
/ -X- _ O
Style -X- _ O
/ -X- _ O
Accent -X- _ O
classiﬁcation -X- _ O
accuracies -X- _ O
using -X- _ O
representations -X- _ O
from -X- _ O
different -X- _ O
layers -X- _ O
learned -X- _ O
during -X- _ O
the -X- _ O
training -X- _ O
of -X- _ O
our -X- _ O
ASR -X- _ B-TaskName
WER -X- _ I-TaskName
prediction -X- _ I-TaskName
system -X- _ O
. -X- _ O

We -X- _ O
visualize -X- _ O
an -X- _ O
example -X- _ O
of -X- _ O
utterance -X- _ O
representations -X- _ O
from -X- _ O
C2 -X- _ O
( -X- _ O
TXT+RAW -X- _ O
- -X- _ O
SIG -X- _ O
) -X- _ O
layer -X- _ O
in -X- _ O
ﬁgure -X- _ O
2 -X- _ O
using -X- _ O
the -X- _ O
t -X- _ O
- -X- _ O
SNE -X- _ O
. -X- _ O

For -X- _ O
a -X- _ O
ﬁxed -X- _ O
utterance -X- _ O
duration -X- _ O
4sD -X- _ O
< -X- _ O
5s -X- _ O
( -X- _ O
716 -X- _ O
speech -X- _ O
turns -X- _ O
) -X- _ O
and -X- _ O
5s -X- _ O
D -X- _ O
< -X- _ O
6s -X- _ O

( -X- _ O
489 -X- _ O
speech -X- _ O
turns -X- _ O
) -X- _ O
, -X- _ O
non -X- _ O
spontaneous -X- _ O
utterances -X- _ O
are -X- _ O
plotted -X- _ O
in -X- _ O
blue -X- _ O
while -X- _ O
spontaneous -X- _ O
ones -X- _ O
are -X- _ O
in -X- _ O
pink -X- _ O
. -X- _ O

The -X- _ O
C2 -X- _ O
layer -X- _ O
produces -X- _ O
clusters -X- _ O
which -X- _ O
shows -X- _ O
that -X- _ O
spontaneous -X- _ O
utterances -X- _ O
are -X- _ O
in -X- _ O
the -X- _ O
upper -X- _ O
- -X- _ O
left -X- _ O
part -X- _ O

Figure -X- _ O
2 -X- _ O
: -X- _ O
Visualization -X- _ O
of -X- _ O
utterance -X- _ O
representations -X- _ O
from -X- _ O
C2 -X- _ O
layer -X- _ O
for -X- _ O
different -X- _ O
speech -X- _ O
styles -X- _ O
( -X- _ O
S -X- _ O
spontaneous -X- _ O
- -X- _ O
NS -X- _ O
non -X- _ O
spontaneous -X- _ O
) -X- _ O
- -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
utt -X- _ O
. -X- _ O

length -X- _ O
is -X- _ O
of -X- _ O
the -X- _ O
2D -X- _ O
space -X- _ O
. -X- _ O

This -X- _ O
suggests -X- _ O
that -X- _ O
C2 -X- _ O
hidden -X- _ O
representation -X- _ O
captures -X- _ O
a -X- _ O
weak -X- _ O
signal -X- _ O
about -X- _ O
speaking -X- _ O
style -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
ﬁgure -X- _ O
3 -X- _ O
is -X- _ O
the -X- _ O
confusion -X- _ O
matrix -X- _ O
produced -X- _ O
using -X- _ O
C2 -X- _ O
( -X- _ O
TXT+RAW -X- _ O
- -X- _ O
SIG -X- _ O
) -X- _ O
layer -X- _ O
. -X- _ O

The -X- _ O
classiﬁers -X- _ O
very -X- _ O
well -X- _ O
predicted -X- _ O
TELSONNE -X- _ O
category -X- _ O
( -X- _ O
Accuracy -X- _ B-MetricName
of -X- _ O
82 -X- _ B-MetricValue
% -X- _ I-MetricValue
) -X- _ O
, -X- _ O
which -X- _ O
contains -X- _ O
many -X- _ O
phone -X- _ O
calls -X- _ O
from -X- _ O
the -X- _ O
radio -X- _ O
listeners -X- _ O
. -X- _ O

This -X- _ O
show -X- _ O
is -X- _ O
rather -X- _ O
different -X- _ O
from -X- _ O
the -X- _ O
4 -X- _ O
other -X- _ O
shows -X- _ O
in -X- _ O
DEV -X- _ O
( -X- _ O
broadcast -X- _ O
debates -X- _ O
and -X- _ O
news -X- _ O
) -X- _ O
. -X- _ O

Figure -X- _ O
3 -X- _ O
: -X- _ O
Confusion -X- _ O
matrix -X- _ O
for -X- _ O
SHOW -X- _ O
classiﬁcation -X- _ O
using -X- _ O
C2 -X- _ O
( -X- _ O
TXT+RAW -X- _ O
- -X- _ O
SIG -X- _ O
) -X- _ O
layer -X- _ O
as -X- _ O
input -X- _ O
, -X- _ O
evaluated -X- _ O
on -X- _ O
DEV -X- _ O
5 -X- _ O
Multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
We -X- _ O
have -X- _ O
seen -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
section -X- _ O
that -X- _ O
, -X- _ O
while -X- _ O
training -X- _ O
an -X- _ O
ASR -X- _ O
performance -X- _ O
prediction -X- _ O
system -X- _ O
, -X- _ O
hidden -X- _ O
layers -X- _ O
convey -X- _ O
a -X- _ O
clear -X- _ O
signal -X- _ O
about -X- _ O
speech -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
and -X- _ O
show -X- _ O
. -X- _ O

This -X- _ O
suggests -X- _ O
that -X- _ O
these -X- _ O
3 -X- _ O
types -X- _ O
of -X- _ O
information -X- _ O
might -X- _ O
be -X- _ O
useful -X- _ O
to -X- _ O
structure -X- _ O
the -X- _ O
deep -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
models -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
investigate -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
knowl -X- _ O
- -X- _ O
edge -X- _ O
of -X- _ O
these -X- _ O
labels -X- _ O
( -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
, -X- _ O
show -X- _ O
) -X- _ O
at -X- _ O
training -X- _ O
time -X- _ O
on -X- _ O
prediction -X- _ O
systems -X- _ O
qualities -X- _ O
. -X- _ O

For -X- _ O
this -X- _ O
, -X- _ O
we -X- _ O
perform -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
providing -X- _ O
the -X- _ O
additional -X- _ O
information -X- _ O
about -X- _ O
broadcast -X- _ O
type -X- _ O
, -X- _ O
speech -X- _ O
style -X- _ O
and -X- _ O
speaker -X- _ O
’s -X- _ O
accent -X- _ O
during -X- _ O
training -X- _ O
. -X- _ O

The -X- _ O
architecture -X- _ O
of -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
model -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
WER -X- _ B-TaskName
prediction -X- _ I-TaskName
model -X- _ O
of -X- _ O
Figure -X- _ O
1 -X- _ O
but -X- _ O
we -X- _ O
add -X- _ O
additional -X- _ O
outputs -X- _ O
: -X- _ O
a -X- _ O
softmax -X- _ O
function -X- _ O
is -X- _ O
added -X- _ O
for -X- _ O
each -X- _ O
new -X- _ O
classiﬁcation -X- _ O
task -X- _ O
after -X- _ O
the -X- _ O
last -X- _ O
fully -X- _ O
connected -X- _ O
layer -X- _ O
( -X- _ O
C2 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
output -X- _ O
dimension -X- _ O
depends -X- _ O
on -X- _ O
the -X- _ O
task -X- _ O
: -X- _ O
6 -X- _ O
for -X- _ O
SHOW -X- _ O
and -X- _ O
2 -X- _ O
for -X- _ O
STYLE -X- _ O
and -X- _ O
ACCENT -X- _ O
tasks -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
full -X- _ O
( -X- _ O
unbalanced -X- _ O
) -X- _ O
data -X- _ O
set -X- _ O
described -X- _ O
in -X- _ O
tables -X- _ O
1 -X- _ O
and -X- _ O
2 -X- _ O
. -X- _ O

Training -X- _ O
of -X- _ O
the -X- _ O
multitask -X- _ O
model -X- _ O
usesAdadelta -X- _ O
update -X- _ O
rule -X- _ O
and -X- _ O
all -X- _ O
parameters -X- _ O
are -X- _ O
initialized -X- _ O
from -X- _ O
scratch -X- _ O
( -X- _ O
8.70 -X- _ O
M -X- _ O
) -X- _ O
. -X- _ O

Models -X- _ O
are -X- _ O
performed -X- _ O
for -X- _ O
50 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
with -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
32 -X- _ B-HyperparameterValue
. -X- _ O

MAE -X- _ B-MetricName
is -X- _ O
used -X- _ O
as -X- _ O
the -X- _ O
loss -X- _ O
function -X- _ O
for -X- _ O
WER -X- _ O
prediction -X- _ O
task -X- _ O
while -X- _ O
cross -X- _ O
- -X- _ O
entropy -X- _ O
loss -X- _ O
is -X- _ O
used -X- _ O
for -X- _ O
the -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
composite -X- _ O
( -X- _ O
multitask -X- _ O
) -X- _ O
loss -X- _ O
, -X- _ O
we -X- _ O
assign -X- _ O
a -X- _ O
weight -X- _ B-HyperparameterName
of -X- _ O
1 -X- _ B-HyperparameterValue
for -X- _ O
MAE -X- _ B-MetricName
loss -X- _ O
( -X- _ O
main -X- _ O
task -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
smaller -X- _ O
weight -X- _ B-HyperparameterName
of -X- _ O
0.3 -X- _ B-HyperparameterValue
( -X- _ O
tuned -X- _ O
using -X- _ O
a -X- _ O
grid -X- _ O
search -X- _ O
on -X- _ O
DEV -X- _ O
dataset -X- _ O
) -X- _ O
for -X- _ O
cross -X- _ B-MetricName
- -X- _ I-MetricName
entropy -X- _ I-MetricName
( -X- _ O
secondary -X- _ O
classiﬁcation -X- _ O
task -X- _ O
) -X- _ O
loss -X- _ O
( -X- _ O
es -X- _ O
) -X- _ O
. -X- _ O

After -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
take -X- _ O
the -X- _ O
model -X- _ O
that -X- _ O
lead -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
MAE -X- _ B-MetricName
onDEV -X- _ O
set -X- _ O
and -X- _ O
report -X- _ O
its -X- _ O
performance -X- _ O
on -X- _ O
TEST -X- _ O
. -X- _ O

We -X- _ O
build -X- _ O
several -X- _ O
models -X- _ O
that -X- _ O
simultaneously -X- _ O
address -X- _ O
1 -X- _ O
, -X- _ O
2 -X- _ O
, -X- _ O
3 -X- _ O
and -X- _ O
4 -X- _ O
tasks -X- _ O
. -X- _ O

The -X- _ O
models -X- _ O
are -X- _ O
evaluated -X- _ O
with -X- _ O
a -X- _ O
speciﬁc -X- _ O
metric -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
: -X- _ O
MAE -X- _ B-MetricName
& -X- _ O
Kendall7for -X- _ B-MetricName
WER -X- _ O
prediction -X- _ O
task -X- _ O
and -X- _ O
Accuracy -X- _ B-MetricName
for -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
. -X- _ O

Table -X- _ O
5 -X- _ O
summarizes -X- _ O
the -X- _ O
experimental -X- _ O
results -X- _ O
on -X- _ O
DEV -X- _ O
and -X- _ O
TEST -X- _ O
sets -X- _ O
, -X- _ O
separated -X- _ O
by -X- _ O
two -X- _ O
vertical -X- _ O
bars -X- _ O
( -X- _ O
jj -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
considered -X- _ O
the -X- _ O
mono -X- _ O
- -X- _ O
task -X- _ O
model -X- _ O
described -X- _ O
in -X- _ O
( -X- _ O
Elloumi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
( -X- _ O
and -X- _ O
summarized -X- _ O
in -X- _ O
section -X- _ O
3 -X- _ O
) -X- _ O
as -X- _ O
a -X- _ O
baseline -X- _ O
system -X- _ O
. -X- _ O

We -X- _ O
recall -X- _ O
that -X- _ O
we -X- _ O
evaluated -X- _ O
the -X- _ O
SHOW -X- _ O
classiﬁcation -X- _ O
task -X- _ O
only -X- _ O
on -X- _ O
the -X- _ O
DEV -X- _ O
set -X- _ O
( -X- _ O
TEST -X- _ O
broadcast -X- _ O
programs -X- _ O
are -X- _ O
new -X- _ O
and -X- _ O
were -X- _ O
unseen -X- _ O
in -X- _ O
the -X- _ O
TRAIN -X- _ O
) -X- _ O
. -X- _ O

First -X- _ O
of -X- _ O
all -X- _ O
, -X- _ O
we -X- _ O
notice -X- _ O
that -X- _ O
performance -X- _ O
of -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
in -X- _ O
muti -X- _ O
- -X- _ O
task -X- _ O
scenarios -X- _ O
are -X- _ O
very -X- _ O
good -X- _ O
: -X- _ O
we -X- _ O
are -X- _ O
able -X- _ O
to -X- _ O
train -X- _ O
efﬁcient -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
systems -X- _ O
that -X- _ O
simultaneously -X- _ O
tag -X- _ O
the -X- _ O
analyzed -X- _ O
utterances -X- _ O
according -X- _ O
to -X- _ O
their -X- _ O
speech -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
and -X- _ O
broadcast -X- _ O
program -X- _ O
origin -X- _ O
. -X- _ O

Such -X- _ O
multitask -X- _ O
systems -X- _ O
might -X- _ O
be -X- _ O
useful -X- _ O
diagnostic -X- _ O
tools -X- _ O
to -X- _ O
analyze -X- _ O
and -X- _ O
predict -X- _ B-TaskName
ASR -X- _ I-TaskName
on -X- _ O
large -X- _ O
speech -X- _ O
collections -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
our -X- _ O
best -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
systems -X- _ O
disvalues -X- _ O

14ModelsPerformance -X- _ O
prediction -X- _ O
task -X- _ O
Classiﬁcation -X- _ O
tasks -X- _ O
MAE -X- _ B-MetricName
Kendall -X- _ B-MetricName
SHOW -X- _ O
STYLE -X- _ O
ACCENT -X- _ O
Baseline -X- _ O
: -X- _ O
Mono -X- _ O
- -X- _ O
task -X- _ O
2 -X- _ O
- -X- _ O
task -X- _ O
3 -X- _ O
- -X- _ O
task -X- _ O
4 -X- _ O
- -X- _ O
task -X- _ O
Table -X- _ O
5 -X- _ O
: -X- _ O
Evaluation -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
performance -X- _ O
prediction -X- _ O
with -X- _ O
multi -X- _ O
- -X- _ O
tasks -X- _ O
models -X- _ O
( -X- _ O
DEVjjTEST -X- _ O
) -X- _ O
computed -X- _ O
with -X- _ O
MAE -X- _ B-MetricName
and -X- _ O
Kendall -X- _ B-MetricName
- -X- _ O
secondary -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
accuracy -X- _ B-MetricName
is -X- _ O
also -X- _ O
reported -X- _ O
play -X- _ O
a -X- _ O
better -X- _ O
performance -X- _ O
( -X- _ O
MAE -X- _ B-MetricName
, -X- _ O
Kendall -X- _ B-MetricName
) -X- _ O
than -X- _ O
the -X- _ O
baseline -X- _ O
system -X- _ O
, -X- _ O
which -X- _ O
means -X- _ O
that -X- _ O
the -X- _ O
implicit -X- _ O
information -X- _ O
given -X- _ O
about -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
and -X- _ O
broadcast -X- _ O
program -X- _ O
type -X- _ O
can -X- _ O
be -X- _ O
helpful -X- _ O
to -X- _ O
structure -X- _ O
the -X- _ O
system -X- _ O
’s -X- _ O
predictions -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
in -X- _ O
2 -X- _ O
- -X- _ O
task -X- _ O
case -X- _ O
, -X- _ O
the -X- _ O
best -X- _ O
model -X- _ O
is -X- _ O
obtained -X- _ O
on -X- _ O
WER+SHOW -X- _ B-TaskName
tasks -X- _ O
with -X- _ O
a -X- _ O
difference -X- _ O
of -X- _ O
+0.41 -X- _ B-MetricValue
% -X- _ I-MetricValue
, -X- _ O
+2.25 -X- _ B-MetricValue
% -X- _ I-MetricValue
for -X- _ O
MAE -X- _ B-MetricName
and -X- _ O
Kendall -X- _ B-MetricName
respectively -X- _ O
( -X- _ O
on -X- _ O
DEV -X- _ O
) -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
baseline -X- _ O
on -X- _ O
WER -X- _ B-TaskName
prediction -X- _ I-TaskName
task -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
also -X- _ O
important -X- _ O
to -X- _ O
mention -X- _ O
that -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
multitask -X- _ O
learning -X- _ O
on -X- _ O
the -X- _ O
main -X- _ O
task -X- _ O
( -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
) -X- _ O
is -X- _ O
limited -X- _ O
: -X- _ O
only -X- _ O
slight -X- _ O
improvements -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
are -X- _ O
observed -X- _ O
for -X- _ O
MAE -X- _ B-MetricName
and -X- _ O
Kendall -X- _ B-MetricName
metrics -X- _ O
. -X- _ O

Anyway -X- _ O
, -X- _ O
the -X- _ O
systems -X- _ O
trained -X- _ O
seem -X- _ O
complementary -X- _ O
since -X- _ O
their -X- _ O
combination -X- _ O
( -X- _ O
averaging -X- _ O
, -X- _ O
over -X- _ O
all -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
systems -X- _ O
, -X- _ O
predicted -X- _ B-TaskName
WERs -X- _ I-TaskName
at -X- _ I-TaskName
utterance -X- _ I-TaskName
level -X- _ I-TaskName
) -X- _ O
leads -X- _ O
to -X- _ O
signiﬁcant -X- _ O
performance -X- _ O
improvement -X- _ O
( -X- _ O
MAE -X- _ B-MetricName
and -X- _ O
Kendall -X- _ B-MetricName
) -X- _ O
. -X- _ O

6 -X- _ O
Conclusion -X- _ O
This -X- _ O
paper -X- _ O
presented -X- _ O
an -X- _ O
analysis -X- _ O
of -X- _ O
learned -X- _ O
representations -X- _ O
of -X- _ O
our -X- _ O
deep -X- _ O
ASR -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
system -X- _ O
. -X- _ O

Experiments -X- _ O
show -X- _ O
that -X- _ O
hidden -X- _ O
layers -X- _ O
convey -X- _ O
a -X- _ O
clear -X- _ O
signal -X- _ O
about -X- _ O
speech -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
, -X- _ O
and -X- _ O
broadcast -X- _ O
type -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
proposed -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
learning -X- _ O
approach -X- _ O
to -X- _ O
simultaneously -X- _ O
predict -X- _ B-TaskName
WER -X- _ I-TaskName
and -X- _ O
classify -X- _ O
utterances -X- _ O
according -X- _ O
to -X- _ O
style -X- _ O
, -X- _ O
accent -X- _ O
and -X- _ O
broadcast -X- _ O
program -X- _ O
origin -X- _ O
. -X- _ O

References -X- _ O
Yonatan -X- _ O
Belinkov -X- _ O
and -X- _ O
James -X- _ O
Glass -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Analyzing -X- _ O
hidden -X- _ O
representations -X- _ O
in -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
automatic -X- _ O
speech -X- _ O
recognition -X- _ O
systems -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
, -X- _ O
pages -X- _ O
2438–2448 -X- _ O
. -X- _ O

Yonatan -X- _ O
Belinkov -X- _ O
, -X- _ O
Llu -X- _ O
´ -X- _ O
ıs -X- _ O
M -X- _ O
` -X- _ O
arquez -X- _ O
, -X- _ O
Hassan -X- _ O
Sajjad -X- _ O
, -X- _ O
Nadir -X- _ O
Durrani -X- _ O
, -X- _ O
Fahim -X- _ O
Dalvi -X- _ O
, -X- _ O
and -X- _ O
James -X- _ O
Glass -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Evaluating -X- _ O
layers -X- _ O
of -X- _ O
representation -X- _ O
in -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
on -X- _ O
part -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
speech -X- _ O
and -X- _ O
semantic -X- _ O
tagging -X- _ O
tasks -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
Eighth -X- _ O
International -X- _ O
Joint -X- _ O
Conference -X- _ O
on -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
Volume -X- _ O
1 -X- _ O
: -X- _ O
Long -X- _ O
Papers -X- _ O
) -X- _ O
, -X- _ O
volume -X- _ O
1 -X- _ O
, -X- _ O
pages -X- _ O
Franc -X- _ O
¸ois -X- _ O
Chollet -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
2015 -X- _ O
. -X- _ O

Keras -X- _ O
. -X- _ O

https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
fchollet -X- _ O
/ -X- _ O
keras -X- _ O
. -X- _ O

Wei -X- _ O
Dai -X- _ O
, -X- _ O
Chia -X- _ O
Dai -X- _ O
, -X- _ O
Shuhui -X- _ O
Qu -X- _ O
, -X- _ O
Juncheng -X- _ O
Li -X- _ O
, -X- _ O
and -X- _ O
Samarjit -X- _ O
Das -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Very -X- _ O
deep -X- _ O
convolutional -X- _ O
neural -X- _ O
networks -X- _ O
for -X- _ O
raw -X- _ O
waveforms -X- _ O
. -X- _ O

In -X- _ O
Acoustics -X- _ O
, -X- _ O
Speech -X- _ O
and -X- _ O
Signal -X- _ O
Processing -X- _ O
( -X- _ O
ICASSP -X- _ O
) -X- _ O
, -X- _ O
2017 -X- _ O
IEEE -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
, -X- _ O
pages -X- _ O
421–425 -X- _ O
. -X- _ O

IEEE -X- _ O
. -X- _ O

Zied -X- _ O
Elloumi -X- _ O
, -X- _ O
Laurent -X- _ O
Besacier -X- _ O
, -X- _ O
Olivier -X- _ O
Galibert -X- _ O
, -X- _ O
Juliette -X- _ O
Kahn -X- _ O
, -X- _ O
and -X- _ O
Benjamin -X- _ O
Lecouteux -X- _ O
. -X- _ O
2018 -X- _ O
. -X- _ O

Asr -X- _ B-TaskName
performance -X- _ I-TaskName
prediction -X- _ I-TaskName
on -X- _ O
unseen -X- _ O
broadcast -X- _ O
programs -X- _ O
using -X- _ O
convolutional -X- _ O
neural -X- _ O
networks -X- _ O
. -X- _ O

In -X- _ O
IEEE -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Acoustics -X- _ O
, -X- _ O
Speech -X- _ O
and -X- _ O
Signal -X- _ O
Processing -X- _ O
( -X- _ O
ICASSP -X- _ O
) -X- _ O
. -X- _ O

Sylvain -X- _ O
Galliano -X- _ O
, -X- _ O
Edouard -X- _ O
Geoffrois -X- _ O
, -X- _ O
Djamel -X- _ O
Mostefa -X- _ O
, -X- _ O
Khalid -X- _ O
Choukri -X- _ O
, -X- _ O
Jean -X- _ O
- -X- _ O
Franc -X- _ O
¸ois -X- _ O
Bonastre -X- _ O
, -X- _ O
and -X- _ O
Guillaume -X- _ O
Gravier -X- _ O
. -X- _ O

2005 -X- _ O
. -X- _ O

The -X- _ O
ester -X- _ O
phase -X- _ O
ii -X- _ O
evaluation -X- _ O
campaign -X- _ O
for -X- _ O
the -X- _ O
rich -X- _ O
transcription -X- _ O
of -X- _ O
french -X- _ O
broadcast -X- _ O
news -X- _ O
. -X- _ O

In -X- _ O
Interspeech -X- _ O
, -X- _ O
pages -X- _ O
1149–1152 -X- _ O
. -X- _ O

Guillaume -X- _ O
Gravier -X- _ O
, -X- _ O
Gilles -X- _ O
Adda -X- _ O
, -X- _ O
Niklas -X- _ O
Paulson -X- _ O
, -X- _ O
Matthieu -X- _ O
Carr -X- _ O
´ -X- _ O
e -X- _ O
, -X- _ O
Aude -X- _ O
Giraudel -X- _ O
, -X- _ O
and -X- _ O
Olivier -X- _ O
Galibert -X- _ O
. -X- _ O
2012 -X- _ O
. -X- _ O

The -X- _ O
etape -X- _ O
corpus -X- _ O
for -X- _ O
the -X- _ O
evaluation -X- _ O
of -X- _ O
speech -X- _ O
- -X- _ O
based -X- _ O
tv -X- _ O
content -X- _ O
processing -X- _ O
in -X- _ O
the -X- _ O
french -X- _ O
language -X- _ O
. -X- _ O

In -X- _ O
LREC -X- _ O
- -X- _ O
Eighth -X- _ O
international -X- _ O
conference -X- _ O
on -X- _ O
Language -X- _ O
Resources -X- _ O
and -X- _ O
Evaluation -X- _ O
, -X- _ O
page -X- _ O
na -X- _ O
. -X- _ O

Juliette -X- _ O
Kahn -X- _ O
, -X- _ O
Olivier -X- _ O
Galibert -X- _ O
, -X- _ O
Ludovic -X- _ O
Quintard -X- _ O
, -X- _ O
Matthieu -X- _ O
Carr -X- _ O
´ -X- _ O
e -X- _ O
, -X- _ O
Aude -X- _ O
Giraudel -X- _ O
, -X- _ O
and -X- _ O
Philippe -X- _ O
Joly -X- _ O
. -X- _ O
2012 -X- _ O
. -X- _ O

A -X- _ O
presentation -X- _ O
of -X- _ O
the -X- _ O
repere -X- _ O
challenge -X- _ O
. -X- _ O

In -X- _ O
Content -X- _ O
- -X- _ O
Based -X- _ O
Multimedia -X- _ O
Indexing -X- _ O
( -X- _ O
CBMI -X- _ O
) -X- _ O
, -X- _ O
2012 -X- _ O
10th -X- _ O
International -X- _ O
Workshop -X- _ O
on -X- _ O
, -X- _ O
pages -X- _ O
1–6 -X- _ O
. -X- _ O

IEEE -X- _ O
. -X- _ O

Yoon -X- _ O
Kim -X- _ O
. -X- _ O

2014 -X- _ O
. -X- _ O

Convolutional -X- _ O
neural -X- _ O
networks -X- _ O
for -X- _ O
sentence -X- _ O
classiﬁcation -X- _ O
. -X- _ O

arXiv -X- _ O
preprint -X- _ O

15Diederik -X- _ O
P. -X- _ O
Kingma -X- _ O
and -X- _ O
Jimmy -X- _ O
Ba -X- _ O
. -X- _ O
2014 -X- _ O
. -X- _ O

Adam -X- _ O
: -X- _ O
A -X- _ O
method -X- _ O
for -X- _ O
stochastic -X- _ O
optimization -X- _ O
. -X- _ O

CoRR -X- _ O
, -X- _ O
Laurens -X- _ O
van -X- _ O
der -X- _ O
Maaten -X- _ O
and -X- _ O
Geoffrey -X- _ O
Hinton -X- _ O
. -X- _ O
2008 -X- _ O
. -X- _ O

Visualizing -X- _ O
data -X- _ O
using -X- _ O
t -X- _ O
- -X- _ O
sne -X- _ O
. -X- _ O

Journal -X- _ O
of -X- _ O
machine -X- _ O
learning -X- _ O
research -X- _ O
, -X- _ O
9 -X- _ O
( -X- _ O
Nov -X- _ O
) -X- _ O
:2579–2605 -X- _ O
. -X- _ O

Abdel -X- _ O
- -X- _ O
rahman -X- _ O
Mohamed -X- _ O
, -X- _ O
Geoffrey -X- _ O
Hinton -X- _ O
, -X- _ O
and -X- _ O
Gerald -X- _ O
Penn -X- _ O
. -X- _ O
2012 -X- _ O
. -X- _ O

Understanding -X- _ O
how -X- _ O
deep -X- _ O
belief -X- _ O
networks -X- _ O
perform -X- _ O
acoustic -X- _ O
modelling -X- _ O
. -X- _ O

In -X- _ O
Acoustics -X- _ O
, -X- _ O
Speech -X- _ O
and -X- _ O
Signal -X- _ O
Processing -X- _ O
( -X- _ O
ICASSP -X- _ O
) -X- _ O
, -X- _ O
2012 -X- _ O
IEEE -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
, -X- _ O
pages -X- _ O
4273 -X- _ O
– -X- _ O
Daniel -X- _ O
Povey -X- _ O
, -X- _ O
Arnab -X- _ O
Ghoshal -X- _ O
, -X- _ O
Gilles -X- _ O
Boulianne -X- _ O
, -X- _ O
Lukas -X- _ O
Burget -X- _ O
, -X- _ O
Ondrej -X- _ O
Glembek -X- _ O
, -X- _ O
Nagendra -X- _ O
Goel -X- _ O
, -X- _ O
Mirko -X- _ O
Hannemann -X- _ O
, -X- _ O
Petr -X- _ O
Motlicek -X- _ O
, -X- _ O
Yanmin -X- _ O
Qian -X- _ O
, -X- _ O
Petr -X- _ O
Schwarz -X- _ O
, -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
2011 -X- _ O
. -X- _ O

The -X- _ O
kaldi -X- _ O
speech -X- _ O
recognition -X- _ O
toolkit -X- _ O
. -X- _ O

In -X- _ O
IEEE -X- _ O
2011 -X- _ O
workshop -X- _ O
on -X- _ O
automatic -X- _ O
speech -X- _ O
recognition -X- _ O
and -X- _ O
understanding -X- _ O
, -X- _ O
EPFLCONF-192584 -X- _ O
. -X- _ O

IEEE -X- _ O
Signal -X- _ O
Processing -X- _ O
Society -X- _ O
. -X- _ O

Xing -X- _ O
Shi -X- _ O
, -X- _ O
Inkit -X- _ O
Padhi -X- _ O
, -X- _ O
and -X- _ O
Kevin -X- _ O
Knight -X- _ O
. -X- _ O

2016 -X- _ O
. -X- _ O

Does -X- _ O
string -X- _ O
- -X- _ O
based -X- _ O
neural -X- _ O
mt -X- _ O
learn -X- _ O
source -X- _ O
syntax -X- _ O
? -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2016 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
pages -X- _ O
1526 -X- _ O
– -X- _ O
Shuai -X- _ O
Wang -X- _ O
, -X- _ O
Yanmin -X- _ O
Qian -X- _ O
, -X- _ O
and -X- _ O
Kai -X- _ O
Yu -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

What -X- _ O
does -X- _ O
the -X- _ O
speaker -X- _ O
embedding -X- _ O
encode -X- _ O
? -X- _ O

In -X- _ O
InterZhizheng -X- _ O
Wu -X- _ O
and -X- _ O
Simon -X- _ O
King -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

Investigating -X- _ O
gated -X- _ O
recurrent -X- _ O
neural -X- _ O
networks -X- _ O
for -X- _ O
speech -X- _ O
synthe- -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
2018 -X- _ O
EMNLP -X- _ O
Workshop -X- _ O
BlackboxNLP -X- _ O
: -X- _ O
Analyzing -X- _ O
and -X- _ O
Interpreting -X- _ O
Neural -X- _ O
Networks -X- _ O
for -X- _ O
NLP -X- _ O
, -X- _ O
pages -X- _ O
16–24 -X- _ O
Brussels -X- _ O
, -X- _ O
Belgium -X- _ O
, -X- _ O
November -X- _ O
1 -X- _ O
, -X- _ O
2018 -X- _ O
. -X- _ O

c -X- _ O

2018 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics16Explaining -X- _ O
non -X- _ O
- -X- _ O
linear -X- _ O
Classiﬁer -X- _ O
Decisions -X- _ O
within -X- _ O
Kernel -X- _ O
- -X- _ O
based -X- _ O
Deep -X- _ O
Architectures -X- _ O
Danilo -X- _ O
Croce -X- _ O
and -X- _ O
Daniele -X- _ O
Rossini -X- _ O
and -X- _ O
Roberto -X- _ O
Basili -X- _ O
Department -X- _ O
of -X- _ O
Enterprise -X- _ O
Engineering -X- _ O
University -X- _ O
of -X- _ O
Roma -X- _ O
, -X- _ O
Tor -X- _ O
Vergata -X- _ O
fcroce -X- _ O
, -X- _ O
basilig -X- _ O
@ -X- _ O
info.uniroma2.it -X- _ O
Abstract -X- _ O
Nonlinear -X- _ O
methods -X- _ O
such -X- _ O
as -X- _ O
deep -X- _ O
neural -X- _ O
networks -X- _ O
achieve -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
performances -X- _ O
in -X- _ O
several -X- _ O
semantic -X- _ O
NLP -X- _ O
tasks -X- _ O
. -X- _ O

However -X- _ O
epistemologically -X- _ O
transparent -X- _ O
decisions -X- _ O
are -X- _ O
not -X- _ O
provided -X- _ O
as -X- _ O
for -X- _ O
the -X- _ O
limited -X- _ O
interpretability -X- _ O
of -X- _ O
the -X- _ O
underlying -X- _ O
acquired -X- _ O
neural -X- _ O
models -X- _ O
. -X- _ O

In -X- _ O
neuralbased -X- _ O
semantic -X- _ O
inference -X- _ O
tasks -X- _ O
epistemological -X- _ O
transparency -X- _ O
corresponds -X- _ O
to -X- _ O
the -X- _ O
ability -X- _ O
of -X- _ O
tracing -X- _ O
back -X- _ O
causal -X- _ O
connections -X- _ O
between -X- _ O
the -X- _ O
linguistic -X- _ O
properties -X- _ O
of -X- _ O
a -X- _ O
input -X- _ O
instance -X- _ O
and -X- _ O
the -X- _ O
produced -X- _ O
classiﬁcation -X- _ O
output -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
a -X- _ O
methodology -X- _ O
, -X- _ O
called -X- _ O
Layerwise -X- _ B-MethodName
Relevance -X- _ I-MethodName
Propagation -X- _ I-MethodName
, -X- _ O
over -X- _ O
linguistically -X- _ O
motivated -X- _ O
neural -X- _ O
architectures -X- _ O
, -X- _ O
namely -X- _ O
Kernel -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
Deep -X- _ I-MethodName
Architectures -X- _ I-MethodName
( -X- _ O
KDA -X- _ B-MethodName
) -X- _ O
, -X- _ O
to -X- _ O
guide -X- _ O
argumentations -X- _ O
and -X- _ O
explanation -X- _ O
inferences -X- _ O
. -X- _ O

In -X- _ O
such -X- _ O
a -X- _ O
way -X- _ O
, -X- _ O
each -X- _ O
decision -X- _ O
provided -X- _ O
by -X- _ O
a -X- _ O
KDA -X- _ B-MethodName
can -X- _ O
be -X- _ O
linked -X- _ O
to -X- _ O
real -X- _ O
examples -X- _ O
, -X- _ O
linguistically -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
instance -X- _ O
: -X- _ O
these -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
motivate -X- _ O
the -X- _ O
network -X- _ O
output -X- _ O
. -X- _ O

Quantitative -X- _ O
analysis -X- _ O
shows -X- _ O
that -X- _ O
richer -X- _ O
explanations -X- _ O
about -X- _ O
the -X- _ O
semantic -X- _ O
and -X- _ O
syntagmatic -X- _ O
structures -X- _ O
of -X- _ O
the -X- _ O
examples -X- _ O
characterize -X- _ O
more -X- _ O
convincing -X- _ O
arguments -X- _ O
in -X- _ O
two -X- _ O
tasks -X- _ O
, -X- _ O
i.e. -X- _ O
question -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
and -X- _ O
semantic -X- _ O
role -X- _ O
labeling -X- _ O
. -X- _ O

1 -X- _ O
Introduction -X- _ O
Nonlinear -X- _ O
methods -X- _ O
such -X- _ O
as -X- _ O
deep -X- _ O
neural -X- _ O
networks -X- _ O
achieve -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
performances -X- _ O
in -X- _ O
several -X- _ O
challenging -X- _ O
problems -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
image -X- _ O
classiﬁcation -X- _ O
or -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
( -X- _ O
NLP -X- _ O
) -X- _ O
. -X- _ O

However -X- _ O
the -X- _ O
traditional -X- _ O
AI -X- _ O
criticism -X- _ O
still -X- _ O
holds -X- _ O
: -X- _ O
they -X- _ O
are -X- _ O
not -X- _ O
epistemologically -X- _ O
transparent -X- _ O
, -X- _ O
as -X- _ O
for -X- _ O
the -X- _ O
limited -X- _ O
interpretability -X- _ O
of -X- _ O
the -X- _ O
neural -X- _ O
inferences -X- _ O
. -X- _ O

In -X- _ O
a -X- _ O
question -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
( -X- _ O
QC -X- _ B-TaskName
) -X- _ O
task -X- _ O
, -X- _ O
e.g. -X- _ O
( -X- _ O
Li -X- _ O
and -X- _ O
Roth -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
, -X- _ O
this -X- _ O
is -X- _ O
particularly -X- _ O
evident -X- _ O
. -X- _ O

The -X- _ O
category -X- _ O
describing -X- _ O
the -X- _ O
target -X- _ O
of -X- _ O
a -X- _ O
request -X- _ O
is -X- _ O
relevant -X- _ O
in -X- _ O
question -X- _ O
answering -X- _ O
to -X- _ O
optimize -X- _ O
the -X- _ O
laterstages -X- _ O
of -X- _ O
search -X- _ O
and -X- _ O
answer -X- _ O
detection -X- _ O
, -X- _ O
and -X- _ O
its -X- _ O
interpretation -X- _ O
depends -X- _ O
on -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
semantic -X- _ O
and -X- _ O
syntactic -X- _ O
properties -X- _ O
of -X- _ O
the -X- _ O
question -X- _ O
. -X- _ O

Epistemological -X- _ O
transparency -X- _ O
corresponds -X- _ O
here -X- _ O
to -X- _ O
the -X- _ O
ability -X- _ O
of -X- _ O
tracing -X- _ O
back -X- _ O
the -X- _ O
connections -X- _ O
between -X- _ O
linguistic -X- _ O
properties -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
question -X- _ O
and -X- _ O
the -X- _ O
proposed -X- _ O
question -X- _ O
category -X- _ O
. -X- _ O

An -X- _ O
example -X- _ O
- -X- _ O
driven -X- _ O
machine -X- _ O
learning -X- _ O
model -X- _ O
should -X- _ O
be -X- _ O
able -X- _ O
to -X- _ O
provide -X- _ O
causal -X- _ O
relations -X- _ O
between -X- _ O
the -X- _ O
input -X- _ O
semantic -X- _ O
aspect -X- _ O
and -X- _ O
the -X- _ O
properties -X- _ O
of -X- _ O
the -X- _ O
question -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
given -X- _ O
the -X- _ O
prediction -X- _ O
” -X- _ O
What -X- _ O
is -X- _ O
the -X- _ O
capital -X- _ O
of -X- _ O
Zimbabwe -X- _ O
? -X- _ O
” -X- _ O
refers -X- _ O
to -X- _ O
a -X- _ O
Location -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
the -X- _ O
system -X- _ O
to -X- _ O
motivate -X- _ O
it -X- _ O
with -X- _ O
a -X- _ O
sentence -X- _ O
such -X- _ O
as -X- _ O
: -X- _ O
Since -X- _ O
it -X- _ O
seems -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
What -X- _ O
is -X- _ O
the -X- _ O
capital -X- _ O
of -X- _ O
California -X- _ O
? -X- _ O
” -X- _ O
which -X- _ O
also -X- _ O
refers -X- _ O
to -X- _ O
a -X- _ O
Location -X- _ O
. -X- _ O

Notice -X- _ O
how -X- _ O
in -X- _ O
neural -X- _ O
learning -X- _ O
, -X- _ O
as -X- _ O
for -X- _ O
example -X- _ O
in -X- _ O
Multilayer -X- _ B-MethodName
Perceptrons -X- _ I-MethodName
, -X- _ O
Long -X- _ O
Short -X- _ O
- -X- _ O
Term -X- _ O
Memory -X- _ O
Networks -X- _ O
, -X- _ O
( -X- _ O
Hochreiter -X- _ O
and -X- _ O
Schmidhuber -X- _ O
, -X- _ O
1997 -X- _ O
) -X- _ O
, -X- _ O
or -X- _ O
the -X- _ O
more -X- _ O
recent -X- _ O
Attention -X- _ O
- -X- _ O
based -X- _ O
Networks -X- _ O
( -X- _ O
Larochelle -X- _ O
and -X- _ O
Hinton -X- _ O
, -X- _ O
2010 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
network -X- _ O
parameters -X- _ O
have -X- _ O
no -X- _ O
clear -X- _ O
conceptual -X- _ O
counterpart -X- _ O
. -X- _ O

Using -X- _ O
the -X- _ O
Layerwise -X- _ B-MethodName
Relevance -X- _ I-MethodName
Propagation -X- _ I-MethodName
( -X- _ O
LRP -X- _ B-MethodName
) -X- _ O
( -X- _ O
Bach -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
approach -X- _ O
, -X- _ O
the -X- _ O
classiﬁcation -X- _ O
decisions -X- _ O
of -X- _ O
a -X- _ O
multilayer -X- _ O
perceptron -X- _ O
are -X- _ O
decomposed -X- _ O
backward -X- _ O
across -X- _ O
the -X- _ O
network -X- _ O
layers -X- _ O
, -X- _ O
and -X- _ O
evidence -X- _ O
about -X- _ O
the -X- _ O
contribution -X- _ O
of -X- _ O
individual -X- _ O
input -X- _ O
fragments -X- _ O
( -X- _ O
i.e. -X- _ O
layer -X- _ O
0 -X- _ O
) -X- _ O
to -X- _ O
the -X- _ O
ﬁnal -X- _ O
decision -X- _ O
is -X- _ O
gathered -X- _ O
. -X- _ O

Evaluation -X- _ O
against -X- _ O
images -X- _ O
( -X- _ O
i.e. -X- _ O
the -X- _ O
MNIST -X- _ O
and -X- _ O
ILSVRC -X- _ O
data -X- _ O
sets -X- _ O
) -X- _ O
suggests -X- _ O
that -X- _ O
LRP -X- _ B-MethodName
activates -X- _ O
meaningful -X- _ O
associations -X- _ O
between -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
fragments -X- _ O
, -X- _ O
and -X- _ O
this -X- _ O
corresponds -X- _ O
to -X- _ O
tracing -X- _ O
back -X- _ O
meaningful -X- _ O
causal -X- _ O
connections -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
a -X- _ O
similar -X- _ O
mechanism -X- _ O
over -X- _ O
the -X- _ O
linguistically -X- _ O
motivated -X- _ O
network -X- _ O
architectures -X- _ O
, -X- _ O
as -X- _ O
they -X- _ O
have -X- _ O
been -X- _ O
recently -X- _ O
proposed -X- _ O
in -X- _ O
( -X- _ O
Croce -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
: -X- _ O
Kernel -X- _ O
- -X- _ O
based -X- _ O
Deep -X- _ O
network -X- _ O
architectures -X- _ O
aim -X- _ O
at -X- _ O
integrating -X- _ O
syntactic -X- _ O
/ -X- _ O
semantic -X- _ O
information -X- _ O
derived -X- _ O
from -X- _ O
the -X- _ O
adoption -X- _ O
of -X- _ O
Tree -X- _ O
Kernels -X- _ O
( -X- _ O
Collins -X- _ O
and -X- _ O
Duffy -X- _ O
, -X- _ O

172001 -X- _ O
) -X- _ O

within -X- _ O
neural -X- _ O
- -X- _ O
based -X- _ O
learning -X- _ O
. -X- _ O

Here -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
inferences -X- _ O
of -X- _ O
such -X- _ O
architectures -X- _ O
can -X- _ O
be -X- _ O
motivated -X- _ O
by -X- _ O
simply -X- _ O
applying -X- _ O
the -X- _ O
LRP -X- _ B-MethodName
method -X- _ O
, -X- _ O
which -X- _ O
allows -X- _ O
to -X- _ O
trace -X- _ O
back -X- _ O
causal -X- _ O
associations -X- _ O
between -X- _ O
the -X- _ O
semantic -X- _ O
classiﬁcation -X- _ O
and -X- _ O
the -X- _ O
examples -X- _ O
expressed -X- _ O
by -X- _ O
parse -X- _ O
tree -X- _ O
- -X- _ O
based -X- _ O
metrics -X- _ O
. -X- _ O

Evaluation -X- _ O
of -X- _ O
the -X- _ O
LRP -X- _ B-MethodName
algorithm -X- _ O
to -X- _ O
the -X- _ O
problem -X- _ O
of -X- _ O
explaining -X- _ O
the -X- _ O
system -X- _ O
decisions -X- _ O
allows -X- _ O
to -X- _ O
demonstrate -X- _ O
the -X- _ O
meaningful -X- _ O
impact -X- _ O
of -X- _ O
LRP -X- _ B-MethodName
on -X- _ O
semantic -X- _ O
transparency -X- _ O
: -X- _ O
users -X- _ O
faced -X- _ O
with -X- _ O
explanations -X- _ O
are -X- _ O
better -X- _ O
oriented -X- _ O
to -X- _ O
accept -X- _ O
or -X- _ O
reject -X- _ O
the -X- _ O
system -X- _ O
decisions -X- _ O
, -X- _ O
thus -X- _ O
improving -X- _ O
the -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
overall -X- _ O
application -X- _ O
accuracy -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
rest -X- _ O
of -X- _ O
the -X- _ O
paper -X- _ O
, -X- _ O
section -X- _ O
2 -X- _ O
reports -X- _ O
related -X- _ O
works -X- _ O
. -X- _ O

In -X- _ O
section -X- _ O
3 -X- _ O
we -X- _ O
describe -X- _ O
the -X- _ O
Kernelbased -X- _ B-MethodName
Deep -X- _ I-MethodName
Architecture -X- _ I-MethodName
( -X- _ O
KDA -X- _ B-MethodName
) -X- _ O
while -X- _ O
section -X- _ O
4 -X- _ O
illustrates -X- _ O
the -X- _ O
details -X- _ O
of -X- _ O
LRP -X- _ B-MethodName
and -X- _ O
how -X- _ O
it -X- _ O
connects -X- _ O
to -X- _ O
KDAs -X- _ B-MethodName
. -X- _ O

In -X- _ O
section -X- _ O
5 -X- _ O
we -X- _ O
propose -X- _ O
both -X- _ O
a -X- _ O
novel -X- _ O
model -X- _ O
to -X- _ O
generate -X- _ O
explanations -X- _ O
of -X- _ O
a -X- _ O
network -X- _ O
prediction -X- _ O
and -X- _ O
an -X- _ O
evaluation -X- _ O
methodology -X- _ O
. -X- _ O

In -X- _ O
section -X- _ O
6 -X- _ O
we -X- _ O
provide -X- _ O
experimental -X- _ O
evidences -X- _ O
of -X- _ O
the -X- _ O
overall -X- _ O
system -X- _ O
’s -X- _ O
effectiveness -X- _ O
against -X- _ O
two -X- _ O
semantic -X- _ O
tasks -X- _ O
, -X- _ O
question -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
and -X- _ O
frame -X- _ O
- -X- _ O
based -X- _ O
argument -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
in -X- _ O
the -X- _ O
semantic -X- _ O
role -X- _ O
labeling -X- _ O
chain -X- _ O
. -X- _ O

Lastly -X- _ O
, -X- _ O
in -X- _ O
section -X- _ O
7 -X- _ O
conclusions -X- _ O
are -X- _ O
derived -X- _ O
. -X- _ O

2 -X- _ O
Related -X- _ O
Work -X- _ O
Linguistically -X- _ O
motivated -X- _ O
explanatory -X- _ O
methods -X- _ O
should -X- _ O
provide -X- _ O
semantically -X- _ O
clear -X- _ O
justiﬁcations -X- _ O
about -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
textual -X- _ O
inferences -X- _ O
. -X- _ O

Methods -X- _ O
making -X- _ O
the -X- _ O
neural -X- _ O
learning -X- _ O
more -X- _ O
readable -X- _ O
are -X- _ O
usually -X- _ O
designed -X- _ O
to -X- _ O
trace -X- _ O
back -X- _ O
the -X- _ O
portions -X- _ O
of -X- _ O
the -X- _ O
network -X- _ O
input -X- _ O
that -X- _ O
mostly -X- _ O
contributed -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
decision -X- _ O
. -X- _ O

Network -X- _ O
propagation -X- _ O
techniques -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
patterns -X- _ O
of -X- _ O
a -X- _ O
given -X- _ O
input -X- _ O
item -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
an -X- _ O
image -X- _ O
) -X- _ O
that -X- _ O
are -X- _ O
linked -X- _ O
to -X- _ O
the -X- _ O
particular -X- _ O
deep -X- _ O
neural -X- _ O
network -X- _ O
prediction -X- _ O
as -X- _ O
in -X- _ O
( -X- _ O
Erhan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2010 -X- _ O
; -X- _ O
Zeiler -X- _ O
and -X- _ O
Fergus -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
. -X- _ O

Usually -X- _ O
, -X- _ O
these -X- _ O
are -X- _ O
based -X- _ O
on -X- _ O
backward -X- _ O
algorithms -X- _ O
that -X- _ O
layer -X- _ O
- -X- _ O
wise -X- _ O
reuse -X- _ O
arc -X- _ O
weights -X- _ O
to -X- _ O
propagate -X- _ O
the -X- _ O
prediction -X- _ O
from -X- _ O
the -X- _ O
output -X- _ O
down -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
, -X- _ O
thus -X- _ O
leading -X- _ O
to -X- _ O
the -X- _ O
re -X- _ O
- -X- _ O
creation -X- _ O
of -X- _ O
meaningful -X- _ O
patterns -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
space -X- _ O
. -X- _ O

Typical -X- _ O
examples -X- _ O
are -X- _ O
deconvolution -X- _ O
heatmaps -X- _ O
, -X- _ O
used -X- _ O
to -X- _ O
approximate -X- _ O
through -X- _ O
Taylor -X- _ O
series -X- _ O
the -X- _ O
partial -X- _ O
derivatives -X- _ O
at -X- _ O
each -X- _ O
layer -X- _ O
( -X- _ O
Simonyan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
, -X- _ O
or -X- _ O
the -X- _ O
so -X- _ O
- -X- _ O
called -X- _ O
Layer -X- _ B-MethodName
- -X- _ I-MethodName
wise -X- _ I-MethodName
Relevance -X- _ I-MethodName
Propagation -X- _ I-MethodName
( -X- _ O
LRP -X- _ B-MethodName
) -X- _ O
, -X- _ O
that -X- _ O
redistributes -X- _ O
back -X- _ O
positive -X- _ O
and -X- _ O
negative -X- _ O
evidence -X- _ O
across -X- _ O
the -X- _ O
laySeveral -X- _ O
efforts -X- _ O
have -X- _ O
been -X- _ O
made -X- _ O
in -X- _ O
the -X- _ O
perspec -X- _ O
- -X- _ O
tive -X- _ O
of -X- _ O
providing -X- _ O
explanations -X- _ O
of -X- _ O
a -X- _ O
neural -X- _ O
classiﬁer -X- _ O
, -X- _ O
often -X- _ O
by -X- _ O
focusing -X- _ O
into -X- _ O
highlighting -X- _ O
an -X- _ O
handful -X- _ O
of -X- _ O
crucial -X- _ O
features -X- _ O
( -X- _ O
Baehrens -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2010 -X- _ O
) -X- _ O
or -X- _ O
deriving -X- _ O
simpler -X- _ O
, -X- _ O
more -X- _ O
readable -X- _ O
models -X- _ O
from -X- _ O
a -X- _ O
complex -X- _ O
one -X- _ O
, -X- _ O
e.g. -X- _ O
a -X- _ O
binary -X- _ O
decision -X- _ O
tree -X- _ O
( -X- _ O
Frosst -X- _ O
and -X- _ O
Hinton -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
or -X- _ O
by -X- _ O
local -X- _ O
approximation -X- _ O
with -X- _ O
linear -X- _ O
models -X- _ O
( -X- _ O
Ribeiro -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
although -X- _ O
they -X- _ O
can -X- _ O
explicitly -X- _ O
show -X- _ O
the -X- _ O
representations -X- _ O
learned -X- _ O
in -X- _ O
the -X- _ O
speciﬁc -X- _ O
hidden -X- _ O
neurons -X- _ O
( -X- _ O
Frosst -X- _ O
and -X- _ O
Hinton -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
these -X- _ O
approaches -X- _ O
base -X- _ O
their -X- _ O
effectiveness -X- _ O
on -X- _ O
the -X- _ O
user -X- _ O
ability -X- _ O
to -X- _ O
study -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
reasoning -X- _ O
and -X- _ O
of -X- _ O
the -X- _ O
accountability -X- _ O
as -X- _ O
a -X- _ O
side -X- _ O
effect -X- _ O
of -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
selected -X- _ O
features -X- _ O
: -X- _ O
this -X- _ O
can -X- _ O
be -X- _ O
very -X- _ O
hard -X- _ O
in -X- _ O
tasks -X- _ O
where -X- _ O
boundaries -X- _ O
between -X- _ O
classes -X- _ O
are -X- _ O
not -X- _ O
well -X- _ O
deﬁned -X- _ O
. -X- _ O

Sometimes -X- _ O
, -X- _ O
explanations -X- _ O
are -X- _ O
associated -X- _ O
to -X- _ O
vector -X- _ O
representations -X- _ O
as -X- _ O
in -X- _ O
( -X- _ O
Ribeiro -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
i.e. -X- _ O
bag -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
word -X- _ O
in -X- _ O
case -X- _ O
of -X- _ O
text -X- _ O
classiﬁcation -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
clearly -X- _ O
weak -X- _ O
at -X- _ O
capturing -X- _ O
signiﬁcant -X- _ O
linguistic -X- _ O
abstractions -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
involved -X- _ O
syntactic -X- _ O
relations -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
model -X- _ O
which -X- _ O
allows -X- _ O
to -X- _ O
provide -X- _ O
explanations -X- _ O
that -X- _ O
are -X- _ O
easily -X- _ O
interpretable -X- _ O
even -X- _ O
by -X- _ O
non -X- _ O
- -X- _ O
expert -X- _ O
users -X- _ O
, -X- _ O
as -X- _ O
they -X- _ O
are -X- _ O
expressed -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
and -X- _ O
are -X- _ O
hence -X- _ O
a -X- _ O
more -X- _ O
natural -X- _ O
solution -X- _ O
. -X- _ O

It -X- _ O
implicitly -X- _ O
captures -X- _ O
lexical -X- _ O
, -X- _ O
semantic -X- _ O
and -X- _ O
syntactic -X- _ O
generalizations -X- _ O
through -X- _ O
the -X- _ O
generation -X- _ O
of -X- _ O
a -X- _ O
linguistically -X- _ O
ﬂuent -X- _ O
explanation -X- _ O
of -X- _ O
predictions -X- _ O
: -X- _ O
as -X- _ O
this -X- _ O
is -X- _ O
exploit -X- _ O
linguistic -X- _ O
analogies -X- _ O
it -X- _ O
provides -X- _ O
a -X- _ O
more -X- _ O
transparent -X- _ O
and -X- _ O
epistemologically -X- _ O
coherent -X- _ O
view -X- _ O
on -X- _ O
the -X- _ O
system -X- _ O
’s -X- _ O
decision -X- _ O
. -X- _ O

3 -X- _ O

A -X- _ O
Kernel -X- _ O
- -X- _ O
based -X- _ O
Deep -X- _ O
Architecture -X- _ O
In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
ﬁrst -X- _ O
describe -X- _ O
the -X- _ O
Nystr -X- _ O
¨om -X- _ O
method -X- _ O
for -X- _ O
generating -X- _ O
low -X- _ O
dimensional -X- _ O
embeddings -X- _ O
that -X- _ O
approximate -X- _ O
high -X- _ O
dimensional -X- _ O
kernel -X- _ O
spaces -X- _ O
. -X- _ O

Then -X- _ O
we -X- _ O
will -X- _ O
review -X- _ O
the -X- _ O
Kernel -X- _ O
- -X- _ O
based -X- _ O
Deep -X- _ O
Architecture -X- _ O
discussed -X- _ O
in -X- _ O
( -X- _ O
Croce -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
that -X- _ O
efﬁciently -X- _ O
combines -X- _ O
kernel -X- _ O
methods -X- _ O
and -X- _ O
deep -X- _ O
learning -X- _ O
by -X- _ O
using -X- _ O
a -X- _ O
Nystr -X- _ O
¨om -X- _ O
layer -X- _ O
into -X- _ O
a -X- _ O
neural -X- _ O
architecture -X- _ O
. -X- _ O

Given -X- _ O
an -X- _ O
input -X- _ O
dataset -X- _ O
D -X- _ O
, -X- _ O
a -X- _ O
kernelK -X- _ O
( -X- _ O
oi -X- _ O
; -X- _ O
oj -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
similarity -X- _ O
function -X- _ O
over -X- _ O
D2that -X- _ O
corresponds -X- _ O
to -X- _ O
a -X- _ O
dot -X- _ O
product -X- _ O
in -X- _ O
the -X- _ O
implicit -X- _ O
kernel -X- _ O
space -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
K -X- _ O
( -X- _ O
oi -X- _ O
; -X- _ O
oj -X- _ O
) -X- _ O
= -X- _ O
 -X- _ O
( -X- _ O
oi -X- _ O
) -X- _ O
 -X- _ O
( -X- _ O
oj -X- _ O
) -X- _ O
. -X- _ O

Kernel -X- _ O
functions -X- _ O
are -X- _ O
used -X- _ O
by -X- _ O
learning -X- _ O
algorithms -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
Support -X- _ O
Vector -X- _ O
Machines -X- _ O
( -X- _ O
Shawe -X- _ O
- -X- _ O
Taylor -X- _ O
and -X- _ O
Cristianini -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
to -X- _ O
operate -X- _ O
only -X- _ O
implicitly -X- _ O
on -X- _ O
instances -X- _ O
in -X- _ O
the -X- _ O
kernel -X- _ O
space -X- _ O
, -X- _ O
by -X- _ O
never -X- _ O
accessing -X- _ O
their -X- _ O
explicit -X- _ O
deﬁnition -X- _ O
. -X- _ O

Let -X- _ O
us -X- _ O
apply -X- _ O
the -X- _ O
projection -X- _ O
functionover -X- _ O
all -X- _ O
examples -X- _ O
from -X- _ O
Dto -X- _ O
derive -X- _ O
representations -X- _ O
, -X- _ O
~ -X- _ O
xdenoting -X- _ O
the -X- _ O
rows -X- _ O
of -X- _ O
the -X- _ O
matrix -X- _ O

Nyström -X- _ O
Projection -X- _ O
… -X- _ O
… -X- _ O
… -X- _ O
K -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
l1 -X- _ O
) -X- _ O
K -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
l2 -X- _ O
) -X- _ O
K -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
ld -X- _ O
) -X- _ O
hidden -X- _ O
layers -X- _ O
classification -X- _ O
layer -X- _ O
landmarks -X- _ O
… -X- _ O
Nyström -X- _ O
layer -X- _ O
input -X- _ O
layer -X- _ O
x -X- _ O
Figure -X- _ O
1 -X- _ O
: -X- _ O
Kernel -X- _ O
- -X- _ O
based -X- _ O
Deep -X- _ O
Architecture -X- _ O
. -X- _ O

X -X- _ O
. -X- _ O

The -X- _ O
Gram -X- _ O
matrix -X- _ O
can -X- _ O
always -X- _ O
be -X- _ O
computed -X- _ O
asG -X- _ O
= -X- _ O
XX -X- _ O
> -X- _ O
, -X- _ O
with -X- _ O
each -X- _ O
single -X- _ O
element -X- _ O
corresponding -X- _ O
to -X- _ O
Gij= -X- _ O
 -X- _ O
( -X- _ O
oi -X- _ O
) -X- _ O
 -X- _ O
( -X- _ O
oj -X- _ O
) -X- _ O
= -X- _ O
K -X- _ O
( -X- _ O
oi -X- _ O
; -X- _ O
oj -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
aim -X- _ O
of -X- _ O
the -X- _ O
Nystr -X- _ O
¨om -X- _ O
method -X- _ O
is -X- _ O
to -X- _ O
derive -X- _ O
a -X- _ O
new -X- _ O
low -X- _ O
- -X- _ O
dimensional -X- _ O
embedding -X- _ O
~xin -X- _ O
al -X- _ O
- -X- _ O
dimensional -X- _ O
space -X- _ O
, -X- _ O
withl -X- _ O

nso -X- _ O
that -X- _ O
~G=~X -X- _ O
~ -X- _ O
X -X- _ O
> -X- _ O
and -X- _ O
~ -X- _ O
GG. -X- _ O
This -X- _ O
is -X- _ O
obtained -X- _ O
by -X- _ O
generating -X- _ O
an -X- _ O
approximation -X- _ O
~GofGusing -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
lcolumns -X- _ O
of -X- _ O
the -X- _ O
matrix -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
a -X- _ O
selection -X- _ O
of -X- _ O
a -X- _ O
subset -X- _ O
LD -X- _ O
of -X- _ O
the -X- _ O
available -X- _ O
examples -X- _ O
, -X- _ O
called -X- _ O
landmarks -X- _ O
. -X- _ O

Suppose -X- _ O
we -X- _ O
randomly -X- _ O
sample -X- _ O
lcolumns -X- _ O
ofG -X- _ O
, -X- _ O
and -X- _ O
letC2RjDjl -X- _ O
be -X- _ O
the -X- _ O
matrix -X- _ O
of -X- _ O
these -X- _ O
sampled -X- _ O
columns -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
rearrange -X- _ O
the -X- _ O
columns -X- _ O
and -X- _ O
rows -X- _ O
of -X- _ O
Gand -X- _ O
deﬁneX= -X- _ O

[ -X- _ O
X1X2 -X- _ O
] -X- _ O
such -X- _ O
that -X- _ O
: -X- _ O
X -X- _ O
> -X- _ O
andC=W -X- _ O
X -X- _ O
> -X- _ O
whereW -X- _ O
= -X- _ O
X -X- _ O
> -X- _ O
tains -X- _ O
only -X- _ O
landmarks -X- _ O
. -X- _ O

The -X- _ O
Nystr -X- _ O
¨om -X- _ O
approximation -X- _ O
can -X- _ O
be -X- _ O
deﬁned -X- _ O
as -X- _ O
: -X- _ O
whereWydenotes -X- _ O
the -X- _ O
Moore -X- _ O
- -X- _ O
Penrose -X- _ O
inverse -X- _ O
of -X- _ O
W. -X- _ O
The -X- _ O
Singular -X- _ O
Value -X- _ O
Decomposition -X- _ O
( -X- _ O
SVD -X- _ O
) -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
obtain -X- _ O
Wyas -X- _ O
it -X- _ O
follows -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
Wis -X- _ O
decomposed -X- _ O
so -X- _ O
that -X- _ O
W -X- _ O
= -X- _ O
USV -X- _ O
> -X- _ O
, -X- _ O
whereUand -X- _ O
Vare -X- _ O
both -X- _ O
orthogonal -X- _ O
matrices -X- _ O
, -X- _ O
and -X- _ O
Sis -X- _ O
a -X- _ O
diagonal -X- _ O
matrix -X- _ O
containing -X- _ O
the -X- _ O
( -X- _ O
non -X- _ O
- -X- _ O
zero -X- _ O
) -X- _ O
singular -X- _ O
values -X- _ O
ofWon -X- _ O
its -X- _ O
diagonal -X- _ O
. -X- _ O

Since -X- _ O
Wis -X- _ O
symmetric -X- _ O
and -X- _ O
positive -X- _ O
deﬁnite -X- _ O
, -X- _ O
W -X- _ O
= -X- _ O
USU -X- _ O
> -X- _ O
. -X- _ O

Then -X- _ O
Wy -X- _ O
= -X- _ O
US 1U -X- _ O
> -X- _ O
=US 1 -X- _ O
G~G -X- _ O
= -X- _ O
CUS 1 -X- _ O
Given -X- _ O
an -X- _ O
input -X- _ O
example -X- _ O
o2 -X- _ O
D -X- _ O
, -X- _ O
a -X- _ O
new -X- _ O
lowdimensional -X- _ O
representation -X- _ O
~~ -X- _ O

xcan -X- _ O
be -X- _ O
thus -X- _ O
determined -X- _ O
by -X- _ O
considering -X- _ O
the -X- _ O
corresponding -X- _ O
item -X- _ O
of -X- _ O
Cas -X- _ O
where~ -X- _ O
cis -X- _ O
the -X- _ O
vector -X- _ O
whose -X- _ O
dimensions -X- _ O
contain -X- _ O
the -X- _ O
evaluations -X- _ O
of -X- _ O
the -X- _ O
kernel -X- _ O
function -X- _ O
between -X- _ O
o -X- _ O
and -X- _ O
each -X- _ O
landmark -X- _ O
oj2L -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
the -X- _ O
method -X- _ O
producesl -X- _ O
- -X- _ O
dimensional -X- _ O
vectors -X- _ O
. -X- _ O

Notice -X- _ O
that -X- _ O
an -X- _ O
optimal -X- _ O
selection -X- _ O
of -X- _ O
landmarks -X- _ O
can -X- _ O
be -X- _ O
expected -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
Gram -X- _ O
Matrix -X- _ O
approximation -X- _ O
error -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
uniform -X- _ O
sampling -X- _ O
without -X- _ O
replacement -X- _ O
policy -X- _ O
is -X- _ O
adopted -X- _ O
: -X- _ O
it -X- _ O
is -X- _ O
in -X- _ O
fact -X- _ O
theoretically -X- _ O
and -X- _ O
empirically -X- _ O
shown -X- _ O
in -X- _ O
Kumar -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2012 -X- _ O
) -X- _ O
to -X- _ O
achieve -X- _ O
results -X- _ O
comparable -X- _ O
with -X- _ O
alternative -X- _ O
but -X- _ O
( -X- _ O
more -X- _ O
complex -X- _ O
) -X- _ O
selection -X- _ O
policies -X- _ O
. -X- _ O

In -X- _ O
( -X- _ O
Croce -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
Nystr -X- _ O
¨om -X- _ O
representation -X- _ O
~~ -X- _ O
xhas -X- _ O
been -X- _ O
used -X- _ O
as -X- _ O
input -X- _ O
within -X- _ O
neural -X- _ O
network -X- _ O
architectures -X- _ O
. -X- _ O

In -X- _ O
fact -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
labeled -X- _ O
dataset -X- _ O
L -X- _ O
= -X- _ O
f -X- _ O
( -X- _ O
o -X- _ O
; -X- _ O
y -X- _ O
) -X- _ O
jo2D -X- _ O
; -X- _ O
y2Yg -X- _ O
, -X- _ O
whereorefers -X- _ O
to -X- _ O
a -X- _ O
generic -X- _ O
instance -X- _ O
and -X- _ O
yis -X- _ O
its -X- _ O
associated -X- _ O
class -X- _ O
, -X- _ O
a -X- _ O
Multi -X- _ B-MethodName
- -X- _ I-MethodName
Layer -X- _ I-MethodName
Perceptron -X- _ I-MethodName
( -X- _ O
MLP -X- _ B-MethodName
) -X- _ O
architecture -X- _ O
can -X- _ O
be -X- _ O
deﬁned -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
speciﬁc -X- _ O
Nystr -X- _ O
¨om -X- _ O
layer -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
Nystr -X- _ O
¨om -X- _ O
embeddings -X- _ O
of -X- _ O
Eq -X- _ O
. -X- _ O
2 -X- _ O
. -X- _ O

Such -X- _ O
Kernel -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
Deep -X- _ I-MethodName
Architecture -X- _ I-MethodName
( -X- _ O
KDA -X- _ B-MethodName
) -X- _ O
has -X- _ O
an -X- _ O
input -X- _ O
layer -X- _ O
, -X- _ O
aNystr -X- _ O
¨om -X- _ O
layer -X- _ O
, -X- _ O
a -X- _ O
possibly -X- _ O
empty -X- _ O
sequence -X- _ O
of -X- _ O
non -X- _ O
- -X- _ O
linear -X- _ O
hidden -X- _ O
layers -X- _ O
and -X- _ O
a -X- _ O
ﬁnal -X- _ O
classiﬁcation -X- _ O
layer -X- _ O
, -X- _ O
which -X- _ O
produces -X- _ O
the -X- _ O
output -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
. -X- _ O

19The -X- _ O
input -X- _ O
layer -X- _ O
corresponds -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
vector~ -X- _ O
c -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
row -X- _ O
of -X- _ O
the -X- _ O
Cmatrix -X- _ O
associated -X- _ O
to -X- _ O
an -X- _ O
example -X- _ O
o -X- _ O
. -X- _ O

The -X- _ O
input -X- _ O
layer -X- _ O
is -X- _ O
mapped -X- _ O
to -X- _ O
the -X- _ O
Nystr -X- _ O
¨om -X- _ O
layer -X- _ O
, -X- _ O
through -X- _ O
the -X- _ O
projection -X- _ O
in -X- _ O
Equation -X- _ O
2 -X- _ O
. -X- _ O

Notice -X- _ O
that -X- _ O
the -X- _ O
embedding -X- _ O
provides -X- _ O
also -X- _ O
the -X- _ O
proper -X- _ O
weights -X- _ O
, -X- _ O
deﬁned -X- _ O
by -X- _ O
US 1 -X- _ O
2 -X- _ O
, -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
mapping -X- _ O
can -X- _ O
be -X- _ O
expressed -X- _ O
through -X- _ O
the -X- _ O
Nystr -X- _ O
¨om -X- _ O
matrixHNy -X- _ O
= -X- _ O
US 1 -X- _ O
2 -X- _ O
: -X- _ O
it -X- _ O
corresponds -X- _ O
to -X- _ O
a -X- _ O
pretrained -X- _ O
stage -X- _ O
derived -X- _ O
through -X- _ O
SVD -X- _ O
. -X- _ O

Formally -X- _ O
, -X- _ O
the -X- _ O
low -X- _ O
- -X- _ O
dimensional -X- _ O
embedding -X- _ O
of -X- _ O
an -X- _ O
input -X- _ O
example -X- _ O
The -X- _ O
resulting -X- _ O
outcome -X- _ O
~~ -X- _ O

xis -X- _ O
the -X- _ O
input -X- _ O
to -X- _ O
one -X- _ O
or -X- _ O
more -X- _ O
non -X- _ O
- -X- _ O
linear -X- _ O
hidden -X- _ O
layers -X- _ O
. -X- _ O

Each -X- _ O
t -X- _ O
- -X- _ O
th -X- _ O
hidden -X- _ O
layer -X- _ O
is -X- _ O
realized -X- _ O
through -X- _ O
a -X- _ O
matrix -X- _ O
Ht2Rht 1ht -X- _ O
and -X- _ O
a -X- _ O
bias -X- _ O
vector -X- _ O
~bt2R1ht -X- _ O
, -X- _ O
wherehtdenotes -X- _ O
the -X- _ O
desired -X- _ O
hidden -X- _ O
layer -X- _ O
dimensionality -X- _ O
. -X- _ O

Clearly -X- _ O
, -X- _ O
given -X- _ O
thatHNy2Rll -X- _ O
, -X- _ O
h0 -X- _ O
= -X- _ O
l. -X- _ O
The -X- _ O
ﬁrst -X- _ O
hidden -X- _ O
layer -X- _ O
in -X- _ O
fact -X- _ O
receives -X- _ O
in -X- _ O
input -X- _ O
~~ -X- _ O
x=~ -X- _ O

cHNy -X- _ O
, -X- _ O
that -X- _ O
corresponds -X- _ O
to -X- _ O
the -X- _ O
t= -X- _ O
0 -X- _ O
layer -X- _ O

input~ -X- _ O
x0=~~ -X- _ O
x -X- _ O
and -X- _ O
its -X- _ O
computation -X- _ O
is -X- _ O
formally -X- _ O
expressed -X- _ O
by -X- _ O
~ -X- _ O
x1 -X- _ O
= -X- _ O
f -X- _ O
( -X- _ O
~ -X- _ O
x0H1+~b1 -X- _ O
) -X- _ O
, -X- _ O
wherefis -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
linear -X- _ O
activation -X- _ O
function -X- _ O
. -X- _ O

In -X- _ O
general -X- _ O
, -X- _ O
the -X- _ O
generic -X- _ O
t -X- _ O
- -X- _ O
th -X- _ O
layer -X- _ O
is -X- _ O
modeled -X- _ O
as -X- _ O
: -X- _ O
The -X- _ O
ﬁnal -X- _ O
layer -X- _ O
of -X- _ O
KDA -X- _ B-MethodName
is -X- _ O
the -X- _ O
classiﬁcation -X- _ O
layer -X- _ O
, -X- _ O
realized -X- _ O
through -X- _ O
the -X- _ O
output -X- _ O
matrix -X- _ O
HOand -X- _ O
the -X- _ O
output -X- _ O
bias -X- _ O
vector -X- _ O
~bO. -X- _ O
Their -X- _ O
dimensionality -X- _ O
depends -X- _ O
on -X- _ O
the -X- _ O
dimensionality -X- _ O
of -X- _ O
the -X- _ O
last -X- _ O
hidden -X- _ O
layer -X- _ O
( -X- _ O
called -X- _ O
O 1 -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
numberjYjof -X- _ O
different -X- _ O
classes -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
HO2RhO 1jYjand -X- _ O
~ -X- _ O
bO2R1jYj -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

In -X- _ O
particular -X- _ O
, -X- _ O
this -X- _ O
layer -X- _ O
computes -X- _ O
a -X- _ O
linear -X- _ O
classiﬁcation -X- _ O
function -X- _ O
with -X- _ O
a -X- _ O
softmax -X- _ O
operator -X- _ O
so -X- _ O
that -X- _ O
^y -X- _ O
= -X- _ O
softmax -X- _ O
( -X- _ O
~ -X- _ O
xO 1HO+~bO -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
standard -X- _ O
dropout -X- _ O
, -X- _ O
a -X- _ O
L2regularization -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
norm -X- _ O
of -X- _ O
each -X- _ O
layer -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
the -X- _ O
KDA -X- _ B-MethodName
is -X- _ O
trained -X- _ O
by -X- _ O
optimizing -X- _ O
a -X- _ O
loss -X- _ O
function -X- _ O
made -X- _ O
of -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
two -X- _ O
factors -X- _ O
: -X- _ O
ﬁrst -X- _ O
, -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
entropy -X- _ O
function -X- _ O
between -X- _ O
the -X- _ O
gold -X- _ O
classes -X- _ O
and -X- _ O
the -X- _ O
predicted -X- _ O
ones -X- _ O
; -X- _ O
second -X- _ O
the -X- _ O
L2regularization -X- _ O
, -X- _ O
whose -X- _ O
importance -X- _ O
is -X- _ O
regulated -X- _ O
by -X- _ O
a -X- _ O
metaparameter. -X- _ O
The -X- _ O
ﬁnal -X- _ O
loss -X- _ O
function -X- _ O
is -X- _ O
thus -X- _ O
( -X- _ O
o -X- _ O
; -X- _ O
y -X- _ O
) -X- _ O
2Lylog -X- _ O
( -X- _ O
^y -X- _ O
) -X- _ O
+X -X- _ O
H2fHtg -X- _ O
[ -X- _ O
fHOgjjHjj2 -X- _ O
where -X- _ O
^yare -X- _ O
the -X- _ O
softmax -X- _ O
values -X- _ O
computed -X- _ O
by -X- _ O
the -X- _ O
network -X- _ O
and -X- _ O
yare -X- _ O
the -X- _ O
true -X- _ O
one -X- _ O
- -X- _ O
hot -X- _ O
encoding -X- _ O
values -X- _ O
associated -X- _ O
with -X- _ O
the -X- _ O
example -X- _ O
from -X- _ O
the -X- _ O
labeled -X- _ O
training -X- _ O
datasetL -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
worth -X- _ O
noticing -X- _ O
that -X- _ O
the -X- _ O
network -X- _ O
is -X- _ O
stimulated -X- _ O
with -X- _ O
an -X- _ O
input -X- _ O
vector -X- _ O
cwhich -X- _ O
contains -X- _ O
the -X- _ O
kernel -X- _ O
evaluations -X- _ O
K -X- _ O
( -X- _ O
s -X- _ O
; -X- _ O
li -X- _ O
) -X- _ O
between -X- _ O
each -X- _ O
example -X- _ O
and -X- _ O
the -X- _ O
landmarks -X- _ O
. -X- _ O

When -X- _ O
using -X- _ O
linguistic -X- _ O
kernels -X- _ O
( -X- _ O
such -X- _ O
as -X- _ O
Semantic -X- _ O
Tree -X- _ O
Kernels -X- _ O
) -X- _ O
this -X- _ O
measure -X- _ O
corresponds -X- _ O
to -X- _ O
a -X- _ O
syntactic -X- _ O
/ -X- _ O
semantic -X- _ O
similarity -X- _ O
between -X- _ O
the -X- _ O
xand -X- _ O
the -X- _ O
subset -X- _ O
of -X- _ O
examples -X- _ O
used -X- _ O
for -X- _ O
the -X- _ O
space -X- _ O
reconstruction -X- _ O
( -X- _ O
made -X- _ O
available -X- _ O
through -X- _ O
the -X- _ O
Nystr -X- _ O
¨om -X- _ O
method -X- _ O
) -X- _ O
. -X- _ O

Once -X- _ O
stimulated -X- _ O
, -X- _ O
the -X- _ O
network -X- _ O
will -X- _ O
provide -X- _ O
an -X- _ O
output -X- _ O
. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
give -X- _ O
an -X- _ O
explanation -X- _ O
to -X- _ O
a -X- _ O
network -X- _ O
decision -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
discuss -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
section -X- _ O
how -X- _ O
to -X- _ O
revert -X- _ O
the -X- _ O
propagation -X- _ O
process -X- _ O
connecting -X- _ O
output -X- _ O
and -X- _ O
input -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
side -X- _ O
effect -X- _ O
we -X- _ O
will -X- _ O
be -X- _ O
able -X- _ O
to -X- _ O
determine -X- _ O
those -X- _ O
landmarks -X- _ O
mostly -X- _ O
affecting -X- _ O
the -X- _ O
ﬁnal -X- _ O
decision -X- _ O
and -X- _ O
which -X- _ O
are -X- _ O
more -X- _ O
semantically -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
instance -X- _ O
. -X- _ O

4 -X- _ O
Layer -X- _ O
- -X- _ O
wise -X- _ O
Relevance -X- _ O
Propagation -X- _ O
in -X- _ O
Kernel -X- _ O
- -X- _ O
based -X- _ O
Deep -X- _ O
Architectures -X- _ O
Layer -X- _ B-MethodName
- -X- _ I-MethodName
wise -X- _ I-MethodName
Relevance -X- _ I-MethodName
propagation -X- _ I-MethodName
( -X- _ O
LRP -X- _ B-MethodName
, -X- _ O
presented -X- _ O
in -X- _ O
( -X- _ O
Bach -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
framework -X- _ O
which -X- _ O
allows -X- _ O
to -X- _ O
decompose -X- _ O
the -X- _ O
prediction -X- _ O
of -X- _ O
a -X- _ O
deep -X- _ O
neural -X- _ O
network -X- _ O
computed -X- _ O
over -X- _ O
a -X- _ O
sample -X- _ O
, -X- _ O
e.g. -X- _ O
an -X- _ O
image -X- _ O
, -X- _ O
down -X- _ O
to -X- _ O
relevance -X- _ O
scores -X- _ O
for -X- _ O
the -X- _ O
single -X- _ O
input -X- _ O
dimensions -X- _ O
of -X- _ O
the -X- _ O
sample -X- _ O
such -X- _ O
as -X- _ O
subpixels -X- _ O
of -X- _ O
an -X- _ O
image -X- _ O
. -X- _ O

More -X- _ O
formally -X- _ O
, -X- _ O
let -X- _ O
f -X- _ O
: -X- _ O
Rd -X- _ O
! -X- _ O
R+be -X- _ O
a -X- _ O
positive -X- _ O
real -X- _ O
- -X- _ O
valued -X- _ O
function -X- _ O
taking -X- _ O
a -X- _ O
vector -X- _ O
x2Rd -X- _ O
as -X- _ O
input -X- _ O
. -X- _ O

The -X- _ O
function -X- _ O
fcan -X- _ O
quantify -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
xbeing -X- _ O
in -X- _ O
a -X- _ O
certain -X- _ O
class -X- _ O
. -X- _ O

The -X- _ O
Layer -X- _ O
- -X- _ O
wise -X- _ O
Relevance -X- _ O
Propagation -X- _ O
assigns -X- _ O
to -X- _ O
each -X- _ O
dimension -X- _ O
, -X- _ O
or -X- _ O
feature -X- _ O
, -X- _ O
xda -X- _ O
relevance -X- _ O
score -X- _ O
dsuch -X- _ O
that -X- _ O
: -X- _ O
f -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
P -X- _ O
Features -X- _ O
whose -X- _ O
score -X- _ O
is -X- _ O
R -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
correspond -X- _ O
to -X- _ O
evidence -X- _ O
in -X- _ O
favor -X- _ O
or -X- _ O
against -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
classiﬁcation -X- _ O
. -X- _ O

In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
LRP -X- _ B-MethodName
allows -X- _ O
to -X- _ O
identify -X- _ O
fragments -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
playing -X- _ O
key -X- _ O
roles -X- _ O
in -X- _ O
the -X- _ O
decision -X- _ O
, -X- _ O
by -X- _ O
propagating -X- _ O
relevance -X- _ O
backwards -X- _ O
. -X- _ O

Let -X- _ O
us -X- _ O
suppose -X- _ O
to -X- _ O
know -X- _ O
the -X- _ O
relevance -X- _ O
score -X- _ O
R -X- _ O
( -X- _ O
l+1 -X- _ O
) -X- _ O
j -X- _ O
of -X- _ O
a -X- _ O
neuronjat -X- _ O
network -X- _ O
layer -X- _ O
l+ -X- _ O
1 -X- _ O
, -X- _ O
then -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
decomposed -X- _ O
into -X- _ O
messages -X- _ O
i -X- _ O
jsent -X- _ O
to -X- _ O
neurons -X- _ O
iin -X- _ O
layerl -X- _ O
: -X- _ O
j -X- _ O
= -X- _ O
X -X- _ O
Hence -X- _ O
it -X- _ O
derives -X- _ O
that -X- _ O
the -X- _ O
relevance -X- _ O
of -X- _ O
a -X- _ O
neuron -X- _ O
iat -X- _ O
layerlcan -X- _ O
be -X- _ O
deﬁned -X- _ O
as -X- _ O
: -X- _ O
R -X- _ O
( -X- _ O
l -X- _ O
) -X- _ O
i -X- _ O
= -X- _ O
X -X- _ O

20Note -X- _ O
that -X- _ O
5 -X- _ O
and -X- _ O
6 -X- _ O
are -X- _ O
such -X- _ O
that -X- _ O
4 -X- _ O
holds -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
adopted -X- _ O
the -X- _ O
-rule -X- _ O
deﬁned -X- _ O
in -X- _ O
( -X- _ O
Bach -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
messages -X- _ O
R -X- _ O
( -X- _ O
l -X- _ O
; -X- _ O
l+1 -X- _ O
) -X- _ O

i -X- _ O
j -X- _ O
: -X- _ O
i -X- _ O

j -X- _ O
= -X- _ O
zij -X- _ O
zj+sign -X- _ O
( -X- _ O
zj -X- _ O
) -X- _ O
R -X- _ O
( -X- _ O
l+1 -X- _ O
) -X- _ O

j -X- _ O
wherezij -X- _ O
= -X- _ O
xiwijand -X- _ O
> -X- _ O
0is -X- _ O
a -X- _ O
numerical -X- _ O
stabilizing -X- _ O
term -X- _ O
and -X- _ O
must -X- _ O
be -X- _ O
small -X- _ O
. -X- _ O

The -X- _ O
informative -X- _ O
value -X- _ O
is -X- _ O
justiﬁed -X- _ O
by -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
the -X- _ O
weights -X- _ O
zij -X- _ O
are -X- _ O
linked -X- _ O
to -X- _ O
the -X- _ O
activation -X- _ O
weights -X- _ O
wijof -X- _ O
the -X- _ O
input -X- _ O
neurons -X- _ O
. -X- _ O

If -X- _ O
we -X- _ O
apply -X- _ O
it -X- _ O
to -X- _ O
a -X- _ O
KDA -X- _ B-MethodName
processing -X- _ O
linguistic -X- _ O
observations -X- _ O
, -X- _ O
then -X- _ O
LRP -X- _ B-MethodName
implicitly -X- _ O
traces -X- _ O
back -X- _ O
the -X- _ O
syntactic -X- _ O
, -X- _ O
semantic -X- _ O
and -X- _ O
lexical -X- _ O
relations -X- _ O
between -X- _ O
the -X- _ O
example -X- _ O
and -X- _ O
the -X- _ O
landmarks -X- _ O
, -X- _ O
thus -X- _ O
it -X- _ O
selects -X- _ O
the -X- _ O
landmarks -X- _ O
whose -X- _ O
presences -X- _ O
were -X- _ O
the -X- _ O
most -X- _ O
inﬂuential -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
predicted -X- _ O
structure -X- _ O
in -X- _ O
the -X- _ O
sentence -X- _ O
. -X- _ O

Indeed -X- _ O
, -X- _ O
each -X- _ O
landmark -X- _ O
is -X- _ O
uniquely -X- _ O
associated -X- _ O
to -X- _ O
an -X- _ O
entry -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
vector -X- _ O
~ -X- _ O
c -X- _ O
, -X- _ O
as -X- _ O
illustrated -X- _ O
in -X- _ O
Sec -X- _ O
3 -X- _ O
. -X- _ O
5 -X- _ O
Explanatory -X- _ O
Models -X- _ O
Justiﬁcations -X- _ O
for -X- _ O
the -X- _ O
KDA -X- _ B-MethodName
emissions -X- _ O
can -X- _ O
be -X- _ O
obtained -X- _ O
by -X- _ O
explaining -X- _ O
the -X- _ O
evidence -X- _ O
in -X- _ O
favour -X- _ O
or -X- _ O
against -X- _ O
a -X- _ O
class -X- _ O
using -X- _ O
landmarks -X- _ O
f`gas -X- _ O
examples -X- _ O
. -X- _ O

The -X- _ O
idea -X- _ O
is -X- _ O
to -X- _ O
select -X- _ O
those -X- _ O
f`gthat -X- _ O
the -X- _ O
LRP -X- _ B-MethodName
method -X- _ O
produces -X- _ O
as -X- _ O
the -X- _ O
most -X- _ O
active -X- _ O
elements -X- _ O
in -X- _ O
layer -X- _ O
0 -X- _ O
. -X- _ O

Once -X- _ O
such -X- _ O
active -X- _ O
landmarks -X- _ O
are -X- _ O
detected -X- _ O
, -X- _ O
an -X- _ O
Explanatory -X- _ O
Model -X- _ O
is -X- _ O
a -X- _ O
function -X- _ O
in -X- _ O
charge -X- _ O
to -X- _ O
compile -X- _ O
the -X- _ O
linguistically -X- _ O
ﬂuent -X- _ O
explanation -X- _ O
by -X- _ O
using -X- _ O
analogies -X- _ O
or -X- _ O
differences -X- _ O
with -X- _ O
the -X- _ O
input -X- _ O
case -X- _ O
. -X- _ O

The -X- _ O
semantic -X- _ O
expressiveness -X- _ O
of -X- _ O
such -X- _ O
analogies -X- _ O
makes -X- _ O
the -X- _ O
resulting -X- _ O
explanation -X- _ O
clear -X- _ O
and -X- _ O
increases -X- _ O
the -X- _ O
user -X- _ O
conﬁdence -X- _ O
on -X- _ O
the -X- _ O
system -X- _ O
reliability -X- _ O
. -X- _ O

When -X- _ O
a -X- _ O
sentencesis -X- _ O
classiﬁed -X- _ O
, -X- _ O
LRP -X- _ B-MethodName
assigns -X- _ O
activation -X- _ O
scoresrs -X- _ O
` -X- _ O
to -X- _ O
each -X- _ O
individual -X- _ O
landmark -X- _ O
` -X- _ O
: -X- _ O
letL -X- _ O
( -X- _ O
+ -X- _ O
) -X- _ O
( -X- _ O
orL -X- _ O
( -X- _ O
  -X- _ O
) -X- _ O
) -X- _ O
denote -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
landmarks -X- _ O
with -X- _ O
positive -X- _ O
( -X- _ O
or -X- _ O
negative -X- _ O
) -X- _ O
activation -X- _ O
score -X- _ O
. -X- _ O

Formally -X- _ O
, -X- _ O
every -X- _ O
explanation -X- _ O
is -X- _ O
characterized -X- _ O
by -X- _ O
a -X- _ O
triplee -X- _ O
= -X- _ O
hs -X- _ O
; -X- _ O
C -X- _ O
; -X- _ O

iwheresis -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
, -X- _ O
Cis -X- _ O
the -X- _ O
predicted -X- _ O
label -X- _ O
and -X- _ O

is -X- _ O
the -X- _ O
modality -X- _ O
of -X- _ O
the -X- _ O
explanation -X- _ O
: -X- _ O


= -X- _ O
+1 -X- _ O
for -X- _ O
positive -X- _ O
( -X- _ O
i.e. -X- _ O
acceptance -X- _ O
) -X- _ O
statements -X- _ O
while -X- _ O

= -X- _ O
 1correspond -X- _ O
to -X- _ O
rejections -X- _ O
of -X- _ O
the -X- _ O
decision -X- _ O
C. -X- _ O
A -X- _ O
landmark -X- _ O
` -X- _ O
ispositively -X- _ O
activated -X- _ O
for -X- _ O
a -X- _ O
given -X- _ O
sentencesif -X- _ O
there -X- _ O
are -X- _ O
not -X- _ O
more -X- _ O
than -X- _ O
k 1other -X- _ O
active -X- _ O
landmarks -X- _ O
` -X- _ O
0whose -X- _ O
activation -X- _ O
value -X- _ O
is -X- _ O
higher -X- _ O
than -X- _ O
the -X- _ O
one -X- _ O
for -X- _ O
` -X- _ O
, -X- _ O
i.e. -X- _ O
Similarly -X- _ O
, -X- _ O
a -X- _ O
landmark -X- _ O
is -X- _ O
negatively -X- _ O
activated -X- _ O
wherekis -X- _ O
a -X- _ O
parameter -X- _ O
used -X- _ O
to -X- _ O
make -X- _ O
explanation -X- _ O
depending -X- _ O
on -X- _ O
not -X- _ O
more -X- _ O
than -X- _ O
klandmarks -X- _ O
, -X- _ O
denoted -X- _ O
byLk -X- _ O
. -X- _ O

Positively -X- _ O
( -X- _ O
or -X- _ O
negative -X- _ O
) -X- _ O
active -X- _ O
landmarks -X- _ O
inLkare -X- _ O
assigned -X- _ O
to -X- _ O
an -X- _ O
activation -X- _ O
value -X- _ O
not -X- _ O
activated -X- _ O
landmarks -X- _ O
. -X- _ O

Given -X- _ O
the -X- _ O
explanation -X- _ O
e -X- _ O
= -X- _ O
hs -X- _ O
; -X- _ O
C -X- _ O
; -X- _ O

i -X- _ O
, -X- _ O
a -X- _ O
landmark -X- _ O
` -X- _ O
whose -X- _ O
( -X- _ O
known -X- _ O
) -X- _ O
class -X- _ O
is -X- _ O
C`isconsistent -X- _ O
( -X- _ O
orinconsistent -X- _ O
) -X- _ O
witheaccording -X- _ O
to -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
the -X- _ O
following -X- _ O
function -X- _ O
: -X- _ O
is -X- _ O
positive -X- _ O
( -X- _ O
or -X- _ O
negative -X- _ O
, -X- _ O
respectively -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
 -X- _ O
( -X- _ O
C0 -X- _ O
; -X- _ O
C -X- _ O
) -X- _ O

= -X- _ O
2kron -X- _ O
( -X- _ O
C0 -X- _ O
= -X- _ O
C -X- _ O
) -X- _ O
 1andkron -X- _ O
is -X- _ O
the -X- _ O
Kronecker -X- _ O
delta -X- _ O
. -X- _ O

An -X- _ O
explanatory -X- _ O
model -X- _ O
is -X- _ O
then -X- _ O
a -X- _ O
function -X- _ O
M -X- _ O
( -X- _ O
e -X- _ O
; -X- _ O
Lk -X- _ O
) -X- _ O
which -X- _ O
maps -X- _ O
an -X- _ O
explanation -X- _ O
e -X- _ O
, -X- _ O
a -X- _ O
sub -X- _ O
set -X- _ O
Lkof -X- _ O
the -X- _ O
active -X- _ O
andconsistent -X- _ O
landmarks -X- _ O
Lfore -X- _ O
into -X- _ O
a -X- _ O
sentence -X- _ O
fin -X- _ O
natural -X- _ O
language -X- _ O
. -X- _ O

Of -X- _ O
course -X- _ O
several -X- _ O
deﬁnitions -X- _ O
for -X- _ O
M -X- _ O
( -X- _ O
e -X- _ O
; -X- _ O
Lk -X- _ O
) -X- _ O
are -X- _ O
possible -X- _ O
. -X- _ O

A -X- _ O
general -X- _ O
explanatory -X- _ O
model -X- _ O
would -X- _ O
be -X- _ O
: -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
> -X- _ O
: -X- _ O
’ -X- _ O

sisCsince -X- _ O
it -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
` -X- _ O
’ -X- _ O
’ -X- _ O
sis -X- _ O
notCsince -X- _ O
it -X- _ O
is -X- _ O
different -X- _ O
from`which -X- _ O
isC -X- _ O
’ -X- _ O
’ -X- _ O
sisCbut -X- _ O
I -X- _ O
do -X- _ O
n’t -X- _ O
know -X- _ O
why -X- _ O
’ -X- _ O
ifL -X- _ O
; -X- _ O
whereL -X- _ O
kare -X- _ O
the -X- _ O
partition -X- _ O
of -X- _ O
landmarks -X- _ O
with -X- _ O
positive -X- _ O
and -X- _ O
negative -X- _ O
relevance -X- _ O
scores -X- _ O
in -X- _ O
Lk -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

Here -X- _ O
we -X- _ O
introduce -X- _ O
three -X- _ O
explanatory -X- _ O
models -X- _ O
we -X- _ O
used -X- _ O
during -X- _ O
experimental -X- _ O
evaluation -X- _ O
: -X- _ O
( -X- _ O
Basic -X- _ O
Model -X- _ O
) -X- _ O

The -X- _ O
ﬁrst -X- _ O
model -X- _ O
is -X- _ O
the -X- _ O
simplest -X- _ O
. -X- _ O

It -X- _ O
returns -X- _ O
an -X- _ O
analogy -X- _ O
only -X- _ O
with -X- _ O
the -X- _ O
( -X- _ O
unique -X- _ O
) -X- _ O
consistent -X- _ O
landmark -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
positive -X- _ O
score -X- _ O
if -X- _ O

= -X- _ O
1 -X- _ O
and -X- _ O
lowest -X- _ O
negative -X- _ O
score -X- _ O
when -X- _ O

= -X- _ O
 1 -X- _ O
. -X- _ O

In -X- _ O
case -X- _ O
no -X- _ O
active -X- _ O
and -X- _ O
consistent -X- _ O
landmark -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
, -X- _ O
the -X- _ O
Basic -X- _ O
Model -X- _ O
returns -X- _ O
a -X- _ O
phrase -X- _ O
stating -X- _ O
only -X- _ O
the -X- _ O
predicted -X- _ O
class -X- _ O
, -X- _ O
with -X- _ O
no -X- _ O
explanation -X- _ O
. -X- _ O

As -X- _ O
an -X- _ O
example -X- _ O
the -X- _ O
explanation -X- _ O
of -X- _ O
an -X- _ O
accepted -X- _ O
decision -X- _ O
in -X- _ O
an -X- _ O
argument -X- _ O
classiﬁcation -X- _ O
task -X- _ O
, -X- _ O
described -X- _ O
by -X- _ O
the -X- _ O
triple -X- _ O
e1 -X- _ O
= -X- _ O
h’Put -X- _ O
this -X- _ O
plate -X- _ O
in -X- _ O
the -X- _ O
center -X- _ O
of -X- _ O
the -X- _ O
table -X- _ O
’ -X- _ O
; -X- _ O
THEME -X- _ O
PLACING -X- _ O
; -X- _ O
1i -X- _ O
, -X- _ O
would -X- _ O
be -X- _ O
mapped -X- _ O
by -X- _ O
the -X- _ O
model -X- _ O
into -X- _ O
: -X- _ O

21I -X- _ O
think -X- _ O
” -X- _ O
this -X- _ O
plate -X- _ O
” -X- _ O
isTHEME -X- _ O
ofPLACING -X- _ O
in -X- _ O
” -X- _ O
Robot -X- _ O
PUT -X- _ O
this -X- _ O
plate -X- _ O
in -X- _ O
the -X- _ O
center -X- _ O
of -X- _ O
the -X- _ O
table -X- _ O
” -X- _ O
since -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
the -X- _ O
soap -X- _ O
” -X- _ O
in -X- _ O
” -X- _ O
Can -X- _ O
you -X- _ O
PUT -X- _ O
the -X- _ O
soap -X- _ O
in -X- _ O
the -X- _ O
washing -X- _ O
machine -X- _ O
? -X- _ O
” -X- _ O
. -X- _ O

( -X- _ O
Multiplicative -X- _ O
Model -X- _ O
) -X- _ O

In -X- _ O
a -X- _ O
second -X- _ O
model -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O
multiplicative -X- _ O
, -X- _ O
the -X- _ O
system -X- _ O
makes -X- _ O
reference -X- _ O
to -X- _ O
up -X- _ O
to -X- _ O
k1kanalogies -X- _ O
with -X- _ O
positively -X- _ O
( -X- _ O
or -X- _ O
negatively -X- _ O
) -X- _ O
active -X- _ O
and -X- _ O
consistent -X- _ O
landmarks -X- _ O
. -X- _ O

Given -X- _ O
the -X- _ O
above -X- _ O
explanation -X- _ O
e1 -X- _ O
, -X- _ O
andk1= -X- _ O
2 -X- _ O
, -X- _ O
it -X- _ O
would -X- _ O
return -X- _ O
: -X- _ O
I -X- _ O
think -X- _ O
” -X- _ O
this -X- _ O
plate -X- _ O
” -X- _ O
isTHEME -X- _ O
ofPLACING -X- _ O
in -X- _ O
” -X- _ O
Robot -X- _ O
PUT -X- _ O
this -X- _ O
plate -X- _ O
in -X- _ O
the -X- _ O
center -X- _ O
of -X- _ O
the -X- _ O
table -X- _ O
” -X- _ O
since -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
the -X- _ O
soap -X- _ O
” -X- _ O
in -X- _ O
” -X- _ O
Can -X- _ O
you -X- _ O
PUT -X- _ O
” -X- _ O
the -X- _ O
soap -X- _ O
” -X- _ O
in -X- _ O
the -X- _ O
washing -X- _ O
machine -X- _ O
? -X- _ O
” -X- _ O

and -X- _ O
it -X- _ O
is -X- _ O
also -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
my -X- _ O
coat -X- _ O
” -X- _ O
in -X- _ O
” -X- _ O
HANG -X- _ O
my -X- _ O
coat -X- _ O
in -X- _ O
the -X- _ O
closet -X- _ O
in -X- _ O
the -X- _ O
bedroom -X- _ O
” -X- _ O
. -X- _ O

( -X- _ O
Contrastive -X- _ O
Model -X- _ O
) -X- _ O

The -X- _ O
last -X- _ O
proposed -X- _ O
model -X- _ O
is -X- _ O
more -X- _ O
complex -X- _ O
since -X- _ O
it -X- _ O
returns -X- _ O
both -X- _ O
a -X- _ O
positive -X- _ O
( -X- _ O
whether -X- _ O

= -X- _ O
1 -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
negative -X- _ O
( -X- _ O


= -X- _ O
 1 -X- _ O
) -X- _ O
analogy -X- _ O
by -X- _ O
selecting -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
the -X- _ O
most -X- _ O
positively -X- _ O
relevant -X- _ O
and -X- _ O
the -X- _ O
most -X- _ O
negatively -X- _ O
relevant -X- _ O
consistent -X- _ O
landmark -X- _ O
: -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
given -X- _ O
e1 -X- _ O
, -X- _ O
it -X- _ O
could -X- _ O
return -X- _ O
: -X- _ O
I -X- _ O
think -X- _ O
” -X- _ O
this -X- _ O
plate -X- _ O
” -X- _ O
is -X- _ O
the -X- _ O
THEME -X- _ O
ofPLACING -X- _ O
in -X- _ O
” -X- _ O
Robot -X- _ O
PUT -X- _ O
this -X- _ O
plate -X- _ O
in -X- _ O
the -X- _ O
center -X- _ O
of -X- _ O
the -X- _ O
table -X- _ O
” -X- _ O
since -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
the -X- _ O
soap -X- _ O
” -X- _ O
which -X- _ O
is -X- _ O
in -X- _ O
” -X- _ O
Can -X- _ O
you -X- _ O
PUT -X- _ O
the -X- _ O
soap -X- _ O
in -X- _ O
the -X- _ O
washing -X- _ O
machine -X- _ O
” -X- _ O
and -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
the -X- _ O
GOAL -X- _ O
ofPLACING -X- _ O
since -X- _ O
different -X- _ O
from -X- _ O
” -X- _ O
on -X- _ O
the -X- _ O
counter -X- _ O
” -X- _ O
in -X- _ O
” -X- _ O
PUT -X- _ O
the -X- _ O
plate -X- _ O
on -X- _ O
the -X- _ O
counter -X- _ O
” -X- _ O
. -X- _ O

5.1 -X- _ O
Using -X- _ O
information -X- _ O
theory -X- _ O
for -X- _ O
validating -X- _ O
explanations -X- _ O
LetP -X- _ O
( -X- _ O
Cjs -X- _ O
) -X- _ O
andP -X- _ O
( -X- _ O
Cjs -X- _ O
; -X- _ O
e -X- _ O
) -X- _ O
be -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
the -X- _ O
prior -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
classiﬁcation -X- _ O
of -X- _ O
sbeing -X- _ O
correct -X- _ O
and -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
classiﬁcation -X- _ O
being -X- _ O
correct -X- _ O
given -X- _ O
an -X- _ O
explanation -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
both -X- _ O
indicate -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
conﬁdence -X- _ O
the -X- _ O
user -X- _ O
has -X- _ O
in -X- _ O
the -X- _ O
classiﬁer -X- _ O
( -X- _ O
i.e. -X- _ O
the -X- _ O
KDA -X- _ B-MethodName
) -X- _ O
given -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
available -X- _ O
information -X- _ O
, -X- _ O
i.e. -X- _ O
with -X- _ O
and -X- _ O
without -X- _ O
explanation -X- _ O
. -X- _ O

Three -X- _ O
explanations -X- _ O
are -X- _ O
possible -X- _ O
: -X- _ O
Useful -X- _ O
explanations -X- _ O
: -X- _ O
these -X- _ O
are -X- _ O
explanations -X- _ O
such -X- _ O
thatCis -X- _ O
correct -X- _ O
and -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
; -X- _ O
e -X- _ O
) -X- _ O
> -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
) -X- _ O
orCis -X- _ O
not -X- _ O
correct -X- _ O
and -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
; -X- _ O
e -X- _ O
) -X- _ O
< -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
) -X- _ O

Useless -X- _ O
explanations -X- _ O
: -X- _ O
they -X- _ O
are -X- _ O
explanations -X- _ O
such -X- _ O
thatP -X- _ O
( -X- _ O
Cjs -X- _ O
; -X- _ O
e -X- _ O
) -X- _ O
= -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
) -X- _ O
Misleading -X- _ O
explanations -X- _ O
: -X- _ O
they -X- _ O
are -X- _ O
explanations -X- _ O
such -X- _ O
that -X- _ O
Cis -X- _ O
correct -X- _ O
and -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
; -X- _ O
e -X- _ O
) -X- _ O
< -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
) -X- _ O
orCis -X- _ O
not -X- _ O
correct -X- _ O
and -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
; -X- _ O
e -X- _ O
) -X- _ O
> -X- _ O

P -X- _ O
( -X- _ O
Cjs -X- _ O
) -X- _ O
The -X- _ O
core -X- _ O
idea -X- _ O
is -X- _ O
that -X- _ O
semantically -X- _ O
coherent -X- _ O
and -X- _ O
exhaustive -X- _ O
explanations -X- _ O
must -X- _ O
indicate -X- _ O
correct -X- _ O
classiﬁcations -X- _ O
whereas -X- _ O
incoherent -X- _ O
or -X- _ O
non -X- _ O
- -X- _ O
existent -X- _ O
explanations -X- _ O
must -X- _ O
hint -X- _ O
towards -X- _ O
wrong -X- _ O
classiﬁcations -X- _ O
. -X- _ O

Given -X- _ O
the -X- _ O
above -X- _ O
probabilities -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
measure -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
an -X- _ O
explanation -X- _ O
by -X- _ O
computing -X- _ O
the -X- _ O
achieved -X- _ O
Information -X- _ O
Gain -X- _ O
( -X- _ O
Kononenko -X- _ O
and -X- _ O
Bratko -X- _ O
, -X- _ O
1991 -X- _ O
) -X- _ O
: -X- _ O
the -X- _ O
posterior -X- _ O
probability -X- _ O
is -X- _ O
expected -X- _ O
to -X- _ O
grow -X- _ O
w.r.t -X- _ O
. -X- _ O

to -X- _ O
the -X- _ O
prior -X- _ O
one -X- _ O
for -X- _ O
correct -X- _ O
decisions -X- _ O
when -X- _ O
a -X- _ O
good -X- _ O
explanation -X- _ O
is -X- _ O
available -X- _ O
against -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
, -X- _ O
while -X- _ O
decreasing -X- _ O
for -X- _ O
bad -X- _ O
or -X- _ O
confusing -X- _ O
explanations -X- _ O
. -X- _ O

The -X- _ O
intuition -X- _ O
behind -X- _ O
Information -X- _ O
Gain -X- _ O
is -X- _ O
that -X- _ O
it -X- _ O
measures -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
information -X- _ O
( -X- _ O
provided -X- _ O
in -X- _ O
number -X- _ O
of -X- _ O
bits -X- _ O
) -X- _ O
gained -X- _ O
by -X- _ O
the -X- _ O
explanation -X- _ O
about -X- _ O
the -X- _ O
user -X- _ O
decision -X- _ O
of -X- _ O
accepting -X- _ O
the -X- _ O
system -X- _ O
classiﬁcation -X- _ O
on -X- _ O
an -X- _ O
incoming -X- _ O
sentence -X- _ O
s -X- _ O
. -X- _ O

A -X- _ O
positive -X- _ O
gain -X- _ O
indicates -X- _ O
that -X- _ O
the -X- _ O
probability -X- _ O
ampliﬁes -X- _ O
towards -X- _ O
the -X- _ O
right -X- _ O
decisions -X- _ O
, -X- _ O
and -X- _ O
declines -X- _ O
with -X- _ O
errors -X- _ O
. -X- _ O

We -X- _ O
will -X- _ O
let -X- _ O
users -X- _ O
to -X- _ O
judge -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
explanation -X- _ O
and -X- _ O
assign -X- _ O
them -X- _ O
a -X- _ O
posterior -X- _ O
probability -X- _ O
that -X- _ O
increases -X- _ O
along -X- _ O
with -X- _ O
better -X- _ O
judgments -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
way -X- _ O
we -X- _ O
have -X- _ O
a -X- _ O
measure -X- _ O
of -X- _ O
how -X- _ O
convincing -X- _ O
the -X- _ O
system -X- _ O
is -X- _ O
about -X- _ O
its -X- _ O
decisions -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
how -X- _ O
weak -X- _ O
it -X- _ O
is -X- _ O
to -X- _ O
clarify -X- _ O
erroneous -X- _ O
cases -X- _ O
. -X- _ O

To -X- _ O
compare -X- _ O
the -X- _ O
overall -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
different -X- _ O
explanatory -X- _ O
models -X- _ O
M -X- _ O
, -X- _ O
the -X- _ O
Information -X- _ B-MetricName
Gain -X- _ I-MetricName
is -X- _ O
measured -X- _ O
against -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
explanations -X- _ O
generated -X- _ O
by -X- _ O
Mand -X- _ O
then -X- _ O
normalized -X- _ O
throughout -X- _ O
the -X- _ O
collection -X- _ O
’s -X- _ O
entropy -X- _ O
Eas -X- _ O
follows -X- _ O
: -X- _ O

Ir=1 -X- _ O
E1 -X- _ O
jTsjjTsjX -X- _ O
whereTsis -X- _ O
the -X- _ O
explanations -X- _ O
collection -X- _ O
and -X- _ O
I -X- _ O
( -X- _ O
j -X- _ O
) -X- _ O
is -X- _ O
the -X- _ O
Information -X- _ O
Gain -X- _ O
of -X- _ O
explanation -X- _ O
j. -X- _ O
6 -X- _ O

Experimental -X- _ O
Evaluation -X- _ O

The -X- _ O
effectiveness -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
approach -X- _ O
has -X- _ O
been -X- _ O
measured -X- _ O
against -X- _ O
two -X- _ O
different -X- _ O
semantic -X- _ O
processing -X- _ O
tasks -X- _ O
, -X- _ O
i.e. -X- _ O
question -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
and -X- _ O
argument -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
in -X- _ O
semantic -X- _ O
role -X- _ O
labeling -X- _ O
. -X- _ O

The -X- _ O
Nystrom -X- _ O
projection -X- _ O
has -X- _ O
been -X- _ O
implemented -X- _ O
in -X- _ O
the -X- _ O
KeLP -X- _ O
framework -X- _ O
( -X- _ O
Filice -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
1 -X- _ O
, -X- _ O
the -X- _ O
neural -X- _ O
network -X- _ O
and -X- _ O
LRP -X- _ B-MethodName
have -X- _ O
been -X- _ O
implemented -X- _ O
in -X- _ O
Tensorﬂow2 -X- _ O
, -X- _ O
with -X- _ O
1 -X- _ O
and -X- _ O
2 -X- _ O
hidden -X- _ O
layers -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
whose -X- _ O
dimensionality -X- _ O
corresponds -X- _ O
to -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
involved -X- _ O
Nystrom -X- _ O
landmarks -X- _ O
( -X- _ O
500 -X- _ O
and -X- _ O
200 -X- _ O
, -X- _ O
re1http -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
www.kelp-ml.org -X- _ O
2https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
www.tensorﬂow.org -X- _ O

22Category -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
; -X- _ O
e -X- _ O
) -X- _ O
1 P -X- _ O
( -X- _ O
Cjs -X- _ O
; -X- _ O
e -X- _ O
) -X- _ O

Table -X- _ O
1 -X- _ O
: -X- _ O
Posterior -X- _ O
probabilities -X- _ O
w.r.t -X- _ O
. -X- _ O

quality -X- _ O
categories -X- _ O
Class -X- _ O
Incoher -X- _ O
.Bad -X- _ O

Weak -X- _ O
Good -X- _ O
V -X- _ O
.Good -X- _ O

Table -X- _ O
2 -X- _ O
: -X- _ O
Weights -X- _ O
for -X- _ O
the -X- _ O
Cohen -X- _ B-MetricName
’s -X- _ I-MetricName
Kappa -X- _ I-MetricName
wstatistics -X- _ O
spectively -X- _ O
, -X- _ O
randomly -X- _ O
selected3 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
adoption -X- _ O
of -X- _ O
dropout -X- _ O
regularization -X- _ O
in -X- _ O
hidden -X- _ O
and -X- _ O
ﬁnal -X- _ O
layers -X- _ O
. -X- _ O

For -X- _ O
both -X- _ O
tasks -X- _ O
, -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
have -X- _ O
been -X- _ O
optimized -X- _ O
via -X- _ O
grid -X- _ O
- -X- _ O
search -X- _ O
. -X- _ O

The -X- _ O
Adam -X- _ O
optimizer -X- _ O
has -X- _ O
been -X- _ O
applied -X- _ O
to -X- _ O
minimize -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
entropy -X- _ O
loss -X- _ O
function -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
epoch -X- _ O
( -X- _ O
500 -X- _ O
) -X- _ O
training -X- _ O
, -X- _ O
each -X- _ O
fed -X- _ O
with -X- _ O
batches -X- _ O
of -X- _ O
size -X- _ O
256 -X- _ O
. -X- _ O

We -X- _ O
adopted -X- _ O
an -X- _ O
early -X- _ O
stop -X- _ O
strategy -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
best -X- _ O
model -X- _ O
was -X- _ O
selected -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
performance -X- _ O
over -X- _ O
the -X- _ O
development -X- _ O
set -X- _ O
. -X- _ O

For -X- _ O
evaluating -X- _ O
our -X- _ O
explanation -X- _ O
method -X- _ O
, -X- _ O
we -X- _ O
deﬁned -X- _ O
ﬁve -X- _ O
quality -X- _ O
categories -X- _ O
and -X- _ O
associated -X- _ O
them -X- _ O
to -X- _ O
values -X- _ O
for -X- _ O
the -X- _ O
posteriori -X- _ O
probability -X- _ O
P -X- _ O
( -X- _ O
Cjs -X- _ O
; -X- _ O
e -X- _ O
) -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
. -X- _ O

We -X- _ O
gathered -X- _ O
into -X- _ O
explanation -X- _ O
datasets -X- _ O
hundreds -X- _ O
of -X- _ O
explanations -X- _ O
from -X- _ O
the -X- _ O
three -X- _ O
models -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
and -X- _ O
presented -X- _ O
them -X- _ O
to -X- _ O
a -X- _ O
pool -X- _ O
of -X- _ O
annotators -X- _ O
( -X- _ O
further -X- _ O
details -X- _ O
in -X- _ O
related -X- _ O
subsections -X- _ O
) -X- _ O
for -X- _ O
independent -X- _ O
labeling -X- _ O
; -X- _ O
annotators -X- _ O
had -X- _ O
no -X- _ O
information -X- _ O
of -X- _ O
the -X- _ O
correctness -X- _ O
of -X- _ O
the -X- _ O
system -X- _ O
emissions -X- _ O
but -X- _ O
just -X- _ O
knowledge -X- _ O
about -X- _ O
the -X- _ O
dataset -X- _ O
entropy -X- _ O
. -X- _ O

We -X- _ O
addressed -X- _ O
their -X- _ O
consensus -X- _ O
by -X- _ O
measuring -X- _ O
a -X- _ O
weighted -X- _ B-MetricName
Cohen -X- _ I-MetricName
’s -X- _ I-MetricName
Kappa -X- _ I-MetricName
. -X- _ O

6.1 -X- _ O
Question -X- _ B-TaskName
Classiﬁcation -X- _ I-TaskName
In -X- _ O
our -X- _ O
ﬁrst -X- _ O
evaluation -X- _ O
, -X- _ O
we -X- _ O
replicated -X- _ O
the -X- _ O
experiments -X- _ O
reported -X- _ O
by -X- _ O
( -X- _ O
Croce -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
the -X- _ O
question -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
task -X- _ O
. -X- _ O

We -X- _ O
thus -X- _ O
used -X- _ O
the -X- _ O
UIUC -X- _ B-DatasetName
dataset -X- _ O
( -X- _ O
Li -X- _ O
and -X- _ O
Roth -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
, -X- _ O
including -X- _ O
a -X- _ O
training -X- _ O
and -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
5452 -X- _ O
and500questions -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
organized -X- _ O
in -X- _ O
6 -X- _ O
coarse -X- _ O
- -X- _ O
grained -X- _ O
classes -X- _ O
( -X- _ O
asENTITY -X- _ O
orHUMAN -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
generated -X- _ O
Nystrom -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
Compositionally -X- _ O
Smoothed -X- _ O
Partial -X- _ O
Tree -X- _ O
Kernel -X- _ O
( -X- _ O
Annesi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
function -X- _ O
with -X- _ O
default -X- _ O
parameters -X- _ O
== -X- _ O
0:4 -X- _ O
. -X- _ O

Using -X- _ O
500 -X- _ O
marks -X- _ O
but -X- _ O
statistically -X- _ O
signiﬁcant -X- _ O
results -X- _ O
have -X- _ O
not -X- _ O
been -X- _ O
measured -X- _ O
( -X- _ O
not -X- _ O
reported -X- _ O
here -X- _ O
due -X- _ O
to -X- _ O
space -X- _ O
limitations -X- _ O
) -X- _ O
.QC -X- _ B-TaskName
SRL -X- _ B-TaskName
- -X- _ O
AC -X- _ B-TaskName
Multiplicative -X- _ O
0.514 -X- _ O
0.662 -X- _ O
Table -X- _ O
3 -X- _ O
: -X- _ O
Information -X- _ B-HyperparameterName
gains -X- _ I-HyperparameterName
for -X- _ O
the -X- _ O
three -X- _ O
Explanatory -X- _ O
Models -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
SRL -X- _ B-TaskName
- -X- _ O
AC -X- _ B-TaskName
and -X- _ O
QC -X- _ B-TaskName
datasets -X- _ O
. -X- _ O

kwis -X- _ O
the -X- _ O
weighted -X- _ O
Cohen -X- _ O
’s -X- _ O
Kappa -X- _ O
w -X- _ O
. -X- _ O
landmarks -X- _ O
, -X- _ O
the -X- _ O
KDA -X- _ B-MethodName
accuracy -X- _ B-MetricName
was -X- _ O
92:6 -X- _ B-MetricValue
% -X- _ I-MetricValue
. -X- _ O

A -X- _ O
group -X- _ O
of -X- _ O
3 -X- _ O
annotators -X- _ O
evaluated -X- _ O
an -X- _ O
explanation -X- _ O
dataset -X- _ O
of -X- _ O
300 -X- _ O
explanations -X- _ O
( -X- _ O
perfectly -X- _ O
balanced -X- _ O
between -X- _ O
correct -X- _ O
and -X- _ O
not -X- _ O
correct -X- _ O
classiﬁcation -X- _ O
) -X- _ O
, -X- _ O
composed -X- _ O
of -X- _ O
100 -X- _ O
explanations -X- _ O
for -X- _ O
each -X- _ O
model -X- _ O
. -X- _ O

Performances -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
. -X- _ O

All -X- _ O
three -X- _ O
explanatory -X- _ O
models -X- _ O
were -X- _ O
able -X- _ O
to -X- _ O
gain -X- _ O
more -X- _ O
than -X- _ O
half -X- _ O
the -X- _ O
required -X- _ O
information -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
ascertain -X- _ O
the -X- _ O
correctness -X- _ O
of -X- _ O
the -X- _ O
classiﬁcation -X- _ O
. -X- _ O

Consider -X- _ O
: -X- _ O
I -X- _ O
think -X- _ O
” -X- _ O
What -X- _ O
year -X- _ O
did -X- _ O
Oklahoma -X- _ O
become -X- _ O
a -X- _ O
state -X- _ O
? -X- _ O
” -X- _ O

refers -X- _ O
to -X- _ O
aNUMBER -X- _ O
since -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
The -X- _ O
ﬁlm -X- _ O
Jaws -X- _ O
was -X- _ O
made -X- _ O
in -X- _ O
what -X- _ O
year -X- _ O
? -X- _ O
” -X- _ O

The -X- _ O
model -X- _ O
provided -X- _ O
an -X- _ O
evidently -X- _ O
coherent -X- _ O
analogy -X- _ O
, -X- _ O
but -X- _ O
this -X- _ O
is -X- _ O
a -X- _ O
easy -X- _ O
case -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
occurrence -X- _ O
in -X- _ O
both -X- _ O
questions -X- _ O
of -X- _ O
very -X- _ O
discriminative -X- _ O
words -X- _ O
, -X- _ O
i.e -X- _ O
” -X- _ O
what -X- _ O
year -X- _ O
” -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
system -X- _ O
is -X- _ O
also -X- _ O
able -X- _ O
to -X- _ O
capture -X- _ O
semantic -X- _ O
similarities -X- _ O
when -X- _ O
both -X- _ O
syntactic -X- _ O
and -X- _ O
lexical -X- _ O
features -X- _ O
are -X- _ O
different -X- _ O
. -X- _ O

E.g. -X- _ O
: -X- _ O
I -X- _ O
think -X- _ O
” -X- _ O
Where -X- _ O
is -X- _ O
the -X- _ O
Mall -X- _ O
of -X- _ O
the -X- _ O
America -X- _ O
? -X- _ O
” -X- _ O
refers -X- _ O
to -X- _ O
a -X- _ O
LOCATION -X- _ O
since -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
What -X- _ O
town -X- _ O
was -X- _ O
the -X- _ O
setting -X- _ O
for -X- _ O
The -X- _ O
Music -X- _ O
Man -X- _ O
? -X- _ O
” -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
an -X- _ O
high -X- _ O
- -X- _ O
quality -X- _ O
explanation -X- _ O
since -X- _ O
the -X- _ O
system -X- _ O
provided -X- _ O
an -X- _ O
analogy -X- _ O
with -X- _ O
a -X- _ O
landmark -X- _ O
requesting -X- _ O
the -X- _ O
same -X- _ O
ﬁne -X- _ O
- -X- _ O
grained -X- _ O
category -X- _ O
but -X- _ O
with -X- _ O
little -X- _ O
sharing -X- _ O
of -X- _ O
lexical -X- _ O
and -X- _ O
syntactic -X- _ O
information -X- _ O
( -X- _ O
note -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
the -X- _ O
absence -X- _ O
in -X- _ O
the -X- _ O
landmark -X- _ O
of -X- _ O
the -X- _ O
very -X- _ O
discriminative -X- _ O
word -X- _ O
” -X- _ O
where -X- _ O
” -X- _ O
) -X- _ O
. -X- _ O

Let -X- _ O
us -X- _ O
now -X- _ O
consider -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
wrong -X- _ O
classiﬁcations -X- _ O
: -X- _ O
I -X- _ O
think -X- _ O
” -X- _ O
Mexican -X- _ O
pesos -X- _ O
are -X- _ O
worth -X- _ O
what -X- _ O
in -X- _ O
U.S. -X- _ O
dollars -X- _ O
? -X- _ O
” -X- _ O
refers -X- _ O
to -X- _ O
a -X- _ O
DESCRIPTION -X- _ O
since -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
What -X- _ O
is -X- _ O
the -X- _ O
Bernoulli -X- _ O
Principle -X- _ O
? -X- _ O
” -X- _ O

The -X- _ O
system -X- _ O
provided -X- _ O
an -X- _ O
explanation -X- _ O
that -X- _ O
is -X- _ O
not -X- _ O
possible -X- _ O
to -X- _ O
easily -X- _ O
interpret -X- _ O
: -X- _ O
indeed -X- _ O
it -X- _ O
was -X- _ O
labeled -X- _ O
as -X- _ O
[ -X- _ O
Incoherent -X- _ O
] -X- _ O
by -X- _ O
all -X- _ O
the -X- _ O
annotators -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
system -X- _ O
effectiveness -X- _ O
is -X- _ O
limited -X- _ O
in -X- _ O
case -X- _ O
of -X- _ O
negative -X- _ O
modality -X- _ O
for -X- _ O
correct -X- _ O
classiﬁcations -X- _ O
. -X- _ O

In -X- _ O
these -X- _ O
cases -X- _ O
explanations -X- _ O
, -X- _ O
albeit -X- _ O
coherent -X- _ O
, -X- _ O
can -X- _ O
be -X- _ O
trivial -X- _ O
and -X- _ O
do -X- _ O
not -X- _ O
actually -X- _ O
help -X- _ O
in -X- _ O
reducing -X- _ O
uncertainty -X- _ O
about -X- _ O
the -X- _ O
correct -X- _ O
target -X- _ O
class -X- _ O
. -X- _ O

The -X- _ O
explanation -X- _ O

23I -X- _ O
think -X- _ O
” -X- _ O
What -X- _ O
is -X- _ O
angiotensin -X- _ O
? -X- _ O
” -X- _ O
does -X- _ O
not -X- _ O
refer -X- _ O
to -X- _ O
a -X- _ O
NUM -X- _ O
since -X- _ O
different -X- _ O
from -X- _ O
” -X- _ O
What -X- _ O
was -X- _ O
Einstein -X- _ O
’s -X- _ O
IQ -X- _ O
? -X- _ O
” -X- _ O
. -X- _ O

is -X- _ O
correct -X- _ O
but -X- _ O
obvious -X- _ O
. -X- _ O

As -X- _ O
an -X- _ O
alternative -X- _ O
, -X- _ O
a -X- _ O
negative -X- _ O
analogy -X- _ O
with -X- _ O
a -X- _ O
very -X- _ O
likely -X- _ O
class -X- _ O
, -X- _ O
i.e. -X- _ O
ENTITY -X- _ O
orDESCRIPTION -X- _ O
, -X- _ O
would -X- _ O
have -X- _ O
provided -X- _ O
more -X- _ O
useful -X- _ O
information -X- _ O
for -X- _ O
disambiguation -X- _ O
. -X- _ O

A -X- _ O
second -X- _ O
challenge -X- _ O
is -X- _ O
represented -X- _ O
by -X- _ O
inherently -X- _ O
ambiguous -X- _ O
questions -X- _ O
. -X- _ O

The -X- _ O
following -X- _ O
explanation -X- _ O
I -X- _ O
think -X- _ O
” -X- _ O
What -X- _ O
is -X- _ O
the -X- _ O
sales -X- _ O
tax -X- _ O
in -X- _ O
Minnesota -X- _ O
? -X- _ O
” -X- _ O
refers -X- _ O
to -X- _ O
a -X- _ O
NUMBER -X- _ O
since -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
What -X- _ O
is -X- _ O
the -X- _ O
population -X- _ O
of -X- _ O
Mozambique -X- _ O
? -X- _ O
” -X- _ O
and -X- _ O
does -X- _ O
not -X- _ O
refer -X- _ O
to -X- _ O
a -X- _ O
ENTITY -X- _ O
since -X- _ O
different -X- _ O
from -X- _ O
” -X- _ O
What -X- _ O
is -X- _ O
a -X- _ O
fear -X- _ O
of -X- _ O
slime -X- _ O
? -X- _ O
” -X- _ O
. -X- _ O

tells -X- _ O
why -X- _ O
NUMBER -X- _ O
is -X- _ O
a -X- _ O
more -X- _ O
likely -X- _ O
class -X- _ O
than -X- _ O
ENTITY -X- _ O
. -X- _ O

Although -X- _ O
seemingly -X- _ O
correct -X- _ O
, -X- _ O
this -X- _ O
is -X- _ O
a -X- _ O
mistake -X- _ O
, -X- _ O
as -X- _ O
ENTITY -X- _ O
is -X- _ O
the -X- _ O
proper -X- _ O
decision -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
explanation -X- _ O
is -X- _ O
perfectly -X- _ O
ﬁne -X- _ O
, -X- _ O
as -X- _ O
it -X- _ O
well -X- _ O
expresses -X- _ O
the -X- _ O
decision -X- _ O
’s -X- _ O
rationale -X- _ O
: -X- _ O
lack -X- _ O
of -X- _ O
contextual -X- _ O
information -X- _ O
in -X- _ O
the -X- _ O
question -X- _ O
is -X- _ O
here -X- _ O
the -X- _ O
main -X- _ O
cause -X- _ O
of -X- _ O
the -X- _ O
error -X- _ O
. -X- _ O

6.2 -X- _ O
Argument -X- _ O
Classiﬁcation -X- _ O
Semantic -X- _ O
role -X- _ O
labeling -X- _ O
( -X- _ O
SRL -X- _ B-TaskName
( -X- _ O
Palmer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2010 -X- _ O
) -X- _ O
) -X- _ O
consists -X- _ O
in -X- _ O
detecting -X- _ O
the -X- _ O
semantic -X- _ O
arguments -X- _ O
associated -X- _ O
with -X- _ O
the -X- _ O
predicate -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
and -X- _ O
their -X- _ O
classiﬁcation -X- _ O
into -X- _ O
their -X- _ O
speciﬁc -X- _ O
roles -X- _ O
( -X- _ O
Fillmore -X- _ O
( -X- _ O
1985 -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
given -X- _ O
the -X- _ O
sentence -X- _ O
” -X- _ O
Bring -X- _ O
the -X- _ O
fruit -X- _ O
onto -X- _ O
the -X- _ O
dining -X- _ O
table -X- _ O
” -X- _ O
, -X- _ O
the -X- _ O
task -X- _ O
would -X- _ O
be -X- _ O
to -X- _ O
recognize -X- _ O
the -X- _ O
verb -X- _ O
” -X- _ O
bring -X- _ O
” -X- _ O
as -X- _ O
evoking -X- _ O
the -X- _ O
BRINGING -X- _ O
frame -X- _ O
, -X- _ O
with -X- _ O
its -X- _ O
roles -X- _ O
, -X- _ O
T -X- _ O
HEME -X- _ O
for -X- _ O
” -X- _ O
the -X- _ O
fruit -X- _ O
” -X- _ O
and -X- _ O
G -X- _ O
OAL -X- _ O
for -X- _ O
” -X- _ O
onto -X- _ O
the -X- _ O
dining -X- _ O
table -X- _ O
” -X- _ O
. -X- _ O

Argument -X- _ O
classiﬁcation -X- _ O
corresponds -X- _ O
to -X- _ O
the -X- _ O
subtask -X- _ O
of -X- _ O
assigning -X- _ O
labels -X- _ O
to -X- _ O
the -X- _ O
sentence -X- _ O
fragments -X- _ O
spanning -X- _ O
individual -X- _ O
roles -X- _ O
. -X- _ O

As -X- _ O
proposed -X- _ O
in -X- _ O
( -X- _ O
Moschitti -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
, -X- _ O
SRL -X- _ B-TaskName
can -X- _ O
be -X- _ O
modeled -X- _ O
as -X- _ O
a -X- _ O
multi -X- _ O
classiﬁcation -X- _ O
task -X- _ O
over -X- _ O
each -X- _ O
parse -X- _ O
tree -X- _ O
node -X- _ O
n -X- _ O
, -X- _ O
where -X- _ O
argument -X- _ O
spans -X- _ O
reﬂect -X- _ O
sub -X- _ O
- -X- _ O
sentences -X- _ O
covered -X- _ O
by -X- _ O
the -X- _ O
tree -X- _ O
rooted -X- _ O
at -X- _ O
n. -X- _ O
Consistently -X- _ O
with -X- _ O
( -X- _ O
Croce -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
, -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
the -X- _ O
KDA -X- _ B-MethodName
has -X- _ O
been -X- _ O
empowered -X- _ O
with -X- _ O
a -X- _ O
Smoothed -X- _ O
Partial -X- _ O
Tree -X- _ O
Kernel -X- _ O
, -X- _ O
operating -X- _ O
over -X- _ O
Grammatical -X- _ O
Relation -X- _ O
Centered -X- _ O
Tree -X- _ O
( -X- _ O
GRCT -X- _ O
) -X- _ O
derived -X- _ O
from -X- _ O
dependency -X- _ O
grammar -X- _ O
. -X- _ O

We -X- _ O
used -X- _ O
the -X- _ O
HuRIC -X- _ B-DatasetName
dataset -X- _ O
( -X- _ O
Bastianelli -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
including -X- _ O
over -X- _ O
650 -X- _ O
annotated -X- _ O
transcriptions -X- _ O
of -X- _ O
spoken -X- _ O
robotic -X- _ O
commands -X- _ O
, -X- _ O
organized -X- _ O
in -X- _ O
18frames -X- _ O
and -X- _ O
about -X- _ O
60arguments4 -X- _ O
. -X- _ O

We -X- _ O
extracted -X- _ O
single -X- _ O
arguments -X- _ O
from -X- _ O
each -X- _ O
HuRIC -X- _ B-DatasetName
example -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
total -X- _ O
of -X- _ O
1 -X- _ O
; -X- _ O
300instances -X- _ O
. -X- _ O

We -X- _ O
run -X- _ O
experiments -X- _ O
with -X- _ O
a -X- _ O
methodology -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
one -X- _ O
described -X- _ O
in -X- _ O
Sec -X- _ O
4http -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
sag.art.uniroma2.it -X- _ O
/ -X- _ O
lu4r.html6.1 -X- _ O
, -X- _ O
but -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
limited -X- _ O
data -X- _ O
size -X- _ O
we -X- _ O
performed -X- _ O
extensive -X- _ O
10 -X- _ B-HyperparameterValue
- -X- _ O
fold -X- _ B-HyperparameterName
cross -X- _ I-HyperparameterName
- -X- _ I-HyperparameterName
validation -X- _ I-HyperparameterName
, -X- _ O
optimizing -X- _ O
network -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
via -X- _ O
grid -X- _ O
- -X- _ O
search -X- _ O
for -X- _ O
each -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

We -X- _ O
generated -X- _ O
Nystrom -X- _ O
representation -X- _ O
of -X- _ O
a -X- _ O
equally -X- _ O
- -X- _ O
weighted -X- _ O
linear -X- _ O
combination -X- _ O
of -X- _ O
SPTK -X- _ O
function -X- _ O
with -X- _ O
default -X- _ O
parameters -X- _ O
== -X- _ O
0:4and -X- _ O
of -X- _ O
linear -X- _ O
kernel -X- _ O
function -X- _ O
applied -X- _ O
to -X- _ O
sparse -X- _ O
vector -X- _ O
representing -X- _ O
the -X- _ O
instance -X- _ O
frame -X- _ O
. -X- _ O

With -X- _ O
these -X- _ O
settings -X- _ O
, -X- _ O
the -X- _ O
KDA -X- _ B-MethodName
accuracy -X- _ O
was -X- _ O
96:1 -X- _ O
% -X- _ O
. -X- _ O

We -X- _ O
sampled -X- _ O
692 -X- _ O
explanations -X- _ O
almost -X- _ O
equally -X- _ O
distributed -X- _ O
among -X- _ O
the -X- _ O
3 -X- _ O
explanatory -X- _ O
models -X- _ O
. -X- _ O

Two -X- _ O
annotators -X- _ O
were -X- _ O
involved -X- _ O
. -X- _ O

Results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Tab -X- _ O
3 -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
task -X- _ O
, -X- _ O
all -X- _ O
models -X- _ O
were -X- _ O
able -X- _ O
to -X- _ O
gain -X- _ O
more -X- _ O
than -X- _ O
two -X- _ O
thirds -X- _ O
of -X- _ O
needed -X- _ O
information -X- _ O
. -X- _ O

The -X- _ O
alike -X- _ O
scores -X- _ O
of -X- _ O
the -X- _ O
three -X- _ O
models -X- _ O
are -X- _ O
probably -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
narrow -X- _ O
linguistic -X- _ O
domain -X- _ O
of -X- _ O
the -X- _ O
corpus -X- _ O
and -X- _ O
the -X- _ O
well -X- _ O
- -X- _ O
deﬁned -X- _ O
semantic -X- _ O
boundaries -X- _ O
between -X- _ O
the -X- _ O
arguments -X- _ O
. -X- _ O

To -X- _ O
show -X- _ O
the -X- _ O
capability -X- _ O
of -X- _ O
such -X- _ O
models -X- _ O
, -X- _ O
let -X- _ O
us -X- _ O
consider -X- _ O
: -X- _ O
I -X- _ O
think -X- _ O
” -X- _ O
the -X- _ O
washer -X- _ O
” -X- _ O
is -X- _ O
the -X- _ O
CONTAINING -X- _ O
OBJECT -X- _ O
of -X- _ O
CLOSURE -X- _ O
in -X- _ O
” -X- _ O
Robot -X- _ O
can -X- _ O
you -X- _ O
OPEN -X- _ O
the -X- _ O
washer -X- _ O
? -X- _ O
” -X- _ O

since -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
the -X- _ O
jar -X- _ O
” -X- _ O
in -X- _ O
” -X- _ O
CLOSE -X- _ O
the -X- _ O
jar -X- _ O
” -X- _ O
and -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
the -X- _ O
THEME -X- _ O
ofBRINGING -X- _ O
since -X- _ O
different -X- _ O
from -X- _ O
” -X- _ O
the -X- _ O
jar -X- _ O
” -X- _ O
in -X- _ O
” -X- _ O
TAKE -X- _ O
the -X- _ O
jar -X- _ O
to -X- _ O
the -X- _ O
table -X- _ O
of -X- _ O
the -X- _ O
kitchen -X- _ O
” -X- _ O
. -X- _ O

I -X- _ O
think -X- _ O
” -X- _ O
me -X- _ O
” -X- _ O
is -X- _ O
the -X- _ O
BENEFICIARY -X- _ O
ofBRINGING -X- _ O
in -X- _ O
” -X- _ O
I -X- _ O
would -X- _ O
like -X- _ O
some -X- _ O
cutlery -X- _ O
can -X- _ O
you -X- _ O
GET -X- _ O
mesome -X- _ O
? -X- _ O
” -X- _ O
since -X- _ O
similar -X- _ O
to -X- _ O
” -X- _ O
me -X- _ O
” -X- _ O
in -X- _ O
” -X- _ O
BRING -X- _ O
mea -X- _ O
fork -X- _ O
from -X- _ O
the -X- _ O
press -X- _ O
. -X- _ O
” -X- _ O

and -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
the -X- _ O
COTHEME -X- _ O
ofCOTHEME -X- _ O
since -X- _ O
different -X- _ O
from -X- _ O
” -X- _ O
me -X- _ O
” -X- _ O
in -X- _ O
” -X- _ O
Would -X- _ O
you -X- _ O
please -X- _ O
FOLLOW -X- _ O
meto -X- _ O
the -X- _ O
kitchen -X- _ O
? -X- _ O
” -X- _ O
. -X- _ O

The -X- _ O
above -X- _ O
commands -X- _ O
have -X- _ O
very -X- _ O
limited -X- _ O
lexical -X- _ O
overlap -X- _ O
with -X- _ O
retrieved -X- _ O
landmarks -X- _ O
. -X- _ O

Nevertheless -X- _ O
, -X- _ O
the -X- _ O
analogies -X- _ O
make -X- _ O
explanations -X- _ O
quite -X- _ O
effective -X- _ O
: -X- _ O
explanatory -X- _ O
models -X- _ O
seems -X- _ O
to -X- _ O
successfully -X- _ O
capture -X- _ O
semantic -X- _ O
and -X- _ O
syntactic -X- _ O
relations -X- _ O
among -X- _ O
input -X- _ O
instances -X- _ O
and -X- _ O
closely -X- _ O
related -X- _ O
landmarks -X- _ O
. -X- _ O

7 -X- _ O
Conclusion -X- _ O
This -X- _ O
paper -X- _ O
investigated -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
a -X- _ O
novel -X- _ O
method -X- _ O
to -X- _ O
generate -X- _ O
epistemologically -X- _ O
transparent -X- _ O
and -X- _ O
linguistically -X- _ O
ﬂuid -X- _ O
explanations -X- _ O
for -X- _ O
a -X- _ O
neural -X- _ O
predictor -X- _ O
emissions -X- _ O
. -X- _ O

The -X- _ O
proposed -X- _ O
approach -X- _ O
applies -X- _ O
LRP -X- _ B-MethodName
to -X- _ O
a -X- _ O
KDA -X- _ B-MethodName
to -X- _ O
backpropagate -X- _ O
and -X- _ O
redistribute -X- _ O
the -X- _ O
prediction -X- _ O
to -X- _ O
input -X- _ O
entries -X- _ O
. -X- _ O

It -X- _ O
then -X- _ O
produces -X- _ O
a -X- _ O
sentence -X- _ O
exploiting -X- _ O
analogies -X- _ O
with -X- _ O
landmarks -X- _ O
, -X- _ O
according -X- _ O
to -X- _ O
different -X- _ O
explanatory -X- _ O
models -X- _ O
. -X- _ O

Moreover -X- _ O
a -X- _ O
novel -X- _ O
evaluation -X- _ O
methodology -X- _ O
based -X- _ O
on -X- _ O
Information -X- _ O
Theory -X- _ O
is -X- _ O
provided -X- _ O
. -X- _ O

Empirical -X- _ O
investigations -X- _ O
carried -X- _ O
out -X- _ O
against -X- _ O
the -X- _ O
QC -X- _ B-TaskName
and -X- _ O
AC -X- _ B-TaskName
tasks -X- _ O
conﬁrm -X- _ O
that -X- _ O
the -X- _ O
explanatory -X- _ O
models -X- _ O
contribute -X- _ O
to -X- _ O
increase -X- _ O
the -X- _ O
user -X- _ O
conﬁdence -X- _ O
in -X- _ O
the -X- _ O
machine -X- _ O
correct -X- _ O
responses -X- _ O
. -X- _ O

24References -X- _ O
Paolo -X- _ O
Annesi -X- _ O
, -X- _ O
Danilo -X- _ O
Croce -X- _ O
, -X- _ O
and -X- _ O
Roberto -X- _ O
Basili -X- _ O
. -X- _ O

2014 -X- _ O
. -X- _ O

Semantic -X- _ O
compositionality -X- _ O
in -X- _ O
tree -X- _ O
kernels -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
CIKM -X- _ O
2014 -X- _ O
. -X- _ O

ACM -X- _ O
. -X- _ O

Sebastian -X- _ O
Bach -X- _ O
, -X- _ O
Alexander -X- _ O
Binder -X- _ O
, -X- _ O
Gregoire -X- _ O
Montavon -X- _ O
, -X- _ O
Frederick -X- _ O
Klauschen -X- _ O
, -X- _ O
Klaus -X- _ O
- -X- _ O
Robert -X- _ O
Mller -X- _ O
, -X- _ O
and -X- _ O
Wojciech -X- _ O
Samek -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

On -X- _ O
pixel -X- _ O
- -X- _ O
wise -X- _ O
explanations -X- _ O
for -X- _ O
non -X- _ O
- -X- _ O
linear -X- _ O
classiﬁer -X- _ O
decisions -X- _ O
by -X- _ O
layer -X- _ O
- -X- _ O
wise -X- _ O
relevance -X- _ O
propagation -X- _ O
. -X- _ O

PLOS -X- _ O
ONE -X- _ O
, -X- _ O
10 -X- _ O
( -X- _ O
7 -X- _ O
) -X- _ O
. -X- _ O

David -X- _ O
Baehrens -X- _ O
, -X- _ O
Timon -X- _ O
Schroeter -X- _ O
, -X- _ O
Stefan -X- _ O
Harmeling -X- _ O
, -X- _ O
Motoaki -X- _ O
Kawanabe -X- _ O
, -X- _ O
Katja -X- _ O
Hansen -X- _ O
, -X- _ O
and -X- _ O
KlausRobert -X- _ O
M -X- _ O
¨uller -X- _ O
. -X- _ O

2010 -X- _ O
. -X- _ O

How -X- _ O
to -X- _ O
explain -X- _ O
individual -X- _ O
classiﬁcation -X- _ O
decisions -X- _ O
. -X- _ O

J. -X- _ O
Mach -X- _ O
. -X- _ O

Learn -X- _ O
. -X- _ O

Res -X- _ O
. -X- _ O
, -X- _ O
Emanuele -X- _ O
Bastianelli -X- _ O
, -X- _ O
Giuseppe -X- _ O
Castellucci -X- _ O
, -X- _ O
Danilo -X- _ O
Croce -X- _ O
, -X- _ O
Luca -X- _ O
Iocchi -X- _ O
, -X- _ O
Roberto -X- _ O
Basili -X- _ O
, -X- _ O
and -X- _ O
Daniele -X- _ O
Nardi -X- _ O
. -X- _ O

2014 -X- _ O
. -X- _ O

Huric -X- _ B-DatasetName
: -X- _ O
a -X- _ O
human -X- _ O
robot -X- _ O
interaction -X- _ O
corpus -X- _ O
. -X- _ O

In -X- _ O
LREC -X- _ O
, -X- _ O
pages -X- _ O
4519–4526 -X- _ O
. -X- _ O

European -X- _ O
Language -X- _ O
Resources -X- _ O
Association -X- _ O
( -X- _ O
ELRA -X- _ O
) -X- _ O
. -X- _ O

Michael -X- _ O
Collins -X- _ O
and -X- _ O
Nigel -X- _ O
Duffy -X- _ O
. -X- _ O

2001 -X- _ O
. -X- _ O

New -X- _ O
ranking -X- _ O
algorithms -X- _ O
for -X- _ O
parsing -X- _ O
and -X- _ O
tagging -X- _ O
: -X- _ O
Kernels -X- _ O
over -X- _ O
discrete -X- _ O
structures -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
voted -X- _ O
perceptron -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
40th -X- _ O
Annual -X- _ O
Meeting -X- _ O
on -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
( -X- _ O
ACL -X- _ O
’ -X- _ O
02 -X- _ O
) -X- _ O
, -X- _ O
July -X- _ O
7 -X- _ O
- -X- _ O
12 -X- _ O
, -X- _ O
2002 -X- _ O
, -X- _ O
Philadelphia -X- _ O
, -X- _ O
PA -X- _ O
, -X- _ O
USA -X- _ O
, -X- _ O
pages -X- _ O
263–270 -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
Morristown -X- _ O
, -X- _ O
NJ -X- _ O
, -X- _ O
USA -X- _ O
. -X- _ O

Danilo -X- _ O
Croce -X- _ O
, -X- _ O
Simone -X- _ O
Filice -X- _ O
, -X- _ O
Giuseppe -X- _ O
Castellucci -X- _ O
, -X- _ O
and -X- _ O
Roberto -X- _ O
Basili -X- _ O
. -X- _ O

2017 -X- _ O
. -X- _ O

Deep -X- _ O
learning -X- _ O
in -X- _ O
semantic -X- _ O
kernel -X- _ O
spaces -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
55th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
( -X- _ O
Volume -X- _ O
1 -X- _ O
: -X- _ O
Long -X- _ O
Papers -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
345–354 -X- _ O
, -X- _ O
Vancouver -X- _ O
, -X- _ O
Canada -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Danilo -X- _ O
Croce -X- _ O
, -X- _ O
Alessandro -X- _ O
Moschitti -X- _ O
, -X- _ O
and -X- _ O
Roberto -X- _ O
Basili -X- _ O
. -X- _ O

2011 -X- _ O
. -X- _ O

Structured -X- _ O
lexical -X- _ O
similarity -X- _ O
via -X- _ O
convolution -X- _ O
kernels -X- _ O
on -X- _ O
dependency -X- _ O
trees -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
Dumitru -X- _ O
Erhan -X- _ O
, -X- _ O
Aaron -X- _ O
Courville -X- _ O
, -X- _ O
and -X- _ O
Yoshua -X- _ O
Bengio -X- _ O
. -X- _ O

2010 -X- _ O
. -X- _ O

Understanding -X- _ O
representations -X- _ O
learned -X- _ O
in -X- _ O
deep -X- _ O
architectures -X- _ O
. -X- _ O

Technical -X- _ O
Report -X- _ O
1355 -X- _ O
, -X- _ O
Universit´e -X- _ O
de -X- _ O
Montr -X- _ O
´ -X- _ O
eal -X- _ O
/ -X- _ O
DIRO -X- _ O
. -X- _ O

Simone -X- _ O
Filice -X- _ O
, -X- _ O
Giuseppe -X- _ O
Castellucci -X- _ O
, -X- _ O
Giovanni -X- _ O
Da -X- _ O
San -X- _ O
Martino -X- _ O
, -X- _ O
Alessandro -X- _ O
Moschitti -X- _ O
, -X- _ O
Danilo -X- _ O
Croce -X- _ O
, -X- _ O
and -X- _ O
Roberto -X- _ O
Basili -X- _ O
. -X- _ O

2018 -X- _ O
. -X- _ O

Kelp -X- _ O
: -X- _ O
a -X- _ O
kernel -X- _ O
- -X- _ O
based -X- _ O
learning -X- _ O
platform -X- _ O
. -X- _ O

Journal -X- _ O
of -X- _ O
Machine -X- _ O
Learning -X- _ O
Research -X- _ O
, -X- _ O
Charles -X- _ O
J. -X- _ O
Fillmore -X- _ O
. -X- _ O
1985 -X- _ O
. -X- _ O

Frames -X- _ O
and -X- _ O
the -X- _ O
semantics -X- _ O
of -X- _ O
understanding -X- _ O
. -X- _ O

Quaderni -X- _ O
di -X- _ O
Semantica -X- _ O
, -X- _ O
6 -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
:222 -X- _ O
– -X- _ O
Nicholas -X- _ O
Frosst -X- _ O
and -X- _ O
Geoffrey -X- _ O
Hinton -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Distilling -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
into -X- _ O
a -X- _ O
soft -X- _ O
decision -X- _ O
. -X- _ O

CEUR -X- _ O
Workshop -X- _ O
Proceedings -X- _ O
, -X- _ O
2071.Sepp -X- _ O

Hochreiter -X- _ O
and -X- _ O
J -X- _ O
¨urgen -X- _ O
Schmidhuber -X- _ O
. -X- _ O

1997 -X- _ O
. -X- _ O

Long -X- _ O
short -X- _ O
- -X- _ O
term -X- _ O
memory -X- _ O
. -X- _ O

Neural -X- _ O
Comput -X- _ O
. -X- _ O

, -X- _ O
9 -X- _ O
( -X- _ O
8 -X- _ O
) -X- _ O
:1735 -X- _ O
– -X- _ O
Igor -X- _ O
Kononenko -X- _ O
and -X- _ O
Ivan -X- _ O
Bratko -X- _ O
. -X- _ O

1991 -X- _ O
. -X- _ O

Informationbased -X- _ O
evaluation -X- _ O
criterion -X- _ O
for -X- _ O
classiﬁer -X- _ O
’s -X- _ O
performance -X- _ O
. -X- _ O

Machine -X- _ O
Learning -X- _ O
, -X- _ O
6 -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
:67–80 -X- _ O
. -X- _ O

Sanjiv -X- _ O
Kumar -X- _ O
, -X- _ O
Mehryar -X- _ O
Mohri -X- _ O
, -X- _ O
and -X- _ O
Ameet -X- _ O
Talwalkar -X- _ O
. -X- _ O
2012 -X- _ O
. -X- _ O

Sampling -X- _ O
methods -X- _ O
for -X- _ O
the -X- _ O
nystr -X- _ O
¨om -X- _ O
method -X- _ O
. -X- _ O

Hugo -X- _ O
Larochelle -X- _ O
and -X- _ O
Geoffrey -X- _ O
E. -X- _ O
Hinton -X- _ O
. -X- _ O

2010 -X- _ O
. -X- _ O

Learning -X- _ O
to -X- _ O
combine -X- _ O
foveal -X- _ O
glimpses -X- _ O
with -X- _ O
a -X- _ O
third -X- _ O
- -X- _ O
order -X- _ O
boltzmann -X- _ O
machine -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
( -X- _ O
NIPS -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
1243 -X- _ O
– -X- _ O
Xin -X- _ O
Li -X- _ O
and -X- _ O
Dan -X- _ O
Roth -X- _ O
. -X- _ O
2006 -X- _ O
. -X- _ O

Learning -X- _ O
question -X- _ O
classiﬁers -X- _ O
: -X- _ O
the -X- _ O
role -X- _ O
of -X- _ O
semantic -X- _ O
information -X- _ O
. -X- _ O

Natural -X- _ O
Language -X- _ O
Engineering -X- _ O
, -X- _ O
12 -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
:229–249 -X- _ O
. -X- _ O

Alessandro -X- _ O
Moschitti -X- _ O
, -X- _ O
Daniele -X- _ O
Pighin -X- _ O
, -X- _ O
and -X- _ O
Roberto -X- _ O
Basili -X- _ O
. -X- _ O

2008 -X- _ O
. -X- _ O

Tree -X- _ O
kernels -X- _ O
for -X- _ O
semantic -X- _ O
role -X- _ O
labeling -X- _ O
. -X- _ O

Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
34 -X- _ O
. -X- _ O
M.S. -X- _ O
Palmer -X- _ O
, -X- _ O
D. -X- _ O
Gildea -X- _ O
, -X- _ O
and -X- _ O
N. -X- _ O
Xue -X- _ O
. -X- _ O

2010 -X- _ O
. -X- _ O

Semantic -X- _ O
Role -X- _ O
Labeling -X- _ O
. -X- _ O

Online -X- _ O
access -X- _ O
: -X- _ O
IEEE -X- _ O
( -X- _ O
Institute -X- _ O
of -X- _ O
Electrical -X- _ O
and -X- _ O
Electronics -X- _ O
Engineers -X- _ O
) -X- _ O
IEEE -X- _ O
Morgan -X- _ O
& -X- _ O
Claypool -X- _ O
Synthesis -X- _ O
eBooks -X- _ O
Library -X- _ O
. -X- _ O

Morgan -X- _ O
& -X- _ O
Claypool -X- _ O
Publishers -X- _ O
. -X- _ O

Marco -X- _ O
T -X- _ O
´ -X- _ O
ulio -X- _ O
Ribeiro -X- _ O
, -X- _ O
Sameer -X- _ O
Singh -X- _ O
, -X- _ O
and -X- _ O
Carlos -X- _ O
Guestrin -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

” -X- _ O
why -X- _ O
should -X- _ O
I -X- _ O
trust -X- _ O
you -X- _ O
? -X- _ O
” -X- _ O
: -X- _ O
Explaining -X- _ O
the -X- _ O
predictions -X- _ O
of -X- _ O
any -X- _ O
classiﬁer -X- _ O
. -X- _ O

CoRR -X- _ O
, -X- _ O
John -X- _ O
Shawe -X- _ O
- -X- _ O
Taylor -X- _ O
and -X- _ O
Nello -X- _ O
Cristianini -X- _ O
. -X- _ O
2004 -X- _ O
. -X- _ O

Kernel -X- _ O
Methods -X- _ O
for -X- _ O
Pattern -X- _ O
Analysis -X- _ O
. -X- _ O

Cambridge -X- _ O
University -X- _ O
Press -X- _ O
, -X- _ O
Cambridge -X- _ O
, -X- _ O
UK -X- _ O
. -X- _ O

Karen -X- _ O
Simonyan -X- _ O
, -X- _ O
Andrea -X- _ O
Vedaldi -X- _ O
, -X- _ O
and -X- _ O
Andrew -X- _ O
Zisserman -X- _ O
. -X- _ O
2013 -X- _ O
. -X- _ O

Deep -X- _ O
inside -X- _ O
convolutional -X- _ O
networks -X- _ O
: -X- _ O
Visualising -X- _ O
image -X- _ O
classiﬁcation -X- _ O
models -X- _ O
and -X- _ O
saliency -X- _ O
Matthew -X- _ O
D. -X- _ O
Zeiler -X- _ O
and -X- _ O
Rob -X- _ O
Fergus -X- _ O
. -X- _ O
2013 -X- _ O
. -X- _ O

Visualizing -X- _ O
and -X- _ O
understanding -X- _ O
convolutional -X- _ O
networks -X- _ O
. -X- _ O

CoRR -X- _ O
, -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
2018 -X- _ O
EMNLP -X- _ O
Workshop -X- _ O
BlackboxNLP -X- _ O
: -X- _ O
Analyzing -X- _ O
and -X- _ O
Interpreting -X- _ O
Neural -X- _ O
Networks -X- _ O
for -X- _ O
NLP -X- _ O
, -X- _ O
pages -X- _ O
25–29 -X- _ O
Brussels -X- _ O
, -X- _ O
Belgium -X- _ O
, -X- _ O
November -X- _ O
1 -X- _ O
, -X- _ O
2018 -X- _ O
. -X- _ O

c -X- _ O

2018 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics25Nightmare -X- _ O
at -X- _ O
test -X- _ O
time -X- _ O
: -X- _ O
How -X- _ O
punctuation -X- _ O
prevents -X- _ O
parsers -X- _ O
from -X- _ O
generalizing -X- _ O
Anders -X- _ O
Søgaard1Miryam -X- _ O
de -X- _ O
Lhoneux2Isabelle -X- _ O
Augenstein1 -X- _ O

University -X- _ O
of -X- _ O
Copenhagen -X- _ O
Uppsala -X- _ O
University -X- _ O
Abstract -X- _ O
Punctuation -X- _ O
is -X- _ O
a -X- _ O
strong -X- _ O
indicator -X- _ O
of -X- _ O
syntactic -X- _ O
structure -X- _ O
, -X- _ O
and -X- _ O
parsers -X- _ O
trained -X- _ O
on -X- _ O
text -X- _ O
with -X- _ O
punctuation -X- _ O
often -X- _ O
rely -X- _ O
heavily -X- _ O
on -X- _ O
this -X- _ O
signal -X- _ O
. -X- _ O

Punctuation -X- _ O
is -X- _ O
a -X- _ O
diversion -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
since -X- _ O
human -X- _ O
language -X- _ O
processing -X- _ O
does -X- _ O
not -X- _ O
rely -X- _ O
on -X- _ O
punctuation -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
extent -X- _ O
, -X- _ O
and -X- _ O
in -X- _ O
informal -X- _ O
texts -X- _ O
, -X- _ O
we -X- _ O
therefore -X- _ O
often -X- _ O
leave -X- _ O
out -X- _ O
punctuation -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
use -X- _ O
punctuation -X- _ O
ungrammatically -X- _ O
for -X- _ O
emphatic -X- _ O
or -X- _ O
creative -X- _ O
purposes -X- _ O
, -X- _ O
or -X- _ O
simply -X- _ O
by -X- _ O
mistake -X- _ O
. -X- _ O

We -X- _ O
show -X- _ O
that -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
dependency -X- _ O
parsers -X- _ O
are -X- _ O
sensitive -X- _ O
to -X- _ O
both -X- _ O
absence -X- _ O
of -X- _ O
punctuation -X- _ O
and -X- _ O
to -X- _ O
alternative -X- _ O
uses -X- _ O
; -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
neural -X- _ O
parsers -X- _ O
tend -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
sensitive -X- _ O
than -X- _ O
vintage -X- _ O
parsers -X- _ O
; -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
training -X- _ O
neural -X- _ O
parsers -X- _ O
without -X- _ O
punctuation -X- _ O
outperforms -X- _ O
all -X- _ O
out -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
box -X- _ O
parsers -X- _ O
across -X- _ O
all -X- _ O
scenarios -X- _ O
where -X- _ O
punctuation -X- _ O
departs -X- _ O
from -X- _ O
standard -X- _ O
punctuation -X- _ O
. -X- _ O

Our -X- _ O
main -X- _ O
experiments -X- _ O
are -X- _ O
on -X- _ O
synthetically -X- _ O
corrupted -X- _ O
data -X- _ O
to -X- _ O
study -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
punctuation -X- _ O
in -X- _ O
isolation -X- _ O
and -X- _ O
avoid -X- _ O
potential -X- _ O
confounds -X- _ O
, -X- _ O
but -X- _ O
we -X- _ O
also -X- _ O
show -X- _ O
effects -X- _ O
on -X- _ O
out -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
domain -X- _ O
data -X- _ O
. -X- _ O

1 -X- _ O
Introduction -X- _ O
We -X- _ O
study -X- _ O
the -X- _ O
sensitivity -X- _ O
of -X- _ O
modern -X- _ O
dependency -X- _ O
parsers -X- _ O
to -X- _ O
punctuation -X- _ O
. -X- _ O

While -X- _ O
punctuation -X- _ O
was -X- _ O
originally -X- _ O
motivated -X- _ O
by -X- _ O
reading -X- _ O
aloud -X- _ O
, -X- _ O
serving -X- _ O
the -X- _ O
purpose -X- _ O
of -X- _ O
“ -X- _ O
breath -X- _ O
marks -X- _ O
” -X- _ O
( -X- _ O
Baldwin -X- _ O
and -X- _ O
Coady -X- _ O
, -X- _ O
1978 -X- _ O
) -X- _ O
, -X- _ O
many -X- _ O
modern -X- _ O
- -X- _ O
day -X- _ O
punctuation -X- _ O
systems -X- _ O
are -X- _ O
designed -X- _ O
to -X- _ O
facilitate -X- _ O
grammatical -X- _ O
disambiguation -X- _ O
. -X- _ O

This -X- _ O
paper -X- _ O
aims -X- _ O
to -X- _ O
show -X- _ O
that -X- _ O
for -X- _ O
this -X- _ O
reason -X- _ O
, -X- _ O
punctuation -X- _ O
can -X- _ O
signiﬁcantly -X- _ O
hurt -X- _ O
the -X- _ O
generalization -X- _ O
ability -X- _ O
of -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
syntactic -X- _ O
parsers -X- _ O
. -X- _ O

In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
syntactic -X- _ O
parsers -X- _ O
become -X- _ O
too -X- _ O
reliant -X- _ O
on -X- _ O
punctuation -X- _ O
and -X- _ O
therefore -X- _ O
suffer -X- _ O
from -X- _ O
the -X- _ O
absence -X- _ O
or -X- _ O
creative -X- _ O
uses -X- _ O
of -X- _ O
punctuation -X- _ O
. -X- _ O

Such -X- _ O
uses -X- _ O
are -X- _ O
abundant -X- _ O
; -X- _ O
see -X- _ O
Table -X- _ O
1 -X- _ O
for -X- _ O
examples -X- _ O
from -X- _ O
Twitter -X- _ O
. -X- _ O

Such -X- _ O
situations -X- _ O
, -X- _ O
where -X- _ O
highly -X- _ O
predictive -X- _ O
features -X- _ O
are -X- _ O
absent -X- _ O
or -X- _ O
distorted -X- _ O
at -X- _ O
test -X- _ O
time -X- _ O
, -X- _ O
were -X- _ O
referred -X- _ O
to -X- _ O
in -X- _ O
Globerson -X- _ O
and -X- _ O
Roweis -X- _ O
( -X- _ O
2006 -X- _ O
) -X- _ O
as -X- _ O
nightmare -X- _ O
at -X- _ O
test -X- _ O
time -X- _ O
. -X- _ O

Human -X- _ O
reading -X- _ O
is -X- _ O
very -X- _ O
robust -X- _ O
to -X- _ O
variation -X- _ O
in -X- _ O
punctuation -X- _ O
( -X- _ O
Baldwin -X- _ O
and -X- _ O
Coady -X- _ O
, -X- _ O
No -X- _ O
punctuation -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
i -X- _ O
have -X- _ O
so -X- _ O
many -X- _ O
questions -X- _ O
i -X- _ O
do -X- _ O
nt -X- _ O
know -X- _ O
where -X- _ O
to -X- _ O
start -X- _ O
Creative -X- _ O
punctuation -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
What -X- _ O
. -X- _ O

The -X- _ O
. -X- _ O

Fuck -X- _ O
. -X- _ O

Ever -X- _ O
. -X- _ O

Dot -X- _ O
. -X- _ O

Com -X- _ O
Both -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
I -X- _ O
feel -X- _ O
like -X- _ O
... -X- _ O

idk -X- _ O
... -X- _ O
idk -X- _ O
... -X- _ O
idk -X- _ O
man -X- _ O
. -X- _ O

Nvm -X- _ O
I -X- _ O
’m -X- _ O
good -X- _ O
. -X- _ O

Table -X- _ O
1 -X- _ O
: -X- _ O
Examples -X- _ O
of -X- _ O
uses -X- _ O
of -X- _ O
punctuation -X- _ O
1978 -X- _ O
) -X- _ O
; -X- _ O
so -X- _ O
creative -X- _ O
use -X- _ O
of -X- _ O
punctuation -X- _ O
does -X- _ O
not -X- _ O
hurt -X- _ O
human -X- _ O
reading -X- _ O
performance -X- _ O
. -X- _ O

In -X- _ O
effect -X- _ O
, -X- _ O
sensitivity -X- _ O
to -X- _ O
punctuation -X- _ O
is -X- _ O
a -X- _ O
major -X- _ O
obstacle -X- _ O
that -X- _ O
prevents -X- _ O
our -X- _ O
syntactic -X- _ O
parser -X- _ O
from -X- _ O
achieving -X- _ O
human -X- _ O
- -X- _ O
level -X- _ O
robustness -X- _ O
. -X- _ O

The -X- _ O
generalization -X- _ O
ability -X- _ O
of -X- _ O
a -X- _ O
dependency -X- _ O
parser -X- _ O
is -X- _ O
usually -X- _ O
measured -X- _ O
by -X- _ O
evaluating -X- _ O
its -X- _ O
accuracy -X- _ O
on -X- _ O
held -X- _ O
- -X- _ O
out -X- _ O
data -X- _ O
, -X- _ O
our -X- _ O
yardstick -X- _ O
to -X- _ O
prevent -X- _ O
over-ﬁtting -X- _ O
, -X- _ O
i.e. -X- _ O
we -X- _ O
deﬁne -X- _ O
the -X- _ O
degree -X- _ O
to -X- _ O
which -X- _ O
a -X- _ O
parser -X- _ O
has -X- _ O
over-ﬁtted -X- _ O
to -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
as -X- _ O
the -X- _ O
difference -X- _ O
between -X- _ O
performance -X- _ O
on -X- _ O
training -X- _ O
data -X- _ O
and -X- _ O
performance -X- _ O
on -X- _ O
the -X- _ O
held -X- _ O
- -X- _ O
out -X- _ O
data -X- _ O
. -X- _ O

This -X- _ O
practice -X- _ O
is -X- _ O
poor -X- _ O
when -X- _ O
data -X- _ O
is -X- _ O
not -X- _ O
i.i.d -X- _ O
. -X- _ O
, -X- _ O
since -X- _ O
the -X- _ O
heldout -X- _ O
data -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
assumed -X- _ O
to -X- _ O
be -X- _ O
representative -X- _ O
; -X- _ O
in -X- _ O
such -X- _ O
cases -X- _ O
, -X- _ O
little -X- _ O
or -X- _ O
no -X- _ O
over-ﬁtting -X- _ O
does -X- _ O
not -X- _ O
guarantee -X- _ O
our -X- _ O
parsers -X- _ O
have -X- _ O
learned -X- _ O
important -X- _ O
linguistic -X- _ O
generalizations -X- _ O
: -X- _ O
Rather -X- _ O
, -X- _ O
the -X- _ O
parsers -X- _ O
may -X- _ O
have -X- _ O
over-ﬁtted -X- _ O
to -X- _ O
superﬁcial -X- _ O
cues -X- _ O
that -X- _ O
are -X- _ O
present -X- _ O
in -X- _ O
both -X- _ O
the -X- _ O
training -X- _ O
and -X- _ O
test -X- _ O
datasets -X- _ O
( -X- _ O
Jo -X- _ O
and -X- _ O
Bengio -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
argue -X- _ O
that -X- _ O
punctuation -X- _ O
signs -X- _ O
are -X- _ O
superﬁcial -X- _ O
cues -X- _ O
preventing -X- _ O
modern -X- _ O
parsers -X- _ O
from -X- _ O
learning -X- _ O
appropriately -X- _ O
high -X- _ O
- -X- _ O
level -X- _ O
abstractions -X- _ O
from -X- _ O
our -X- _ O
datasets -X- _ O
. -X- _ O

Contributions -X- _ O
We -X- _ O
evaluate -X- _ O
three -X- _ O
neural -X- _ O
dependency -X- _ O
parsers -X- _ O
for -X- _ O
English -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
two -X- _ O
older -X- _ O
alternatives -X- _ O
, -X- _ O
on -X- _ O
a -X- _ O
standard -X- _ O
benchmark -X- _ O
, -X- _ O
before -X- _ O
and -X- _ O
after -X- _ O
stripping -X- _ O
punctuation -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
after -X- _ O
injecting -X- _ O
more -X- _ O
punctuation -X- _ O
signs -X- _ O
in -X- _ O
the -X- _ O
benchmark -X- _ O
. -X- _ O

26John -X- _ O
, -X- _ O
27 -X- _ O
, -X- _ O
likes -X- _ O
jazz -X- _ O
.nsubj -X- _ O
punctamod -X- _ O
punctdobjpunct -X- _ O
Figure -X- _ O
1 -X- _ O
: -X- _ O
Punctuation -X- _ O
in -X- _ O
Stanford -X- _ O
dependencies -X- _ O
We -X- _ O
show -X- _ O
that -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
projective -X- _ O
parsers -X- _ O
are -X- _ O
, -X- _ O
unsurprisingly -X- _ O
, -X- _ O
more -X- _ O
sensitive -X- _ O
to -X- _ O
punctuation -X- _ O
injection -X- _ O
than -X- _ O
non -X- _ O
- -X- _ O
projective -X- _ O
ones -X- _ O
, -X- _ O
since -X- _ O
punctuation -X- _ O
injection -X- _ O
may -X- _ O
introduce -X- _ O
crossing -X- _ O
edges -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
neural -X- _ O
parsers -X- _ O
are -X- _ O
more -X- _ O
sensitive -X- _ O
than -X- _ O
vintage -X- _ O
parsers -X- _ O
. -X- _ O

The -X- _ O
latter -X- _ O
is -X- _ O
our -X- _ O
main -X- _ O
contribution -X- _ O
, -X- _ O
but -X- _ O
we -X- _ O
also -X- _ O
show -X- _ O
that -X- _ O
training -X- _ O
a -X- _ O
neural -X- _ O
parser -X- _ O
without -X- _ O
punctuation -X- _ O
outperforms -X- _ O
all -X- _ O
parsers -X- _ O
trained -X- _ O
in -X- _ O
a -X- _ O
regular -X- _ O
fashion -X- _ O
across -X- _ O
all -X- _ O
punctuation -X- _ O
scenarios -X- _ O
. -X- _ O

Our -X- _ O
experiments -X- _ O
are -X- _ O
on -X- _ O
semi -X- _ O
- -X- _ O
synthetic -X- _ O
data -X- _ O
to -X- _ O
control -X- _ O
for -X- _ O
confounds -X- _ O
, -X- _ O
but -X- _ O
we -X- _ O
also -X- _ O
show -X- _ O
the -X- _ O
parser -X- _ O
trained -X- _ O
without -X- _ O
punctuation -X- _ O
is -X- _ O
superior -X- _ O
on -X- _ O
real -X- _ O
data -X- _ O
with -X- _ O
non -X- _ O
- -X- _ O
standard -X- _ O
punctuation -X- _ O
. -X- _ O

2 -X- _ O
Punctuation -X- _ O
in -X- _ O
Stanford -X- _ O
dependencies -X- _ O
Dependency -X- _ O
annotation -X- _ O
Dependency -X- _ O
annotation -X- _ O
refers -X- _ O
to -X- _ O
the -X- _ O
manual -X- _ O
assignment -X- _ O
of -X- _ O
syntactic -X- _ O
structures -X- _ O
to -X- _ O
sentences -X- _ O
, -X- _ O
following -X- _ O
one -X- _ O
of -X- _ O
several -X- _ O
sets -X- _ O
of -X- _ O
available -X- _ O
annotation -X- _ O
guidelines -X- _ O
. -X- _ O

This -X- _ O
paper -X- _ O
focuses -X- _ O
exclusively -X- _ O
on -X- _ O
the -X- _ O
Stanford -X- _ O
dependencies -X- _ O
annotation -X- _ O
scheme -X- _ O
( -X- _ O
de -X- _ O
Marneffe -X- _ O
and -X- _ O
Manning -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
. -X- _ O

This -X- _ O
scheme -X- _ O
restricts -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
possible -X- _ O
syntactic -X- _ O
structures -X- _ O
to -X- _ O
single -X- _ O
- -X- _ O
rooted -X- _ O
, -X- _ O
ordered -X- _ O
, -X- _ O
possibly -X- _ O
non -X- _ O
- -X- _ O
projective -X- _ O
trees -X- _ O
whose -X- _ O
edges -X- _ O
are -X- _ O
uniquely -X- _ O
labeled -X- _ O
by -X- _ O
a -X- _ O
single -X- _ O
dependency -X- _ O
label -X- _ O
. -X- _ O

Punctuation -X- _ O
Punctuation -X- _ O
should -X- _ O
be -X- _ O
distinguished -X- _ O
from -X- _ O
diacritics -X- _ O
and -X- _ O
logographs -X- _ O
. -X- _ O

The -X- _ O
two -X- _ O
most -X- _ O
frequently -X- _ O
used -X- _ O
punctuation -X- _ O
signs -X- _ O
are -X- _ O
periods -X- _ O
and -X- _ O
commas -X- _ O
. -X- _ O

Periods -X- _ O
( -X- _ O
“ -X- _ O
. -X- _ O
” -X- _ O
) -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
are -X- _ O
potentially -X- _ O
ambiguous -X- _ O
with -X- _ O
other -X- _ O
uses -X- _ O
of -X- _ O
dots -X- _ O
, -X- _ O
typically -X- _ O
indicating -X- _ O
omissions -X- _ O
or -X- _ O
pauses -X- _ O
. -X- _ O

When -X- _ O
dots -X- _ O
are -X- _ O
used -X- _ O
emphatically -X- _ O
and -X- _ O
creatively -X- _ O
it -X- _ O
is -X- _ O
hard -X- _ O
to -X- _ O
maintain -X- _ O
this -X- _ O
distinction -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
will -X- _ O
simply -X- _ O
refer -X- _ O
to -X- _ O
dots -X- _ O
andcommas -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
. -X- _ O

We -X- _ O
ignore -X- _ O
other -X- _ O
punctuation -X- _ O
signs -X- _ O
, -X- _ O
including -X- _ O
dashes -X- _ O
, -X- _ O
question -X- _ O
and -X- _ O
exclamation -X- _ O
marks -X- _ O
, -X- _ O
and -X- _ O
colons -X- _ O
and -X- _ O
semicolons -X- _ O
. -X- _ O

Punctuation -X- _ O
is -X- _ O
, -X- _ O
among -X- _ O
other -X- _ O
things -X- _ O
, -X- _ O
used -X- _ O
to -X- _ O
mark -X- _ O
boundaries -X- _ O
between -X- _ O
constituents -X- _ O
of -X- _ O
written -X- _ O
language -X- _ O
. -X- _ O

Space -X- _ O
characters -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
separate -X- _ O
words -X- _ O
, -X- _ O
albeit -X- _ O
sometimes -X- _ O
inconsistently -X- _ O
. -X- _ O

Spacing -X- _ O
is -X- _ O
a -X- _ O
fairly -X- _ O
recent -X- _ O
innovation -X- _ O
in -X- _ O
writing -X- _ O
; -X- _ O
classical -X- _ O
Latin -X- _ O
and -X- _ O
Greek -X- _ O
did -X- _ O
not -X- _ O
leave -X- _ O
spaces -X- _ O
betweenwords -X- _ O
, -X- _ O
and -X- _ O
many -X- _ O
Asian -X- _ O
languages -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
Thai -X- _ O
and -X- _ O
Lao -X- _ O
, -X- _ O
still -X- _ O
do -X- _ O
not -X- _ O
. -X- _ O

A -X- _ O
period -X- _ O
is -X- _ O
typically -X- _ O
used -X- _ O
to -X- _ O
mark -X- _ O
the -X- _ O
end -X- _ O
of -X- _ O
a -X- _ O
grammatical -X- _ O
sentence -X- _ O
, -X- _ O
and -X- _ O
commas -X- _ O
are -X- _ O
often -X- _ O
used -X- _ O
to -X- _ O
separate -X- _ O
clauses -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
punctuation -X- _ O
also -X- _ O
correlates -X- _ O
strongly -X- _ O
with -X- _ O
properties -X- _ O
of -X- _ O
syntactic -X- _ O
structures -X- _ O
and -X- _ O
is -X- _ O
therefore -X- _ O
very -X- _ O
predictive -X- _ O
of -X- _ O
dependency -X- _ O
structures -X- _ O
. -X- _ O

Variation -X- _ O
in -X- _ O
punctuation -X- _ O
is -X- _ O
often -X- _ O
observed -X- _ O
in -X- _ O
informal -X- _ O
texts -X- _ O
, -X- _ O
but -X- _ O
variation -X- _ O
may -X- _ O
also -X- _ O
be -X- _ O
the -X- _ O
result -X- _ O
of -X- _ O
errors -X- _ O
. -X- _ O

Punctuation -X- _ O
errors -X- _ O
are -X- _ O
by -X- _ O
far -X- _ O
the -X- _ O
most -X- _ O
frequent -X- _ O
error -X- _ O
type -X- _ O
in -X- _ O
scientiﬁc -X- _ O
writing -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
( -X- _ O
Remse -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

Modern -X- _ O
parsers -X- _ O
should -X- _ O
be -X- _ O
robust -X- _ O
to -X- _ O
such -X- _ O
variation -X- _ O
, -X- _ O
just -X- _ O
like -X- _ O
humans -X- _ O
are -X- _ O
( -X- _ O
Baldwin -X- _ O
and -X- _ O
Coady -X- _ O
, -X- _ O
1978 -X- _ O
) -X- _ O
. -X- _ O

Punctuation -X- _ O
in -X- _ O
Stanford -X- _ O
dependencies -X- _ O

In -X- _ O
the -X- _ O
Stanford -X- _ O
dependencies -X- _ O
( -X- _ O
de -X- _ O
Marneffe -X- _ O
and -X- _ O
Manning -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
, -X- _ O
periods -X- _ O
attach -X- _ O
to -X- _ O
root -X- _ O
tokens -X- _ O
, -X- _ O
and -X- _ O
commas -X- _ O
attach -X- _ O
to -X- _ O
their -X- _ O
left -X- _ O
neighbor -X- _ O
or -X- _ O
to -X- _ O
root -X- _ O
tokens -X- _ O
; -X- _ O
see -X- _ O
Figure -X- _ O
1 -X- _ O
. -X- _ O

3 -X- _ O
Experiments -X- _ O
This -X- _ O
section -X- _ O
describes -X- _ O
how -X- _ O
we -X- _ O
remove -X- _ O
and -X- _ O
inject -X- _ O
punctuation -X- _ O
( -X- _ O
our -X- _ O
perturbation -X- _ O
maps -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
details -X- _ O
of -X- _ O
the -X- _ O
parsers -X- _ O
used -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

Perturbation -X- _ O
maps -X- _ O
Since -X- _ O
dots -X- _ O
consistently -X- _ O
attach -X- _ O
to -X- _ O
the -X- _ O
root -X- _ O
token -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
, -X- _ O
and -X- _ O
commas -X- _ O
attach -X- _ O
to -X- _ O
their -X- _ O
left -X- _ O
neighbour -X- _ O
or -X- _ O
to -X- _ O
the -X- _ O
root -X- _ O
token -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
remove -X- _ O
and -X- _ O
inject -X- _ O
additional -X- _ O
punctuation -X- _ O
in -X- _ O
a -X- _ O
sentence -X- _ O
without -X- _ O
affecting -X- _ O
the -X- _ O
rest -X- _ O
of -X- _ O
its -X- _ O
syntactic -X- _ O
structure -X- _ O
and -X- _ O
without -X- _ O
violating -X- _ O
the -X- _ O
wellformedness -X- _ O
of -X- _ O
dependency -X- _ O
trees -X- _ O
. -X- _ O

Note -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
that -X- _ O
injecting -X- _ O
a -X- _ O
root -X- _ O
- -X- _ O
dominated -X- _ O
dot -X- _ O
or -X- _ O
comma -X- _ O
may -X- _ O
lead -X- _ O
to -X- _ O
crossing -X- _ O
edges -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
turn -X- _ O
a -X- _ O
projective -X- _ O
dependency -X- _ O
tree -X- _ O
into -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
projective -X- _ O
one -X- _ O
. -X- _ O

This -X- _ O
may -X- _ O
lead -X- _ O
to -X- _ O
cascading -X- _ O
errors -X- _ O
for -X- _ O
projective -X- _ B-MethodName
dependency -X- _ I-MethodName
parsers -X- _ I-MethodName
( -X- _ O
Ng -X- _ O
and -X- _ O
Curran -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
our -X- _ O
experiments -X- _ O
, -X- _ O
arc -X- _ O
- -X- _ O
eager -X- _ O
M -X- _ B-MethodName
ALTPARSER -X- _ I-MethodName
and -X- _ O
S -X- _ B-MethodName
TANFORD -X- _ I-MethodName
are -X- _ O
the -X- _ O
only -X- _ O
projective -X- _ O
parsers -X- _ O
. -X- _ O

We -X- _ O
therefore -X- _ O
propose -X- _ O
two -X- _ O
perturbation -X- _ O
maps -X- _ O
( -X- _ O
Jo -X- _ O
and -X- _ O
Bengio -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
: -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
simply -X- _ O
removing -X- _ O
punctuation -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
a -X- _ O
simple -X- _ O
injection -X- _ O
scheme -X- _ O
with -X- _ O
two -X- _ O
parameters -X- _ O
and. -X- _ O
Let -X- _ O
a -X- _ O
dependency -X- _ O
structure -X- _ O
be -X- _ O
an -X- _ O
ordered -X- _ O
tree -X- _ O
with -X- _ O
nnodes -X- _ O
decorated -X- _ O
with -X- _ O
words -X- _ O
ject -X- _ O
a -X- _ O
comma -X- _ O
at -X- _ O
position -X- _ O
iwith -X- _ O
probability -X- _ O
and -X- _ O
move -X- _ O
nodes -X- _ O
ijnto -X- _ O
positionsj+ -X- _ O
1 -X- _ O
, -X- _ O
increasing -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
graph -X- _ O
by -X- _ O
1 -X- _ O
; -X- _ O
and -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
inject -X- _ O
a -X- _ O
dot -X- _ O
at -X- _ O
positioni+1with -X- _ O
probability -X- _ O
and -X- _ O
move -X- _ O
nodes -X- _ O
i -X- _ O
< -X- _ O
jnto -X- _ O
positionsj+ -X- _ O
1 -X- _ O
, -X- _ O
increasing -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
graph -X- _ O
by -X- _ O
1 -X- _ O
. -X- _ O

If -X- _ O
we -X- _ O
follow -X- _ O
standard -X- _ O
methodology -X- _ O

27Parser -X- _ O
Neural -X- _ O
Trans.-based -X- _ O
Projective -X- _ O
UUP -X- _ B-MethodName
ARSER -X- _ I-MethodName
D -X- _ B-MethodName
D -X- _ I-MethodName
KG -X- _ I-MethodName
RAPHS -X- _ I-MethodName
D -X- _ I-MethodName
MALTPARSER -X- _ O
D -X- _ O
D -X- _ O
TURBO -X- _ B-MethodName
PARSER -X- _ I-MethodName
STANFORD -X- _ B-MethodName
D -X- _ O
D -X- _ O
D -X- _ O
Table -X- _ O
2 -X- _ O
: -X- _ O
Our -X- _ O
dependency -X- _ O
parsers -X- _ O
and -X- _ O
ignore -X- _ O
punctuation -X- _ O
when -X- _ O
evaluating -X- _ O
parsers -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
compare -X- _ O
evaluations -X- _ O
before -X- _ O
and -X- _ O
after -X- _ O
applying -X- _ O
the -X- _ O
injection -X- _ O
scheme -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
equally -X- _ O
straightforward -X- _ O
to -X- _ O
remove -X- _ O
punctuation -X- _ O
without -X- _ O
affecting -X- _ O
the -X- _ O
rest -X- _ O
of -X- _ O
the -X- _ O
dependency -X- _ O
tree -X- _ O
. -X- _ O

Each -X- _ O
element -X- _ O
wi -X- _ O
to -X- _ O
the -X- _ O
right -X- _ O
of -X- _ O
punctuation -X- _ O
nodes -X- _ O
wj -X- _ O
( -X- _ O
i -X- _ O
> -X- _ O
j -X- _ O
) -X- _ O
moves -X- _ O
to -X- _ O
the -X- _ O
left -X- _ O
( -X- _ O
j 1 -X- _ O
) -X- _ O
for -X- _ O
every -X- _ O
punctuation -X- _ O
item -X- _ O
, -X- _ O
decreasing -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
by -X- _ O
1 -X- _ O
each -X- _ O
time -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
both -X- _ O
removing -X- _ O
punctuation -X- _ O
and -X- _ O
our -X- _ O
injection -X- _ O
scheme -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
perturbation -X- _ O
maps -X- _ O
( -X- _ O
Jo -X- _ O
and -X- _ O
Bengio -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
of -X- _ O
our -X- _ O
dataset -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
following -X- _ O
important -X- _ O
properties -X- _ O
: -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
grammatical -X- _ O
structure -X- _ O
recognizability -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
human -X- _ O
ability -X- _ O
to -X- _ O
correctly -X- _ O
process -X- _ O
sentences -X- _ O
, -X- _ O
is -X- _ O
preserved -X- _ O
( -X- _ O
Baldwin -X- _ O
and -X- _ O
Coady -X- _ O
, -X- _ O
1978 -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
surface -X- _ O
statistical -X- _ O
regularities -X- _ O
are -X- _ O
qualitatively -X- _ O
different -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
there -X- _ O
exists -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
trivial -X- _ O
generalization -X- _ O
map -X- _ O
between -X- _ O
the -X- _ O
original -X- _ O
dataset -X- _ O
and -X- _ O
the -X- _ O
perturbed -X- _ O
version -X- _ O
. -X- _ O

These -X- _ O
properties -X- _ O
mean -X- _ O
we -X- _ O
can -X- _ O
use -X- _ O
our -X- _ O
punctuation -X- _ O
injection -X- _ O
scheme -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
sensitivity -X- _ O
of -X- _ O
neural -X- _ O
dependency -X- _ O
parsers -X- _ O
to -X- _ O
the -X- _ O
surface -X- _ O
statistical -X- _ O
regularities -X- _ O
involving -X- _ O
dots -X- _ O
and -X- _ O
commas -X- _ O
( -X- _ O
Jo -X- _ O
and -X- _ O
Bengio -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Since -X- _ O
human -X- _ O
reading -X- _ O
is -X- _ O
largely -X- _ O
unaffected -X- _ O
by -X- _ O
erroneous -X- _ O
punctuation -X- _ O
, -X- _ O
we -X- _ O
may -X- _ O
expect -X- _ O
parsers -X- _ O
to -X- _ O
be -X- _ O
robust -X- _ O
to -X- _ O
absence -X- _ O
of -X- _ O
punctuation -X- _ O
and -X- _ O
punctuation -X- _ O
injection -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
. -X- _ O

Our -X- _ O
results -X- _ O
clearly -X- _ O
show -X- _ O
this -X- _ O
is -X- _ O
not -X- _ O
the -X- _ O
case -X- _ O
; -X- _ O
in -X- _ O
fact -X- _ O
, -X- _ O
recently -X- _ O
proposed -X- _ O
neural -X- _ O
dependency -X- _ O
parsers -X- _ O
are -X- _ O
very -X- _ O
sensitive -X- _ O
to -X- _ O
differences -X- _ O
in -X- _ O
punctuation -X- _ O
. -X- _ O

Our -X- _ O
dependency -X- _ O
parsers -X- _ O
We -X- _ O
use -X- _ O
ﬁve -X- _ O
parsers -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
: -X- _ O
the -X- _ O
Uppsala -X- _ B-MethodName
parser -X- _ I-MethodName
( -X- _ O
UUPARSER -X- _ B-MethodName
) -X- _ O
( -X- _ O
de -X- _ O
Lhoneux -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017a -X- _ O
, -X- _ O
b -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
graphbased -X- _ B-MethodName
parser -X- _ I-MethodName
proposed -X- _ O
in -X- _ O
( -X- _ O
Kiperwasser -X- _ O
and -X- _ O
Goldberg -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
( -X- _ O
KG -X- _ B-MethodName
RAPHS -X- _ I-MethodName
) -X- _ O
, -X- _ O
the -X- _ O
arc -X- _ O
- -X- _ O
eager -X- _ O
M -X- _ B-MethodName
ALTPARSER -X- _ I-MethodName
( -X- _ O
Nivre -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2007 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
T -X- _ B-MethodName
URBO -X- _ I-MethodName
PARSER -X- _ I-MethodName
( -X- _ O
Fern -X- _ O
´ -X- _ O
andez -X- _ O
- -X- _ O
Gonz -X- _ O
´ -X- _ O
alez -X- _ O
and -X- _ O
Martins -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
STANFORD -X- _ B-MethodName
parser -X- _ I-MethodName
( -X- _ O
Chen -X- _ O
and -X- _ O
Manning -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

UUP -X- _ B-MethodName
ARSER -X- _ I-MethodName
is -X- _ O
a -X- _ O
neural -X- _ O
transition -X- _ O
- -X- _ O
based -X- _ O
dependency -X- _ O
parser -X- _ O
, -X- _ O
while -X- _ O
KG -X- _ B-MethodName
RAPHS -X- _ I-MethodName
is -X- _ O
a -X- _ O
neural -X- _ O
graphbased -X- _ O
parser -X- _ O
. -X- _ O

M -X- _ B-MethodName
ALTPARSER -X- _ I-MethodName
is -X- _ O
a -X- _ O
more -X- _ O
traditional -X- _ O
transition -X- _ O
- -X- _ O
based -X- _ O
parser -X- _ O
, -X- _ O
and -X- _ O
T -X- _ B-MethodName
URBO -X- _ I-MethodName
PARSER -X- _ I-MethodName
is -X- _ O
a -X- _ O
more -X- _ O
traditional -X- _ O
graph -X- _ O
- -X- _ O
based -X- _ O
parser -X- _ O
. -X- _ O

Fi -X- _ O
- -X- _ O
nally -X- _ O
, -X- _ O
the -X- _ O
S -X- _ B-MethodName
TANFORD -X- _ I-MethodName
parser -X- _ O
is -X- _ O
a -X- _ O
projective -X- _ O
, -X- _ O
neural -X- _ O
transition -X- _ O
- -X- _ O
based -X- _ O
dependency -X- _ O
parser -X- _ O
. -X- _ O

All -X- _ O
parsers -X- _ O
rely -X- _ O
on -X- _ O
predicted -X- _ O
part -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
speech -X- _ O
tags -X- _ O
, -X- _ O
except -X- _ O
UUPARSER -X- _ B-MethodName
( -X- _ O
which -X- _ O
does -X- _ O
not -X- _ O
rely -X- _ O
on -X- _ O
part -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
speech -X- _ O
information -X- _ O
at -X- _ O
all -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
T -X- _ O
URBO -X- _ O
TAGGER -X- _ O
to -X- _ O
obtain -X- _ O
those -X- _ O
. -X- _ O

See -X- _ O
Table -X- _ O
2 -X- _ O
for -X- _ O
an -X- _ O
overview -X- _ O
of -X- _ O
our -X- _ O
parsers -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
evaluate -X- _ O
three -X- _ O
non -X- _ O
- -X- _ O
standard -X- _ O
versions -X- _ O
of -X- _ O
the -X- _ O
UUP -X- _ B-MethodName
ARSER -X- _ I-MethodName
, -X- _ O
namely -X- _ O
, -X- _ O
a -X- _ O
parser -X- _ O
trained -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
parameters -X- _ O
as -X- _ O
the -X- _ O
offthe -X- _ O
- -X- _ O
shelf -X- _ O
parser -X- _ O
( -X- _ O
de -X- _ O
Lhoneux -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017b -X- _ O
) -X- _ O
, -X- _ O
but -X- _ O
which -X- _ O
simply -X- _ O
ignores -X- _ O
dots -X- _ O
and -X- _ O
commas -X- _ O
completely -X- _ O
( -X- _ O
N -X- _ O
OPUNCT -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
two -X- _ O
heavily -X- _ O
regularised -X- _ O
versions -X- _ O
of -X- _ O
the -X- _ O
parser -X- _ O
trained -X- _ O
in -X- _ O
the -X- _ O
standard -X- _ O
fashion -X- _ O
: -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
a -X- _ O
version -X- _ O
trained -X- _ O
with -X- _ O
the -X- _ O
drop -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
out -X- _ I-HyperparameterName
parameter -X- _ I-HyperparameterName
set -X- _ O
to -X- _ O
0.8 -X- _ B-HyperparameterValue
( -X- _ O
zeros -X- _ O
out -X- _ O
80 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
activations -X- _ O
) -X- _ O
; -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
a -X- _ O
version -X- _ O
with -X- _ O
the -X- _ O
gradient -X- _ B-HyperparameterName
clipping -X- _ I-HyperparameterName
parameter -X- _ I-HyperparameterName
set -X- _ O
to -X- _ O
0.075 -X- _ B-HyperparameterValue
. -X- _ O

We -X- _ O
do -X- _ O
so -X- _ O
to -X- _ O
answer -X- _ O
the -X- _ O
question -X- _ O
of -X- _ O
whether -X- _ O
more -X- _ O
heavily -X- _ O
regularized -X- _ O
dependency -X- _ O
parsers -X- _ O
are -X- _ O
less -X- _ O
sensitive -X- _ O
to -X- _ O
punctuation -X- _ O
( -X- _ O
they -X- _ O
are -X- _ O
not -X- _ O
) -X- _ O
. -X- _ O

4 -X- _ O
Results -X- _ O
and -X- _ O
analysis -X- _ O
We -X- _ O
discuss -X- _ O
the -X- _ O
sensitivity -X- _ O
of -X- _ O
off -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
shelf -X- _ O
dependency -X- _ O
parsers -X- _ O
to -X- _ O
our -X- _ O
perturbation -X- _ O
maps -X- _ O
, -X- _ O
comparing -X- _ O
to -X- _ O
a -X- _ O
parser -X- _ O
trained -X- _ O
after -X- _ O
removing -X- _ O
punctuation -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
to -X- _ O
heavily -X- _ O
regularised -X- _ O
versions -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
parser -X- _ O
. -X- _ O

No -X- _ O
punctuation -X- _ O
We -X- _ O
ﬁrst -X- _ O
test -X- _ O
our -X- _ O
parsers -X- _ O
on -X- _ O
a -X- _ O
version -X- _ O
of -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
where -X- _ O
we -X- _ O
strip -X- _ O
away -X- _ O
all -X- _ O
punctuation -X- _ O
. -X- _ O

The -X- _ O
data -X- _ O
thus -X- _ O
consists -X- _ O
of -X- _ O
newswire -X- _ O
( -X- _ O
WSJ -X- _ B-DatasetName
22 -X- _ I-DatasetName
) -X- _ O
with -X- _ O
punctuation -X- _ O
removed -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
Example -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
but -X- _ O
indomain -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
are -X- _ O
in -X- _ O
the -X- _ O
second -X- _ O
results -X- _ O
column -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
relative -X- _ O
increases -X- _ O
in -X- _ O
error -X- _ O
listed -X- _ O
in -X- _ O
the -X- _ O
third -X- _ O
results -X- _ O
column -X- _ O
. -X- _ O

The -X- _ O
drop -X- _ O
induced -X- _ O
by -X- _ O
removing -X- _ O
punctuation -X- _ O
is -X- _ O
quite -X- _ O
dramatic -X- _ O
: -X- _ O
The -X- _ O
UUP -X- _ O
ARSER -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
suffers -X- _ O
an -X- _ O
absolute -X- _ O
drop -X- _ O
of -X- _ O
5.4 -X- _ B-MetricValue
% -X- _ I-MetricValue
LAS -X- _ B-MetricName
or -X- _ O
an -X- _ O
error -X- _ B-MetricName
increase -X- _ O
of -X- _ O
67 -X- _ B-MetricValue
% -X- _ I-MetricValue
. -X- _ O

For -X- _ O
every -X- _ O
three -X- _ O
mistakes -X- _ O
, -X- _ O
UUP -X- _ O
ARSER -X- _ O
does -X- _ O
, -X- _ O
stripping -X- _ O
away -X- _ O
punctuation -X- _ O
makes -X- _ O
it -X- _ O
introduce -X- _ O
another -X- _ O
two -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
, -X- _ O
generally -X- _ O
, -X- _ O
the -X- _ O
relative -X- _ O
increase -X- _ O
in -X- _ O
error -X- _ O
is -X- _ O
much -X- _ O
higher -X- _ O
for -X- _ O
the -X- _ O
three -X- _ O
neural -X- _ O
parsers -X- _ O
, -X- _ O
and -X- _ O
that -X- _ O
the -X- _ O
regularisation -X- _ O
strategies -X- _ O
( -X- _ O
drop -X- _ O
- -X- _ O
out -X- _ O
and -X- _ O
gradient -X- _ O
clipping -X- _ O
) -X- _ O
do -X- _ O
not -X- _ O
seem -X- _ O
to -X- _ O
help -X- _ O
much -X- _ O
. -X- _ O

Comma -X- _ O
and -X- _ O
dot -X- _ O
injection -X- _ O
At -X- _ O
medium -X- _ O
injection -X- _ O
rates -X- _ O
, -X- _ O
all -X- _ O
parsers -X- _ O
are -X- _ O
sensitive -X- _ O
to -X- _ O
punctuation -X- _ O
injection -X- _ O
. -X- _ O

With -X- _ O
= -X- _ O
0:05 -X- _ O
; -X- _ O

= -X- _ O
0:05 -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
all -X- _ O
parsers -X- _ O
perform -X- _ O
worse -X- _ O
than -X- _ O
in -X- _ O
the -X- _ O
absence -X- _ O
of -X- _ O
punctuation -X- _ O
. -X- _ O

Our -X- _ O
main -X- _ O
observation -X- _ O
is -X- _ O
, -X- _ O
again -X- _ O
, -X- _ O
that -X- _ O
neural -X- _ O
parsers -X- _ O
suffer -X- _ O
higher -X- _ O
relative -X- _ O
increases -X- _ O
in -X- _ O
errors -X- _ O

28ENGLISH -X- _ O
PENN -X- _ O
TREEBANK -X- _ O
( -X- _ O
CORRUPTED -X- _ O
) -X- _ O

O -X- _ O
UT -X- _ O
- -X- _ O
OF -X- _ O
- -X- _ O
DOMAIN -X- _ O
Table -X- _ O
3 -X- _ O
: -X- _ O
Labeled -X- _ B-MetricName
attachment -X- _ I-MetricName
scores -X- _ I-MetricName
with -X- _ O
punctuation -X- _ O
removed -X- _ O
. -X- _ O

All -X- _ O
parsers -X- _ O
suffer -X- _ O
from -X- _ O
absence -X- _ O
of -X- _ O
or -X- _ O
additional -X- _ O
punctuation -X- _ O
. -X- _ O

The -X- _ O
relative -X- _ O
increase -X- _ O
in -X- _ O
error -X- _ O
( -X- _ O
1 -X- _ O
- -X- _ O
BL -X- _ O
1 -X- _ O
- -X- _ O
SYS 1 -X- _ O
; -X- _ O
with -X- _ O
BLperformance -X- _ B-MetricName
on -X- _ O
original -X- _ O
text -X- _ O
; -X- _ O
SYS -X- _ B-MetricName
performance -X- _ I-MetricName
under -X- _ O
NO -X- _ O
PUNCT -X- _ O
and= -X- _ O

0:1 -X- _ O
; -X- _ O
= -X- _ O
0:1 -X- _ O
, -X- _ O
resp -X- _ O
. -X- _ O
) -X- _ O

for -X- _ O
neural -X- _ O
parsers -X- _ O
is -X- _ O
higher -X- _ O
than -X- _ O
for -X- _ O
non -X- _ O
- -X- _ O
neural -X- _ O
parsers -X- _ O
. -X- _ O

G -X- _ B-MetricName
WEB -X- _ I-MetricName
and -X- _ O
FOSTER -X- _ B-MetricName
scores -X- _ O
are -X- _ O
on -X- _ O
development -X- _ O
sentences -X- _ O
( -X- _ O
of -X- _ O
at -X- _ O
least -X- _ O
ﬁve -X- _ O
words -X- _ O
) -X- _ O
with -X- _ O
no -X- _ O
punctuation -X- _ O
. -X- _ O

than -X- _ O
vintage -X- _ O
parsers -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
the -X- _ O
M -X- _ B-MethodName
ALTPARSER -X- _ I-MethodName
is -X- _ O
a -X- _ O
projective -X- _ O
parser -X- _ O
and -X- _ O
therefore -X- _ O
has -X- _ O
a -X- _ O
higher -X- _ O
relative -X- _ O
increase -X- _ O
in -X- _ O
error -X- _ O
; -X- _ O
but -X- _ O
T -X- _ B-MethodName
URBO -X- _ I-MethodName
PARSER -X- _ I-MethodName
is -X- _ O
much -X- _ O
more -X- _ O
robust -X- _ O
than -X- _ O
the -X- _ O
other -X- _ O
parsers -X- _ O
. -X- _ O

That -X- _ O
said -X- _ O
, -X- _ O
it -X- _ O
still -X- _ O
does -X- _ O
much -X- _ O
worse -X- _ O
than -X- _ O
the -X- _ O
UUP -X- _ O
ARSER -X- _ O
trained -X- _ O
without -X- _ O
punctuation -X- _ O
. -X- _ O

Evaluation -X- _ O
on -X- _ O
informal -X- _ O
text -X- _ O
with -X- _ O
non -X- _ O
- -X- _ O
standard -X- _ O
punctuation -X- _ O
We -X- _ O
also -X- _ O
evaluate -X- _ O
the -X- _ O
models -X- _ O
on -X- _ O
sentences -X- _ O
with -X- _ O
non -X- _ O
- -X- _ O
standard -X- _ O
punctuation -X- _ O
in -X- _ O
the -X- _ O
development -X- _ O
sections -X- _ O
in -X- _ O
the -X- _ O
Google -X- _ O
Web -X- _ O
Treebank -X- _ O
with -X- _ O
informal -X- _ O
text -X- _ O
( -X- _ O
from -X- _ O
Yahoo -X- _ O
Answers -X- _ O
and -X- _ O
user -X- _ O
reviews -X- _ O
) -X- _ O
. -X- _ O

Speciﬁcally -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
the -X- _ O
models -X- _ O
on -X- _ O
sentences -X- _ O
with -X- _ O
more -X- _ O
than -X- _ O
one -X- _ O
dot -X- _ O
. -X- _ O

Again -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
neural -X- _ O
dependency -X- _ O
parser -X- _ O
trained -X- _ O
without -X- _ O
punctuation -X- _ O
is -X- _ O
superior -X- _ O
to -X- _ O
the -X- _ O
other -X- _ O
parsers -X- _ O
. -X- _ O

5 -X- _ O
Related -X- _ O
work -X- _ O
Punctuation -X- _ O
in -X- _ O
parsing -X- _ O
Spitkovsky -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2011 -X- _ O
) -X- _ O
introduced -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
splitting -X- _ O
sentences -X- _ O
at -X- _ O
punctuation -X- _ O
and -X- _ O
imposing -X- _ O
parsing -X- _ B-TaskName
restrictions -X- _ O
over -X- _ O
the -X- _ O
fragments -X- _ O
and -X- _ O
observed -X- _ O
signiﬁcant -X- _ O
improvements -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
unsupervised -X- _ O
parsing -X- _ O
. -X- _ O

Ng -X- _ O
and -X- _ O
Curran -X- _ O
( -X- _ O
2015 -X- _ O
) -X- _ O
aim -X- _ O
to -X- _ O
prevent -X- _ O
cascading -X- _ O
errors -X- _ O
by -X- _ O
enforcing -X- _ O
correct -X- _ O
punctuation -X- _ O
arcs -X- _ O
. -X- _ O

They -X- _ O
restrict -X- _ O
themselves -X- _ O
to -X- _ O
projective -X- _ B-TaskName
dependency -X- _ I-TaskName
parsing -X- _ I-TaskName
; -X- _ O
erroneous -X- _ O
punctuation -X- _ O
arcs -X- _ O
do -X- _ O
not -X- _ O
lead -X- _ O
to -X- _ O
cascading -X- _ O
errors -X- _ O
in -X- _ O
non -X- _ O
- -X- _ O
projective -X- _ B-TaskName
dependency -X- _ I-TaskName
parsing -X- _ I-TaskName
. -X- _ O

Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
motivated -X- _ O
by -X- _ O
the -X- _ O
same -X- _ O
observation -X- _ O
, -X- _ O
treat -X- _ O
punctuation -X- _ O
marks -X- _ O
as -X- _ O
properties -X- _ O
of -X- _ O
their -X- _ O
neighboring -X- _ O
words -X- _ O
rather -X- _ O
than -X- _ O
as -X- _ O
individual -X- _ O
tokens -X- _ O
, -X- _ O
showing -X- _ O
improvements -X- _ O
on -X- _ O
in -X- _ O
- -X- _ O
domain -X- _ O
data -X- _ O
. -X- _ O

Breaking -X- _ O
NLP -X- _ O
models -X- _ O
Jia -X- _ O
and -X- _ O
Liang -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
show -X- _ O
how -X- _ O
machine -X- _ O
reading -X- _ O
models -X- _ O
can -X- _ O
easily -X- _ O
be -X- _ O
broken -X- _ O
with -X- _ O
distractor -X- _ O
sentences -X- _ O
at -X- _ O
test -X- _ O
timeand -X- _ O
propose -X- _ O
an -X- _ O
alternative -X- _ O
evaluation -X- _ O
scheme -X- _ O
, -X- _ O
and -X- _ O
Belinkov -X- _ O
and -X- _ O
Bisk -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
show -X- _ O
how -X- _ O
susceptible -X- _ O
character -X- _ O
- -X- _ O
based -X- _ O
machine -X- _ O
translation -X- _ O
models -X- _ O
are -X- _ O
to -X- _ O
noise -X- _ O
. -X- _ O

Both -X- _ O
papers -X- _ O
are -X- _ O
similar -X- _ O
to -X- _ O
ours -X- _ O
in -X- _ O
evaluating -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
models -X- _ O
under -X- _ O
corruptions -X- _ O
of -X- _ O
the -X- _ O
data -X- _ O
. -X- _ O

There -X- _ O
was -X- _ O
recently -X- _ O
a -X- _ O
workshop -X- _ O
dedicated -X- _ O
to -X- _ O
evaluation -X- _ O
of -X- _ O
NLP -X- _ O
models -X- _ O
under -X- _ O
human -X- _ O
adversarial -X- _ O
example -X- _ O
selection -X- _ O
( -X- _ O
Ettinger -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Historically -X- _ O
, -X- _ O
NLP -X- _ O
models -X- _ O
were -X- _ O
rarely -X- _ O
evaluated -X- _ O
on -X- _ O
synthetic -X- _ O
or -X- _ O
otherwise -X- _ O
adversarial -X- _ O
data -X- _ O
, -X- _ O
but -X- _ O
we -X- _ O
believe -X- _ O
this -X- _ O
is -X- _ O
a -X- _ O
fruitful -X- _ O
research -X- _ O
direction -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
largely -X- _ O
a -X- _ O
philosophical -X- _ O
question -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
believe -X- _ O
a -X- _ O
philosophical -X- _ O
argument -X- _ O
is -X- _ O
in -X- _ O
order -X- _ O
. -X- _ O

John -X- _ O
Dewey -X- _ O
( -X- _ O
John -X- _ O
Dewey -X- _ O
, -X- _ O
1910 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
American -X- _ O
philosopher -X- _ O
, -X- _ O
distinguishes -X- _ O
three -X- _ O
modes -X- _ O
of -X- _ O
thinking -X- _ O
: -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
common -X- _ O
reasoning -X- _ O
, -X- _ O
which -X- _ O
identiﬁes -X- _ O
pattern -X- _ O
in -X- _ O
available -X- _ O
, -X- _ O
historical -X- _ O
data -X- _ O
, -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
empirical -X- _ O
thinking -X- _ O
, -X- _ O
which -X- _ O
collects -X- _ O
new -X- _ O
data -X- _ O
to -X- _ O
vary -X- _ O
the -X- _ O
experimental -X- _ O
conditions -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
iii -X- _ O
) -X- _ O
experimental -X- _ O
thinking -X- _ O
, -X- _ O
which -X- _ O
actively -X- _ O
modiﬁes -X- _ O
the -X- _ O
conditions -X- _ O
in -X- _ O
controlled -X- _ O
experiments -X- _ O
to -X- _ O
isolate -X- _ O
the -X- _ O
relevant -X- _ O
variables -X- _ O
. -X- _ O

We -X- _ O
believe -X- _ O
recent -X- _ O
work -X- _ O
on -X- _ O
breaking -X- _ O
NLP -X- _ O
models -X- _ O
is -X- _ O
an -X- _ O
attempt -X- _ O
to -X- _ O
introduce -X- _ O
experimental -X- _ O
thinking -X- _ O
into -X- _ O
NLP -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
otherwise -X- _ O
been -X- _ O
limited -X- _ O
– -X- _ O
or -X- _ O
handicapped -X- _ O
in -X- _ O
Dewey -X- _ O
’s -X- _ O
words -X- _ O
– -X- _ O
by -X- _ O
what -X- _ O
data -X- _ O
happens -X- _ O
to -X- _ O
be -X- _ O
available -X- _ O
. -X- _ O

6 -X- _ O
Conclusions -X- _ O
We -X- _ O
evaluate -X- _ O
the -X- _ O
sensitivity -X- _ O
of -X- _ O
ﬁve -X- _ O
dependency -X- _ O
parsers -X- _ O
to -X- _ O
variations -X- _ O
in -X- _ O
punctuation -X- _ O
, -X- _ O
showing -X- _ O
that -X- _ O
available -X- _ O
neural -X- _ O
parsers -X- _ O
tend -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
sensitive -X- _ O
to -X- _ O
such -X- _ O
variation -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
show -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
that -X- _ O
training -X- _ O
neural -X- _ O
parsers -X- _ O
without -X- _ O
punctuation -X- _ O
provides -X- _ O
a -X- _ O
robust -X- _ O
model -X- _ O
that -X- _ O
is -X- _ O
better -X- _ O
than -X- _ O
any -X- _ O
offthe -X- _ O
- -X- _ O
shelf -X- _ O
parsers -X- _ O
. -X- _ O

29Acknowledgments -X- _ O
We -X- _ O
thank -X- _ O
CSC -X- _ O
in -X- _ O
Helsinki -X- _ O
and -X- _ O
Sigma2 -X- _ O
in -X- _ O
Oslo -X- _ O
for -X- _ O
providing -X- _ O
the -X- _ O
computational -X- _ O
resources -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
experiments -X- _ O
, -X- _ O
through -X- _ O
NeIC -X- _ O
- -X- _ O
NLPL -X- _ O
( -X- _ O
www.nlpl.eu -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
ﬁrst -X- _ O
author -X- _ O
was -X- _ O
supported -X- _ O
by -X- _ O
an -X- _ O
ERC -X- _ O
Starting -X- _ O
Grant -X- _ O
. -X- _ O

References -X- _ O
Scott -X- _ O
Baldwin -X- _ O
and -X- _ O
James -X- _ O
Coady -X- _ O
. -X- _ O

1978 -X- _ O
. -X- _ O

Psycholinguistic -X- _ O
approaches -X- _ O
to -X- _ O
a -X- _ O
theory -X- _ O
of -X- _ O
punctuation -X- _ O
. -X- _ O

Journal -X- _ O
Yonatan -X- _ O
Belinkov -X- _ O
and -X- _ O
Yonatan -X- _ O
Bisk -X- _ O
. -X- _ O

2018 -X- _ O
. -X- _ O

Synthetic -X- _ O
and -X- _ O
Natural -X- _ O
Noise -X- _ O
Both -X- _ O
Break -X- _ O
Neural -X- _ O
Machine -X- _ O
Translation -X- _ O
. -X- _ O

Danqi -X- _ O
Chen -X- _ O
and -X- _ O
Christopher -X- _ O
Manning -X- _ O
. -X- _ O

2014 -X- _ O
. -X- _ O

A -X- _ O
fast -X- _ O
and -X- _ O
accurate -X- _ O
dependency -X- _ O
parser -X- _ O
using -X- _ O
neural -X- _ O
networks -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
EMNLP -X- _ O
, -X- _ O
pages -X- _ O
740–750 -X- _ O
. -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Allyson -X- _ O
Ettinger -X- _ O
, -X- _ O
Sudha -X- _ O
Rao -X- _ O
, -X- _ O
Hal -X- _ O
Daum -X- _ O
´ -X- _ O
e -X- _ O
III -X- _ O
, -X- _ O
and -X- _ O
Emily -X- _ O
M. -X- _ O
Bender -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Towards -X- _ O
Linguistically -X- _ O
Generalizable -X- _ O
NLP -X- _ O
Systems -X- _ O
: -X- _ O
A -X- _ O
Workshop -X- _ O
and -X- _ O
Shared -X- _ O
Task -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
First -X- _ O
Workshop -X- _ O
on -X- _ O
Building -X- _ O
Linguistically -X- _ O
Generalizable -X- _ O
NLP -X- _ O
Systems -X- _ O
, -X- _ O
pages -X- _ O
1–10 -X- _ O
, -X- _ O
Copenhagen -X- _ O
, -X- _ O
Denmark -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Daniel -X- _ O
Fern -X- _ O
´ -X- _ O
andez -X- _ O
- -X- _ O
Gonz -X- _ O
´ -X- _ O
alez -X- _ O
and -X- _ O
Andr -X- _ O
´ -X- _ O
e -X- _ O
F. -X- _ O
T. -X- _ O
Martins -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

Parsing -X- _ O
as -X- _ O
reduction -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
ACL -X- _ O
, -X- _ O
pages -X- _ O
1523–1533 -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Amir -X- _ O
Globerson -X- _ O
and -X- _ O
Sam -X- _ O
Roweis -X- _ O
. -X- _ O

2006 -X- _ O
. -X- _ O

Nightmare -X- _ O
at -X- _ O
test -X- _ O
time -X- _ O
: -X- _ O
robust -X- _ O
learning -X- _ O
by -X- _ O
feature -X- _ O
deletion -X- _ O
. -X- _ O

In -X- _ O
ICML -X- _ O
. -X- _ O

Robin -X- _ O
Jia -X- _ O
and -X- _ O
Percy -X- _ O
Liang -X- _ O
. -X- _ O

2017 -X- _ O
. -X- _ O

Adversarial -X- _ O
Examples -X- _ O
for -X- _ O
Evaluating -X- _ O
Reading -X- _ O
Comprehension -X- _ O
Systems -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
EMNLP -X- _ O
. -X- _ O

Jason -X- _ O
Jo -X- _ O
and -X- _ O
Yoshua -X- _ O
Bengio -X- _ O
. -X- _ O

2017 -X- _ O
. -X- _ O

Measuring -X- _ O
the -X- _ O
tendency -X- _ O
of -X- _ O
CNNs -X- _ O
to -X- _ O
Learn -X- _ O
Surface -X- _ O
Statistical -X- _ O
RegularJohn -X- _ O
Dewey -X- _ O
. -X- _ O
1910 -X- _ O
. -X- _ O

How -X- _ O
we -X- _ O
think -X- _ O
. -X- _ O

Dover -X- _ O
. -X- _ O

Eliyahu -X- _ O
Kiperwasser -X- _ O
and -X- _ O
Yoav -X- _ O
Goldberg -X- _ O
. -X- _ O

2016 -X- _ O
. -X- _ O

Simple -X- _ O
and -X- _ O
accurate -X- _ O
dependency -X- _ B-TaskName
parsing -X- _ I-TaskName
using -X- _ O
bidirectional -X- _ O
LSTM -X- _ O
feature -X- _ O
representations -X- _ O
. -X- _ O

4:313–327 -X- _ O
. -X- _ O

Miryam -X- _ O
de -X- _ O
Lhoneux -X- _ O
, -X- _ O
Yan -X- _ O
Shao -X- _ O
, -X- _ O
Ali -X- _ O
Basirat -X- _ O
, -X- _ O
Eliyahu -X- _ O
Kiperwasser -X- _ O
, -X- _ O
Sara -X- _ O
Stymne -X- _ O
, -X- _ O
Yoav -X- _ O
Goldberg -X- _ O
, -X- _ O
and -X- _ O
Joakim -X- _ O
Nivre -X- _ O
. -X- _ O
2017a -X- _ O
. -X- _ O

From -X- _ O
raw -X- _ O
text -X- _ O
to -X- _ O
universal -X- _ O
dependencies -X- _ O
- -X- _ O
look -X- _ O
, -X- _ O
no -X- _ O
tags -X- _ O
! -X- _ O

pages -X- _ O
207–217 -X- _ O
, -X- _ O
Vancouver -X- _ O
, -X- _ O
Canada -X- _ O
. -X- _ O

Miryam -X- _ O
de -X- _ O
Lhoneux -X- _ O
, -X- _ O
Sara -X- _ O
Stymne -X- _ O
, -X- _ O
and -X- _ O
Joakim -X- _ O
Nivre -X- _ O
. -X- _ O
2017b -X- _ O
. -X- _ O

Arc -X- _ B-TaskName
- -X- _ I-TaskName
hybrid -X- _ I-TaskName
non -X- _ I-TaskName
- -X- _ I-TaskName
projective -X- _ I-TaskName
dependency -X- _ I-TaskName
parsing -X- _ I-TaskName
with -X- _ O
a -X- _ O
static -X- _ O
- -X- _ O
dynamic -X- _ O
oracle -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
15th -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Parsing -X- _ O
Technologies -X- _ O
, -X- _ O
pages -X- _ O
99–104 -X- _ O
, -X- _ O
Pisa -X- _ O
, -X- _ O
Italy -X- _ O
. -X- _ O

Ji -X- _ O
Ma -X- _ O
, -X- _ O
Yue -X- _ O
Zhang -X- _ O
, -X- _ O
and -X- _ O
Jingbo -X- _ O
Zhu -X- _ O
. -X- _ O
2014 -X- _ O
. -X- _ O

Punctuation -X- _ O
processing -X- _ O
for -X- _ O
projective -X- _ B-TaskName
dependency -X- _ I-TaskName
parsing -X- _ I-TaskName
. -X- _ O

InACL -X- _ O
. -X- _ O

Marie -X- _ O
- -X- _ O
Catherine -X- _ O
de -X- _ O
Marneffe -X- _ O
and -X- _ O
Chris -X- _ O
Manning -X- _ O
. -X- _ O
2008 -X- _ O
. -X- _ O

The -X- _ O
Stanford -X- _ O
typed -X- _ O
dependencies -X- _ O
representation -X- _ O
. -X- _ O

In -X- _ O
Coling -X- _ O
Workshop -X- _ O
on -X- _ O
Cross -X- _ O
- -X- _ O
Framework -X- _ O
and -X- _ O
Cross -X- _ O
- -X- _ O
Domain -X- _ O
Parser -X- _ O
Evaluation -X- _ O
. -X- _ O

Dominick -X- _ O
Ng -X- _ O
and -X- _ O
James -X- _ O
Curran -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

Identifying -X- _ O
cascading -X- _ O
errors -X- _ O
using -X- _ O
constraints -X- _ O
in -X- _ O
dependency -X- _ B-TaskName
parsing -X- _ I-TaskName
. -X- _ O

In -X- _ O
ACL -X- _ O
. -X- _ O

Joakim -X- _ O
Nivre -X- _ O
, -X- _ O
Johan -X- _ O
Hall -X- _ O
, -X- _ O
Jens -X- _ O
Nilsson -X- _ O
, -X- _ O
Atanas -X- _ O
Chanev -X- _ O
, -X- _ O
G -X- _ O
¨uls -X- _ O
¸en -X- _ O

Eryi -X- _ O
˘git -X- _ O
, -X- _ O
Sandra -X- _ O
K -X- _ O
¨ubler -X- _ O
, -X- _ O
Svetoslav -X- _ O
Marinov -X- _ O
, -X- _ O
and -X- _ O
Erwin -X- _ O
Marsi -X- _ O
. -X- _ O

2007 -X- _ O
. -X- _ O

MaltParser -X- _ O
: -X- _ O
A -X- _ O
language -X- _ O
- -X- _ O
independent -X- _ O
system -X- _ O
for -X- _ O
data -X- _ B-TaskName
- -X- _ I-TaskName
driven -X- _ I-TaskName
dependency -X- _ I-TaskName
parsing -X- _ I-TaskName
. -X- _ O

Natural -X- _ O
Language -X- _ O
Engineering -X- _ O
, -X- _ O
Madeline -X- _ O
Remse -X- _ O
, -X- _ O
Mohsen -X- _ O
Mesgar -X- _ O
, -X- _ O
and -X- _ O
Michael -X- _ O
Strube -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

Feature -X- _ O
- -X- _ O
rich -X- _ O
error -X- _ O
detection -X- _ O
in -X- _ O
scientiﬁc -X- _ O
writing -X- _ O
using -X- _ O
logistic -X- _ O
regression -X- _ O
. -X- _ O

In -X- _ O
BEA -X- _ O
. -X- _ O

Valentin -X- _ O
Spitkovsky -X- _ O
, -X- _ O
Hiyan -X- _ O
Alshawi -X- _ O
, -X- _ O
and -X- _ O
Dan -X- _ O
Jurafsky -X- _ O
. -X- _ O
2011 -X- _ O
. -X- _ O

Punctuation -X- _ O
: -X- _ O
Making -X- _ O
a -X- _ O
point -X- _ O
in -X- _ O
unsupervised -X- _ O
dependency -X- _ B-TaskName
parsing -X- _ I-TaskName
. -X- _ O

In -X- _ O
CoNLL -X- _ O
. -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
2018 -X- _ O
EMNLP -X- _ O
Workshop -X- _ O
BlackboxNLP -X- _ O
: -X- _ O
Analyzing -X- _ O
and -X- _ O
Interpreting -X- _ O
Neural -X- _ O
Networks -X- _ O
for -X- _ O
NLP -X- _ O
, -X- _ O
pages -X- _ O
30–39 -X- _ O
Brussels -X- _ O
, -X- _ O
Belgium -X- _ O
, -X- _ O
November -X- _ O
1 -X- _ O
, -X- _ O
2018 -X- _ O
. -X- _ O

c -X- _ O

2018 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics30Evaluating -X- _ O
Textual -X- _ O
Representations -X- _ O
through -X- _ O
Image -X- _ O
Generation -X- _ O
Graham -X- _ O
Spinks -X- _ O
Department -X- _ O
of -X- _ O
Computer -X- _ O
Science -X- _ O
KU -X- _ O
Leuven -X- _ O
, -X- _ O
Belgium -X- _ O
graham.spinks -X- _ O
@ -X- _ O
cs.kuleuven.beMarie -X- _ O
- -X- _ O
Francine -X- _ O
Moens -X- _ O
Department -X- _ O
of -X- _ O
Computer -X- _ O
Science -X- _ O
KU -X- _ O
Leuven -X- _ O
, -X- _ O
Belgium -X- _ O
sien.moens -X- _ O
@ -X- _ O
cs.kuleuven.be -X- _ O

Abstract -X- _ O
We -X- _ O
present -X- _ O
a -X- _ O
methodology -X- _ O
for -X- _ O
determining -X- _ B-TaskName
the -X- _ I-TaskName
quality -X- _ I-TaskName
of -X- _ I-TaskName
textual -X- _ I-TaskName
representations -X- _ I-TaskName
through -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
generate -X- _ O
images -X- _ O
from -X- _ O
them -X- _ O
. -X- _ O

Continuous -X- _ O
representations -X- _ O
of -X- _ O
textual -X- _ O
input -X- _ O
are -X- _ O
ubiquitous -X- _ O
in -X- _ O
modern -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
techniques -X- _ O
either -X- _ O
at -X- _ O
the -X- _ O
core -X- _ O
of -X- _ O
machine -X- _ O
learning -X- _ O
algorithms -X- _ O
or -X- _ O
as -X- _ O
the -X- _ O
by -X- _ O
- -X- _ O
product -X- _ O
at -X- _ O
any -X- _ O
given -X- _ O
layer -X- _ O
of -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
. -X- _ O

While -X- _ O
current -X- _ O
techniques -X- _ O
to -X- _ O
evaluate -X- _ O
such -X- _ O
representations -X- _ O
focus -X- _ O
on -X- _ O
their -X- _ O
performance -X- _ O
on -X- _ O
particular -X- _ O
tasks -X- _ O
, -X- _ O
they -X- _ O
do -X- _ O
n’t -X- _ O
provide -X- _ O
a -X- _ O
clear -X- _ O
understanding -X- _ O
of -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
informational -X- _ O
detail -X- _ O
that -X- _ O
is -X- _ O
stored -X- _ O
within -X- _ O
them -X- _ O
, -X- _ O
especially -X- _ O
their -X- _ O
ability -X- _ O
to -X- _ O
represent -X- _ O
spatial -X- _ O
information -X- _ O
. -X- _ O

The -X- _ O
central -X- _ O
premise -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
is -X- _ O
that -X- _ O
visual -X- _ O
inspection -X- _ O
or -X- _ O
analysis -X- _ O
is -X- _ O
the -X- _ O
most -X- _ O
convenient -X- _ O
method -X- _ O
to -X- _ O
quickly -X- _ O
and -X- _ O
accurately -X- _ O
determine -X- _ O
information -X- _ O
content -X- _ O
. -X- _ O

Through -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
neural -X- _ O
networks -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
new -X- _ O
technique -X- _ O
to -X- _ O
compare -X- _ B-TaskName
the -X- _ I-TaskName
quality -X- _ I-TaskName
of -X- _ I-TaskName
textual -X- _ I-TaskName
representations -X- _ I-TaskName
by -X- _ O
visualizing -X- _ O
their -X- _ O
information -X- _ O
content -X- _ O
. -X- _ O

The -X- _ O
method -X- _ O
is -X- _ O
illustrated -X- _ O
on -X- _ O
a -X- _ O
medical -X- _ O
dataset -X- _ O
where -X- _ O
the -X- _ O
correct -X- _ O
representation -X- _ O
of -X- _ O
spatial -X- _ O
information -X- _ O
and -X- _ O
shorthands -X- _ O
are -X- _ O
of -X- _ O
particular -X- _ O
importance -X- _ O
. -X- _ O

For -X- _ O
four -X- _ O
different -X- _ O
well -X- _ O
- -X- _ O
known -X- _ O
textual -X- _ O
representations -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
with -X- _ O
a -X- _ O
quantitative -X- _ O
analysis -X- _ O
that -X- _ O
some -X- _ O
representations -X- _ O
are -X- _ O
consistently -X- _ O
able -X- _ O
to -X- _ O
deliver -X- _ O
higher -X- _ O
quality -X- _ O
visualizations -X- _ O
of -X- _ O
the -X- _ O
information -X- _ O
content -X- _ O
. -X- _ O

Additionally -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
quantitative -X- _ O
analysis -X- _ O
technique -X- _ O
correlates -X- _ O
with -X- _ O
the -X- _ O
judgment -X- _ O
of -X- _ O
a -X- _ O
human -X- _ O
expert -X- _ O
evaluator -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
alignment -X- _ O
. -X- _ O

1 -X- _ O
Introduction -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
a -X- _ O
method -X- _ O
is -X- _ O
proposed -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
a -X- _ O
textual -X- _ O
representation -X- _ O
by -X- _ O
conditioning -X- _ O
an -X- _ O
image -X- _ O
generation -X- _ O
network -X- _ O
on -X- _ O
it -X- _ O
. -X- _ O

Neural -X- _ O
networks -X- _ O
implicitly -X- _ O
construct -X- _ O
representations -X- _ O
of -X- _ O
a -X- _ O
textual -X- _ O
input -X- _ O
by -X- _ O
learning -X- _ O
which -X- _ O
features -X- _ O
are -X- _ O
important -X- _ O
for -X- _ O
the -X- _ O
task -X- _ O
at -X- _ O
hand -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
not -X- _ O
immediately -X- _ O
possible -X- _ O
however -X- _ O
to -X- _ O
assess -X- _ O
the -X- _ O
levelof -X- _ O
detail -X- _ O
and -X- _ O
structure -X- _ O
that -X- _ O
is -X- _ O
retained -X- _ O
in -X- _ O
such -X- _ O
a -X- _ O
representation -X- _ O
. -X- _ O

Many -X- _ O
systems -X- _ O
often -X- _ O
complement -X- _ O
or -X- _ O
replace -X- _ O
the -X- _ O
input -X- _ O
with -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
representations -X- _ O
that -X- _ O
have -X- _ O
the -X- _ O
advantage -X- _ O
of -X- _ O
being -X- _ O
constructed -X- _ O
with -X- _ O
a -X- _ O
larger -X- _ O
unlabeled -X- _ O
corpus -X- _ O
. -X- _ O

Depending -X- _ O
on -X- _ O
the -X- _ O
task -X- _ O
, -X- _ O
this -X- _ O
practice -X- _ O
sometimes -X- _ O
signiﬁcantly -X- _ O
improves -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
network -X- _ O
( -X- _ O
Turian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2010 -X- _ O
) -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
one -X- _ O
hand -X- _ O
, -X- _ O
this -X- _ O
is -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
a -X- _ O
larger -X- _ O
unlabeled -X- _ O
corpus -X- _ O
which -X- _ O
reduces -X- _ O
data -X- _ O
sparsity -X- _ O
and -X- _ O
thus -X- _ O
improves -X- _ O
generalization -X- _ O
accuracy -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
representations -X- _ O
often -X- _ O
contain -X- _ O
higherlevel -X- _ O
features -X- _ O
that -X- _ O
are -X- _ O
fundamental -X- _ O
for -X- _ O
the -X- _ O
task -X- _ O
they -X- _ O
are -X- _ O
trained -X- _ O
for -X- _ O
. -X- _ O

A -X- _ O
neural -X- _ O
network -X- _ O
in -X- _ O
a -X- _ O
separate -X- _ O
task -X- _ O
can -X- _ O
thus -X- _ O
rely -X- _ O
on -X- _ O
those -X- _ O
features -X- _ O
without -X- _ O
having -X- _ O
to -X- _ O
discover -X- _ O
them -X- _ O
all -X- _ O
over -X- _ O
again -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
ﬁeld -X- _ O
of -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
advances -X- _ O
and -X- _ O
machine -X- _ O
learning -X- _ O
models -X- _ O
expand -X- _ O
to -X- _ O
include -X- _ O
multimodal -X- _ O
information -X- _ O
, -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
understanding -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
detail -X- _ O
and -X- _ O
information -X- _ O
that -X- _ O
is -X- _ O
retained -X- _ O
in -X- _ O
a -X- _ O
textual -X- _ O
representation -X- _ O
only -X- _ O
grows -X- _ O
. -X- _ O

Obtained -X- _ O
representations -X- _ O
can -X- _ O
be -X- _ O
employed -X- _ O
in -X- _ O
additional -X- _ O
tasks -X- _ O
( -X- _ O
for -X- _ O
example -X- _ O
generation -X- _ O
, -X- _ O
translation -X- _ O
, -X- _ O
summarization -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O
) -X- _ O

depending -X- _ O
on -X- _ O
their -X- _ O
ability -X- _ O
to -X- _ O
capture -X- _ O
certain -X- _ O
types -X- _ O
of -X- _ O
information -X- _ O
. -X- _ O

The -X- _ O
medical -X- _ O
domain -X- _ O
in -X- _ O
particular -X- _ O
might -X- _ O
beneﬁt -X- _ O
from -X- _ O
a -X- _ O
better -X- _ O
understanding -X- _ O
of -X- _ O
representations -X- _ O
as -X- _ O
the -X- _ O
industry -X- _ O
moves -X- _ O
to -X- _ O
adopt -X- _ O
deep -X- _ O
learning -X- _ O
methods -X- _ O
in -X- _ O
increasingly -X- _ O
intricate -X- _ O
applications -X- _ O
and -X- _ O
researchers -X- _ O
attempt -X- _ O
to -X- _ O
extract -X- _ O
and -X- _ O
utilize -X- _ O
more -X- _ O
complex -X- _ O
information -X- _ O
structures -X- _ O
. -X- _ O

An -X- _ O
example -X- _ O
is -X- _ O
spatial -X- _ O
information -X- _ O
which -X- _ O
is -X- _ O
an -X- _ O
important -X- _ O
quantity -X- _ O
in -X- _ O
many -X- _ O
natural -X- _ O
language -X- _ O
applications -X- _ O
, -X- _ O
yet -X- _ O
no -X- _ O
explicit -X- _ O
methodology -X- _ O
exists -X- _ O
that -X- _ O
indicates -X- _ O
to -X- _ O
what -X- _ O
extent -X- _ O
that -X- _ O
information -X- _ O
is -X- _ O
present -X- _ O
in -X- _ O
textual -X- _ O
representations -X- _ O
. -X- _ O

In -X- _ O
many -X- _ O
medical -X- _ O
settings -X- _ O
, -X- _ O
a -X- _ O
correct -X- _ O
understanding -X- _ O
and -X- _ O
representation -X- _ O
of -X- _ O
such -X- _ O
information -X- _ O
is -X- _ O
crucial -X- _ O
. -X- _ O

In -X- _ O
thorax -X- _ O
radiography -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
focus -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
textual -X- _ O
captions -X- _ O
often -X- _ O
include -X- _ O
detailed -X- _ O
ﬁndings -X- _ O
which -X- _ O
relate -X- _ O
to -X- _ O
speciﬁc -X- _ O
areas -X- _ O
in -X- _ O
an -X- _ O
X -X- _ O
- -X- _ O
Ray -X- _ O
. -X- _ O

Clinical -X- _ O
texts -X- _ O
in -X- _ O
general -X- _ O
, -X- _ O
add -X- _ O
an -X- _ O
extra -X- _ O
level -X- _ O
of -X- _ O
com- -X- _ O

31plexity -X- _ O
as -X- _ O
they -X- _ O
often -X- _ O
lack -X- _ O
syntactic -X- _ O
structure -X- _ O
and -X- _ O
employ -X- _ O
many -X- _ O
shorthands -X- _ O
. -X- _ O

Images -X- _ O
differ -X- _ O
from -X- _ O
texts -X- _ O
in -X- _ O
the -X- _ O
sense -X- _ O
that -X- _ O
the -X- _ O
retained -X- _ O
information -X- _ O
and -X- _ O
generalization -X- _ O
of -X- _ O
a -X- _ O
representation -X- _ O
are -X- _ O
immediately -X- _ O
apparent -X- _ O
for -X- _ O
a -X- _ O
human -X- _ O
observer -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
not -X- _ O
surprising -X- _ O
that -X- _ O
the -X- _ O
’ -X- _ O
human -X- _ B-MetricName
perceptual -X- _ I-MetricName
score -X- _ I-MetricName
’ -X- _ O
is -X- _ O
a -X- _ O
frequently -X- _ O
used -X- _ O
metric -X- _ O
to -X- _ O
evaluate -X- _ O
image -X- _ O
generation -X- _ O
systems -X- _ O
( -X- _ O
Borji -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
method -X- _ O
to -X- _ O
assess -X- _ B-TaskName
the -X- _ I-TaskName
quality -X- _ I-TaskName
of -X- _ I-TaskName
textual -X- _ I-TaskName
representations -X- _ I-TaskName
. -X- _ O

By -X- _ O
creating -X- _ O
images -X- _ O
from -X- _ O
different -X- _ O
textual -X- _ O
representations -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
some -X- _ O
representations -X- _ O
lack -X- _ O
the -X- _ O
necessary -X- _ O
information -X- _ O
to -X- _ O
lead -X- _ O
to -X- _ O
detailed -X- _ O
high -X- _ O
- -X- _ O
quality -X- _ O
images -X- _ O
. -X- _ O

The -X- _ O
textual -X- _ O
representations -X- _ O
are -X- _ O
evaluated -X- _ O
both -X- _ O
by -X- _ O
comparing -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
produced -X- _ O
images -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
images -X- _ O
in -X- _ O
the -X- _ O
test -X- _ O
data -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
alignment -X- _ O
between -X- _ O
images -X- _ O
and -X- _ O
captions -X- _ O
. -X- _ O

The -X- _ O
outcome -X- _ O
is -X- _ O
determined -X- _ O
both -X- _ O
by -X- _ O
a -X- _ O
qualitative -X- _ O
( -X- _ O
human -X- _ B-MetricName
perceptual -X- _ I-MetricName
scores -X- _ I-MetricName
) -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
a -X- _ O
quantitative -X- _ O
( -X- _ O
divergence -X- _ B-MetricName
scores -X- _ I-MetricName
) -X- _ O
measure -X- _ O
. -X- _ O

To -X- _ O
calculate -X- _ O
the -X- _ O
divergence -X- _ B-MetricName
scores -X- _ I-MetricName
, -X- _ O
we -X- _ O
rely -X- _ O
on -X- _ O
the -X- _ O
methodology -X- _ O
that -X- _ O
estimates -X- _ O
distance -X- _ O
between -X- _ O
two -X- _ O
distributions -X- _ O
as -X- _ O
introduced -X- _ O
by -X- _ O
( -X- _ O
Danihelka -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
extend -X- _ O
it -X- _ O
to -X- _ O
estimate -X- _ O
how -X- _ O
well -X- _ O
image -X- _ O
and -X- _ O
text -X- _ O
are -X- _ O
aligned -X- _ O
in -X- _ O
the -X- _ O
generated -X- _ O
content -X- _ O
. -X- _ O

As -X- _ O
we -X- _ O
show -X- _ O
in -X- _ O
the -X- _ O
results -X- _ O
, -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
architectures -X- _ O
are -X- _ O
indeed -X- _ O
suitable -X- _ O
to -X- _ O
get -X- _ O
an -X- _ O
immediate -X- _ O
visual -X- _ O
estimate -X- _ O
of -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
representation -X- _ O
and -X- _ O
the -X- _ O
information -X- _ O
contained -X- _ O
within -X- _ O
. -X- _ O

We -X- _ O
will -X- _ O
evaluate -X- _ O
several -X- _ O
common -X- _ O
textual -X- _ O
representations -X- _ O
that -X- _ O
were -X- _ O
constructed -X- _ O
with -X- _ O
unsupervised -X- _ O
learning -X- _ O
techniques -X- _ O
on -X- _ O
both -X- _ O
a -X- _ O
relatively -X- _ O
straightforward -X- _ O
conditional -X- _ B-MethodName
GAN -X- _ I-MethodName
as -X- _ O
well -X- _ O
as -X- _ O
on -X- _ O
a -X- _ O
more -X- _ O
advanced -X- _ O
StackGan -X- _ B-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
which -X- _ O
uses -X- _ O
several -X- _ O
stages -X- _ O
and -X- _ O
a -X- _ O
conditioning -X- _ O
mechanism -X- _ O
that -X- _ O
augments -X- _ O
the -X- _ O
textual -X- _ O
representation -X- _ O
. -X- _ O

The -X- _ O
contributions -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
are -X- _ O
: -X- _ O
The -X- _ O
formulation -X- _ O
of -X- _ O
a -X- _ O
methodology -X- _ O
to -X- _ O
visualize -X- _ B-TaskName
and -X- _ I-TaskName
evaluate -X- _ I-TaskName
the -X- _ I-TaskName
information -X- _ I-TaskName
and -X- _ I-TaskName
quality -X- _ I-TaskName
of -X- _ I-TaskName
different -X- _ I-TaskName
textual -X- _ I-TaskName
representations -X- _ I-TaskName
. -X- _ O

The -X- _ O
extension -X- _ O
of -X- _ O
a -X- _ O
GAN -X- _ B-MethodName
evaluation -X- _ O
measure -X- _ O
to -X- _ O
evaluate -X- _ O
alignment -X- _ O
of -X- _ O
output -X- _ O
with -X- _ O
conditional -X- _ O
information -X- _ O
. -X- _ O

2 -X- _ O
Motivation -X- _ O
and -X- _ O
background -X- _ O
To -X- _ O
understand -X- _ O
the -X- _ O
motivation -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
necessary -X- _ O
to -X- _ O
understand -X- _ O
some -X- _ O
background -X- _ O
on -X- _ O
the -X- _ O
different -X- _ O
types -X- _ O
of -X- _ O
textual -X- _ O
representations -X- _ O
and -X- _ O
why -X- _ O
better -X- _ O
evaluation -X- _ O
methods -X- _ O
are -X- _ O
necessary -X- _ O
. -X- _ O

As -X- _ O
weuse -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
models -X- _ O
for -X- _ O
evaluation -X- _ O
purposes -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
discuss -X- _ O
related -X- _ O
research -X- _ O
in -X- _ O
that -X- _ O
area -X- _ O
. -X- _ O

2.1 -X- _ O
Textual -X- _ O
Representations -X- _ O

A -X- _ O
textual -X- _ O
representation -X- _ O
is -X- _ O
usually -X- _ O
a -X- _ O
vector -X- _ O
associated -X- _ O
with -X- _ O
a -X- _ O
piece -X- _ O
of -X- _ O
text -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
be -X- _ O
a -X- _ O
character -X- _ O
, -X- _ O
word -X- _ O
, -X- _ O
sentence -X- _ O
, -X- _ O
paragraph -X- _ O
or -X- _ O
document -X- _ O
. -X- _ O

In -X- _ O
its -X- _ O
simplest -X- _ O
form -X- _ O
, -X- _ O
a -X- _ O
representation -X- _ O
can -X- _ O
be -X- _ O
a -X- _ O
symbolic -X- _ O
ID -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
in -X- _ O
a -X- _ O
one -X- _ O
- -X- _ O
hot -X- _ O
vector -X- _ O
where -X- _ O
each -X- _ O
dimension -X- _ O
represents -X- _ O
an -X- _ O
ID -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
essentially -X- _ O
a -X- _ O
discrete -X- _ O
, -X- _ O
symbolic -X- _ O
representation -X- _ O
that -X- _ O
is -X- _ O
very -X- _ O
sparse -X- _ O
in -X- _ O
information -X- _ O
as -X- _ O
by -X- _ O
deﬁnition -X- _ O
only -X- _ O
one -X- _ O
dimension -X- _ O
is -X- _ O
non -X- _ O
- -X- _ O
zero -X- _ O
. -X- _ O

They -X- _ O
are -X- _ O
also -X- _ O
somewhat -X- _ O
arbitrary -X- _ O
in -X- _ O
the -X- _ O
sense -X- _ O
that -X- _ O
two -X- _ O
texts -X- _ O
that -X- _ O
are -X- _ O
near -X- _ O
each -X- _ O
other -X- _ O
in -X- _ O
the -X- _ O
code -X- _ O
space -X- _ O
do -X- _ O
n’t -X- _ O
necessarily -X- _ O
share -X- _ O
a -X- _ O
similar -X- _ O
meaning -X- _ O
or -X- _ O
syntax -X- _ O
. -X- _ O

More -X- _ O
efﬁcient -X- _ O
methods -X- _ O
assign -X- _ O
particular -X- _ O
handengineered -X- _ O
or -X- _ O
automatically -X- _ O
extracted -X- _ O
features -X- _ O
to -X- _ O
a -X- _ O
lower -X- _ O
- -X- _ O
dimensional -X- _ O
vector -X- _ O
. -X- _ O

One -X- _ O
feature -X- _ O
can -X- _ O
be -X- _ O
stored -X- _ O
in -X- _ O
exactly -X- _ O
one -X- _ O
dimension -X- _ O
or -X- _ O
it -X- _ O
could -X- _ O
be -X- _ O
shared -X- _ O
over -X- _ O
many -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
we -X- _ O
will -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
latter -X- _ O
, -X- _ O
also -X- _ O
referred -X- _ O
to -X- _ O
as -X- _ O
distributed -X- _ O
representations -X- _ O
or -X- _ O
word -X- _ O
embeddings -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
traditional -X- _ O
method -X- _ O
to -X- _ O
represent -X- _ O
sentences -X- _ O
in -X- _ O
recent -X- _ O
neural -X- _ O
network -X- _ O
related -X- _ O
research -X- _ O
. -X- _ O

They -X- _ O
are -X- _ O
dense -X- _ O
, -X- _ O
low -X- _ O
- -X- _ O
dimensional -X- _ O
and -X- _ O
real -X- _ O
- -X- _ O
valued -X- _ O
( -X- _ O
Turian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2010 -X- _ O
) -X- _ O
. -X- _ O

Texts -X- _ O
that -X- _ O
contain -X- _ O
similar -X- _ O
concepts -X- _ O
or -X- _ O
meaning -X- _ O
for -X- _ O
a -X- _ O
typical -X- _ O
task -X- _ O
end -X- _ O
up -X- _ O
near -X- _ O
each -X- _ O
other -X- _ O
in -X- _ O
such -X- _ O
a -X- _ O
distributed -X- _ O
representation -X- _ O
space -X- _ O
which -X- _ O
serves -X- _ O
as -X- _ O
a -X- _ O
proxy -X- _ O
for -X- _ O
generalized -X- _ O
, -X- _ O
semantic -X- _ O
information -X- _ O
storage -X- _ O
. -X- _ O

Word -X- _ O
embeddings -X- _ O
can -X- _ O
be -X- _ O
built -X- _ O
with -X- _ O
unsupervised -X- _ O
training -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
by -X- _ O
leveraging -X- _ O
positional -X- _ O
information -X- _ O
of -X- _ O
texts -X- _ O
in -X- _ O
a -X- _ O
corpus -X- _ O
; -X- _ O
with -X- _ O
weakly -X- _ O
supervised -X- _ O
training -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
in -X- _ O
an -X- _ O
adversarial -X- _ O
setting -X- _ O
; -X- _ O
or -X- _ O
with -X- _ O
supervision -X- _ O
of -X- _ O
output -X- _ O
labels -X- _ O
. -X- _ O

While -X- _ O
this -X- _ O
paper -X- _ O
focuses -X- _ O
on -X- _ O
unsupervised -X- _ O
and -X- _ O
weakly -X- _ O
supervised -X- _ O
methods -X- _ O
only -X- _ O
, -X- _ O
the -X- _ O
methods -X- _ O
that -X- _ O
are -X- _ O
described -X- _ O
here -X- _ O
are -X- _ O
applicable -X- _ O
to -X- _ O
supervised -X- _ O
representations -X- _ O
as -X- _ O
well -X- _ O
. -X- _ O

Well -X- _ O
- -X- _ O
known -X- _ O
methods -X- _ O
of -X- _ O
creating -X- _ O
word -X- _ O
embeddings -X- _ O
are -X- _ O
the -X- _ O
word2vec -X- _ O
algorithms -X- _ O
, -X- _ O
introduced -X- _ O
by -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2013a -X- _ O
) -X- _ O
. -X- _ O

Word -X- _ O
embeddings -X- _ O
are -X- _ O
usually -X- _ O
constructed -X- _ O
with -X- _ O
neural -X- _ O
networks -X- _ O
that -X- _ O
predict -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
a -X- _ O
word -X- _ O
in -X- _ O
a -X- _ O
text -X- _ O
document -X- _ O
. -X- _ O

They -X- _ O
are -X- _ O
able -X- _ O
to -X- _ O
scale -X- _ O
to -X- _ O
large -X- _ O
training -X- _ O
corpora -X- _ O
, -X- _ O
thus -X- _ O
representing -X- _ O
large -X- _ O
amounts -X- _ O
of -X- _ O
information -X- _ O
and -X- _ O
features -X- _ O
in -X- _ O
a -X- _ O
relatively -X- _ O
small -X- _ O
amount -X- _ O
of -X- _ O
dimensions -X- _ O
. -X- _ O

While -X- _ O
word2vec -X- _ O
word -X- _ O
embeddings -X- _ O
solely -X- _ O
operate -X- _ O
on -X- _ O
the -X- _ O
word -X- _ O
level -X- _ O
, -X- _ O
extensions -X- _ O
have -X- _ O
been -X- _ O
made -X- _ O
that -X- _ O
include -X- _ O
information -X- _ O
at -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
characters -X- _ O
( -X- _ O
e.g. -X- _ O
char -X- _ O
- -X- _ O
CNN -X- _ O
- -X- _ O
RNN -X- _ O
( -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
or -X- _ O
at -X- _ O
higher -X- _ O

32levels -X- _ O
such -X- _ O
as -X- _ O
sentences -X- _ O
, -X- _ O
paragraphs -X- _ O
or -X- _ O
documents -X- _ O
. -X- _ O

( -X- _ O
e.g. -X- _ O
skipthought -X- _ O
vectors -X- _ O
( -X- _ O
Kiros -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
or -X- _ O
doc2vec -X- _ O
( -X- _ O
Le -X- _ O
and -X- _ O
Mikolov -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

While -X- _ O
these -X- _ O
methods -X- _ O
usually -X- _ O
are -X- _ O
trained -X- _ O
on -X- _ O
tasks -X- _ O
that -X- _ O
reproduce -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
a -X- _ O
textual -X- _ O
component -X- _ O
, -X- _ O
autoencoders -X- _ O
( -X- _ O
AE -X- _ O
) -X- _ O
are -X- _ O
trained -X- _ O
to -X- _ O
recreate -X- _ O
the -X- _ O
original -X- _ O
text -X- _ O
in -X- _ O
its -X- _ O
entirety -X- _ O
while -X- _ O
implicitly -X- _ O
learning -X- _ O
a -X- _ O
compact -X- _ O
, -X- _ O
distributed -X- _ O
representation -X- _ O
as -X- _ O
well -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
along -X- _ O
the -X- _ O
way -X- _ O
. -X- _ O

A -X- _ O
recent -X- _ O
method -X- _ O
that -X- _ O
builds -X- _ O
on -X- _ O
the -X- _ O
autoencoder -X- _ O
approach -X- _ O
is -X- _ O
an -X- _ O
Adversarially -X- _ B-MethodName
Regularized -X- _ I-MethodName
Autoencoder -X- _ I-MethodName
( -X- _ O
ARAE -X- _ B-MethodName
) -X- _ O
( -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Here -X- _ O
, -X- _ O
the -X- _ O
representation -X- _ O
is -X- _ O
built -X- _ O
explicitly -X- _ O
from -X- _ O
an -X- _ O
encoder -X- _ O
that -X- _ O
is -X- _ O
trained -X- _ O
as -X- _ O
part -X- _ O
of -X- _ O
an -X- _ O
autoencoder -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
a -X- _ O
conventional -X- _ O
Generative -X- _ B-MethodName
Adversarial -X- _ I-MethodName
Network -X- _ I-MethodName
( -X- _ O
GAN -X- _ B-MethodName
) -X- _ O
. -X- _ O

Such -X- _ O
representations -X- _ O
contain -X- _ O
semantic -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
sentence -X- _ O
but -X- _ O
also -X- _ O
discriminative -X- _ O
information -X- _ O
that -X- _ O
allows -X- _ O
the -X- _ O
adversarial -X- _ O
network -X- _ O
to -X- _ O
distinguish -X- _ O
real -X- _ O
samples -X- _ O
from -X- _ O
fake -X- _ O
ones -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
a -X- _ O
smoother -X- _ O
semantic -X- _ O
transition -X- _ O
is -X- _ O
apparent -X- _ O
while -X- _ O
traversing -X- _ O
the -X- _ O
representation -X- _ O
space -X- _ O
when -X- _ O
compared -X- _ O
to -X- _ O
an -X- _ O
autoencoder -X- _ O
. -X- _ O

Spinks -X- _ O
and -X- _ O
Moens -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
have -X- _ O
applied -X- _ O
this -X- _ O
technique -X- _ O
to -X- _ O
create -X- _ O
textual -X- _ O
representations -X- _ O
of -X- _ O
X -X- _ O
- -X- _ O
Ray -X- _ O
captions -X- _ O
and -X- _ O
generate -X- _ O
textual -X- _ O
output -X- _ O
with -X- _ O
low -X- _ O
perplexity -X- _ O
. -X- _ O

The -X- _ O
quality -X- _ O
of -X- _ O
distributed -X- _ O
vectors -X- _ O
can -X- _ O
be -X- _ O
assessed -X- _ O
with -X- _ O
similarity -X- _ O
tasks -X- _ O
that -X- _ O
give -X- _ O
a -X- _ O
rough -X- _ O
measure -X- _ O
of -X- _ O
semantic -X- _ O
and -X- _ O
syntactic -X- _ O
information -X- _ O
( -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013a -X- _ O
, -X- _ O
c -X- _ O
) -X- _ O
but -X- _ O
studies -X- _ O
by -X- _ O
Faruqui -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2016 -X- _ O
) -X- _ O
and -X- _ O
Linzen -X- _ O
( -X- _ O
2016 -X- _ O
) -X- _ O
indeed -X- _ O
suggest -X- _ O
that -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
word -X- _ O
similarity -X- _ O
tasks -X- _ O
for -X- _ O
the -X- _ O
evaluation -X- _ O
of -X- _ O
word -X- _ O
vectors -X- _ O
is -X- _ O
problematic -X- _ O
and -X- _ O
may -X- _ O
lead -X- _ O
to -X- _ O
incorrect -X- _ O
inferences -X- _ O
. -X- _ O

Schnabel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2015 -X- _ O
) -X- _ O
have -X- _ O
evaluated -X- _ O
embeddings -X- _ O
with -X- _ O
a -X- _ O
range -X- _ O
of -X- _ O
methods -X- _ O
, -X- _ O
both -X- _ O
intrinsic -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
semantic -X- _ O
and -X- _ O
syntactic -X- _ O
similarity -X- _ O
, -X- _ O
and -X- _ O
extrinsic -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
noun -X- _ O
phrase -X- _ O
chunking -X- _ O
and -X- _ O
sentiment -X- _ O
classiﬁcation -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
extrinsic -X- _ O
tasks -X- _ O
, -X- _ O
they -X- _ O
found -X- _ O
that -X- _ O
different -X- _ O
representations -X- _ O
performed -X- _ O
best -X- _ O
for -X- _ O
different -X- _ O
tasks -X- _ O
, -X- _ O
suggesting -X- _ O
that -X- _ O
perhaps -X- _ O
there -X- _ O
is -X- _ O
n’t -X- _ O
one -X- _ O
optimal -X- _ O
representation -X- _ O
for -X- _ O
all -X- _ O
tasks -X- _ O
. -X- _ O

Such -X- _ O
studies -X- _ O
suggest -X- _ O
that -X- _ O
better -X- _ O
methodologies -X- _ O
and -X- _ O
more -X- _ O
research -X- _ O
is -X- _ O
needed -X- _ O
into -X- _ O
methods -X- _ O
that -X- _ O
accurately -X- _ O
assess -X- _ O
the -X- _ O
value -X- _ O
of -X- _ O
different -X- _ O
continuous -X- _ O
representations -X- _ O
. -X- _ O

This -X- _ O
paper -X- _ O
addresses -X- _ O
this -X- _ O
by -X- _ O
focusing -X- _ O
on -X- _ O
the -X- _ O
evaluation -X- _ O
of -X- _ O
the -X- _ O
information -X- _ O
content -X- _ O
of -X- _ O
the -X- _ O
representation -X- _ O
rather -X- _ O
than -X- _ O
any -X- _ O
task -X- _ O
- -X- _ O
oriented -X- _ O
metric -X- _ O
. -X- _ O

Lazaridou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2015 -X- _ O
) -X- _ O
also -X- _ O
worked -X- _ O
towards -X- _ O
a -X- _ O
visualization -X- _ O
method -X- _ O
for -X- _ O
text -X- _ O
representations -X- _ O
by -X- _ O
averaging -X- _ O
images -X- _ O
of -X- _ O
the -X- _ O
nearest -X- _ O
neighbors -X- _ O
vectors -X- _ O
after -X- _ O
a -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
mapping -X- _ O
. -X- _ O

Contrary -X- _ O
to -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
their -X- _ O
approach -X- _ O
did -X- _ O
not -X- _ O
in -X- _ O
- -X- _ O
clude -X- _ O
any -X- _ O
evaluation -X- _ O
mechanism -X- _ O
of -X- _ O
the -X- _ O
outcome -X- _ O
and -X- _ O
only -X- _ O
focused -X- _ O
on -X- _ O
individual -X- _ O
words -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
construct -X- _ O
distributed -X- _ O
representations -X- _ O
of -X- _ O
sentences -X- _ O
with -X- _ O
several -X- _ O
unsupervised -X- _ O
methods -X- _ O
mentioned -X- _ O
above -X- _ O
. -X- _ O

Subsequently -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
new -X- _ O
methodology -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
learned -X- _ O
word -X- _ O
embeddings -X- _ O
by -X- _ O
generating -X- _ O
images -X- _ O
from -X- _ O
them -X- _ O
, -X- _ O
thus -X- _ O
visualizing -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
detail -X- _ O
and -X- _ O
information -X- _ O
retained -X- _ O
in -X- _ O
the -X- _ O
different -X- _ O
embeddings -X- _ O
. -X- _ O

To -X- _ O
understand -X- _ O
our -X- _ O
methodology -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
useful -X- _ O
to -X- _ O
discuss -X- _ O
some -X- _ O
background -X- _ O
on -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
models -X- _ O
and -X- _ O
, -X- _ O
more -X- _ O
in -X- _ O
general -X- _ O
, -X- _ O
generative -X- _ O
models -X- _ O
. -X- _ O

2.2 -X- _ O
Generative -X- _ O
models -X- _ O
Recent -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
models -X- _ O
rely -X- _ O
on -X- _ O
advances -X- _ O
in -X- _ O
generative -X- _ O
models -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
probabilistic -X- _ O
models -X- _ O
that -X- _ O
estimate -X- _ O
a -X- _ O
distribution -X- _ O
given -X- _ O
a -X- _ O
certain -X- _ O
input -X- _ O
. -X- _ O

Such -X- _ O
generative -X- _ O
systems -X- _ O
have -X- _ O
shown -X- _ O
impressive -X- _ O
progress -X- _ O
in -X- _ O
the -X- _ O
creation -X- _ O
of -X- _ O
realistic -X- _ O
data -X- _ O
, -X- _ O
most -X- _ O
notably -X- _ O
with -X- _ O
Generative -X- _ B-MethodName
Adversarial -X- _ I-MethodName
Networks -X- _ I-MethodName
( -X- _ O
GANs -X- _ B-MethodName
) -X- _ O
( -X- _ O
Goodfellow -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
original -X- _ O
formulation -X- _ O
, -X- _ O
GANs -X- _ B-MethodName
are -X- _ O
trained -X- _ O
by -X- _ O
alternately -X- _ O
improving -X- _ O
a -X- _ O
generator -X- _ O
network -X- _ O
, -X- _ O
G -X- _ O
, -X- _ O
which -X- _ O
aims -X- _ O
to -X- _ O
create -X- _ O
realistic -X- _ O
samples -X- _ O
and -X- _ O
a -X- _ O
discriminator -X- _ O
network -X- _ O
, -X- _ O
D -X- _ O
, -X- _ O
which -X- _ O
tries -X- _ O
to -X- _ O
distinguish -X- _ O
real -X- _ O
samples -X- _ O
from -X- _ O
generated -X- _ O
ones -X- _ O
. -X- _ O

As -X- _ O
training -X- _ O
such -X- _ O
an -X- _ O
architecture -X- _ O
tends -X- _ O
to -X- _ O
be -X- _ O
unstable -X- _ O
, -X- _ O
several -X- _ O
improvements -X- _ O
have -X- _ O
been -X- _ O
proposed -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
the -X- _ O
Wasserstein -X- _ B-MethodName
GAN -X- _ I-MethodName
( -X- _ O
WGAN -X- _ B-MethodName
) -X- _ O
( -X- _ O
Arjovsky -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
formulation -X- _ O
the -X- _ O
discriminator -X- _ O
is -X- _ O
replaced -X- _ O
by -X- _ O
a -X- _ O
critic -X- _ O
, -X- _ O
f -X- _ O
, -X- _ O
that -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
approximate -X- _ O
the -X- _ O
Earth -X- _ O
- -X- _ O
Mover -X- _ O
distance -X- _ O
( -X- _ O
EM -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
EM -X- _ O
is -X- _ O
an -X- _ O
estimate -X- _ O
of -X- _ O
the -X- _ O
minimum -X- _ O
amount -X- _ O
of -X- _ O
effort -X- _ O
that -X- _ O
is -X- _ O
necessary -X- _ O
to -X- _ O
displace -X- _ O
one -X- _ O
distribution -X- _ O
to -X- _ O
another -X- _ O
( -X- _ O
Arjovsky -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
loss -X- _ O
function -X- _ O
to -X- _ O
train -X- _ O
a -X- _ O
GAN -X- _ B-MethodName
with -X- _ O
the -X- _ O
Wasserstein -X- _ O
Distance -X- _ O
is -X- _ O
presented -X- _ O
in -X- _ O
Equation -X- _ O
1 -X- _ O
. -X- _ O

min -X- _ O
min -X- _ O
Gmax -X- _ O
whereGis -X- _ O
the -X- _ O
generator -X- _ O
, -X- _ O
fis -X- _ O
the -X- _ O
critic -X- _ O
, -X- _ O
Wis -X- _ O
the -X- _ O
Wasserstein -X- _ O
distance -X- _ O
, -X- _ O
and -X- _ O
PrandPgare -X- _ O
the -X- _ O
real -X- _ O
and -X- _ O
generated -X- _ O
data -X- _ O
distributions -X- _ O
respectively -X- _ O
. -X- _ O

To -X- _ O
ensure -X- _ O
that -X- _ O
the -X- _ O
approximation -X- _ O
to -X- _ O
the -X- _ O
earth -X- _ O
mover -X- _ O
distance -X- _ O
is -X- _ O
valid -X- _ O
, -X- _ O
the -X- _ O
critic -X- _ O
fshould -X- _ O
be -X- _ O
enforced -X- _ O
to -X- _ O
be -X- _ O
1 -X- _ O
- -X- _ O
Lipschitz -X- _ O
continuous -X- _ O
. -X- _ O

( -X- _ O
Arjovsky -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
achieve -X- _ O
this -X- _ O
by -X- _ O
clipping -X- _ O
the -X- _ O
critic -X- _ O
weights -X- _ O
between -X- _ O
[ -X- _ O
 c -X- _ O
; -X- _ O
c -X- _ O
] -X- _ O
, -X- _ O
wherecis -X- _ O
typically -X- _ O
smaller -X- _ O
than -X- _ O
1 -X- _ O
. -X- _ O

Extensions -X- _ O
to -X- _ O
the -X- _ O
GAN -X- _ B-MethodName
setup -X- _ O
have -X- _ O
been -X- _ O
proposed -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
conditional -X- _ O
adversarial -X- _ O
networks -X- _ O
( -X- _ O
Odena -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
progressively -X- _ O

33grown -X- _ O
GANs -X- _ B-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Karras -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
which -X- _ O
have -X- _ O
made -X- _ O
detailed -X- _ O
high -X- _ O
resolution -X- _ O
category -X- _ O
- -X- _ O
dependent -X- _ O
image -X- _ O
generation -X- _ O
possible -X- _ O
. -X- _ O

During -X- _ O
the -X- _ O
training -X- _ O
of -X- _ O
conditional -X- _ B-MethodName
GANs -X- _ I-MethodName
, -X- _ O
the -X- _ O
class -X- _ O
or -X- _ O
label -X- _ O
is -X- _ O
passed -X- _ O
along -X- _ O
to -X- _ O
both -X- _ O
generator -X- _ O
and -X- _ O
discriminator -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
networks -X- _ O
implicitly -X- _ O
learn -X- _ O
relevant -X- _ O
auxiliary -X- _ O
information -X- _ O
which -X- _ O
leads -X- _ O
to -X- _ O
more -X- _ O
detailed -X- _ O
outputs -X- _ O
. -X- _ O

Progressively -X- _ O
grown -X- _ O
GANs -X- _ B-MethodName
rely -X- _ O
on -X- _ O
low -X- _ O
- -X- _ O
resolution -X- _ O
outputs -X- _ O
to -X- _ O
learn -X- _ O
outlines -X- _ O
and -X- _ O
structures -X- _ O
of -X- _ O
images -X- _ O
that -X- _ O
are -X- _ O
reﬁned -X- _ O
into -X- _ O
smooth -X- _ O
visual -X- _ O
output -X- _ O
at -X- _ O
higher -X- _ O
resolutions -X- _ O
. -X- _ O

This -X- _ O
approach -X- _ O
is -X- _ O
also -X- _ O
the -X- _ O
essence -X- _ O
of -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
textto -X- _ O
- -X- _ O
image -X- _ O
architectures -X- _ O
. -X- _ O

Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
have -X- _ O
demonstrated -X- _ O
how -X- _ O
to -X- _ O
produce -X- _ O
realistic -X- _ O
images -X- _ O
conditioned -X- _ O
on -X- _ O
textual -X- _ O
captions -X- _ O
with -X- _ O
a -X- _ O
progressive -X- _ O
GAN -X- _ B-MethodName
network -X- _ O
called -X- _ O
StackGAN -X- _ B-MethodName
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
StackGAN -X- _ B-MethodName
to -X- _ O
visualize -X- _ O
textual -X- _ O
representations -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
a -X- _ O
simpliﬁed -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
architecture -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
GAN -X- _ B-MethodName
. -X- _ O

The -X- _ O
information -X- _ O
and -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
produced -X- _ O
images -X- _ O
allow -X- _ O
us -X- _ O
to -X- _ O
evaluate -X- _ B-TaskName
the -X- _ I-TaskName
quality -X- _ I-TaskName
of -X- _ I-TaskName
the -X- _ I-TaskName
different -X- _ I-TaskName
textual -X- _ I-TaskName
representations -X- _ I-TaskName
. -X- _ O

With -X- _ O
that -X- _ O
goal -X- _ O
we -X- _ O
will -X- _ O
discuss -X- _ O
some -X- _ O
methods -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
visual -X- _ O
output -X- _ O
of -X- _ O
such -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
GANs -X- _ B-MethodName
. -X- _ O

2.3 -X- _ O
Evaluation -X- _ O
measures -X- _ O
As -X- _ O
we -X- _ O
produce -X- _ O
images -X- _ O
from -X- _ O
text -X- _ O
to -X- _ O
determine -X- _ B-TaskName
the -X- _ I-TaskName
quality -X- _ I-TaskName
of -X- _ I-TaskName
the -X- _ I-TaskName
textual -X- _ I-TaskName
representations -X- _ I-TaskName
, -X- _ O
accurate -X- _ O
evaluation -X- _ O
measures -X- _ O
are -X- _ O
needed -X- _ O
to -X- _ O
assess -X- _ O
the -X- _ O
generated -X- _ O
images -X- _ O
. -X- _ O

We -X- _ O
focus -X- _ O
on -X- _ O
evaluation -X- _ O
measures -X- _ O
for -X- _ O
GANs -X- _ B-MethodName
as -X- _ O
it -X- _ O
is -X- _ O
the -X- _ O
only -X- _ O
type -X- _ O
of -X- _ O
architecture -X- _ O
that -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
create -X- _ O
images -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
. -X- _ O

Besides -X- _ O
human -X- _ O
perceptual -X- _ O
scores -X- _ O
, -X- _ O
some -X- _ O
recent -X- _ O
advances -X- _ O
have -X- _ O
been -X- _ O
made -X- _ O
to -X- _ O
assess -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
generated -X- _ O
output -X- _ O
of -X- _ O
GANs -X- _ B-MethodName
. -X- _ O

Some -X- _ O
of -X- _ O
the -X- _ O
most -X- _ O
widely -X- _ O
adopted -X- _ O
measures -X- _ O
are -X- _ O
the -X- _ O
Inception -X- _ B-MetricName
Score -X- _ I-MetricName
( -X- _ O
IS -X- _ B-MetricName
) -X- _ O
( -X- _ O
Salimans -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
Fr -X- _ B-MetricName
´ -X- _ I-MetricName
echet -X- _ I-MetricName
Inception -X- _ I-MetricName
Distance -X- _ I-MetricName
( -X- _ O
FID -X- _ B-MetricName
) -X- _ O
( -X- _ O
Heusel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Both -X- _ O
measures -X- _ O
have -X- _ O
a -X- _ O
reasonable -X- _ O
correlation -X- _ O
with -X- _ O
image -X- _ O
quality -X- _ O
but -X- _ O
also -X- _ O
contain -X- _ O
undesirable -X- _ O
properties -X- _ O
as -X- _ O
explained -X- _ O
by -X- _ O
Borji -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

One -X- _ O
large -X- _ O
problem -X- _ O
is -X- _ O
that -X- _ O
both -X- _ O
use -X- _ O
a -X- _ O
third -X- _ O
- -X- _ O
party -X- _ O
network -X- _ O
that -X- _ O
was -X- _ O
trained -X- _ O
on -X- _ O
a -X- _ O
different -X- _ O
dataset -X- _ O
to -X- _ O
measure -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
generated -X- _ O
data -X- _ O
. -X- _ O

It -X- _ O
therefore -X- _ O
assumes -X- _ O
that -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
dataset -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
generation -X- _ O
task -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
dataset -X- _ O
that -X- _ O
the -X- _ O
third -X- _ O
- -X- _ O
party -X- _ O
network -X- _ O
was -X- _ O
trained -X- _ O
on -X- _ O
. -X- _ O

This -X- _ O
assumption -X- _ O
is -X- _ O
often -X- _ O
not -X- _ O
fulﬁlled -X- _ O
, -X- _ O
particularly -X- _ O
if -X- _ O
specialized -X- _ O
medical -X- _ O
datasets -X- _ O
are -X- _ O
used -X- _ O
. -X- _ O

To -X- _ O
solve -X- _ O
these -X- _ O
issues -X- _ O
, -X- _ O
Danihelka -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
propose -X- _ O
using -X- _ O
divergence -X- _ O
and -X- _ O
distance -X- _ O
functionsthat -X- _ O
are -X- _ O
normally -X- _ O
used -X- _ O
for -X- _ O
training -X- _ O
a -X- _ O
GAN -X- _ B-MethodName
. -X- _ O

I -X- _ O
m -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
show -X- _ O
that -X- _ O
these -X- _ O
metrics -X- _ O
exhibit -X- _ O
consistency -X- _ O
across -X- _ O
various -X- _ O
models -X- _ O
and -X- _ O
ﬁnd -X- _ O
that -X- _ O
they -X- _ O
better -X- _ O
reﬂect -X- _ O
human -X- _ B-MetricName
perceptual -X- _ I-MetricName
scores -X- _ I-MetricName
than -X- _ O
the -X- _ O
IS -X- _ B-MetricName
and -X- _ O
FID -X- _ B-MetricName
. -X- _ O

To -X- _ O
calculate -X- _ O
how -X- _ O
well -X- _ O
the -X- _ O
generated -X- _ O
distribution -X- _ O
has -X- _ O
approached -X- _ O
the -X- _ O
data -X- _ O
distribution -X- _ O
, -X- _ O
an -X- _ O
independent -X- _ O
critic -X- _ O
is -X- _ O
trained -X- _ O
until -X- _ O
convergence -X- _ O
to -X- _ O
distinguish -X- _ O
between -X- _ O
generated -X- _ O
samples -X- _ O
and -X- _ O
samples -X- _ O
from -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
. -X- _ O

The -X- _ O
WGAN -X- _ B-MethodName
loss -X- _ O
is -X- _ O
used -X- _ O
and -X- _ O
the -X- _ O
weights -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
generator -X- _ O
are -X- _ O
no -X- _ O
longer -X- _ O
updated -X- _ O
. -X- _ O

When -X- _ O
applied -X- _ O
to -X- _ O
output -X- _ O
images -X- _ O
, -X- _ O
the -X- _ O
Wasserstein -X- _ O
distance -X- _ O
thus -X- _ O
can -X- _ O
give -X- _ O
an -X- _ O
estimate -X- _ O
of -X- _ O
the -X- _ O
divergence -X- _ O
between -X- _ O
the -X- _ O
generated -X- _ O
and -X- _ O
real -X- _ O
images -X- _ O
. -X- _ O

This -X- _ O
quantity -X- _ O
is -X- _ O
expressed -X- _ O
as -X- _ O
Wqualimage -X- _ O
in -X- _ O
Equation -X- _ O
2 -X- _ O
. -X- _ O

Wqualimage -X- _ O
( -X- _ O
G -X- _ O
; -X- _ O
P -X- _ O
r -X- _ O
; -X- _ O
v -X- _ O
) -X- _ O
= -X- _ O
max -X- _ O
wherePr -X- _ O
; -X- _ O
vrefers -X- _ O
to -X- _ O
the -X- _ O
real -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
validation -X- _ O
data -X- _ O
. -X- _ O

Additionally -X- _ O
, -X- _ O
by -X- _ O
evaluating -X- _ O
the -X- _ O
model -X- _ O
that -X- _ O
is -X- _ O
trained -X- _ O
in -X- _ O
Equation -X- _ O
2 -X- _ O
on -X- _ O
the -X- _ O
training -X- _ O
and -X- _ O
test -X- _ O
set -X- _ O
, -X- _ O
Danihelka -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
suggest -X- _ O
a -X- _ O
method -X- _ O
to -X- _ O
estimate -X- _ O
whether -X- _ O
overﬁtting -X- _ O
has -X- _ O
occurred -X- _ O
. -X- _ O

Indeed -X- _ O
, -X- _ O
if -X- _ O
the -X- _ O
model -X- _ O
generalizes -X- _ O
well -X- _ O
to -X- _ O
the -X- _ O
unseen -X- _ O
examples -X- _ O
in -X- _ O
the -X- _ O
testset -X- _ O
, -X- _ O
the -X- _ O
expected -X- _ O
values -X- _ O
in -X- _ O
Equation -X- _ O
3 -X- _ O
should -X- _ O
be -X- _ O
roughly -X- _ O
the -X- _ O
same -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
equation -X- _ O
Pr -X- _ O
; -X- _ O
te -X- _ O
andPr -X- _ O
; -X- _ O
trrefer -X- _ O
to -X- _ O
the -X- _ O
real -X- _ O
distributions -X- _ O
of -X- _ O
the -X- _ O
test -X- _ O
and -X- _ O
training -X- _ O
set -X- _ O
respectively -X- _ O
. -X- _ O

E -X- _ O
[ -X- _ O
Wqualimage -X- _ O
( -X- _ O
G -X- _ O
; -X- _ O
P -X- _ O
r -X- _ O
; -X- _ O
te -X- _ O
) -X- _ O
] -X- _ O

= -X- _ O
E -X- _ O
[ -X- _ O
Wqualimage -X- _ O
( -X- _ O
G -X- _ O
; -X- _ O
P -X- _ O
r -X- _ O
; -X- _ O
tr -X- _ O
) -X- _ O
] -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O

While -X- _ O
this -X- _ O
method -X- _ O
allows -X- _ O
us -X- _ O
to -X- _ O
judge -X- _ O
the -X- _ O
output -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
images -X- _ O
, -X- _ O
and -X- _ O
by -X- _ O
extension -X- _ O
the -X- _ O
textual -X- _ O
representations -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
section -X- _ O
we -X- _ O
will -X- _ O
explain -X- _ O
how -X- _ O
our -X- _ O
methodology -X- _ O
extends -X- _ O
this -X- _ O
approach -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
alignment -X- _ O
between -X- _ O
image -X- _ O
and -X- _ O
text -X- _ O
. -X- _ O

3 -X- _ O
Method -X- _ O
This -X- _ O
paper -X- _ O
proposes -X- _ O
a -X- _ O
methodology -X- _ O
that -X- _ O
evaluates -X- _ B-TaskName
the -X- _ I-TaskName
quality -X- _ I-TaskName
of -X- _ I-TaskName
textual -X- _ I-TaskName
representations -X- _ I-TaskName
by -X- _ O
visualizing -X- _ O
them -X- _ O
with -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
models -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
achieved -X- _ O
in -X- _ O
three -X- _ O
separate -X- _ O
stages -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
subsections -X- _ O
. -X- _ O

3.1 -X- _ O
Train -X- _ O
and -X- _ O
create -X- _ O
a -X- _ O
textual -X- _ O
representation -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
4 -X- _ O
different -X- _ O
textual -X- _ O
representations -X- _ O
are -X- _ O
created -X- _ O
by -X- _ O
training -X- _ O
on -X- _ O
the -X- _ O
captions -X- _ O
of -X- _ O
the -X- _ O

Figure -X- _ O
1 -X- _ O
. -X- _ O

Overview -X- _ O
of -X- _ O
the -X- _ O
methodology -X- _ O
. -X- _ O

A -X- _ O
textual -X- _ O
representation -X- _ O
is -X- _ O
ﬁrst -X- _ O
trained -X- _ O
and -X- _ O
then -X- _ O
fed -X- _ O
as -X- _ O
a -X- _ O
conditional -X- _ O
input -X- _ O
to -X- _ O
a -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
model -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
ﬁgure -X- _ O
a -X- _ O
StackGAN -X- _ B-MethodName
. -X- _ O

The -X- _ O
textual -X- _ O
representation -X- _ O
is -X- _ O
fed -X- _ O
to -X- _ O
both -X- _ O
the -X- _ O
ﬁrst -X- _ O
and -X- _ O
second -X- _ O
stage -X- _ O
of -X- _ O
an -X- _ O
image -X- _ O
StackGAN -X- _ B-MethodName
with -X- _ O
the -X- _ O
goal -X- _ O
of -X- _ O
creating -X- _ O
low- -X- _ O
and -X- _ O
high -X- _ O
- -X- _ O
resolution -X- _ O
images -X- _ O
x1andx2respectively -X- _ O
. -X- _ O

From -X- _ O
the -X- _ O
representation -X- _ O
, -X- _ O
the -X- _ O
augmented -X- _ O
conditioning -X- _ O
embedding -X- _ O
^cis -X- _ O
formed -X- _ O
. -X- _ O

In -X- _ O
a -X- _ O
ﬁnal -X- _ O
step -X- _ O
, -X- _ O
the -X- _ O
visual -X- _ O
output -X- _ O
is -X- _ O
evaluated -X- _ O
. -X- _ O

training -X- _ O
set -X- _ O
using -X- _ O
unsupervised -X- _ O
training -X- _ O
methods -X- _ O
. -X- _ O

As -X- _ O
these -X- _ O
representations -X- _ O
are -X- _ O
compared -X- _ O
afterwards -X- _ O
, -X- _ O
they -X- _ O
each -X- _ O
need -X- _ O
to -X- _ O
have -X- _ O
the -X- _ O
same -X- _ O
, -X- _ O
ﬁxed -X- _ O
dimension -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
ﬁrst -X- _ O
2 -X- _ O
representations -X- _ O
, -X- _ O
the -X- _ O
typical -X- _ O
word2vec -X- _ O
skip -X- _ O
- -X- _ O
gram -X- _ O
word -X- _ O
embeddings -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
build -X- _ O
the -X- _ O
vectors -X- _ O
. -X- _ O

A -X- _ O
representation -X- _ O
for -X- _ O
a -X- _ O
sentence -X- _ O
is -X- _ O
built -X- _ O
by -X- _ O
respectively -X- _ O
summing -X- _ O
and -X- _ O
concatenating -X- _ O
the -X- _ O
individual -X- _ O
word -X- _ O
embeddings -X- _ O
for -X- _ O
the -X- _ O
entire -X- _ O
sequence -X- _ O
. -X- _ O

Such -X- _ O
a -X- _ O
comparison -X- _ O
is -X- _ O
interesting -X- _ O
as -X- _ O
summing -X- _ O
( -X- _ O
or -X- _ O
averaging -X- _ O
) -X- _ O
word -X- _ O
vectors -X- _ O
allows -X- _ O
to -X- _ O
use -X- _ O
high -X- _ O
- -X- _ O
dimensional -X- _ O
word -X- _ O
representations -X- _ O
, -X- _ O
yet -X- _ O
sacriﬁces -X- _ O
word -X- _ O
order -X- _ O
. -X- _ O

Concatenating -X- _ O
on -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
requires -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
low -X- _ O
- -X- _ O
dimensional -X- _ O
word -X- _ O
embeddings -X- _ O
as -X- _ O
the -X- _ O
sentence -X- _ O
dimension -X- _ O
is -X- _ O
ﬁxed -X- _ O
, -X- _ O
but -X- _ O
maintains -X- _ O
word -X- _ O
order -X- _ O
and -X- _ O
has -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
work -X- _ O
well -X- _ O
at -X- _ O
the -X- _ O
input -X- _ O
of -X- _ O
convolutional -X- _ O
networks -X- _ O
( -X- _ O
Kim -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
models -X- _ O
used -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
. -X- _ O

Additionally -X- _ O
, -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
representation -X- _ O
of -X- _ O
an -X- _ O
autoencoder -X- _ O
is -X- _ O
built -X- _ O
. -X- _ O

The -X- _ O
autoencoder -X- _ B-MethodName
, -X- _ O
that -X- _ O
consists -X- _ O
of -X- _ O
a -X- _ O
1 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
LSTM -X- _ B-MethodName
encoder -X- _ O
and -X- _ O
a -X- _ O
1 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
LSTM -X- _ B-MethodName
decoder -X- _ O
, -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
recreate -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
with -X- _ O
a -X- _ O
cross -X- _ O
- -X- _ O
entropy -X- _ O
loss -X- _ O
at -X- _ O
the -X- _ O
word -X- _ O
- -X- _ O
level -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
use -X- _ O
the -X- _ O
representation -X- _ O
produced -X- _ O
by -X- _ O
an -X- _ O
ARAE -X- _ B-MethodName
, -X- _ O
as -X- _ O
in -X- _ O
section -X- _ O
2.1 -X- _ O
. -X- _ O

The -X- _ O
ARAE -X- _ B-MethodName
contains -X- _ O
a -X- _ O
1 -X- _ B-HyperparameterName
- -X- _ O
layer -X- _ B-HyperparameterName
LSTM -X- _ B-HyperparameterName
encoder -X- _ O
and -X- _ O
1 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
LSTM -X- _ B-HyperparameterName
decoder -X- _ O
. -X- _ O

The -X- _ O
generator -X- _ O
and -X- _ O
discriminator -X- _ O
consist -X- _ O
of -X- _ O
3 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
feedforward -X- _ O
networks -X- _ O
. -X- _ O

3.2 -X- _ O
Create -X- _ O
images -X- _ O
from -X- _ O
text -X- _ O
From -X- _ O
these -X- _ O
representations -X- _ O
, -X- _ O
images -X- _ O
are -X- _ O
created -X- _ O
with -X- _ O
a -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
model -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
a -X- _ O
simpleconditional -X- _ O
GAN -X- _ B-MethodName
or -X- _ O
a -X- _ O
more -X- _ O
complex -X- _ O
StackGAN -X- _ B-MethodName
. -X- _ O

In -X- _ O
the -X- _ O
latter -X- _ O
, -X- _ O
a -X- _ O
textual -X- _ O
representation -X- _ O
tis -X- _ O
fed -X- _ O
into -X- _ O
a -X- _ O
fully -X- _ O
- -X- _ O
connected -X- _ O
net -X- _ O
that -X- _ O
creates -X- _ O
a -X- _ O
mean -X- _ O
and -X- _ O
a -X- _ O
variance2from -X- _ O
which -X- _ O
augmented -X- _ O
conditional -X- _ O
representations -X- _ O
^care -X- _ O
generated -X- _ O
. -X- _ O

The -X- _ O
KullbackLeibler -X- _ O
divergence -X- _ O
( -X- _ O
KL -X- _ O
- -X- _ O
loss -X- _ O
) -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
coerce -X- _ O
^cto -X- _ O
approach -X- _ O
a -X- _ O
normal -X- _ O
distribution -X- _ O
N -X- _ O
( -X- _ O
0 -X- _ O
; -X- _ O
I -X- _ O
) -X- _ O
. -X- _ O

This -X- _ O
ensures -X- _ O
smoothness -X- _ O
between -X- _ O
different -X- _ O
input -X- _ O
texts -X- _ O
and -X- _ O
avoids -X- _ O
overﬁtting -X- _ O
when -X- _ O
generating -X- _ O
images -X- _ O
from -X- _ O
captions -X- _ O
( -X- _ O
Doersch -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Larsen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
conditional -X- _ O
vector -X- _ O
^cis -X- _ O
then -X- _ O
concatenated -X- _ O
to -X- _ O
a -X- _ O
noise -X- _ O
vectorz0 -X- _ O
, -X- _ O
sampled -X- _ O
from -X- _ O
a -X- _ O
normal -X- _ O
distribution -X- _ O
, -X- _ O
and -X- _ O
fed -X- _ O
to -X- _ O
the -X- _ O
generator -X- _ O
. -X- _ O

Such -X- _ O
a -X- _ O
StackGAN -X- _ B-MethodName
model -X- _ O
is -X- _ O
trained -X- _ O
in -X- _ O
two -X- _ O
stages -X- _ O
: -X- _ O
at -X- _ O
a -X- _ O
ﬁrst -X- _ O
stage -X- _ O
the -X- _ O
features -X- _ O
of -X- _ O
real -X- _ O
and -X- _ O
generated -X- _ O
images -X- _ O
are -X- _ O
matched -X- _ O
to -X- _ O
produce -X- _ O
lowresolution -X- _ O
images -X- _ O
that -X- _ O
lack -X- _ O
detail -X- _ O
. -X- _ O

During -X- _ O
the -X- _ O
second -X- _ O
stage -X- _ O
, -X- _ O
the -X- _ O
generator -X- _ O
produces -X- _ O
larger -X- _ O
images -X- _ O
, -X- _ O
conditioned -X- _ O
on -X- _ O
both -X- _ O
the -X- _ O
augmented -X- _ O
conditional -X- _ O
vector -X- _ O
^cas -X- _ O

well -X- _ O
as -X- _ O
the -X- _ O
image -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
ﬁrst -X- _ O
stage -X- _ O
. -X- _ O

The -X- _ O
training -X- _ O
is -X- _ O
broken -X- _ O
up -X- _ O
into -X- _ O
the -X- _ O
maximization -X- _ O
of -X- _ O
the -X- _ O
loss -X- _ O
of -X- _ O
Dand -X- _ O
the -X- _ O
minimization -X- _ O
of -X- _ O
the -X- _ O
loss -X- _ O
ofGas -X- _ O
shown -X- _ O
in -X- _ O
Equations -X- _ O
4 -X- _ O
and -X- _ O
5 -X- _ O
for -X- _ O
the -X- _ O
ﬁrst -X- _ O
stage -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
a -X- _ O
traditional -X- _ O
GAN -X- _ B-MethodName
formulation -X- _ O
is -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
StackGAN -X- _ B-MethodName
model -X- _ O
. -X- _ O

max -X- _ O

35min -X- _ O
G1LG1= -X- _ O
wherepzandpdrepresent -X- _ O
the -X- _ O
random -X- _ O
normal -X- _ O
and -X- _ O
data -X- _ O
distribution -X- _ O
respectively -X- _ O
. -X- _ O

tis -X- _ O
the -X- _ O
textual -X- _ O
representation -X- _ O
and -X- _ O
is -X- _ O
a -X- _ O
regularization -X- _ O
parameter -X- _ O
to -X- _ O
balance -X- _ O
the -X- _ O
loss -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
terms -X- _ O
. -X- _ O

Subﬁx -X- _ O
1 -X- _ O
indicates -X- _ O
that -X- _ O
these -X- _ O
equations -X- _ O
relate -X- _ O
to -X- _ O
stage -X- _ O
1 -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
the -X- _ O
StackGAN -X- _ B-MethodName
model -X- _ O
is -X- _ O
distinct -X- _ O
from -X- _ O
more -X- _ O
conventional -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
architectures -X- _ O
not -X- _ O
only -X- _ O
in -X- _ O
the -X- _ O
sense -X- _ O
that -X- _ O
the -X- _ O
former -X- _ O
progressively -X- _ O
constructs -X- _ O
higher -X- _ O
resolution -X- _ O
images -X- _ O
but -X- _ O
also -X- _ O
because -X- _ O
of -X- _ O
the -X- _ O
conditioning -X- _ O
augmentation -X- _ O
. -X- _ O

This -X- _ O
mechanism -X- _ O
is -X- _ O
particularly -X- _ O
important -X- _ O
for -X- _ O
this -X- _ O
experiment -X- _ O
, -X- _ O
as -X- _ O
it -X- _ O
essentially -X- _ O
augments -X- _ O
the -X- _ O
different -X- _ O
textual -X- _ O
representations -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
simple -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
GAN -X- _ B-MethodName
, -X- _ O
which -X- _ O
we -X- _ O
refer -X- _ O
to -X- _ O
as -X- _ O
TTI -X- _ B-MethodName
- -X- _ I-MethodName
GAN -X- _ I-MethodName
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
GAN -X- _ B-MethodName
architecture -X- _ O
without -X- _ O
separate -X- _ O
stages -X- _ O
that -X- _ O
passes -X- _ O
the -X- _ O
textual -X- _ O
representation -X- _ O
to -X- _ O
both -X- _ O
the -X- _ O
generator -X- _ O
and -X- _ O
discriminator -X- _ O
without -X- _ O
modiﬁcations -X- _ O
. -X- _ O

Both -X- _ O
generator -X- _ O
and -X- _ O
discriminator -X- _ O
for -X- _ O
all -X- _ O
text -X- _ O
- -X- _ O
toimage -X- _ O
architectures -X- _ O
( -X- _ O
i.e. -X- _ O
the -X- _ O
TTI -X- _ B-MethodName
- -X- _ I-MethodName
GAN -X- _ I-MethodName
and -X- _ O
both -X- _ O
stage -X- _ O
- -X- _ O
I -X- _ O
and -X- _ O
stage -X- _ O
- -X- _ O
II -X- _ O
StackGAN -X- _ B-MethodName
) -X- _ O
consist -X- _ O
of -X- _ O
a -X- _ O
series -X- _ O
of -X- _ O
convolutional -X- _ O
up- -X- _ O
and -X- _ O
down -X- _ O
- -X- _ O
sampling -X- _ O
blocks -X- _ O
respectively -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
text -X- _ O
embedding -X- _ O
tis -X- _ O
passed -X- _ O
to -X- _ O
the -X- _ O
discriminator -X- _ O
it -X- _ O
is -X- _ O
compressed -X- _ O
with -X- _ O
a -X- _ O
fullyconnected -X- _ O
network -X- _ O
and -X- _ O
replicated -X- _ O
to -X- _ O
match -X- _ O
the -X- _ O
dimensions -X- _ O
of -X- _ O
the -X- _ O
image -X- _ O
. -X- _ O

3.3 -X- _ O
Evaluate -X- _ O
the -X- _ O
output -X- _ O
quality -X- _ O
Evaluating -X- _ O
the -X- _ O
output -X- _ O
quality -X- _ O
will -X- _ O
let -X- _ O
us -X- _ O
judge -X- _ O
the -X- _ O
textual -X- _ O
representation -X- _ O
quality -X- _ O
. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
do -X- _ O
so -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
rely -X- _ O
on -X- _ O
Equation -X- _ O
2 -X- _ O
to -X- _ O
calculate -X- _ O
Wqualimage -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
also -X- _ O
like -X- _ O
to -X- _ O
have -X- _ O
a -X- _ O
rough -X- _ O
idea -X- _ O
of -X- _ O
how -X- _ O
well -X- _ O
the -X- _ O
conditional -X- _ O
information -X- _ O
is -X- _ O
assimilated -X- _ O
in -X- _ O
the -X- _ O
output -X- _ O
. -X- _ O

We -X- _ O
therefore -X- _ O
extend -X- _ O
the -X- _ O
previously -X- _ O
mentioned -X- _ O
setup -X- _ O
to -X- _ O
calculate -X- _ O
the -X- _ O
divergence -X- _ O
between -X- _ O
an -X- _ O
additional -X- _ O
pair -X- _ O
of -X- _ O
distributions -X- _ O
. -X- _ O

Walign -X- _ O
imtxtin -X- _ O
Equation -X- _ O
6 -X- _ O
measures -X- _ O
the -X- _ O
distance -X- _ O
between -X- _ O
the -X- _ O
aligned -X- _ O
image -X- _ O
- -X- _ O
text -X- _ O
distributions -X- _ O
by -X- _ O
also -X- _ O
feeding -X- _ O
the -X- _ O
conditional -X- _ O
information -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
case -X- _ O
the -X- _ O
textual -X- _ O
representations -X- _ O
, -X- _ O
to -X- _ O
the -X- _ O
critic -X- _ O
. -X- _ O

Walign -X- _ O
imtxt -X- _ O
( -X- _ O
G -X- _ O
; -X- _ O
P -X- _ O
r -X- _ O
; -X- _ O
v -X- _ O
) -X- _ O
= -X- _ O
max -X- _ O
wherecis -X- _ O
conditional -X- _ O
information -X- _ O
that -X- _ O
corresponds -X- _ O
to -X- _ O
the -X- _ O
current -X- _ O
data -X- _ O
sample -X- _ O
. -X- _ O

f2is -X- _ O
distinct -X- _ O
and -X- _ O
independent -X- _ O
from -X- _ O
the -X- _ O
critic -X- _ O
f1 -X- _ O
in -X- _ O
Equation -X- _ O

2but -X- _ O
is -X- _ O
also -X- _ O
trained -X- _ O
until -X- _ O
convergence -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
. -X- _ O

The -X- _ O
intuition -X- _ O
behind -X- _ O
Equation -X- _ O
6 -X- _ O
is -X- _ O
that -X- _ O
Walign -X- _ O
imtxtis -X- _ O
a -X- _ O
measure -X- _ O
of -X- _ O
the -X- _ O
distance -X- _ O
between -X- _ O
the -X- _ O
real -X- _ O
and -X- _ O
generated -X- _ O
distributions -X- _ O
with -X- _ O
their -X- _ O
conditional -X- _ O
information -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
Walign -X- _ O
imtxtshould -X- _ O
be -X- _ O
smaller -X- _ O
for -X- _ O
models -X- _ O
that -X- _ O
take -X- _ O
the -X- _ O
conditional -X- _ O
information -X- _ O
into -X- _ O
account -X- _ O
when -X- _ O
creating -X- _ O
the -X- _ O
output -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
the -X- _ O
value -X- _ O
of -X- _ O
Walign -X- _ O
imtxtalso -X- _ O
depends -X- _ O
on -X- _ O
the -X- _ O
chosen -X- _ O
textual -X- _ O
representation -X- _ O
and -X- _ O
can -X- _ O
therefore -X- _ O
not -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
evaluate -X- _ O
alignment -X- _ O
of -X- _ O
the -X- _ O
TTI -X- _ B-MethodName
- -X- _ I-MethodName
GAN -X- _ I-MethodName
model -X- _ O
across -X- _ O
different -X- _ O
representations -X- _ O
. -X- _ O

It -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
the -X- _ O
StackGAN -X- _ B-MethodName
however -X- _ O
as -X- _ O
the -X- _ O
representations -X- _ O
are -X- _ O
coerced -X- _ O
to -X- _ O
approach -X- _ O
a -X- _ O
normal -X- _ O
distribution -X- _ O
with -X- _ O
the -X- _ O
conditioning -X- _ O
augmentation -X- _ O
mechanism -X- _ O
. -X- _ O

We -X- _ O
would -X- _ O
also -X- _ O
like -X- _ O
to -X- _ O
get -X- _ O
an -X- _ O
estimate -X- _ O
for -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
overﬁtting -X- _ O
that -X- _ O
occurs -X- _ O
for -X- _ O
each -X- _ O
textual -X- _ O
representation -X- _ O
. -X- _ O

For -X- _ O
this -X- _ O
we -X- _ O
rely -X- _ O
on -X- _ O
the -X- _ O
insights -X- _ O
of -X- _ O
Equation -X- _ O
3 -X- _ O
. -X- _ O

In -X- _ O
Equations -X- _ O
7 -X- _ O
and -X- _ O
8 -X- _ O
we -X- _ O
suggest -X- _ O
a -X- _ O
simple -X- _ O
method -X- _ O
to -X- _ O
compare -X- _ O
how -X- _ O
much -X- _ O
overﬁtting -X- _ O
occurs -X- _ O
on -X- _ O
both -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
images -X- _ O
itself -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
on -X- _ O
the -X- _ O
alignment -X- _ O
to -X- _ O
the -X- _ O
captions -X- _ O
. -X- _ O

By -X- _ O
taking -X- _ O
the -X- _ O
quotient -X- _ O
of -X- _ O
the -X- _ O
expected -X- _ O
values -X- _ O
of -X- _ O
the -X- _ O
evaluation -X- _ O
ofWqualimage -X- _ O
andWalign -X- _ O
imtxt -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
compare -X- _ O
how -X- _ O
much -X- _ O
overﬁtting -X- _ O
happened -X- _ O
for -X- _ O
each -X- _ O
entity -X- _ O
. -X- _ O

Oqualimage -X- _ O
= -X- _ O
E -X- _ O
[ -X- _ O
Wqualimage -X- _ O
( -X- _ O
G -X- _ O
; -X- _ O
P -X- _ O
r -X- _ O
; -X- _ O
te -X- _ O
) -X- _ O
] -X- _ O
= -X- _ O
E -X- _ O
[ -X- _ O
Wqualimage -X- _ O
( -X- _ O
G -X- _ O
; -X- _ O
P -X- _ O
r -X- _ O
; -X- _ O
tr -X- _ O
) -X- _ O
] -X- _ O
 1 -X- _ O
Oalign -X- _ O
imtxt -X- _ O
= -X- _ O
E -X- _ O
[ -X- _ O
Walign -X- _ O
imtxt -X- _ O
( -X- _ O
G -X- _ O
; -X- _ O
P -X- _ O
r -X- _ O
; -X- _ O
te -X- _ O
) -X- _ O
] -X- _ O
= -X- _ O
E -X- _ O
[ -X- _ O
Walign -X- _ O
imtxt -X- _ O
( -X- _ O
G -X- _ O
; -X- _ O
P -X- _ O
r -X- _ O
; -X- _ O
tr -X- _ O
) -X- _ O
] -X- _ O
 1 -X- _ O
The -X- _ O
entire -X- _ O
setup -X- _ O
of -X- _ O
the -X- _ O
methodology -X- _ O
is -X- _ O
illustrated -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
where -X- _ O
the -X- _ O
StackGAN -X- _ O
architecture -X- _ O
is -X- _ O
used -X- _ O
as -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
architecture -X- _ O
. -X- _ O

4 -X- _ O
Experiments -X- _ O
The -X- _ O
used -X- _ O
dataset -X- _ O
is -X- _ O
the -X- _ O
chest -X- _ B-DatasetName
X -X- _ I-DatasetName
- -X- _ I-DatasetName
Ray -X- _ I-DatasetName
dataset -X- _ I-DatasetName
of -X- _ O
the -X- _ O
National -X- _ O
Library -X- _ O
of -X- _ O
Medicine -X- _ O
, -X- _ O
National -X- _ O
Institutes -X- _ O
of -X- _ O
Health -X- _ O
, -X- _ O
Bethesda -X- _ O
, -X- _ O
MD -X- _ O
, -X- _ O
USA -X- _ O
( -X- _ O
DemnerFushman -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

It -X- _ O
contains -X- _ O
the -X- _ O
ﬁndings -X- _ O
of -X- _ O
the -X- _ O
frontal -X- _ O
and -X- _ O
lateral -X- _ O
X -X- _ O
- -X- _ O
Ray -X- _ O
for -X- _ O
3851 -X- _ O
patients -X- _ O
. -X- _ O

For -X- _ O
this -X- _ O
work -X- _ O
only -X- _ O
the -X- _ O
frontal -X- _ O
X -X- _ O
- -X- _ O
Rays -X- _ O
are -X- _ O
retained -X- _ O
. -X- _ O

Random -X- _ O
crops -X- _ O
are -X- _ O
performed -X- _ O
during -X- _ O
training -X- _ O
for -X- _ O
data -X- _ O
augmentation -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
content -X- _ O
in -X- _ O
the -X- _ O
ﬁndings -X- _ O
is -X- _ O
invariant -X- _ O
to -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
the -X- _ O
sentences -X- _ O
, -X- _ O
up -X- _ O
to -X- _ O
4 -X- _ O
captions -X- _ O
are -X- _ O
created -X- _ O
for -X- _ O
each -X- _ O
X -X- _ O
- -X- _ O
Ray -X- _ O
by -X- _ O
selecting -X- _ O
different -X- _ O
sentences -X- _ O
or -X- _ O
a -X- _ O
different -X- _ O
sentence -X- _ O
order -X- _ O
. -X- _ O

Captions -X- _ O
that -X- _ O
contain -X- _ O
less -X- _ O
than -X- _ O
30 -X- _ O

36Representation -X- _ O
Wqualimage -X- _ O
Table -X- _ O
1 -X- _ O
. -X- _ O

Quantitative -X- _ O
results -X- _ O
of -X- _ O
10 -X- _ O
runs -X- _ O
for -X- _ O
the -X- _ O
TTI -X- _ B-MethodName
- -X- _ I-MethodName
GAN -X- _ I-MethodName
visualization -X- _ O
method -X- _ O
for -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
representations -X- _ O
. -X- _ O

A -X- _ O
lower -X- _ O
Wqualimage -X- _ O
implies -X- _ O
a -X- _ O
better -X- _ O
image -X- _ O
quality -X- _ O
. -X- _ O

( -X- _ O
* -X- _ O
) -X- _ O
For -X- _ O
both -X- _ O
the -X- _ O
autoencoder -X- _ B-MethodName
and -X- _ O
ARAE -X- _ B-MethodName
, -X- _ O
an -X- _ O
outlier -X- _ O
was -X- _ O
removed -X- _ O
. -X- _ O

words -X- _ O
are -X- _ O
padded -X- _ O
to -X- _ O
equal -X- _ O
length -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
maximum -X- _ O
of -X- _ O
30 -X- _ O
words -X- _ O
. -X- _ O

All -X- _ O
words -X- _ O
are -X- _ O
lowercase -X- _ O
and -X- _ O
words -X- _ O
with -X- _ O
a -X- _ O
frequency -X- _ O
of -X- _ O
less -X- _ O
than -X- _ O
5 -X- _ O
occurrences -X- _ O
are -X- _ O
removed -X- _ O
and -X- _ O
replaced -X- _ O
by -X- _ O
an -X- _ O
out -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
vocabulary -X- _ O
marker -X- _ O
. -X- _ O

While -X- _ O
the -X- _ O
dataset -X- _ O
also -X- _ O
contains -X- _ O
diagnosis -X- _ O
labels -X- _ O
for -X- _ O
each -X- _ O
image -X- _ O
, -X- _ O
they -X- _ O
are -X- _ O
not -X- _ O
used -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
. -X- _ O

The -X- _ O
dataset -X- _ O
is -X- _ O
divided -X- _ O
into -X- _ O
training -X- _ O
, -X- _ O
validation -X- _ O
and -X- _ O
test -X- _ O
set -X- _ O
with -X- _ O
80 -X- _ O
% -X- _ O
,10 -X- _ O
% -X- _ O
and10 -X- _ O
% -X- _ O
of -X- _ O
the -X- _ O
data -X- _ O
respectively -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
experiments -X- _ O
we -X- _ O
ﬁrst -X- _ O
create -X- _ O
four -X- _ O
different -X- _ O
textual -X- _ O
representations -X- _ O
on -X- _ O
the -X- _ O
captions -X- _ O
of -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
, -X- _ O
as -X- _ O
detailed -X- _ O
in -X- _ O
section -X- _ O
3.1 -X- _ O
. -X- _ O

Those -X- _ O
representations -X- _ O
are -X- _ O
referred -X- _ O
to -X- _ O
as -X- _ O
word2vec -X- _ B-MethodName
( -X- _ O
sum -X- _ O
) -X- _ O
, -X- _ O
word2vec -X- _ B-MethodName
( -X- _ O
concat -X- _ O
) -X- _ O
, -X- _ O
autoencoder -X- _ B-MethodName
and -X- _ O
ARAE -X- _ B-MethodName
. -X- _ O

To -X- _ O
illustrate -X- _ O
the -X- _ O
methodology -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
ﬁxed -X- _ O
dimension -X- _ O
of -X- _ O
each -X- _ O
representation -X- _ O
to -X- _ O
300 -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
a -X- _ O
standard -X- _ O
dimension -X- _ O
for -X- _ O
such -X- _ O
embeddings -X- _ O
, -X- _ O
initially -X- _ O
used -X- _ O
by -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2013b -X- _ O
) -X- _ O
in -X- _ O
their -X- _ O
analysis -X- _ O
of -X- _ O
distributed -X- _ O
vectors -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
autoencoder -X- _ B-MethodName
and -X- _ O
the -X- _ O
ARAE -X- _ B-MethodName
, -X- _ O
training -X- _ O
is -X- _ O
stopped -X- _ O
when -X- _ O
the -X- _ O
validation -X- _ O
error -X- _ O
of -X- _ O
the -X- _ O
reconstruction -X- _ O
is -X- _ O
minimal -X- _ O
. -X- _ O

To -X- _ O
generate -X- _ O
images -X- _ O
from -X- _ O
the -X- _ O
text -X- _ O
, -X- _ O
the -X- _ O
TTIGAN -X- _ B-MethodName
and -X- _ O
StackGAN -X- _ B-MethodName
models -X- _ O
are -X- _ O
used -X- _ O
as -X- _ O
explained -X- _ O
in -X- _ O
section -X- _ O
3.2 -X- _ O
. -X- _ O

The -X- _ O
latter -X- _ O
produces -X- _ O
images -X- _ O
with -X- _ O
higher -X- _ O
resolution -X- _ O
than -X- _ O
the -X- _ O
former -X- _ O
approach -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
important -X- _ O
as -X- _ O
a -X- _ O
higher -X- _ O
resolution -X- _ O
is -X- _ O
required -X- _ O
to -X- _ O
make -X- _ O
an -X- _ O
accurate -X- _ O
assessment -X- _ O
about -X- _ O
the -X- _ O
alignment -X- _ O
of -X- _ O
the -X- _ O
X -X- _ O
- -X- _ O
Ray -X- _ O
images -X- _ O
to -X- _ O
the -X- _ O
captions -X- _ O
. -X- _ O

The -X- _ O
expected -X- _ O
outcome -X- _ O
is -X- _ O
that -X- _ O
a -X- _ O
textual -X- _ O
representation -X- _ O
that -X- _ O
maintains -X- _ O
sequential -X- _ O
information -X- _ O
performs -X- _ O
better -X- _ O
than -X- _ O
one -X- _ O
that -X- _ O
does -X- _ O
not -X- _ O
. -X- _ O

Additionally -X- _ O
we -X- _ O
expect -X- _ O
a -X- _ O
code -X- _ O
that -X- _ O
lies -X- _ O
on -X- _ O
a -X- _ O
regularized -X- _ O
smooth -X- _ O
space -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
code -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
ARAE -X- _ B-MethodName
, -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
useful -X- _ O
than -X- _ O
a -X- _ O
code -X- _ O
that -X- _ O
does -X- _ O
not -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
perform -X- _ O
two -X- _ O
types -X- _ O
of -X- _ O
experiments -X- _ O
, -X- _ O
for -X- _ O
which -X- _ O
the -X- _ O
concrete -X- _ O
setup -X- _ O
is -X- _ O
as -X- _ O
follows -X- _ O
. -X- _ O

1 -X- _ O
. -X- _ O

As -X- _ O
GAN -X- _ B-MethodName
training -X- _ O
can -X- _ O
be -X- _ O
unstable -X- _ O
, -X- _ O
the -X- _ O
TTIGAN -X- _ B-MethodName
is -X- _ O
trained -X- _ O
10 -X- _ O
times -X- _ O
for -X- _ O
each -X- _ O
represen -X- _ O
- -X- _ O
Representation -X- _ O
WqualimageWalign -X- _ O
imtxt -X- _ O
Table -X- _ O
2 -X- _ O
. -X- _ O

Quantitative -X- _ O
results -X- _ O
for -X- _ O
the -X- _ O
trained -X- _ O
Stage2 -X- _ O

StackGAN -X- _ B-MethodName
visualization -X- _ O
method -X- _ O
for -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
representations -X- _ O
. -X- _ O

A -X- _ O
lower -X- _ O
Wqualimage -X- _ O
and -X- _ O
Walign -X- _ O
imtxtimply -X- _ O
a -X- _ O
better -X- _ O
image -X- _ O
quality -X- _ O
and -X- _ O
alignment -X- _ O
respectively -X- _ O
. -X- _ O

tation -X- _ O
. -X- _ O

From -X- _ O
the -X- _ O
evaluation -X- _ O
of -X- _ O
each -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
measures -X- _ O
for -X- _ O
Wqualimage -X- _ O
, -X- _ O
Oqualimage -X- _ O
andOalign -X- _ O
imtxtwhich -X- _ O
allow -X- _ O
us -X- _ O
to -X- _ O
compare -X- _ O
the -X- _ O
value -X- _ O
of -X- _ O
the -X- _ O
different -X- _ O
representations -X- _ O
. -X- _ O

The -X- _ O
TTI -X- _ B-MethodName
- -X- _ I-MethodName
GAN -X- _ I-MethodName
in -X- _ O
our -X- _ O
setup -X- _ O
produces -X- _ O
images -X- _ O
with -X- _ O
a -X- _ O
resolution -X- _ O
of -X- _ O
64x64 -X- _ O
pixels -X- _ O
. -X- _ O

2 -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
StackGAN -X- _ B-MethodName
, -X- _ O
we -X- _ O
train -X- _ O
one -X- _ O
model -X- _ O
for -X- _ O
each -X- _ O
representation -X- _ O
, -X- _ O
and -X- _ O
train -X- _ O
an -X- _ O
independent -X- _ O
critic -X- _ O
5 -X- _ O
times -X- _ O
for -X- _ O
each -X- _ O
model -X- _ O
. -X- _ O

As -X- _ O
GAN -X- _ B-MethodName
training -X- _ O
can -X- _ O
be -X- _ O
quite -X- _ O
unstable -X- _ O
, -X- _ O
this -X- _ O
experiment -X- _ O
does -X- _ O
not -X- _ O
allow -X- _ O
us -X- _ O
to -X- _ O
judge -X- _ O
the -X- _ O
value -X- _ O
of -X- _ O
the -X- _ O
representations -X- _ O
from -X- _ O
just -X- _ O
one -X- _ O
run -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
our -X- _ O
estimates -X- _ O
for -X- _ O
Wqualimage -X- _ O
andWalign -X- _ O
imtxtto -X- _ O
the -X- _ O
evaluation -X- _ O
of -X- _ O
a -X- _ O
trained -X- _ O
clinician -X- _ O
, -X- _ O
to -X- _ O
conﬁrm -X- _ O
that -X- _ O
our -X- _ O
methodology -X- _ O
correlates -X- _ O
with -X- _ O
human -X- _ O
judgment -X- _ O
, -X- _ O
both -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
quality -X- _ O
and -X- _ O
alignment -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
ﬁrst -X- _ O
stage -X- _ O
of -X- _ O
the -X- _ O
StackGAN -X- _ B-MethodName
we -X- _ O
produce -X- _ O
64x64 -X- _ O
pixel -X- _ O
images -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
second -X- _ O
stage -X- _ O
outputs -X- _ O
higher -X- _ O
resolution -X- _ O
256x256 -X- _ O
pixel -X- _ O
images -X- _ O
. -X- _ O

For -X- _ O
this -X- _ O
experiment -X- _ O
, -X- _ O
was -X- _ O
set -X- _ O
to -X- _ O
0:05andcwas -X- _ B-HyperparameterValue
set -X- _ O
to -X- _ O
0:01 -X- _ B-HyperparameterValue
. -X- _ O

The -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
architectures -X- _ O
are -X- _ O
each -X- _ O
trained -X- _ O
during -X- _ O
120 -X- _ O
epochs -X- _ O
for -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
textual -X- _ O
representations -X- _ O
of -X- _ O
the -X- _ O
captions -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
. -X- _ O

The -X- _ O
image -X- _ O
quality -X- _ O
is -X- _ O
then -X- _ O
assessed -X- _ O
on -X- _ O
the -X- _ O
images -X- _ O
that -X- _ O
are -X- _ O
generated -X- _ O
from -X- _ O
the -X- _ O
captions -X- _ O
of -X- _ O
the -X- _ O
validation -X- _ O
and -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

This -X- _ O
ensures -X- _ O
that -X- _ O
we -X- _ O
check -X- _ O
whether -X- _ O
the -X- _ O
learned -X- _ O
representations -X- _ O
can -X- _ O
generalize -X- _ O
well -X- _ O
to -X- _ O
captions -X- _ O
that -X- _ O
were -X- _ O
never -X- _ O
seen -X- _ O
during -X- _ O
their -X- _ O
construction -X- _ O
. -X- _ O

5 -X- _ O
Results -X- _ O
In -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
generated -X- _ O
images -X- _ O
of -X- _ O
the -X- _ O
TTI -X- _ B-MethodName
- -X- _ I-MethodName
GAN -X- _ I-MethodName
model -X- _ O
are -X- _ O
presented -X- _ O
for -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
representations -X- _ O
. -X- _ O

Over -X- _ O
the -X- _ O
ten -X- _ O
performed -X- _ O
runs -X- _ O
, -X- _ O
the -X- _ O
TTI -X- _ B-MethodName
- -X- _ I-MethodName
GAN -X- _ I-MethodName
training -X- _ O
collapsed -X- _ O
once -X- _ O
for -X- _ O
both -X- _ O
the -X- _ O

37Textual -X- _ O
Results -X- _ O
Representation -X- _ O
# -X- _ O
C -X- _ O
/ -X- _ O
# -X- _ O
N -X- _ O
# -X- _ O
U -X- _ O
Ground -X- _ O
Truth -X- _ O

20 -X- _ O
/ -X- _ O
1=20 -X- _ O
4 -X- _ O
Table -X- _ O
3 -X- _ O
. -X- _ O

Qualitative -X- _ O
assessment -X- _ O
by -X- _ O
clinicians -X- _ O
for -X- _ O
the -X- _ O
produced -X- _ O
images -X- _ O
of -X- _ O
the -X- _ O
StackGAN -X- _ B-MethodName
Stage-2 -X- _ O
model -X- _ O
. -X- _ O

Are -X- _ O
the -X- _ O
caption -X- _ O
and -X- _ O
the -X- _ O
image -X- _ O
congruent -X- _ O
? -X- _ O

( -X- _ O
Congruent -X- _ O
( -X- _ O
C -X- _ O
) -X- _ O
/ -X- _ O
Not -X- _ O
congruent -X- _ O
( -X- _ O
N -X- _ O
) -X- _ O
/ -X- _ O
Unclear -X- _ O
( -X- _ O
U -X- _ O
) -X- _ O
. -X- _ O

Higher -X- _ O
values -X- _ O
of -X- _ O
the -X- _ O
proportion -X- _ O
# -X- _ O
C -X- _ O
/ -X- _ O
# -X- _ O
N -X- _ O
indicate -X- _ O
better -X- _ O
alignment -X- _ O
. -X- _ O

ARAE -X- _ B-MethodName
and -X- _ O
autoencoder -X- _ B-MethodName
representations -X- _ O
. -X- _ O

As -X- _ O
those -X- _ O
runs -X- _ O
were -X- _ O
clear -X- _ O
outliers -X- _ O
originating -X- _ O
from -X- _ O
the -X- _ O
collapse -X- _ O
of -X- _ O
GAN -X- _ B-MethodName
training -X- _ O
, -X- _ O
they -X- _ O
were -X- _ O
removed -X- _ O
from -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
. -X- _ O

As -X- _ O
expected -X- _ O
, -X- _ O
the -X- _ O
ARAE -X- _ B-MethodName
results -X- _ O
do -X- _ O
appear -X- _ O
to -X- _ O
lead -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
overall -X- _ O
image -X- _ O
quality -X- _ O
, -X- _ O
followed -X- _ O
by -X- _ O
the -X- _ O
word2vec -X- _ B-MethodName
( -X- _ O
concat -X- _ O
) -X- _ O
and -X- _ O
autoencoder -X- _ B-MethodName
models -X- _ O
. -X- _ O

The -X- _ O
word2vec -X- _ O
( -X- _ O
sum -X- _ O
) -X- _ O
consistently -X- _ O
leads -X- _ O
to -X- _ O
worse -X- _ O
solutions -X- _ O
. -X- _ O

In -X- _ O
terms -X- _ O
ofOqualimage -X- _ O
, -X- _ O
the -X- _ O
word2vec -X- _ B-MethodName
( -X- _ O
concat -X- _ O
) -X- _ O
model -X- _ O
experiences -X- _ O
less -X- _ O
overﬁtting -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
image -X- _ O
quality -X- _ O
than -X- _ O
the -X- _ O
other -X- _ O
representations -X- _ O
( -X- _ O
11:4 -X- _ O
% -X- _ O
versus -X- _ O
15 50 -X- _ O
% -X- _ O
) -X- _ O
, -X- _ O
suggesting -X- _ O
that -X- _ O
such -X- _ O
concatenated -X- _ O
word2vec -X- _ O
representations -X- _ O
, -X- _ O
that -X- _ O
maintain -X- _ O
word -X- _ O
order -X- _ O
, -X- _ O
generalize -X- _ O
well -X- _ O
. -X- _ O

While -X- _ O
the -X- _ O
Stage-2 -X- _ O
StackGAN -X- _ B-MethodName
results -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
ARAE -X- _ B-MethodName
representations -X- _ O
achieve -X- _ O
the -X- _ O
highest -X- _ O
image -X- _ O
quality -X- _ O
again -X- _ O
, -X- _ O
they -X- _ O
do -X- _ O
n’t -X- _ O
entirely -X- _ O
agree -X- _ O
with -X- _ O
the -X- _ O
TTI -X- _ B-MethodName
- -X- _ I-MethodName
GAN -X- _ I-MethodName
results -X- _ O
. -X- _ O

This -X- _ O
can -X- _ O
be -X- _ O
attributed -X- _ O
to -X- _ O
several -X- _ O
causes -X- _ O
: -X- _ O
1 -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
for -X- _ O
Stage-2 -X- _ O
StackGAN -X- _ B-MethodName
only -X- _ O
include -X- _ O
results -X- _ O
for -X- _ O
1 -X- _ O
trained -X- _ O
model -X- _ O
as -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
compare -X- _ O
the -X- _ O
metrics -X- _ O
for -X- _ O
such -X- _ O
a -X- _ O
model -X- _ O
with -X- _ O
the -X- _ O
human -X- _ O
judgment -X- _ O
scores -X- _ O
; -X- _ O

2 -X- _ O
. -X- _ O

The -X- _ O
Stage-2 -X- _ O
StackGAN -X- _ B-MethodName
training -X- _ O
produces -X- _ O
more -X- _ O
detailed -X- _ O
images -X- _ O
of -X- _ O
higher -X- _ O
resolution -X- _ O
so -X- _ O
consistent -X- _ O
training -X- _ O
is -X- _ O
more -X- _ O
difﬁcult -X- _ O
; -X- _ O
3 -X- _ O
. -X- _ O

The -X- _ O
augmented -X- _ O
conditioning -X- _ O
adds -X- _ O
to -X- _ O
the -X- _ O
original -X- _ O
representation -X- _ O
, -X- _ O
likely -X- _ O
making -X- _ O
the -X- _ O
outcome -X- _ O
for -X- _ O
each -X- _ O
representation -X- _ O
more -X- _ O
similar -X- _ O
. -X- _ O

With -X- _ O
the -X- _ O
exception -X- _ O
of -X- _ O
the -X- _ O
autoencoder -X- _ O
representation -X- _ O
, -X- _ O
the -X- _ O
outcome -X- _ O
of -X- _ O
the -X- _ O
Stage-2 -X- _ O
model -X- _ O
, -X- _ O
which -X- _ O
relies -X- _ O
on -X- _ O
the -X- _ O
outcome -X- _ O
of -X- _ O
the -X- _ O
ﬁrst -X- _ O
stage -X- _ O
, -X- _ O
exhibit -X- _ O
a -X- _ O
lot -X- _ O
more -X- _ O
overﬁtting -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
bothOqualimage -X- _ O
andOalign -X- _ O
imtxtwith -X- _ O
values -X- _ O
that -X- _ O
range -X- _ O
from -X- _ O
126 -X- _ O
% -X- _ O
to498 -X- _ O
% -X- _ O
. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
assess -X- _ O
the -X- _ O
validity -X- _ O
of -X- _ O
the -X- _ O
quantitative -X- _ O
assessment -X- _ O
, -X- _ O
a -X- _ O
trained -X- _ O
clinician -X- _ O
carries -X- _ O
out -X- _ O
a -X- _ O
visual -X- _ O
assessment -X- _ O
of -X- _ O
the -X- _ O
produced -X- _ O
image -X- _ O
samples -X- _ O
. -X- _ O

Werandomly -X- _ O
pick -X- _ O
25 -X- _ O
produced -X- _ O
images -X- _ O
of -X- _ O
the -X- _ O
StackGAN -X- _ B-MethodName
stage-2 -X- _ O
models -X- _ O
for -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
textual -X- _ O
representations -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
selected -X- _ O
25 -X- _ O
true -X- _ O
caption -X- _ O
- -X- _ O
image -X- _ O
pairs -X- _ O
to -X- _ O
compare -X- _ O
the -X- _ O
models -X- _ O
to -X- _ O
. -X- _ O

The -X- _ O
evaluator -X- _ O
was -X- _ O
asked -X- _ O
to -X- _ O
determine -X- _ O
for -X- _ O
each -X- _ O
sample -X- _ O
: -X- _ O
Are -X- _ O
the -X- _ O
caption -X- _ O
and -X- _ O
the -X- _ O
generated -X- _ O
image -X- _ O
congruent -X- _ O
or -X- _ O
conﬂicting -X- _ O
? -X- _ O

( -X- _ O
Congruent -X- _ O
/ -X- _ O
Conﬂicting -X- _ O
/ -X- _ O
Unclear -X- _ O
) -X- _ O

The -X- _ O
evaluator -X- _ O
was -X- _ O
also -X- _ O
asked -X- _ O
for -X- _ O
each -X- _ O
image -X- _ O
if -X- _ O
it -X- _ O
was -X- _ O
clearly -X- _ O
not -X- _ O
a -X- _ O
real -X- _ O
but -X- _ O
generated -X- _ O
X -X- _ O
- -X- _ O
Ray -X- _ O
, -X- _ O
but -X- _ O
did -X- _ O
n’t -X- _ O
ﬁnd -X- _ O
that -X- _ O
to -X- _ O
be -X- _ O
the -X- _ O
case -X- _ O
for -X- _ O
any -X- _ O
of -X- _ O
the -X- _ O
images -X- _ O
. -X- _ O

This -X- _ O
reﬂects -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
all -X- _ O
Wqualimage -X- _ O
appear -X- _ O
to -X- _ O
be -X- _ O
quite -X- _ O
similar -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
while -X- _ O
our -X- _ O
model -X- _ O
produces -X- _ O
an -X- _ O
output -X- _ O
of -X- _ O
256 -X- _ O
by -X- _ O
256 -X- _ O
pixels -X- _ O
, -X- _ O
a -X- _ O
higher -X- _ O
resolution -X- _ O
is -X- _ O
still -X- _ O
desirable -X- _ O
to -X- _ O
make -X- _ O
accurate -X- _ O
judgments -X- _ O
about -X- _ O
the -X- _ O
content -X- _ O
of -X- _ O
such -X- _ O
X -X- _ O
- -X- _ O
Rays -X- _ O
. -X- _ O

In -X- _ O
cases -X- _ O
where -X- _ O
the -X- _ O
clinician -X- _ O
found -X- _ O
that -X- _ O
additional -X- _ O
information -X- _ O
would -X- _ O
be -X- _ O
necessary -X- _ O
to -X- _ O
judge -X- _ O
whether -X- _ O
the -X- _ O
alignment -X- _ O
is -X- _ O
correct -X- _ O
, -X- _ O
the -X- _ O
clinician -X- _ O
was -X- _ O
able -X- _ O
to -X- _ O
respond -X- _ O
with -X- _ O
” -X- _ O
unclear -X- _ O
” -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
this -X- _ O
does -X- _ O
not -X- _ O
mean -X- _ O
that -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
image -X- _ O
was -X- _ O
bad -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
. -X- _ O

From -X- _ O
the -X- _ O
results -X- _ O
, -X- _ O
we -X- _ O
ﬁnd -X- _ O
that -X- _ O
indeed -X- _ O
the -X- _ O
word2vec -X- _ B-MethodName
summation -X- _ O
model -X- _ O
and -X- _ O
the -X- _ O
ARAE -X- _ B-MethodName
model -X- _ O
, -X- _ O
that -X- _ O
obtained -X- _ O
the -X- _ O
best -X- _ O
alignment -X- _ O
scores -X- _ O
Walign -X- _ O
imtxtaccording -X- _ O
to -X- _ O
our -X- _ O
quantitative -X- _ O
measures -X- _ O
, -X- _ O
also -X- _ O
appear -X- _ O
to -X- _ O
be -X- _ O
the -X- _ O
best -X- _ O
aligned -X- _ O
in -X- _ O
the -X- _ O
human -X- _ O
judgment -X- _ O
. -X- _ O

While -X- _ O
the -X- _ O
word2vec -X- _ B-MethodName
concatenation -X- _ O
model -X- _ O
achieved -X- _ O
a -X- _ O
slightly -X- _ O
worse -X- _ O
Walign -X- _ B-MetricName
imtxtscore -X- _ I-MetricName
, -X- _ O
the -X- _ O
clinician -X- _ O
still -X- _ O
judged -X- _ O
its -X- _ O
alignment -X- _ O
to -X- _ O
be -X- _ O
better -X- _ O
than -X- _ O
the -X- _ O
autoencoder -X- _ B-MethodName
model -X- _ O
for -X- _ O
the -X- _ O
selected -X- _ O
samples -X- _ O
, -X- _ O
perhaps -X- _ O
reﬂecting -X- _ O
its -X- _ O
slightly -X- _ O
improved -X- _ O
Wqualimage -X- _ B-MetricName
over -X- _ O
the -X- _ O
autoencoder -X- _ O
model -X- _ O
. -X- _ O

In -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
a -X- _ O
generated -X- _ O
image -X- _ O
of -X- _ O
stage -X- _ O
- -X- _ O
I -X- _ O
and -X- _ O
stage -X- _ O
- -X- _ O
II -X- _ O
is -X- _ O
presented -X- _ O
along -X- _ O
the -X- _ O
architecture -X- _ O
. -X- _ O

While -X- _ O
the -X- _ O
Stage -X- _ O
- -X- _ O
I -X- _ O
images -X- _ O
capture -X- _ O
the -X- _ O
structure -X- _ O
and -X- _ O
main -X- _ O
features -X- _ O
of -X- _ O
the -X- _ O
X -X- _ O
- -X- _ O
Rays -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
clear -X- _ O
improvement -X- _ O
in -X- _ O
quality -X- _ O
for -X- _ O
the -X- _ O
stage -X- _ O
- -X- _ O
II -X- _ O
images -X- _ O
. -X- _ O

6 -X- _ O
Conclusion -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
proposed -X- _ O
a -X- _ O
method -X- _ O
to -X- _ O
determine -X- _ B-TaskName
the -X- _ I-TaskName
quality -X- _ I-TaskName
of -X- _ I-TaskName
textual -X- _ I-TaskName
representations -X- _ I-TaskName
by -X- _ O
visualizing -X- _ O
them -X- _ O
with -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
models -X- _ O
. -X- _ O

After -X- _ O
testing -X- _ O
our -X- _ O
approach -X- _ O
on -X- _ O
four -X- _ O
different -X- _ O
unsupervised -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
models -X- _ O
, -X- _ O
it -X- _ O
appears -X- _ O
that -X- _ O
textual -X- _ O
representations -X- _ O
that -X- _ O
retain -X- _ O
word -X- _ O
order -X- _ O
and -X- _ O
lie -X- _ O
on -X- _ O
a -X- _ O
smooth -X- _ O
representation -X- _ O
space -X- _ O
, -X- _ O
lead -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
quality -X- _ O
of -X- _ O
image -X- _ O
output -X- _ O
. -X- _ O

We -X- _ O
proposed -X- _ O
a -X- _ O
method -X- _ O
to -X- _ O
judge -X- _ O
the -X- _ O
alignment -X- _ O
of -X- _ O
the -X- _ O
captions -X- _ O
with -X- _ O
the -X- _ O
visual -X- _ O
output -X- _ O
which -X- _ O
correlates -X- _ O
with -X- _ O
the -X- _ O
judgment -X- _ O
of -X- _ O

38a -X- _ O
trained -X- _ O
clinician -X- _ O
. -X- _ O

While -X- _ O
only -X- _ O
unsupervised -X- _ O
representations -X- _ O
were -X- _ O
used -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
the -X- _ O
methodology -X- _ O
can -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
other -X- _ O
types -X- _ O
of -X- _ O
textual -X- _ O
representations -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
constitute -X- _ O
a -X- _ O
new -X- _ O
methodology -X- _ O
to -X- _ O
evaluate -X- _ B-TaskName
textual -X- _ I-TaskName
representations -X- _ I-TaskName
through -X- _ O
visualization -X- _ O
and -X- _ O
offer -X- _ O
an -X- _ O
interesting -X- _ O
path -X- _ O
for -X- _ O
future -X- _ O
work -X- _ O
. -X- _ O

The -X- _ O
application -X- _ O
of -X- _ O
the -X- _ O
method -X- _ O
to -X- _ O
more -X- _ O
complex -X- _ O
sentences -X- _ O
, -X- _ O
different -X- _ O
ﬁelds -X- _ O
or -X- _ O
topics -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
development -X- _ O
of -X- _ O
alternative -X- _ O
alignment -X- _ O
measures -X- _ O
are -X- _ O
interesting -X- _ O
possibilities -X- _ O
for -X- _ O
such -X- _ O
research -X- _ O
. -X- _ O

Acknowledgments -X- _ O
We -X- _ O
thank -X- _ O
Dr. -X- _ O
Erwin -X- _ O
Str -X- _ O
¨oker -X- _ O
from -X- _ O
the -X- _ O
UZ -X- _ O
BrusselVUB -X- _ O
for -X- _ O
sharing -X- _ O
his -X- _ O
expertise -X- _ O
in -X- _ O
the -X- _ O
qualitative -X- _ O
assessment -X- _ O
of -X- _ O
generated -X- _ O
samples -X- _ O
. -X- _ O

References -X- _ O
Martin -X- _ O
Arjovsky -X- _ O
, -X- _ O
Soumith -X- _ O
Chintala -X- _ O
, -X- _ O
and -X- _ O
L -X- _ O
´ -X- _ O
eon -X- _ O
Bottou -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Wasserstein -X- _ O
gan -X- _ O
. -X- _ O
arXiv -X- _ O
preprint -X- _ O
Ali -X- _ O
Borji -X- _ O
. -X- _ O
2018 -X- _ O
. -X- _ O

Pros -X- _ O
and -X- _ O
cons -X- _ O
of -X- _ O
gan -X- _ O
evaluation -X- _ O
measures -X- _ O
. -X- _ O

arXiv -X- _ O
preprint -X- _ O
arXiv:1802.03446 -X- _ O
. -X- _ O

Ivo -X- _ O
Danihelka -X- _ O
, -X- _ O
Balaji -X- _ O
Lakshminarayanan -X- _ O
, -X- _ O
Benigno -X- _ O
Uria -X- _ O
, -X- _ O
Daan -X- _ O
Wierstra -X- _ O
, -X- _ O
and -X- _ O
Peter -X- _ O
Dayan -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Comparison -X- _ O
of -X- _ O
maximum -X- _ O
likelihood -X- _ O
and -X- _ O
gan -X- _ O
- -X- _ O
based -X- _ O
training -X- _ O
of -X- _ O
real -X- _ O
nvps -X- _ O
. -X- _ O

arXiv -X- _ O
preprint -X- _ O
arXiv:1705.05263 -X- _ O
. -X- _ O

Dina -X- _ O
Demner -X- _ O
- -X- _ O
Fushman -X- _ O
, -X- _ O
Marc -X- _ O
D -X- _ O
Kohli -X- _ O
, -X- _ O
Marc -X- _ O
B -X- _ O
Rosenman -X- _ O
, -X- _ O
Sonya -X- _ O
E -X- _ O
Shooshan -X- _ O
, -X- _ O
Laritza -X- _ O
Rodriguez -X- _ O
, -X- _ O
Sameer -X- _ O
Antani -X- _ O
, -X- _ O
George -X- _ O
R -X- _ O
Thoma -X- _ O
, -X- _ O
and -X- _ O
Clement -X- _ O
J -X- _ O
McDonald -X- _ O
. -X- _ O
2015 -X- _ O
. -X- _ O

Preparing -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
radiology -X- _ O
examinations -X- _ O
for -X- _ O
distribution -X- _ O
and -X- _ O
retrieval -X- _ O
. -X- _ O

Journal -X- _ O
of -X- _ O
the -X- _ O
American -X- _ O
Medical -X- _ O
Informatics -X- _ O
Association -X- _ O
, -X- _ O
Carl -X- _ O
Doersch -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

Tutorial -X- _ O
on -X- _ O
variational -X- _ O
autoencoders -X- _ O
. -X- _ O

arXiv -X- _ O
preprint -X- _ O
arXiv:1606.05908 -X- _ O
. -X- _ O

Manaal -X- _ O
Faruqui -X- _ O
, -X- _ O
Yulia -X- _ O
Tsvetkov -X- _ O
, -X- _ O
Pushpendre -X- _ O
Rastogi -X- _ O
, -X- _ O
and -X- _ O
Chris -X- _ O
Dyer -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

Problems -X- _ O
with -X- _ O
evaluation -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
using -X- _ O
word -X- _ O
similarity -X- _ O
tasks -X- _ O
. -X- _ O

arXiv -X- _ O
Ian -X- _ O
Goodfellow -X- _ O
, -X- _ O
Jean -X- _ O
Pouget -X- _ O
- -X- _ O
Abadie -X- _ O
, -X- _ O
Mehdi -X- _ O
Mirza -X- _ O
, -X- _ O
Bing -X- _ O
Xu -X- _ O
, -X- _ O
David -X- _ O
Warde -X- _ O
- -X- _ O
Farley -X- _ O
, -X- _ O
Sherjil -X- _ O
Ozair -X- _ O
, -X- _ O
Aaron -X- _ O
Courville -X- _ O
, -X- _ O
and -X- _ O
Yoshua -X- _ O
Bengio -X- _ O
. -X- _ O

2014 -X- _ O
. -X- _ O

Generative -X- _ O
adversarial -X- _ O
nets -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
, -X- _ O
pages -X- _ O
2672–2680 -X- _ O
. -X- _ O

Martin -X- _ O
Heusel -X- _ O
, -X- _ O
Hubert -X- _ O
Ramsauer -X- _ O
, -X- _ O
Thomas -X- _ O
Unterthiner -X- _ O
, -X- _ O
Bernhard -X- _ O
Nessler -X- _ O
, -X- _ O
G -X- _ O
¨unter -X- _ O
Klambauer -X- _ O
, -X- _ O
and -X- _ O
Sepp -X- _ O
Hochreiter -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Gans -X- _ B-MethodName
trained -X- _ O
by -X- _ O
a -X- _ O
two -X- _ O
time -X- _ O
- -X- _ O
scale -X- _ O
update -X- _ O
rule -X- _ O
converge -X- _ O
to -X- _ O
a -X- _ O
nash -X- _ O
equilibrium -X- _ O
. -X- _ O

arXiv -X- _ O
preprint -X- _ O
arXiv:1706.08500 -X- _ O
.Daniel -X- _ O

Jiwoong -X- _ O
I -X- _ O
m -X- _ O
, -X- _ O
He -X- _ O
Ma -X- _ O
, -X- _ O
Graham -X- _ O
Taylor -X- _ O
, -X- _ O
and -X- _ O
Kristin -X- _ O
Branson -X- _ O
. -X- _ O

2018 -X- _ O
. -X- _ O

Quantitatively -X- _ O
evaluating -X- _ O
gans -X- _ B-MethodName
with -X- _ O
divergences -X- _ O
proposed -X- _ O
for -X- _ O
training -X- _ O
. -X- _ O

arXiv -X- _ O
Tero -X- _ O
Karras -X- _ O
, -X- _ O
Timo -X- _ O
Aila -X- _ O
, -X- _ O
Samuli -X- _ O
Laine -X- _ O
, -X- _ O
and -X- _ O
Jaakko -X- _ O
Lehtinen -X- _ O
. -X- _ O

2017 -X- _ O
. -X- _ O

Progressive -X- _ O
growing -X- _ O
of -X- _ O
gans -X- _ B-MethodName
for -X- _ O
improved -X- _ O
quality -X- _ O
, -X- _ O
stability -X- _ O
, -X- _ O
and -X- _ O
variation -X- _ O
. -X- _ O

arXiv -X- _ O
Yoon -X- _ O
Kim -X- _ O
. -X- _ O

2014 -X- _ O
. -X- _ O

Convolutional -X- _ O
neural -X- _ O
networks -X- _ O
for -X- _ O
sentence -X- _ O
classiﬁcation -X- _ O
. -X- _ O

arXiv -X- _ O
preprint -X- _ O
Yoon -X- _ O
Kim -X- _ O
, -X- _ O
Yacine -X- _ O
Jernite -X- _ O
, -X- _ O
David -X- _ O
Sontag -X- _ O
, -X- _ O
and -X- _ O
Alexander -X- _ O
M -X- _ O
Rush -X- _ O
. -X- _ O

2016 -X- _ O
. -X- _ O

Character -X- _ O
- -X- _ O
aware -X- _ O
neural -X- _ O
language -X- _ O
models -X- _ O
. -X- _ O

In -X- _ O
AAAI -X- _ O
, -X- _ O
pages -X- _ O
2741–2749 -X- _ O
. -X- _ O

Yoon -X- _ O
Kim -X- _ O
, -X- _ O
Kelly -X- _ O
Zhang -X- _ O
, -X- _ O
Alexander -X- _ O
M -X- _ O
Rush -X- _ O
, -X- _ O
Yann -X- _ O
LeCun -X- _ O
, -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Adversarially -X- _ O
regularized -X- _ O
autoencoders -X- _ O
for -X- _ O
generating -X- _ O
discrete -X- _ O
structures -X- _ O
. -X- _ O

arXiv -X- _ O
Ryan -X- _ O
Kiros -X- _ O
, -X- _ O
Yukun -X- _ O
Zhu -X- _ O
, -X- _ O
Ruslan -X- _ O
R -X- _ O
Salakhutdinov -X- _ O
, -X- _ O
Richard -X- _ O
Zemel -X- _ O
, -X- _ O
Raquel -X- _ O
Urtasun -X- _ O
, -X- _ O
Antonio -X- _ O
Torralba -X- _ O
, -X- _ O
and -X- _ O
Sanja -X- _ O
Fidler -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

Skip -X- _ O
- -X- _ O
thought -X- _ O
vectors -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
, -X- _ O
Anders -X- _ O
Boesen -X- _ O
Lindbo -X- _ O
Larsen -X- _ O
, -X- _ O
Søren -X- _ O
Kaae -X- _ O
Sønderby -X- _ O
, -X- _ O
Hugo -X- _ O
Larochelle -X- _ O
, -X- _ O
and -X- _ O
Ole -X- _ O
Winther -X- _ O
. -X- _ O
2015 -X- _ O
. -X- _ O

Autoencoding -X- _ O
beyond -X- _ O
pixels -X- _ O
using -X- _ O
a -X- _ O
learned -X- _ O
similarity -X- _ O
metric.arXiv -X- _ O
preprint -X- _ O
arXiv:1512.09300 -X- _ O
. -X- _ O

Angeliki -X- _ O
Lazaridou -X- _ O
, -X- _ O
Dat -X- _ O
Tien -X- _ O
Nguyen -X- _ O
, -X- _ O
and -X- _ O
Marco -X- _ O
Baroni -X- _ O
. -X- _ O
2015 -X- _ O
. -X- _ O

Do -X- _ O
distributed -X- _ O
semantic -X- _ O
models -X- _ O
dream -X- _ O
of -X- _ O
electric -X- _ O
sheep -X- _ O
? -X- _ O

visualizing -X- _ O
word -X- _ O
representations -X- _ O
through -X- _ O
image -X- _ O
synthesis -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
Fourth -X- _ O
Workshop -X- _ O
on -X- _ O
Vision -X- _ O
and -X- _ O
Language -X- _ O
, -X- _ O
pages -X- _ O
Quoc -X- _ O
Le -X- _ O
and -X- _ O
Tomas -X- _ O
Mikolov -X- _ O
. -X- _ O
2014 -X- _ O
. -X- _ O

Distributed -X- _ O
representations -X- _ O
of -X- _ O
sentences -X- _ O
and -X- _ O
documents -X- _ O
. -X- _ O

In -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Machine -X- _ O
Learning -X- _ O
, -X- _ O
pages -X- _ O
Tal -X- _ O
Linzen -X- _ O
. -X- _ O

2016 -X- _ O
. -X- _ O

Issues -X- _ O
in -X- _ O
evaluating -X- _ O
semantic -X- _ O
spaces -X- _ O
using -X- _ O
word -X- _ O
analogies -X- _ O
. -X- _ O

arXiv -X- _ O
preprint -X- _ O
Tomas -X- _ O
Mikolov -X- _ O
, -X- _ O
Kai -X- _ O
Chen -X- _ O
, -X- _ O
Greg -X- _ O
Corrado -X- _ O
, -X- _ O
and -X- _ O
Jeffrey -X- _ O
Dean -X- _ O
. -X- _ O
2013a -X- _ O
. -X- _ O

Efﬁcient -X- _ O
estimation -X- _ O
of -X- _ O
word -X- _ O
representations -X- _ O
in -X- _ O
vector -X- _ O
space -X- _ O
. -X- _ O

arXiv -X- _ O
preprint -X- _ O
Tomas -X- _ O
Mikolov -X- _ O
, -X- _ O
Ilya -X- _ O
Sutskever -X- _ O
, -X- _ O
Kai -X- _ O
Chen -X- _ O
, -X- _ O
Greg -X- _ O
S -X- _ O
Corrado -X- _ O
, -X- _ O
and -X- _ O
Jeff -X- _ O
Dean -X- _ O
. -X- _ O
2013b -X- _ O
. -X- _ O

Distributed -X- _ O
representations -X- _ O
of -X- _ O
words -X- _ O
and -X- _ O
phrases -X- _ O
and -X- _ O
their -X- _ O
compositionality -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
neural -X- _ O
information -X- _ O
processing -X- _ O
Tomas -X- _ O
Mikolov -X- _ O
, -X- _ O
Wen -X- _ O
- -X- _ O
tau -X- _ O
Yih -X- _ O
, -X- _ O
and -X- _ O
Geoffrey -X- _ O
Zweig -X- _ O
. -X- _ O
2013c -X- _ O
. -X- _ O

Linguistic -X- _ O
regularities -X- _ O
in -X- _ O
continuous -X- _ O
space -X- _ O
word -X- _ O
representations -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2013 -X- _ O
Conference -X- _ O
of -X- _ O
the -X- _ O
North -X- _ O
American -X- _ O
Chapter -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
: -X- _ O
Human -X- _ O
Language -X- _ O
Technologies -X- _ O
, -X- _ O
pages -X- _ O
746–751 -X- _ O
. -X- _ O

39Augustus -X- _ O
Odena -X- _ O
, -X- _ O
Christopher -X- _ O
Olah -X- _ O
, -X- _ O
and -X- _ O
Jonathon -X- _ O
Shlens -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

Conditional -X- _ O
image -X- _ O
synthesis -X- _ O
with -X- _ O
auxiliary -X- _ O
classiﬁer -X- _ O
gans -X- _ B-MethodName
. -X- _ O

arXiv -X- _ O
preprint -X- _ O
Tim -X- _ O
Salimans -X- _ O
, -X- _ O
Ian -X- _ O
Goodfellow -X- _ O
, -X- _ O
Wojciech -X- _ O
Zaremba -X- _ O
, -X- _ O
Vicki -X- _ O
Cheung -X- _ O
, -X- _ O
Alec -X- _ O
Radford -X- _ O
, -X- _ O
and -X- _ O
Xi -X- _ O
Chen -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

Improved -X- _ O
techniques -X- _ O
for -X- _ O
training -X- _ O
gans -X- _ B-MethodName
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
, -X- _ O
pages -X- _ O
Tobias -X- _ O
Schnabel -X- _ O
, -X- _ O
Igor -X- _ O
Labutov -X- _ O
, -X- _ O
David -X- _ O
Mimno -X- _ O
, -X- _ O
and -X- _ O
Thorsten -X- _ O
Joachims -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

Evaluation -X- _ O
methods -X- _ O
for -X- _ O
unsupervised -X- _ O
word -X- _ O
embeddings -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2015 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
pages -X- _ O
298–307 -X- _ O
. -X- _ O

Graham -X- _ O
Spinks -X- _ O
and -X- _ O
Marie -X- _ O
- -X- _ O
Francine -X- _ O
Moens -X- _ O
. -X- _ O
2018 -X- _ O
. -X- _ O

Generating -X- _ O
continuous -X- _ O
representations -X- _ O
of -X- _ O
medical -X- _ O
texts -X- _ O
. -X- _ O

NAACL -X- _ O
HLT -X- _ O
2018 -X- _ O
, -X- _ O
page -X- _ O
66 -X- _ O
. -X- _ O

Joseph -X- _ O
Turian -X- _ O
, -X- _ O
Lev -X- _ O
Ratinov -X- _ O
, -X- _ O
and -X- _ O
Yoshua -X- _ O
Bengio -X- _ O
. -X- _ O

2010 -X- _ O
. -X- _ O

Word -X- _ O
representations -X- _ O
: -X- _ O
a -X- _ O
simple -X- _ O
and -X- _ O
general -X- _ O
method -X- _ O
for -X- _ O
semi -X- _ O
- -X- _ O
supervised -X- _ O
learning -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
48th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
pages -X- _ O
384–394 -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Han -X- _ O
Zhang -X- _ O
, -X- _ O
Tao -X- _ O
Xu -X- _ O
, -X- _ O
Hongsheng -X- _ O
Li -X- _ O
, -X- _ O
Shaoting -X- _ O
Zhang -X- _ O
, -X- _ O
Xiaolei -X- _ O
Huang -X- _ O
, -X- _ O
Xiaogang -X- _ O
Wang -X- _ O
, -X- _ O
and -X- _ O
Dimitris -X- _ O
Metaxas -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Stackgan -X- _ O
: -X- _ O

Text -X- _ O
to -X- _ O
photo -X- _ O
- -X- _ O
realistic -X- _ O
image -X- _ O
synthesis -X- _ O
with -X- _ O
stacked -X- _ O
generative -X- _ O
adversarial -X- _ O
networks -X- _ O
. -X- _ O

In -X- _ O
IEEE -X- _ O
Int -X- _ O
. -X- _ O

Conf -X- _ O
. -X- _ O

Comput -X- _ O
. -X- _ O

Vision -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
2018 -X- _ O
EMNLP -X- _ O
Workshop -X- _ O
BlackboxNLP -X- _ O
: -X- _ O
Analyzing -X- _ O
and -X- _ O
Interpreting -X- _ O
Neural -X- _ O
Networks -X- _ O
for -X- _ O
NLP -X- _ O
, -X- _ O
pages -X- _ O
40–46 -X- _ O
Brussels -X- _ O
, -X- _ O
Belgium -X- _ O
, -X- _ O
November -X- _ O
1 -X- _ O
, -X- _ O
2018 -X- _ O
. -X- _ O

c -X- _ O

2018 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics40On -X- _ O
the -X- _ O
Role -X- _ O
of -X- _ O
Text -X- _ O
Preprocessing -X- _ O
in -X- _ O
Neural -X- _ O
Network -X- _ O
Architectures -X- _ O
: -X- _ O
An -X- _ O
Evaluation -X- _ O
Study -X- _ O
on -X- _ O
Text -X- _ O
Categorization -X- _ O
and -X- _ O
Sentiment -X- _ O
Analysis -X- _ O
Jose -X- _ O
Camacho -X- _ O
- -X- _ O
Collados -X- _ O
School -X- _ O
of -X- _ O
Computer -X- _ O
Science -X- _ O
and -X- _ O
Informatics -X- _ O
Cardiff -X- _ O
University -X- _ O
camachocolladosj -X- _ O
@ -X- _ O
cardiff.ac.ukMohammad -X- _ O
Taher -X- _ O
Pilehvar -X- _ O
School -X- _ O
of -X- _ O
Computer -X- _ O
Engineering -X- _ O
Iran -X- _ O
University -X- _ O
of -X- _ O
Science -X- _ O
and -X- _ O
Technology -X- _ O
pilehvar -X- _ O
@ -X- _ O
iust.ac.ir -X- _ O
Abstract -X- _ O
Text -X- _ O
preprocessing -X- _ O
is -X- _ O
often -X- _ O
the -X- _ O
ﬁrst -X- _ O
step -X- _ O
in -X- _ O
the -X- _ O
pipeline -X- _ O
of -X- _ O
a -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
NLP -X- _ O
) -X- _ O
system -X- _ O
, -X- _ O
with -X- _ O
potential -X- _ O
impact -X- _ O
in -X- _ O
its -X- _ O
ﬁnal -X- _ O
performance -X- _ O
. -X- _ O

Despite -X- _ O
its -X- _ O
importance -X- _ O
, -X- _ O
text -X- _ O
preprocessing -X- _ O
has -X- _ O
not -X- _ O
received -X- _ O
much -X- _ O
attention -X- _ O
in -X- _ O
the -X- _ O
deep -X- _ O
learning -X- _ O
literature -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
we -X- _ O
investigate -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
simple -X- _ O
text -X- _ O
preprocessing -X- _ O
decisions -X- _ O
( -X- _ O
particularly -X- _ O
tokenizing -X- _ O
, -X- _ O
lemmatizing -X- _ O
, -X- _ O
lowercasing -X- _ O
and -X- _ O
multiword -X- _ O
grouping -X- _ O
) -X- _ O
on -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
a -X- _ O
standard -X- _ O
neural -X- _ O
text -X- _ O
classiﬁer -X- _ O
. -X- _ O

We -X- _ O
perform -X- _ O
an -X- _ O
extensive -X- _ O
evaluation -X- _ O
on -X- _ O
standard -X- _ O
benchmarks -X- _ O
from -X- _ O
text -X- _ O
categorization -X- _ O
and -X- _ O
sentiment -X- _ O
analysis -X- _ O
. -X- _ O

While -X- _ O
our -X- _ O
experiments -X- _ O
show -X- _ O
that -X- _ O
a -X- _ O
simple -X- _ O
tokenization -X- _ O
of -X- _ O
input -X- _ O
text -X- _ O
is -X- _ O
generally -X- _ O
adequate -X- _ O
, -X- _ O
they -X- _ O
also -X- _ O
highlight -X- _ O
signiﬁcant -X- _ O
degrees -X- _ O
of -X- _ O
variability -X- _ O
across -X- _ O
preprocessing -X- _ O
techniques -X- _ O
. -X- _ O

This -X- _ O
reveals -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
paying -X- _ O
attention -X- _ O
to -X- _ O
this -X- _ O
usually -X- _ O
- -X- _ O
overlooked -X- _ O
step -X- _ O
in -X- _ O
the -X- _ O
pipeline -X- _ O
, -X- _ O
particularly -X- _ O
when -X- _ O
comparing -X- _ O
different -X- _ O
models -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
our -X- _ O
evaluation -X- _ O
provides -X- _ O
insights -X- _ O
into -X- _ O
the -X- _ O
best -X- _ O
preprocessing -X- _ O
practices -X- _ O
for -X- _ O
training -X- _ O
word -X- _ O
embeddings -X- _ O
. -X- _ O

1 -X- _ O

Introduction -X- _ O
Words -X- _ O
are -X- _ O
often -X- _ O
considered -X- _ O
as -X- _ O
the -X- _ O
basic -X- _ O
constituents -X- _ O
of -X- _ O
texts -X- _ O
for -X- _ O
many -X- _ O
languages -X- _ O
, -X- _ O
including -X- _ O
English.1The -X- _ O
ﬁrst -X- _ O
module -X- _ O
in -X- _ O
an -X- _ O
NLP -X- _ O
pipeline -X- _ O
is -X- _ O
a -X- _ O
tokenizer -X- _ O
which -X- _ O
transforms -X- _ O
texts -X- _ O
to -X- _ O
sequences -X- _ O
of -X- _ O
words -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
in -X- _ O
practise -X- _ O
, -X- _ O
other -X- _ O
preprocessing -X- _ O
techniques -X- _ O
can -X- _ O
be -X- _ O
( -X- _ O
and -X- _ O
are -X- _ O
) -X- _ O
further -X- _ O
used -X- _ O
together -X- _ O
with -X- _ O
tokenization -X- _ O
. -X- _ O

These -X- _ O
include -X- _ O
lemmatization -X- _ O
, -X- _ O
lowercasing -X- _ O
and -X- _ O
multiword -X- _ O
grouping -X- _ O
, -X- _ O
among -X- _ O
others -X- _ O
. -X- _ O

Although -X- _ O
these -X- _ O
preprocessing -X- _ O
decisions -X- _ O
have -X- _ O
in -X- _ O
NLP -X- _ O
in -X- _ O
general -X- _ O
and -X- _ O
text -X- _ O
classiﬁcation -X- _ O
in -X- _ O
particular -X- _ O
, -X- _ O
recent -X- _ O
work -X- _ O
has -X- _ O
also -X- _ O
considered -X- _ O
other -X- _ O
linguistic -X- _ O
units -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
characters -X- _ O
( -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Xiao -X- _ O
and -X- _ O
Cho -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
or -X- _ O
word -X- _ O
senses -X- _ O
( -X- _ O
Li -X- _ O
and -X- _ O
Jurafsky -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Flekova -X- _ O
and -X- _ O
Gurevych -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Pilehvar -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

These -X- _ O
techniques -X- _ O
require -X- _ O
a -X- _ O
different -X- _ O
kind -X- _ O
of -X- _ O
preprocessing -X- _ O
and -X- _ O
, -X- _ O
while -X- _ O
they -X- _ O
have -X- _ O
been -X- _ O
shown -X- _ O
effective -X- _ O
in -X- _ O
various -X- _ O
settings -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
work -X- _ O
we -X- _ O
only -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
mainstream -X- _ O
word -X- _ O
- -X- _ O
based -X- _ O
models.been -X- _ O
studied -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
conventional -X- _ O
text -X- _ O
classiﬁcation -X- _ O
techniques -X- _ O
( -X- _ O
Leopold -X- _ O
and -X- _ O
Kindermann -X- _ O
, -X- _ O
2002 -X- _ O
; -X- _ O
Uysal -X- _ O
and -X- _ O
Gunal -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
little -X- _ O
attention -X- _ O
has -X- _ O
been -X- _ O
paid -X- _ O
to -X- _ O
them -X- _ O
in -X- _ O
the -X- _ O
more -X- _ O
recent -X- _ O
neural -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
. -X- _ O

The -X- _ O
most -X- _ O
similar -X- _ O
study -X- _ O
to -X- _ O
ours -X- _ O
is -X- _ O
Zhang -X- _ O
and -X- _ O
LeCun -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
analyzed -X- _ O
different -X- _ O
encoding -X- _ O
levels -X- _ O
for -X- _ O
English -X- _ O
and -X- _ O
Asian -X- _ O
languages -X- _ O
such -X- _ O
as -X- _ O
Chinese -X- _ O
, -X- _ O
Japanese -X- _ O
and -X- _ O
Korean -X- _ O
. -X- _ O

As -X- _ O
opposed -X- _ O
to -X- _ O
our -X- _ O
work -X- _ O
, -X- _ O
their -X- _ O
analysis -X- _ O
was -X- _ O
focused -X- _ O
on -X- _ O
UTF-8 -X- _ O
bytes -X- _ O
, -X- _ O
characters -X- _ O
, -X- _ O
words -X- _ O
, -X- _ O
romanized -X- _ O
characters -X- _ O
and -X- _ O
romanized -X- _ O
words -X- _ O
as -X- _ O
encoding -X- _ O
levels -X- _ O
, -X- _ O
rather -X- _ O
than -X- _ O
the -X- _ O
preprocessing -X- _ O
techniques -X- _ O
analyzed -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
. -X- _ O

Additionally -X- _ O
, -X- _ O
word -X- _ O
embeddings -X- _ O
have -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
play -X- _ O
an -X- _ O
important -X- _ O
role -X- _ O
in -X- _ O
boosting -X- _ O
the -X- _ O
generalization -X- _ O
capabilities -X- _ O
of -X- _ O
neural -X- _ O
systems -X- _ O
( -X- _ O
Goldberg -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Camacho -X- _ O
- -X- _ O
Collados -X- _ O
and -X- _ O
Pilehvar -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
while -X- _ O
some -X- _ O
studies -X- _ O
have -X- _ O
focused -X- _ O
on -X- _ O
intrinsically -X- _ O
analyzing -X- _ O
the -X- _ O
role -X- _ O
of -X- _ O
lemmatization -X- _ O
in -X- _ O
their -X- _ O
underlying -X- _ O
training -X- _ O
corpus -X- _ O
( -X- _ O
Ebert -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Kuznetsov -X- _ O
and -X- _ O
Gurevych -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
impact -X- _ O
on -X- _ O
their -X- _ O
extrinsic -X- _ O
performance -X- _ O
when -X- _ O
integrated -X- _ O
into -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
architecture -X- _ O
has -X- _ O
remained -X- _ O
understudied.2 -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
role -X- _ O
of -X- _ O
preprocessing -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
, -X- _ O
particularly -X- _ O
in -X- _ O
how -X- _ O
it -X- _ O
is -X- _ O
split -X- _ O
into -X- _ O
individual -X- _ O
( -X- _ O
meaning -X- _ O
- -X- _ O
bearing -X- _ O
) -X- _ O
tokens -X- _ O
and -X- _ O
how -X- _ O
it -X- _ O
affects -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
standard -X- _ O
neural -X- _ O
text -X- _ O
classiﬁcation -X- _ O
models -X- _ O
based -X- _ O
on -X- _ O
Convolutional -X- _ B-MethodName
Neural -X- _ I-MethodName
Networks -X- _ I-MethodName
( -X- _ O
LeCun -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2010 -X- _ O
; -X- _ O
Kim -X- _ O
, -X- _ O
2014 -X- _ O
, -X- _ O
CNN -X- _ B-MethodName
) -X- _ O
. -X- _ O

CNNs -X- _ B-MethodName
have -X- _ O
proven -X- _ O
to -X- _ O
be -X- _ O
effective -X- _ O
in -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
NLP -X- _ O
applications -X- _ O
, -X- _ O
including -X- _ O
text -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
tasks -X- _ O
such -X- _ O
as -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
( -X- _ O
Johnson -X- _ O
and -X- _ O
Zhang -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Tang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Xiao -X- _ O
and -X- _ O
Cho -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Conneau -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
polarity -X- _ B-TaskName
detection -X- _ I-TaskName
portant -X- _ O
role -X- _ O
but -X- _ O
also -X- _ O
its -X- _ O
nature -X- _ O
, -X- _ O
domain -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O

Levy -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2015 -X- _ O
) -X- _ O
also -X- _ O
showed -X- _ O
how -X- _ O
small -X- _ O
hyperparameter -X- _ O
variations -X- _ O
may -X- _ O
have -X- _ O
an -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
these -X- _ O
considerations -X- _ O
remain -X- _ O
out -X- _ O
of -X- _ O
the -X- _ O
scope -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
. -X- _ O

41 -X- _ O
( -X- _ O
Kalchbrenner -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Kim -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Dos -X- _ O
Santos -X- _ O
and -X- _ O
Gatti -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Yin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
the -X- _ O
tasks -X- _ O
considered -X- _ O
in -X- _ O
this -X- _ O
work -X- _ O
. -X- _ O

The -X- _ O
goal -X- _ O
of -X- _ O
our -X- _ O
evaluation -X- _ O
study -X- _ O
is -X- _ O
to -X- _ O
ﬁnd -X- _ O
answers -X- _ O
to -X- _ O
the -X- _ O
following -X- _ O
two -X- _ O
questions -X- _ O
: -X- _ O
1 -X- _ O
. -X- _ O
Are -X- _ O
neural -X- _ O
network -X- _ O
architectures -X- _ O
( -X- _ O
in -X- _ O
particular -X- _ O
CNNs -X- _ B-MethodName
) -X- _ O
affected -X- _ O
by -X- _ O
seemingly -X- _ O
small -X- _ O
preprocessing -X- _ O
decisions -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
? -X- _ O

2 -X- _ O
. -X- _ O
Does -X- _ O
the -X- _ O
preprocessing -X- _ O
of -X- _ O
the -X- _ O
embeddings -X- _ O
’ -X- _ O
underlying -X- _ O
training -X- _ O
corpus -X- _ O
have -X- _ O
an -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
ﬁnal -X- _ O
performance -X- _ O
of -X- _ O
a -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
neural -X- _ O
network -X- _ O
text -X- _ O
classiﬁer -X- _ O
? -X- _ O

According -X- _ O
to -X- _ O
our -X- _ O
experiments -X- _ O
in -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
and -X- _ O
polarity -X- _ B-TaskName
detection -X- _ I-TaskName
, -X- _ O
these -X- _ O
decisions -X- _ O
are -X- _ O
important -X- _ O
in -X- _ O
certain -X- _ O
cases -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
we -X- _ O
shed -X- _ O
some -X- _ O
light -X- _ O
on -X- _ O
the -X- _ O
motivations -X- _ O
of -X- _ O
each -X- _ O
preprocessing -X- _ O
decision -X- _ O
and -X- _ O
provide -X- _ O
some -X- _ O
hints -X- _ O
on -X- _ O
how -X- _ O
to -X- _ O
normalize -X- _ O
the -X- _ O
input -X- _ O
corpus -X- _ O
to -X- _ O
better -X- _ O
suit -X- _ O
each -X- _ O
setting -X- _ O
. -X- _ O

The -X- _ O
accompanying -X- _ O
materials -X- _ O
of -X- _ O
this -X- _ O
submission -X- _ O
can -X- _ O
be -X- _ O
downloaded -X- _ O
at -X- _ O
the -X- _ O
following -X- _ O
repository -X- _ O
: -X- _ O

https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
pedrada88 -X- _ O
/ -X- _ O
preproc -X- _ O
- -X- _ O
textclassiﬁcation -X- _ O
. -X- _ O

2 -X- _ O
Text -X- _ O
Preprocessing -X- _ O
Given -X- _ O
an -X- _ O
input -X- _ O
text -X- _ O
, -X- _ O
words -X- _ O
are -X- _ O
gathered -X- _ O
as -X- _ O
input -X- _ O
units -X- _ O
of -X- _ O
classiﬁcation -X- _ O
models -X- _ O
through -X- _ O
tokenization -X- _ O
. -X- _ O

We -X- _ O
refer -X- _ O
to -X- _ O
the -X- _ O
corpus -X- _ O
which -X- _ O
is -X- _ O
only -X- _ O
tokenized -X- _ O
as -X- _ O
vanilla -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
given -X- _ O
the -X- _ O
sentence -X- _ O
“ -X- _ O
Apple -X- _ O
is -X- _ O
asking -X- _ O
its -X- _ O
manufacturers -X- _ O
to -X- _ O
move -X- _ O
MacBook -X- _ O
Air -X- _ O
production -X- _ O
to -X- _ O
the -X- _ O
United -X- _ O
States -X- _ O
. -X- _ O
” -X- _ O

( -X- _ O
running -X- _ O
example -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
vanilla -X- _ O
tokenized -X- _ O
text -X- _ O
would -X- _ O
be -X- _ O
as -X- _ O
follows -X- _ O
( -X- _ O
white -X- _ O
spaces -X- _ O
delimiting -X- _ O
different -X- _ O
word -X- _ O
units -X- _ O
) -X- _ O
: -X- _ O
Apple -X- _ O
is -X- _ O
asking -X- _ O
its -X- _ O
manufacturers -X- _ O
to -X- _ O
move -X- _ O
MacBook -X- _ O
Air -X- _ O
production -X- _ O
to -X- _ O
the -X- _ O
United -X- _ O
States -X- _ O
. -X- _ O

We -X- _ O
additionally -X- _ O
consider -X- _ O
three -X- _ O
simple -X- _ O
preprocessing -X- _ O
techniques -X- _ O
to -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
an -X- _ O
input -X- _ O
text -X- _ O
: -X- _ O
lowercasing -X- _ O
( -X- _ O
Section -X- _ O
2.1 -X- _ O
) -X- _ O
, -X- _ O
lemmatizing -X- _ O
( -X- _ O
Section -X- _ O
2.2 -X- _ O
) -X- _ O
and -X- _ O
multiword -X- _ O
grouping -X- _ O
( -X- _ O
Section -X- _ O
2.3 -X- _ O
) -X- _ O
. -X- _ O

2.1 -X- _ O

Lowercasing -X- _ O
This -X- _ O
is -X- _ O
the -X- _ O
simplest -X- _ O
preprocessing -X- _ O
technique -X- _ O
which -X- _ O
consists -X- _ O
of -X- _ O
lowercasing -X- _ O
each -X- _ O
single -X- _ O
token -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
: -X- _ O
apple -X- _ O
is -X- _ O
asking -X- _ O
its -X- _ O
manufacturers -X- _ O
to -X- _ O
move -X- _ O
macbook -X- _ O
air -X- _ O
production -X- _ O
to -X- _ O
the -X- _ O
united -X- _ O
states -X- _ O
.Due -X- _ O
to -X- _ O
its -X- _ O
simplicity -X- _ O
, -X- _ O
lowercasing -X- _ O
has -X- _ O
been -X- _ O
a -X- _ O
popular -X- _ O
practice -X- _ O
in -X- _ O
modules -X- _ O
of -X- _ O
deep -X- _ O
learning -X- _ O
libraries -X- _ O
and -X- _ O
word -X- _ O
embedding -X- _ O
packages -X- _ O
( -X- _ O
Pennington -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Faruqui -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

Despite -X- _ O
its -X- _ O
desirable -X- _ O
property -X- _ O
of -X- _ O
reducing -X- _ O
sparsity -X- _ O
and -X- _ O
vocabulary -X- _ O
size -X- _ O
, -X- _ O
lowercasing -X- _ O
may -X- _ O
negatively -X- _ O
impact -X- _ O
system -X- _ O
’s -X- _ O
performance -X- _ O
by -X- _ O
increasing -X- _ O
ambiguity -X- _ O
. -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
the -X- _ O
Apple -X- _ O
company -X- _ O
in -X- _ O
our -X- _ O
example -X- _ O
and -X- _ O
theapple -X- _ O
fruit -X- _ O
would -X- _ O
be -X- _ O
considered -X- _ O
as -X- _ O
identical -X- _ O
entities -X- _ O
. -X- _ O

2.2 -X- _ O
Lemmatizing -X- _ O
The -X- _ O
process -X- _ O
of -X- _ O
lemmatizing -X- _ O
consists -X- _ O
of -X- _ O
replacing -X- _ O
a -X- _ O
given -X- _ O
token -X- _ O
with -X- _ O
its -X- _ O
corresponding -X- _ O
lemma -X- _ O
: -X- _ O

Apple -X- _ O
be -X- _ O
ask -X- _ O
its -X- _ O
manufacturer -X- _ O
to -X- _ O
move -X- _ O
MacBook -X- _ O
Air -X- _ O
production -X- _ O
to -X- _ O
the -X- _ O
United -X- _ O
States -X- _ O
. -X- _ O

Lemmatization -X- _ O
has -X- _ O
been -X- _ O
traditionally -X- _ O
a -X- _ O
standard -X- _ O
preprocessing -X- _ O
technique -X- _ O
for -X- _ O
linear -X- _ O
text -X- _ O
classiﬁcation -X- _ O
systems -X- _ O
( -X- _ O
Mullen -X- _ O
and -X- _ O
Collier -X- _ O
, -X- _ O
2004 -X- _ O
; -X- _ O
Toman -X- _ O
is -X- _ O
rarely -X- _ O
used -X- _ O
as -X- _ O
a -X- _ O
preprocessing -X- _ O
stage -X- _ O
in -X- _ O
neuralbased -X- _ O
systems -X- _ O
. -X- _ O

The -X- _ O
main -X- _ O
idea -X- _ O
behind -X- _ O
lemmatization -X- _ O
is -X- _ O
to -X- _ O
reduce -X- _ O
sparsity -X- _ O
, -X- _ O
as -X- _ O
different -X- _ O
inﬂected -X- _ O
forms -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
lemma -X- _ O
may -X- _ O
occur -X- _ O
infrequently -X- _ O
( -X- _ O
or -X- _ O
not -X- _ O
at -X- _ O
all -X- _ O
) -X- _ O
during -X- _ O
training -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
this -X- _ O
may -X- _ O
come -X- _ O
at -X- _ O
the -X- _ O
cost -X- _ O
of -X- _ O
neglecting -X- _ O
important -X- _ O
syntactic -X- _ O
nuances -X- _ O
. -X- _ O

2.3 -X- _ O
Multiword -X- _ O
grouping -X- _ O
This -X- _ O
last -X- _ O
preprocessing -X- _ O
technique -X- _ O
consists -X- _ O
of -X- _ O
grouping -X- _ O
consecutive -X- _ O
tokens -X- _ O
together -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
token -X- _ O
if -X- _ O
found -X- _ O
in -X- _ O
a -X- _ O
given -X- _ O
inventory -X- _ O
: -X- _ O
Apple -X- _ O
is -X- _ O
asking -X- _ O
its -X- _ O
manufacturers -X- _ O
to -X- _ O
move -X- _ O
MacBook -X- _ O
Air -X- _ O
production -X- _ O
to -X- _ O
the -X- _ O
United -X- _ O
States -X- _ O
. -X- _ O

The -X- _ O
motivation -X- _ O
behind -X- _ O
this -X- _ O
step -X- _ O
lies -X- _ O
in -X- _ O
the -X- _ O
idiosyncratic -X- _ O
nature -X- _ O
of -X- _ O
multiword -X- _ O
expressions -X- _ O
( -X- _ O
Sag -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
, -X- _ O
e.g. -X- _ O
United -X- _ O
States -X- _ O
in -X- _ O
the -X- _ O
example -X- _ O
. -X- _ O

The -X- _ O
meaning -X- _ O
of -X- _ O
these -X- _ O
multiword -X- _ O
expressions -X- _ O
are -X- _ O
often -X- _ O
hardly -X- _ O
traceable -X- _ O
from -X- _ O
their -X- _ O
individual -X- _ O
tokens -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
treating -X- _ O
multiwords -X- _ O
as -X- _ O
single -X- _ O
units -X- _ O
may -X- _ O
lead -X- _ O
to -X- _ O
better -X- _ O
training -X- _ O
of -X- _ O
a -X- _ O
given -X- _ O
model -X- _ O
. -X- _ O

Because -X- _ O
of -X- _ O
this -X- _ O
, -X- _ O
word -X- _ O
embedding -X- _ O
toolkits -X- _ O
such -X- _ O
as -X- _ O
Word2vec -X- _ B-MethodName
propose -X- _ O
statistical -X- _ O
approaches -X- _ O
for -X- _ O
extracting -X- _ O
these -X- _ O
multiwords -X- _ O
, -X- _ O
or -X- _ O
directly -X- _ O
include -X- _ O
multiwords -X- _ O
along -X- _ O
with -X- _ O
single -X- _ O
words -X- _ O
in -X- _ O
their -X- _ O
pretrained -X- _ O
embedding -X- _ O
spaces -X- _ O
( -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013b -X- _ O
) -X- _ O
. -X- _ O

423 -X- _ O
Evaluation -X- _ O
We -X- _ O
considered -X- _ O
two -X- _ O
tasks -X- _ O
for -X- _ O
our -X- _ O
experiments -X- _ O
: -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
, -X- _ O
i.e. -X- _ O
assigning -X- _ O
a -X- _ O
topic -X- _ O
to -X- _ O
a -X- _ O
given -X- _ O
document -X- _ O
from -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
deﬁned -X- _ O
set -X- _ O
of -X- _ O
topics -X- _ O
, -X- _ O
andpolarity -X- _ B-TaskName
detection -X- _ I-TaskName
, -X- _ O
i.e. -X- _ O
detecting -X- _ O
if -X- _ O
the -X- _ O
sentiment -X- _ O
of -X- _ O
a -X- _ O
given -X- _ O
piece -X- _ O
of -X- _ O
text -X- _ O
is -X- _ O
positive -X- _ O
or -X- _ O
negative -X- _ O
( -X- _ O
Dong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

Two -X- _ O
different -X- _ O
settings -X- _ O
were -X- _ O
studied -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
word -X- _ O
embedding -X- _ O
’s -X- _ O
training -X- _ O
corpus -X- _ O
and -X- _ O
the -X- _ O
evaluation -X- _ O
dataset -X- _ O
were -X- _ O
preprocessed -X- _ O
in -X- _ O
a -X- _ O
similar -X- _ O
manner -X- _ O
( -X- _ O
Section -X- _ O
3.2 -X- _ O
) -X- _ O
; -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
the -X- _ O
two -X- _ O
were -X- _ O
preprocessed -X- _ O
differently -X- _ O
( -X- _ O
Section -X- _ O
3.3 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
what -X- _ O
follows -X- _ O
we -X- _ O
describe -X- _ O
the -X- _ O
common -X- _ O
experimental -X- _ O
setting -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
datasets -X- _ O
and -X- _ O
preprocessing -X- _ O
used -X- _ O
for -X- _ O
the -X- _ O
evaluation -X- _ O
. -X- _ O

3.1 -X- _ O
Experimental -X- _ O
setup -X- _ O
We -X- _ O
tried -X- _ O
with -X- _ O
two -X- _ O
classiﬁcation -X- _ O
models -X- _ O
. -X- _ O

The -X- _ O
ﬁrst -X- _ O
one -X- _ O
is -X- _ O
a -X- _ O
standard -X- _ O
CNN -X- _ B-MethodName
model -X- _ O
similar -X- _ O
to -X- _ O
that -X- _ O
of -X- _ O
Kim -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
using -X- _ O
ReLU -X- _ O
( -X- _ O
Nair -X- _ O
and -X- _ O
Hinton -X- _ O
, -X- _ O
2010 -X- _ O
) -X- _ O
as -X- _ O
non -X- _ O
- -X- _ O
linear -X- _ O
activation -X- _ O
function -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
second -X- _ O
model -X- _ O
, -X- _ O
we -X- _ O
add -X- _ O
a -X- _ O
recurrent -X- _ O
layer -X- _ O
( -X- _ O
speciﬁcally -X- _ O
an -X- _ O
LSTM -X- _ B-MethodName
( -X- _ O
Hochreiter -X- _ O
and -X- _ O
Schmidhuber -X- _ O
, -X- _ O
1997 -X- _ O
) -X- _ O
) -X- _ O
before -X- _ O
passing -X- _ O
the -X- _ O
pooled -X- _ O
features -X- _ O
directly -X- _ O
to -X- _ O
the -X- _ O
fully -X- _ O
connected -X- _ O
softmax -X- _ O
layer.3The -X- _ O
inclusion -X- _ O
of -X- _ O
this -X- _ O
LSTM -X- _ B-MethodName
layer -X- _ O
has -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
be -X- _ O
able -X- _ O
to -X- _ O
effectively -X- _ O
replace -X- _ O
multiple -X- _ O
layers -X- _ O
of -X- _ O
convolution -X- _ O
and -X- _ O
be -X- _ O
beneﬁcial -X- _ O
particularly -X- _ O
for -X- _ O
large -X- _ O
inputs -X- _ O
( -X- _ O
Xiao -X- _ O
and -X- _ O
Cho -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

These -X- _ O
models -X- _ O
were -X- _ O
used -X- _ O
for -X- _ O
both -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
and -X- _ O
polarity -X- _ B-TaskName
detection -X- _ I-TaskName
tasks -X- _ O
, -X- _ O
with -X- _ O
slight -X- _ O
hyperparameter -X- _ O
variations -X- _ O
given -X- _ O
their -X- _ O
different -X- _ O
natures -X- _ O
( -X- _ O
mainly -X- _ O
in -X- _ O
their -X- _ O
text -X- _ O
size -X- _ O
) -X- _ O
which -X- _ O
were -X- _ O
ﬁxed -X- _ O
across -X- _ O
all -X- _ O
datasets -X- _ O
. -X- _ O

The -X- _ O
embedding -X- _ O
layer -X- _ O
was -X- _ O
initialized -X- _ O
using -X- _ O
300 -X- _ O
- -X- _ O
dimensional -X- _ O
CBOW -X- _ B-MethodName
Word2vec -X- _ B-MethodName
embeddings -X- _ O
( -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013a -X- _ O
) -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
3B -X- _ O
- -X- _ O
word -X- _ O
UMBC -X- _ B-DatasetName
WebBase -X- _ I-DatasetName
corpus -X- _ O
( -X- _ O
Han -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
with -X- _ O
standard -X- _ O
hyperparameters4 -X- _ O
. -X- _ O

Evaluation -X- _ O
datasets -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
task -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
BBC -X- _ B-DatasetName
news -X- _ I-DatasetName
dataset5 -X- _ O
( -X- _ O
Greene -X- _ O
Reuters6 -X- _ O
( -X- _ O
Lewis -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
and -X- _ O
Ohsumed7 -X- _ B-DatasetName
. -X- _ O

in -X- _ O
( -X- _ O
Pilehvar -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
available -X- _ O
at -X- _ O
https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github -X- _ O
. -X- _ O
com -X- _ O
/ -X- _ O
pilehvar -X- _ O
/ -X- _ O
sensecnn -X- _ O
5http -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
mlg.ucd.ie -X- _ O
/ -X- _ O
datasets -X- _ O
/ -X- _ O
bbc.html -X- _ O
( -X- _ O
i.e. -X- _ O
91 -X- _ O
) -X- _ O
and -X- _ O
to -X- _ O
be -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
other -X- _ O
datasets -X- _ O
, -X- _ O
we -X- _ O
reduce -X- _ O
the -X- _ O
dataset -X- _ O
to -X- _ O
its -X- _ O
8 -X- _ O
most -X- _ O
frequent -X- _ O
labels -X- _ O
, -X- _ O
a -X- _ O
reduction -X- _ O
already -X- _ O
performed -X- _ O
in -X- _ O
previous -X- _ O
works -X- _ O
( -X- _ O
Sebastiani -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
. -X- _ O

7ftp -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
medir.ohsu.edu -X- _ O
/ -X- _ O
pub -X- _ O
/ -X- _ O
ohsumedDataset -X- _ O
Type -X- _ O
Labels -X- _ O
# -X- _ O
of -X- _ O
docs -X- _ O
Eval -X- _ O
. -X- _ O

TOPICBBC -X- _ B-DatasetName
News -X- _ I-DatasetName
5 -X- _ O
2,225 -X- _ O
10 -X- _ O
- -X- _ O
cross -X- _ B-DatasetName
20News -X- _ I-DatasetName
News -X- _ O
6 -X- _ O
18,846 -X- _ O
Train -X- _ O
- -X- _ O
test -X- _ O
Reuters -X- _ B-DatasetName
News -X- _ I-DatasetName
8 -X- _ O
9,178 -X- _ O
10 -X- _ O
- -X- _ O
cross -X- _ O
Ohsumed -X- _ B-DatasetName
Medical -X- _ O
23 -X- _ O
23,166 -X- _ O
Train -X- _ O
- -X- _ O
testPOLARITYRTC -X- _ B-DatasetName
Snippets -X- _ I-DatasetName
2 -X- _ O
438,000 -X- _ O
Train -X- _ O
- -X- _ O
test -X- _ O
IMDB -X- _ B-DatasetName
Reviews -X- _ I-DatasetName
2 -X- _ O
50,000 -X- _ O
Train -X- _ O
- -X- _ O
test -X- _ O
Stanford -X- _ B-DatasetName
Phrases -X- _ O
2 -X- _ O
119,783 -X- _ O
10 -X- _ O
- -X- _ O
cross -X- _ O
Table -X- _ O
1 -X- _ O
: -X- _ O

Evaluation -X- _ O
datasets -X- _ O
for -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
and -X- _ O
polarity -X- _ B-TaskName
detection -X- _ I-TaskName
. -X- _ O

PL04 -X- _ O
( -X- _ O
Pang -X- _ O
and -X- _ O
Lee -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
PL058 -X- _ O
( -X- _ O
Pang -X- _ O
and -X- _ O
the -X- _ O
Stanford -X- _ O
sentiment -X- _ O
dataset10 -X- _ O
( -X- _ O
Socher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
, -X- _ O
SF -X- _ O
) -X- _ O
were -X- _ O
considered -X- _ O
for -X- _ O
polarity -X- _ B-TaskName
detection -X- _ I-TaskName
. -X- _ O

Statistics -X- _ O
of -X- _ O
the -X- _ O
versions -X- _ O
of -X- _ O
the -X- _ O
datasets -X- _ O
used -X- _ O
are -X- _ O
displayed -X- _ O
in -X- _ O
Table -X- _ O
1.11For -X- _ O
both -X- _ O
tasks -X- _ O
the -X- _ O
evaluation -X- _ O
was -X- _ O
carried -X- _ O
out -X- _ O
either -X- _ O
by -X- _ O
10 -X- _ O
- -X- _ O
fold -X- _ O
cross -X- _ O
- -X- _ O
validation -X- _ O
or -X- _ O
using -X- _ O
the -X- _ O
train -X- _ O
- -X- _ O
test -X- _ O
splits -X- _ O
of -X- _ O
the -X- _ O
datasets -X- _ O
, -X- _ O
in -X- _ O
case -X- _ O
of -X- _ O
availability -X- _ O
. -X- _ O

Preprocessing -X- _ O
. -X- _ O

Four -X- _ O
different -X- _ O
techniques -X- _ O
( -X- _ O
see -X- _ O
Section -X- _ O
2 -X- _ O
) -X- _ O
were -X- _ O
used -X- _ O
to -X- _ O
preprocess -X- _ O
the -X- _ O
datasets -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
corpus -X- _ O
which -X- _ O
was -X- _ O
used -X- _ O
to -X- _ O
train -X- _ O
word -X- _ O
embeddings -X- _ O
( -X- _ O
i.e. -X- _ O
UMBC -X- _ B-DatasetName
) -X- _ O
. -X- _ O

For -X- _ O
tokenization -X- _ O
and -X- _ O
lemmatization -X- _ O
we -X- _ O
relied -X- _ O
on -X- _ O
Stanford -X- _ O
CoreNLP -X- _ O
( -X- _ O
Manning -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

As -X- _ O
for -X- _ O
multiwords -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
phrases -X- _ O
from -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
Google -X- _ B-MethodName
News -X- _ I-MethodName
Word2vec -X- _ I-MethodName
vectors -X- _ O
, -X- _ O
which -X- _ O
were -X- _ O
obtained -X- _ O
using -X- _ O
a -X- _ O
simple -X- _ O
statistical -X- _ O
approach -X- _ O
( -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
3.2 -X- _ O
Experiment -X- _ O
1 -X- _ O
: -X- _ O
Preprocessing -X- _ O
effect -X- _ O
Table -X- _ O
2 -X- _ O
shows -X- _ O
the -X- _ O
accuracy13of -X- _ B-MetricName
the -X- _ O
classiﬁcation -X- _ O
models -X- _ O
using -X- _ O
our -X- _ O
four -X- _ O
preprocessing -X- _ O
techniques -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
a -X- _ O
certain -X- _ O
variability -X- _ O
of -X- _ O
results -X- _ O
depending -X- _ O
on -X- _ O
the -X- _ O
preprocessing -X- _ O
techniques -X- _ O
used -X- _ O
( -X- _ O
averwww.cs.cornell.edu -X- _ O
/ -X- _ O
people -X- _ O
/ -X- _ O
pabo -X- _ O
/ -X- _ O
movie-review-data -X- _ O
/ -X- _ O
9http -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
www.rottentomatoes.com -X- _ O
10We -X- _ O
mapped -X- _ O
the -X- _ O
numerical -X- _ O
value -X- _ O
of -X- _ O
phrases -X- _ O
to -X- _ O
either -X- _ O
negative -X- _ O
( -X- _ O
from -X- _ O
0 -X- _ O
to -X- _ O
0.4 -X- _ O
) -X- _ O
or -X- _ O
positive -X- _ O
( -X- _ O
from -X- _ O
0.6 -X- _ O
to -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
removing -X- _ O
the -X- _ O
neutral -X- _ O
phrases -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
scale -X- _ O
( -X- _ O
from -X- _ O
0.4 -X- _ O
to -X- _ O
0.6 -X- _ O
) -X- _ O
. -X- _ O

11For -X- _ O
the -X- _ O
datasets -X- _ O
with -X- _ O
train -X- _ O
- -X- _ O
test -X- _ O
partitions -X- _ O
, -X- _ O
the -X- _ O
sizes -X- _ O
of -X- _ O
the -X- _ O
test -X- _ O
sets -X- _ O
are -X- _ O
the -X- _ O
following -X- _ O
: -X- _ O
7,532 -X- _ O
for -X- _ O
20News -X- _ O
; -X- _ O
12,733 -X- _ O
for -X- _ O
Ohsumed -X- _ O
; -X- _ O
25,000 -X- _ O
for -X- _ O
IMDb -X- _ O
; -X- _ O
and -X- _ O
1,000 -X- _ O
for -X- _ O
RTC -X- _ O
. -X- _ O

12For -X- _ O
future -X- _ O
work -X- _ O
it -X- _ O
would -X- _ O
be -X- _ O
interesting -X- _ O
to -X- _ O
explore -X- _ O
more -X- _ O
complex -X- _ O
methods -X- _ O
to -X- _ O
learn -X- _ O
embeddings -X- _ O
for -X- _ O
multiword -X- _ O
expressions -X- _ O
( -X- _ O
Yin -X- _ O
and -X- _ O
Sch -X- _ O
¨utze -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Poliak -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

13Computed -X- _ O
by -X- _ O
averaging -X- _ O
accuracy -X- _ B-MetricName
of -X- _ O
two -X- _ O
different -X- _ O
runs -X- _ O
. -X- _ O

The -X- _ O
statistical -X- _ O
signiﬁcance -X- _ O
was -X- _ O
calculated -X- _ O
according -X- _ O
to -X- _ O
an -X- _ O
unpaired -X- _ O
t -X- _ O
- -X- _ O
test -X- _ O
at -X- _ O
the -X- _ O
5 -X- _ O
% -X- _ O
signiﬁcance -X- _ O
level -X- _ O
. -X- _ O

43Topic -X- _ B-TaskName
categorization -X- _ I-TaskName
Polarity -X- _ B-TaskName
detection -X- _ I-TaskName
Table -X- _ O
2 -X- _ O
: -X- _ O
Accuracy -X- _ B-MetricName
on -X- _ O
the -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
and -X- _ O
polarity -X- _ O
detection -X- _ O
tasks -X- _ O
using -X- _ O
various -X- _ O
preprocessing -X- _ O
techniques -X- _ O
for -X- _ O
the -X- _ O
CNN -X- _ B-MethodName
and -X- _ O
CNN+LSTM -X- _ B-MethodName
models.yindicates -X- _ O
results -X- _ O
that -X- _ O
are -X- _ O
statistically -X- _ O
signiﬁcant -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
the -X- _ O
top -X- _ O
result -X- _ O
. -X- _ O

age -X- _ O
variability14of2:4 -X- _ O
% -X- _ O
for -X- _ O
the -X- _ O
CNN+LSTM -X- _ B-MethodName
model -X- _ O
, -X- _ O
including -X- _ O
a -X- _ O
statistical -X- _ O
signiﬁcance -X- _ O
gap -X- _ O
in -X- _ O
seven -X- _ O
of -X- _ O
the -X- _ O
nine -X- _ O
datasets -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
proves -X- _ O
the -X- _ O
inﬂuence -X- _ O
of -X- _ O
preprocessing -X- _ O
on -X- _ O
the -X- _ O
ﬁnal -X- _ O
results -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
perhaps -X- _ O
not -X- _ O
surprising -X- _ O
that -X- _ O
the -X- _ O
lowest -X- _ O
variance -X- _ O
of -X- _ O
results -X- _ O
is -X- _ O
seen -X- _ O
in -X- _ O
the -X- _ O
datasets -X- _ O
with -X- _ O
the -X- _ O
larger -X- _ O
training -X- _ O
data -X- _ O
( -X- _ O
i.e. -X- _ O
RTC -X- _ O
and -X- _ O
Stanford -X- _ O
) -X- _ O
. -X- _ O

This -X- _ O
suggests -X- _ O
that -X- _ O
the -X- _ O
preprocessing -X- _ O
decisions -X- _ O
are -X- _ O
not -X- _ O
so -X- _ O
important -X- _ O
when -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
is -X- _ O
large -X- _ O
enough -X- _ O
, -X- _ O
but -X- _ O
they -X- _ O
are -X- _ O
indeed -X- _ O
relevant -X- _ O
in -X- _ O
benchmarks -X- _ O
where -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
is -X- _ O
limited -X- _ O
. -X- _ O

As -X- _ O
far -X- _ O
as -X- _ O
the -X- _ O
individual -X- _ O
preprocessing -X- _ O
techniques -X- _ O
are -X- _ O
concerned -X- _ O
, -X- _ O
the -X- _ O
vanilla -X- _ O
setting -X- _ O
( -X- _ O
tokenization -X- _ O
only -X- _ O
) -X- _ O
proves -X- _ O
to -X- _ O
be -X- _ O
consistent -X- _ O
across -X- _ O
datasets -X- _ O
and -X- _ O
tasks -X- _ O
, -X- _ O
as -X- _ O
it -X- _ O
performs -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
ballpark -X- _ O
as -X- _ O
the -X- _ O
best -X- _ O
result -X- _ O
in -X- _ O
8 -X- _ O
of -X- _ O
the -X- _ O
9 -X- _ O
datasets -X- _ O
for -X- _ O
both -X- _ O
models -X- _ O
( -X- _ O
with -X- _ O
no -X- _ O
noticeable -X- _ O
differences -X- _ O
between -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
and -X- _ O
polarity -X- _ B-TaskName
detection -X- _ I-TaskName
) -X- _ O
. -X- _ O

The -X- _ O
only -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
dataset -X- _ O
in -X- _ O
which -X- _ O
tokenization -X- _ O
does -X- _ O
not -X- _ O
seem -X- _ O
enough -X- _ O
is -X- _ O
Ohsumed -X- _ B-DatasetName
, -X- _ O
which -X- _ O
, -X- _ O
unlike -X- _ O
the -X- _ O
more -X- _ O
general -X- _ O
nature -X- _ O
of -X- _ O
other -X- _ O
categorization -X- _ O
datasets -X- _ O
( -X- _ O
news -X- _ O
) -X- _ O
, -X- _ O
belongs -X- _ O
to -X- _ O
a -X- _ O
specialized -X- _ O
domain -X- _ O
( -X- _ O
medical -X- _ O
) -X- _ O
for -X- _ O
which -X- _ O
ﬁne -X- _ O
- -X- _ O
grained -X- _ O
distinctions -X- _ O
are -X- _ O
required -X- _ O
to -X- _ O
classify -X- _ O
cardiovascular -X- _ O
diseases -X- _ O
. -X- _ O

In -X- _ O
particular -X- _ O
for -X- _ O
this -X- _ O
dataset -X- _ O
, -X- _ O
word -X- _ O
embeddings -X- _ O
trained -X- _ O
on -X- _ O
a -X- _ O
general -X- _ O
- -X- _ O
domain -X- _ O
corpus -X- _ O
like -X- _ O
UMBC -X- _ B-DatasetName
may -X- _ O
not -X- _ O
accurately -X- _ O
capture -X- _ O
the -X- _ O
specialized -X- _ O
meaning -X- _ O
of -X- _ O
medical -X- _ O
terms -X- _ O
and -X- _ O
hence -X- _ O
, -X- _ O
sparsity -X- _ O
becomes -X- _ O
an -X- _ O
issue -X- _ O
. -X- _ O

In -X- _ O
fact -X- _ O
, -X- _ O
lowercasing -X- _ O
and -X- _ O
lemmatizing -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
mainly -X- _ O
aimed -X- _ O
at -X- _ O
reducing -X- _ O
sparsity -X- _ O
, -X- _ O
outperform -X- _ O
the -X- _ O
vanilla -X- _ O
setting -X- _ O
by -X- _ O
over -X- _ O
six -X- _ O
points -X- _ O
in -X- _ O
14Average -X- _ O
variability -X- _ O
was -X- _ O
the -X- _ O
result -X- _ O
of -X- _ O
averaging -X- _ O
the -X- _ O
variability -X- _ O
of -X- _ O
each -X- _ O
dataset -X- _ O
, -X- _ O
which -X- _ O
was -X- _ O
computed -X- _ O
as -X- _ O
the -X- _ O
difference -X- _ O
between -X- _ O
the -X- _ O
best -X- _ O
and -X- _ O
the -X- _ O
worst -X- _ O
preprocessing -X- _ O
performances.the -X- _ O

CNN+LSTM -X- _ B-MethodName
setting -X- _ O
and -X- _ O
clearly -X- _ O
outperform -X- _ O
the -X- _ O
other -X- _ O
preprocessing -X- _ O
techniques -X- _ O
on -X- _ O
the -X- _ O
single -X- _ O
CNN -X- _ B-MethodName
model -X- _ O
as -X- _ O
well -X- _ O
. -X- _ O

Nevertheless -X- _ O
, -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
more -X- _ O
complex -X- _ O
preprocessing -X- _ O
techniques -X- _ O
such -X- _ O
as -X- _ O
lemmatization -X- _ O
and -X- _ O
multiword -X- _ O
grouping -X- _ O
does -X- _ O
not -X- _ O
help -X- _ O
in -X- _ O
general -X- _ O
. -X- _ O

Even -X- _ O
though -X- _ O
lemmatization -X- _ O
has -X- _ O
proved -X- _ O
useful -X- _ O
in -X- _ O
conventional -X- _ O
linear -X- _ O
models -X- _ O
as -X- _ O
an -X- _ O
effective -X- _ O
way -X- _ O
to -X- _ O
deal -X- _ O
with -X- _ O
sparsity -X- _ O
( -X- _ O
Mullen -X- _ O
and -X- _ O
Collier -X- _ O
, -X- _ O
2004 -X- _ O
; -X- _ O
Toman -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
, -X- _ O
neural -X- _ O
network -X- _ O
architectures -X- _ O
seem -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
capable -X- _ O
of -X- _ O
overcoming -X- _ O
sparsity -X- _ O
thanks -X- _ O
to -X- _ O
the -X- _ O
generalization -X- _ O
power -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
. -X- _ O

3.3 -X- _ O
Experiment -X- _ O
2 -X- _ O
: -X- _ O
Cross -X- _ O
- -X- _ O
preprocessing -X- _ O
This -X- _ O
experiment -X- _ O
aims -X- _ O
at -X- _ O
studying -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
using -X- _ O
different -X- _ O
word -X- _ O
embeddings -X- _ O
( -X- _ O
with -X- _ O
differently -X- _ O
preprocessed -X- _ O
training -X- _ O
corpora -X- _ O
) -X- _ O
on -X- _ O
tokenized -X- _ O
datasets -X- _ O
( -X- _ O
vanilla -X- _ O
setting -X- _ O
) -X- _ O
. -X- _ O

Table -X- _ O
3 -X- _ O
shows -X- _ O
the -X- _ O
results -X- _ O
for -X- _ O
this -X- _ O
experiment -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
experiment -X- _ O
we -X- _ O
observe -X- _ O
a -X- _ O
different -X- _ O
trend -X- _ O
, -X- _ O
with -X- _ O
multiwordenhanced -X- _ O
vectors -X- _ O
exhibiting -X- _ O
a -X- _ O
better -X- _ O
performance -X- _ O
both -X- _ O
on -X- _ O
the -X- _ O
single -X- _ O
CNN -X- _ B-MethodName
model -X- _ O
( -X- _ O
best -X- _ O
overall -X- _ O
performance -X- _ O
in -X- _ O
seven -X- _ O
of -X- _ O
the -X- _ O
nine -X- _ O
datasets -X- _ O
) -X- _ O
and -X- _ O
on -X- _ O
the -X- _ O
CNN+LSTM -X- _ B-MethodName
model -X- _ O
( -X- _ O
best -X- _ O
performance -X- _ O
in -X- _ O
four -X- _ O
datasets -X- _ O
and -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
ballpark -X- _ O
as -X- _ O
the -X- _ O
best -X- _ O
results -X- _ O
in -X- _ O
four -X- _ O
of -X- _ O
the -X- _ O
remaining -X- _ O
ﬁve -X- _ O
datasets -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
case -X- _ O
the -X- _ O
same -X- _ O
set -X- _ O
of -X- _ O
words -X- _ O
is -X- _ O
learnt -X- _ O
but -X- _ O
single -X- _ O
tokens -X- _ O
inside -X- _ O
multiword -X- _ O
expressions -X- _ O
are -X- _ O
not -X- _ O
trained -X- _ O
. -X- _ O

Instead -X- _ O
, -X- _ O
these -X- _ O
single -X- _ O
tokens -X- _ O
are -X- _ O
considered -X- _ O
in -X- _ O
isolation -X- _ O
only -X- _ O
, -X- _ O
without -X- _ O
the -X- _ O
added -X- _ O
noise -X- _ O
when -X- _ O
considered -X- _ O
inside -X- _ O
the -X- _ O
multiword -X- _ O
expression -X- _ O
as -X- _ O
well -X- _ O
. -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
the -X- _ O
word -X- _ O
Apple -X- _ O
has -X- _ O
a -X- _ O
clearly -X- _ O
different -X- _ O
meaning -X- _ O
in -X- _ O
isolation -X- _ O
from -X- _ O
the -X- _ O
one -X- _ O
inside -X- _ O

44Embedding -X- _ O
PreprocessingTopic -X- _ B-TaskName
categorization -X- _ I-TaskName
Polarity -X- _ B-TaskName
detection -X- _ I-TaskName
Table -X- _ O
3 -X- _ O
: -X- _ O
Cross -X- _ O
- -X- _ O
preprocessing -X- _ O
evaluation -X- _ O
: -X- _ O
accuracy -X- _ B-MetricName
on -X- _ O
the -X- _ O
topic -X- _ B-TaskName
categorization -X- _ I-TaskName
and -X- _ O
polarity -X- _ B-TaskName
detection -X- _ I-TaskName
tasks -X- _ O
using -X- _ O
different -X- _ O
sets -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
to -X- _ O
initialize -X- _ O
the -X- _ O
embedding -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
classiﬁers -X- _ O
. -X- _ O

All -X- _ O
datasets -X- _ O
were -X- _ O
preprocessed -X- _ O
similarly -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
vanilla -X- _ O
setting.yindicates -X- _ O
results -X- _ O
that -X- _ O
are -X- _ O
statistically -X- _ O
signiﬁcant -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
the -X- _ O
top -X- _ O
result -X- _ O
. -X- _ O

the -X- _ O
multiword -X- _ O
expression -X- _ O
BigApple -X- _ O
, -X- _ O
hence -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
beneﬁcial -X- _ O
not -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
word -X- _ O
Applewhen -X- _ O
part -X- _ O
of -X- _ O
this -X- _ O
multiword -X- _ O
expression -X- _ O
. -X- _ O

Interestingly -X- _ O
, -X- _ O
using -X- _ O
multiword -X- _ O
- -X- _ O
wise -X- _ O
embeddings -X- _ O
on -X- _ O
the -X- _ O
vanilla -X- _ O
setting -X- _ O
leads -X- _ O
to -X- _ O
consistently -X- _ O
better -X- _ O
results -X- _ O
than -X- _ O
using -X- _ O
them -X- _ O
on -X- _ O
the -X- _ O
same -X- _ O
multiwordgrouped -X- _ O
preprocessed -X- _ O
dataset -X- _ O
in -X- _ O
eight -X- _ O
of -X- _ O
the -X- _ O
nine -X- _ O
datasets -X- _ O
. -X- _ O

This -X- _ O
could -X- _ O
provide -X- _ O
hints -X- _ O
on -X- _ O
the -X- _ O
excellent -X- _ O
results -X- _ O
provided -X- _ O
by -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
Word2vec -X- _ B-MethodName
embeddings -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
Google -X- _ B-DatasetName
News -X- _ I-DatasetName
corpus -X- _ O
, -X- _ O
which -X- _ O
learns -X- _ O
multiwords -X- _ O
similarly -X- _ O
to -X- _ O
our -X- _ O
setting -X- _ O
. -X- _ O

Apart -X- _ O
from -X- _ O
this -X- _ O
somewhat -X- _ O
surprising -X- _ O
ﬁnding -X- _ O
, -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
the -X- _ O
embeddings -X- _ O
trained -X- _ O
on -X- _ O
a -X- _ O
simple -X- _ O
tokenized -X- _ O
corpus -X- _ O
( -X- _ O
i.e. -X- _ O
vanilla -X- _ O
) -X- _ O
proved -X- _ O
again -X- _ O
competitive -X- _ O
, -X- _ O
as -X- _ O
different -X- _ O
preprocessing -X- _ O
techniques -X- _ O
such -X- _ O
as -X- _ O
lowercasing -X- _ O
and -X- _ O
lemmatizing -X- _ O
do -X- _ O
not -X- _ O
seem -X- _ O
to -X- _ O
help -X- _ O
. -X- _ O

In -X- _ O
fact -X- _ O
, -X- _ O
the -X- _ O
relatively -X- _ O
weaker -X- _ O
performance -X- _ O
of -X- _ O
lemmatization -X- _ O
and -X- _ O
lowercasing -X- _ O
in -X- _ O
this -X- _ O
crossprocessing -X- _ O
experiment -X- _ O
is -X- _ O
somehow -X- _ O
expected -X- _ O
as -X- _ O
the -X- _ O
coverage -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
in -X- _ O
vanilla -X- _ O
- -X- _ O
tokenized -X- _ O
datasets -X- _ O
is -X- _ O
limited -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
many -X- _ O
entities -X- _ O
which -X- _ O
are -X- _ O
capitalized -X- _ O
in -X- _ O
the -X- _ O
datasets -X- _ O
are -X- _ O
not -X- _ O
covered -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
lowercasing -X- _ O
, -X- _ O
and -X- _ O
inﬂected -X- _ O
forms -X- _ O
are -X- _ O
missing -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
lemmatizing -X- _ O
. -X- _ O

4 -X- _ O
Conclusions -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
we -X- _ O
analyzed -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
simple -X- _ O
text -X- _ O
preprocessing -X- _ O
decisions -X- _ O
on -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
a -X- _ O
standard -X- _ O
word -X- _ O
- -X- _ O
based -X- _ O
neural -X- _ O
text -X- _ O
classiﬁer -X- _ O
. -X- _ O

Our -X- _ O
evaluations -X- _ O
highlight -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
being -X- _ O
careful -X- _ O
in -X- _ O
the -X- _ O
choice -X- _ O
of -X- _ O
how -X- _ O
to -X- _ O
preprocess -X- _ O
our -X- _ O
data -X- _ O
and -X- _ O
to -X- _ O
be -X- _ O
consistent -X- _ O
when -X- _ O
comparing -X- _ O
different -X- _ O
systems -X- _ O
. -X- _ O

In -X- _ O
general -X- _ O
, -X- _ O
a -X- _ O
simple -X- _ O
tokenization -X- _ O
works -X- _ O
equally -X- _ O
or -X- _ O
better -X- _ O
than -X- _ O
more -X- _ O
complex -X- _ O
pre -X- _ O
- -X- _ O
processing -X- _ O
techniques -X- _ O
such -X- _ O
as -X- _ O
lemmatization -X- _ O
or -X- _ O
multiword -X- _ O
grouping -X- _ O
, -X- _ O
except -X- _ O
for -X- _ O
domain -X- _ O
- -X- _ O
speciﬁc -X- _ O
datasets -X- _ O
( -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
medical -X- _ O
dataset -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
) -X- _ O
in -X- _ O
which -X- _ O
sole -X- _ O
tokenization -X- _ O
performs -X- _ O
poorly -X- _ O
. -X- _ O

Additionally -X- _ O
, -X- _ O
word -X- _ O
embeddings -X- _ O
trained -X- _ O
on -X- _ O
multiword -X- _ O
- -X- _ O
grouped -X- _ O
corpora -X- _ O
perform -X- _ O
surprisingly -X- _ O
well -X- _ O
when -X- _ O
applied -X- _ O
to -X- _ O
simple -X- _ O
tokenized -X- _ O
datasets -X- _ O
. -X- _ O

This -X- _ O
property -X- _ O
has -X- _ O
often -X- _ O
been -X- _ O
overlooked -X- _ O
and -X- _ O
, -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
we -X- _ O
test -X- _ O
the -X- _ O
hypothesis -X- _ O
for -X- _ O
the -X- _ O
ﬁrst -X- _ O
time -X- _ O
. -X- _ O

In -X- _ O
fact -X- _ O
, -X- _ O
this -X- _ O
ﬁnding -X- _ O
could -X- _ O
partially -X- _ O
explain -X- _ O
the -X- _ O
long -X- _ O
- -X- _ O
lasting -X- _ O
success -X- _ O
of -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
Word2vec -X- _ B-MethodName
embeddings -X- _ O
, -X- _ O
which -X- _ O
speciﬁcally -X- _ O
learn -X- _ O
multiword -X- _ O
embeddings -X- _ O
as -X- _ O
part -X- _ O
of -X- _ O
their -X- _ O
pipeline -X- _ O
( -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013b -X- _ O
) -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
our -X- _ O
analysis -X- _ O
shows -X- _ O
that -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
high -X- _ O
variance -X- _ O
in -X- _ O
the -X- _ O
results -X- _ O
depending -X- _ O
on -X- _ O
the -X- _ O
preprocessing -X- _ O
choice -X- _ O
( -X- _ O
2:4 -X- _ O
% -X- _ O
on -X- _ O
average -X- _ O
for -X- _ O
the -X- _ O
best -X- _ O
performing -X- _ O
model -X- _ O
) -X- _ O
, -X- _ O
especially -X- _ O
when -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
is -X- _ O
not -X- _ O
large -X- _ O
enough -X- _ O
to -X- _ O
generalize -X- _ O
. -X- _ O

Further -X- _ O
analysis -X- _ O
and -X- _ O
experimentation -X- _ O
would -X- _ O
be -X- _ O
required -X- _ O
to -X- _ O
fully -X- _ O
understand -X- _ O
the -X- _ O
signiﬁcance -X- _ O
of -X- _ O
these -X- _ O
results -X- _ O
; -X- _ O
but -X- _ O
, -X- _ O
this -X- _ O
work -X- _ O
can -X- _ O
be -X- _ O
viewed -X- _ O
as -X- _ O
a -X- _ O
starting -X- _ O
point -X- _ O
for -X- _ O
studying -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
text -X- _ O
preprocessing -X- _ O
in -X- _ O
deep -X- _ O
learning -X- _ O
models -X- _ O
. -X- _ O

We -X- _ O
hope -X- _ O
that -X- _ O
our -X- _ O
ﬁndings -X- _ O
will -X- _ O
encourage -X- _ O
future -X- _ O
researchers -X- _ O
to -X- _ O
carefully -X- _ O
select -X- _ O
and -X- _ O
report -X- _ O
these -X- _ O
preprocessing -X- _ O
decisions -X- _ O
when -X- _ O
evaluating -X- _ O
or -X- _ O
comparing -X- _ O
different -X- _ O
models -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
as -X- _ O
future -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
plan -X- _ O
to -X- _ O
extend -X- _ O
our -X- _ O
analysis -X- _ O
to -X- _ O
other -X- _ O
tasks -X- _ O
( -X- _ O
e.g. -X- _ O
question -X- _ O
answering -X- _ O
) -X- _ O
, -X- _ O
languages -X- _ O
( -X- _ O
particularly -X- _ O
morphologically -X- _ O
rich -X- _ O
languages -X- _ O
for -X- _ O
which -X- _ O
these -X- _ O
results -X- _ O
may -X- _ O
vary -X- _ O
) -X- _ O
and -X- _ O
preprocessing -X- _ O
techniques -X- _ O
( -X- _ O
e.g. -X- _ O
stopword -X- _ O
removal -X- _ O
or -X- _ O
part -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
speech -X- _ O
tagging -X- _ O
) -X- _ O
. -X- _ O

45Acknowledgments -X- _ O
Jose -X- _ O
Camacho -X- _ O
- -X- _ O
Collados -X- _ O
is -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
ERC -X- _ O
Starting -X- _ O
Grant -X- _ O
637277 -X- _ O
. -X- _ O

References -X- _ O
Jose -X- _ O
Camacho -X- _ O
- -X- _ O
Collados -X- _ O
and -X- _ O
Mohammad -X- _ O
Taher -X- _ O
Pilehvar -X- _ O
. -X- _ O
2018 -X- _ O
. -X- _ O

From -X- _ O
word -X- _ O
to -X- _ O
sense -X- _ O
embeddings -X- _ O
: -X- _ O
A -X- _ O
survey -X- _ O
on -X- _ O
vector -X- _ O
representations -X- _ O
of -X- _ O
meaning -X- _ O
. -X- _ O

Journal -X- _ O
of -X- _ O
Artiﬁcial -X- _ O
Intelligence -X- _ O
Research -X- _ O
( -X- _ O
JAIR -X- _ O
) -X- _ O
. -X- _ O

Alexis -X- _ O
Conneau -X- _ O
, -X- _ O
Holger -X- _ O
Schwenk -X- _ O
, -X- _ O
Lo -X- _ O
¨ıc -X- _ O
Barrault -X- _ O
, -X- _ O
and -X- _ O
Yann -X- _ O
Lecun -X- _ O
. -X- _ O

2017 -X- _ O
. -X- _ O

Very -X- _ O
deep -X- _ O
convolutional -X- _ O
networks -X- _ O
for -X- _ O
text -X- _ O
classiﬁcation -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
EACL -X- _ O
, -X- _ O
pages -X- _ O
1107–1116 -X- _ O
, -X- _ O
Valencia -X- _ O
, -X- _ O
Spain -X- _ O
. -X- _ O

Li -X- _ O
Dong -X- _ O
, -X- _ O
Furu -X- _ O
Wei -X- _ O
, -X- _ O
Shujie -X- _ O
Liu -X- _ O
, -X- _ O
Ming -X- _ O
Zhou -X- _ O
, -X- _ O
and -X- _ O
Ke -X- _ O
Xu -X- _ O
. -X- _ O
2015 -X- _ O
. -X- _ O

A -X- _ O
statistical -X- _ O
parsing -X- _ O
framework -X- _ O
for -X- _ O
sentiment -X- _ O
classiﬁcation -X- _ O
. -X- _ O

Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

C´ıcero -X- _ O
Nogueira -X- _ O
Dos -X- _ O
Santos -X- _ O
and -X- _ O
Maira -X- _ O
Gatti -X- _ O
. -X- _ O
2014 -X- _ O
. -X- _ O

Deep -X- _ O
convolutional -X- _ O
neural -X- _ O
networks -X- _ O
for -X- _ O
sentiment -X- _ O
analysis -X- _ O
of -X- _ O
short -X- _ O
texts -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
COLING -X- _ O
, -X- _ O
Sebastian -X- _ O
Ebert -X- _ O
, -X- _ O
Thomas -X- _ O
M -X- _ O
¨uller -X- _ O
, -X- _ O
and -X- _ O
Hinrich -X- _ O
Sch -X- _ O
¨utze -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

Lamb -X- _ O
: -X- _ O

A -X- _ O
good -X- _ O
shepherd -X- _ O
of -X- _ O
morphologically -X- _ O
rich -X- _ O
languages -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2016 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
pages -X- _ O
742–752 -X- _ O
. -X- _ O

Manaal -X- _ O
Faruqui -X- _ O
, -X- _ O
Jesse -X- _ O
Dodge -X- _ O
, -X- _ O
Sujay -X- _ O
K. -X- _ O
Jauhar -X- _ O
, -X- _ O
Chris -X- _ O
Dyer -X- _ O
, -X- _ O
Eduard -X- _ O
Hovy -X- _ O
, -X- _ O
and -X- _ O
Noah -X- _ O
A. -X- _ O
Smith -X- _ O
. -X- _ O
2015 -X- _ O
. -X- _ O

Retroﬁtting -X- _ O
word -X- _ O
vectors -X- _ O
to -X- _ O
semantic -X- _ O
lexicons -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
NAACL -X- _ O
, -X- _ O
pages -X- _ O
1606–1615 -X- _ O
. -X- _ O

Lucie -X- _ O
Flekova -X- _ O
and -X- _ O
Iryna -X- _ O
Gurevych -X- _ O
. -X- _ O

2016 -X- _ O
. -X- _ O

Supersense -X- _ O
embeddings -X- _ O
: -X- _ O
A -X- _ O
uniﬁed -X- _ O
model -X- _ O
for -X- _ O
supersense -X- _ O
interpretation -X- _ O
, -X- _ O
prediction -X- _ O
, -X- _ O
and -X- _ O
utilization -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
ACL -X- _ O
, -X- _ O
Berlin -X- _ O
, -X- _ O
Germany -X- _ O
. -X- _ O

Yoav -X- _ O
Goldberg -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

A -X- _ O
primer -X- _ O
on -X- _ O
neural -X- _ O
network -X- _ O
models -X- _ O
for -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
. -X- _ O

Journal -X- _ O
of -X- _ O
Artiﬁcial -X- _ O
Intelligence -X- _ O
Research -X- _ O
, -X- _ O
57:345–420 -X- _ O
. -X- _ O

Derek -X- _ O
Greene -X- _ O
and -X- _ O
P -X- _ O
´ -X- _ O
adraig -X- _ O
Cunningham -X- _ O
. -X- _ O

2006 -X- _ O
. -X- _ O

Practical -X- _ O
solutions -X- _ O
to -X- _ O
the -X- _ O
problem -X- _ O
of -X- _ O
diagonal -X- _ O
dominance -X- _ O
in -X- _ O
kernel -X- _ O
document -X- _ O
clustering -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
23rd -X- _ O
International -X- _ O
conference -X- _ O
on -X- _ O
Machine -X- _ O
learning -X- _ O
, -X- _ O
Lushan -X- _ O
Han -X- _ O
, -X- _ O
Abhay -X- _ O
Kashyap -X- _ O
, -X- _ O
Tim -X- _ O
Finin -X- _ O
, -X- _ O
James -X- _ O
Mayﬁeld -X- _ O
, -X- _ O
and -X- _ O
Jonathan -X- _ O
Weese -X- _ O
. -X- _ O

2013 -X- _ O
. -X- _ O

UMBC -X- _ B-DatasetName
ebiquitycore -X- _ O
: -X- _ O
Semantic -X- _ O
textual -X- _ O
similarity -X- _ O
systems -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
Second -X- _ O
Joint -X- _ O
Conference -X- _ O
on -X- _ O
Lexical -X- _ O
and -X- _ O
Computational -X- _ O
Semantics -X- _ O
, -X- _ O
volume -X- _ O
1 -X- _ O
, -X- _ O
pages -X- _ O
44 -X- _ O
– -X- _ O
Samer -X- _ O
Hassan -X- _ O
, -X- _ O
Rada -X- _ O
Mihalcea -X- _ O
, -X- _ O
and -X- _ O
Carmen -X- _ O
Banea -X- _ O
. -X- _ O

2007 -X- _ O
. -X- _ O

Random -X- _ O
walk -X- _ O
term -X- _ O
weighting -X- _ O
for -X- _ O
improved -X- _ O
text -X- _ O
classiﬁcation -X- _ O
. -X- _ O

International -X- _ O
Journal -X- _ O
of -X- _ O
Semantic -X- _ O
Computing -X- _ O
, -X- _ O
1 -X- _ O
( -X- _ O
04 -X- _ O
) -X- _ O
:421–439.Sepp -X- _ O
Hochreiter -X- _ O
and -X- _ O
J -X- _ O
¨urgen -X- _ O
Schmidhuber -X- _ O
. -X- _ O

1997 -X- _ O
. -X- _ O

Long -X- _ O
short -X- _ O
- -X- _ O
term -X- _ O
memory -X- _ O
. -X- _ O

Neural -X- _ O
computation -X- _ O
, -X- _ O
Rie -X- _ O
Johnson -X- _ O
and -X- _ O
Tong -X- _ O
Zhang -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

Effective -X- _ O
use -X- _ O
of -X- _ O
word -X- _ O
order -X- _ O
for -X- _ O
text -X- _ O
categorization -X- _ O
with -X- _ O
convolutional -X- _ O
neural -X- _ O
networks -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
NAACL -X- _ O
, -X- _ O
pages -X- _ O
103–112 -X- _ O
, -X- _ O
Denver -X- _ O
, -X- _ O
Colorado -X- _ O
. -X- _ O

Nal -X- _ O
Kalchbrenner -X- _ O
, -X- _ O
Edward -X- _ O
Grefenstette -X- _ O
, -X- _ O
and -X- _ O
Phil -X- _ O
Blunsom -X- _ O
. -X- _ O
2014 -X- _ O
. -X- _ O

A -X- _ O
convolutional -X- _ O
neural -X- _ O
network -X- _ O
for -X- _ O
modelling -X- _ O
sentences -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
ACL -X- _ O
, -X- _ O
pages -X- _ O
Yoon -X- _ O
Kim -X- _ O
. -X- _ O
2014 -X- _ O
. -X- _ O

Convolutional -X- _ O
neural -X- _ O
networks -X- _ O
for -X- _ O
sentence -X- _ O
classiﬁcation -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
EMNLP -X- _ O
. -X- _ O

Yoon -X- _ O
Kim -X- _ O
, -X- _ O
Yacine -X- _ O
Jernite -X- _ O
, -X- _ O
David -X- _ O
Sontag -X- _ O
, -X- _ O
and -X- _ O
Alexander -X- _ O
M -X- _ O
Rush -X- _ O
. -X- _ O

2016 -X- _ O
. -X- _ O

Character -X- _ O
- -X- _ O
aware -X- _ O
neural -X- _ O
language -X- _ O
models -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
AAAI -X- _ O
. -X- _ O

Ilia -X- _ O
Kuznetsov -X- _ O
and -X- _ O
Iryna -X- _ O
Gurevych -X- _ O
. -X- _ O

2018 -X- _ O
. -X- _ O

From -X- _ O
text -X- _ O
to -X- _ O
lexicon -X- _ O
: -X- _ O
Bridging -X- _ O
the -X- _ O
gap -X- _ O
between -X- _ O
word -X- _ O
embeddings -X- _ O
and -X- _ O
lexical -X- _ O
resources -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
27th -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
pages -X- _ O
233–244 -X- _ O
. -X- _ O

Ken -X- _ O
Lang -X- _ O
. -X- _ O
1995 -X- _ O
. -X- _ O

Newsweeder -X- _ O
: -X- _ O
Learning -X- _ O
to -X- _ O
ﬁlter -X- _ O
netnews -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
12th -X- _ O
international -X- _ O
conference -X- _ O
on -X- _ O
machine -X- _ O
learning -X- _ O
, -X- _ O
pages -X- _ O
331–339 -X- _ O
. -X- _ O

Yann -X- _ O
LeCun -X- _ O
, -X- _ O
Koray -X- _ O
Kavukcuoglu -X- _ O
, -X- _ O
and -X- _ O
Cl -X- _ O
´ -X- _ O
ement -X- _ O
Farabet -X- _ O
. -X- _ O

2010 -X- _ O
. -X- _ O

Convolutional -X- _ O
networks -X- _ O
and -X- _ O
applications -X- _ O
in -X- _ O
vision -X- _ O
. -X- _ O

In -X- _ O
Circuits -X- _ O
and -X- _ O
Systems -X- _ O
( -X- _ O
ISCAS -X- _ O
) -X- _ O
, -X- _ O
Proceedings -X- _ O
of -X- _ O
2010 -X- _ O
IEEE -X- _ O
International -X- _ O
Symposium -X- _ O
on -X- _ O
, -X- _ O
Edda -X- _ O
Leopold -X- _ O
and -X- _ O
J -X- _ O
¨org -X- _ O
Kindermann -X- _ O
. -X- _ O
2002 -X- _ O
. -X- _ O

Text -X- _ O
categorization -X- _ O
with -X- _ O
support -X- _ O
vector -X- _ O
machines -X- _ O
. -X- _ O

how -X- _ O
to -X- _ O
represent -X- _ O
texts -X- _ O
in -X- _ O
input -X- _ O
space -X- _ O
? -X- _ O

Machine -X- _ O
Learning -X- _ O
, -X- _ O
Omer -X- _ O
Levy -X- _ O
, -X- _ O
Yoav -X- _ O
Goldberg -X- _ O
, -X- _ O
and -X- _ O
Ido -X- _ O
Dagan -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

Improving -X- _ O
distributional -X- _ O
similarity -X- _ O
with -X- _ O
lessons -X- _ O
learned -X- _ O
from -X- _ O
word -X- _ O
embeddings -X- _ O
. -X- _ O

Transactions -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
3:211–225 -X- _ O
. -X- _ O

David -X- _ O
D. -X- _ O
Lewis -X- _ O
, -X- _ O
Yiming -X- _ O
Yang -X- _ O
, -X- _ O
Tony -X- _ O
G -X- _ O
Rose -X- _ O
, -X- _ O
and -X- _ O
Fan -X- _ O
Li -X- _ O
. -X- _ O

2004 -X- _ O
. -X- _ O

Rcv1 -X- _ O
: -X- _ O

A -X- _ O
new -X- _ O
benchmark -X- _ O
collection -X- _ O
for -X- _ O
text -X- _ O
categorization -X- _ O
research -X- _ O
. -X- _ O

Journal -X- _ O
of -X- _ O
machine -X- _ O
learning -X- _ O
research -X- _ O
, -X- _ O
5 -X- _ O
( -X- _ O
Apr -X- _ O
) -X- _ O
:361–397 -X- _ O
. -X- _ O

Jiwei -X- _ O
Li -X- _ O
and -X- _ O
Dan -X- _ O
Jurafsky -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

Do -X- _ O
multi -X- _ O
- -X- _ O
sense -X- _ O
embeddings -X- _ O
improve -X- _ O
natural -X- _ O
language -X- _ O
understanding -X- _ O
? -X- _ O

InProceedings -X- _ O
of -X- _ O
EMNLP -X- _ O
, -X- _ O
Lisbon -X- _ O
, -X- _ O
Portugal -X- _ O
. -X- _ O

Andrew -X- _ O
L. -X- _ O
Maas -X- _ O
, -X- _ O
Raymond -X- _ O
E. -X- _ O
Daly -X- _ O
, -X- _ O
Peter -X- _ O
T. -X- _ O
Pham -X- _ O
, -X- _ O
Dan -X- _ O
Huang -X- _ O
, -X- _ O
Andrew -X- _ O
Y -X- _ O
. -X- _ O

Ng -X- _ O
, -X- _ O
and -X- _ O
Christopher -X- _ O
Potts -X- _ O
. -X- _ O
2011 -X- _ O
. -X- _ O

Learning -X- _ O
word -X- _ O
vectors -X- _ O
for -X- _ O
sentiment -X- _ O
analysis -X- _ O
. -X- _ O

InProceedings -X- _ O
of -X- _ O
ACL -X- _ O
- -X- _ O
HLT -X- _ O
, -X- _ O
pages -X- _ O
142–150 -X- _ O
, -X- _ O
Portland -X- _ O
, -X- _ O
Oregon -X- _ O
, -X- _ O
USA -X- _ O
. -X- _ O

Christopher -X- _ O
D. -X- _ O
Manning -X- _ O
, -X- _ O
Mihai -X- _ O
Surdeanu -X- _ O
, -X- _ O
John -X- _ O
Bauer -X- _ O
, -X- _ O
Jenny -X- _ O
Finkel -X- _ O
, -X- _ O
Steven -X- _ O
J. -X- _ O
Bethard -X- _ O
, -X- _ O
and -X- _ O
David -X- _ O
McClosky -X- _ O
. -X- _ O
2014 -X- _ O
. -X- _ O

The -X- _ O
Stanford -X- _ O
CoreNLP -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
toolkit -X- _ O
. -X- _ O

In -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
( -X- _ O
ACL -X- _ O
) -X- _ O
System -X- _ O
Demonstrations -X- _ O
, -X- _ O

46Tomas -X- _ O
Mikolov -X- _ O
, -X- _ O
Kai -X- _ O
Chen -X- _ O
, -X- _ O
Greg -X- _ O
Corrado -X- _ O
, -X- _ O
and -X- _ O
Jeffrey -X- _ O
Dean -X- _ O
. -X- _ O
2013a -X- _ O
. -X- _ O

Efﬁcient -X- _ O
estimation -X- _ O
of -X- _ O
word -X- _ O
representations -X- _ O
in -X- _ O
vector -X- _ O
space -X- _ O
. -X- _ O

CoRR -X- _ O
, -X- _ O
abs -X- _ O
/ -X- _ O
1301.3781 -X- _ O
. -X- _ O

Tomas -X- _ O
Mikolov -X- _ O
, -X- _ O
Ilya -X- _ O
Sutskever -X- _ O
, -X- _ O
Kai -X- _ O
Chen -X- _ O
, -X- _ O
Greg -X- _ O
S -X- _ O
Corrado -X- _ O
, -X- _ O
and -X- _ O
Jeff -X- _ O
Dean -X- _ O
. -X- _ O
2013b -X- _ O
. -X- _ O

Distributed -X- _ O
representations -X- _ O
of -X- _ O
words -X- _ O
and -X- _ O
phrases -X- _ O
and -X- _ O
their -X- _ O
compositionality -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
neural -X- _ O
information -X- _ O
processing -X- _ O
Tony -X- _ O
Mullen -X- _ O
and -X- _ O
Nigel -X- _ O
Collier -X- _ O
. -X- _ O

2004 -X- _ O
. -X- _ O

Sentiment -X- _ O
analysis -X- _ O
using -X- _ O
support -X- _ O
vector -X- _ O
machines -X- _ O
with -X- _ O
diverse -X- _ O
information -X- _ O
sources -X- _ O
. -X- _ O

In -X- _ O
EMNLP -X- _ O
, -X- _ O
volume -X- _ O
4 -X- _ O
, -X- _ O
pages -X- _ O
412 -X- _ O
– -X- _ O
Vinod -X- _ O
Nair -X- _ O
and -X- _ O
Geoffrey -X- _ O
E. -X- _ O
Hinton -X- _ O
. -X- _ O

2010 -X- _ O
. -X- _ O

Rectiﬁed -X- _ O
linear -X- _ O
units -X- _ O
improve -X- _ O
restricted -X- _ O
boltzmann -X- _ O
machines -X- _ O
. -X- _ O

InProceedings -X- _ O
of -X- _ O
the -X- _ O
27th -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Machine -X- _ O
Learning -X- _ O
( -X- _ O
ICML-10 -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
807–814 -X- _ O
. -X- _ O

Omnipress -X- _ O
. -X- _ O

Bo -X- _ O
Pang -X- _ O
and -X- _ O
Lillian -X- _ O
Lee -X- _ O
. -X- _ O

2004 -X- _ O
. -X- _ O

A -X- _ O
sentimental -X- _ O
education -X- _ O
: -X- _ O
Sentiment -X- _ O
analysis -X- _ O
using -X- _ O
subjectivity -X- _ O
summarization -X- _ O
based -X- _ O
on -X- _ O
minimum -X- _ O
cuts -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
ACL -X- _ O
. -X- _ O

Bo -X- _ O
Pang -X- _ O
and -X- _ O
Lillian -X- _ O
Lee -X- _ O
. -X- _ O

2005 -X- _ O
. -X- _ O

Seeing -X- _ O
stars -X- _ O
: -X- _ O
Exploiting -X- _ O
class -X- _ O
relationships -X- _ O
for -X- _ O
sentiment -X- _ O
categorization -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
rating -X- _ O
scales -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
ACL -X- _ O
. -X- _ O

Jeffrey -X- _ O
Pennington -X- _ O
, -X- _ O
Richard -X- _ O
Socher -X- _ O
, -X- _ O
and -X- _ O
Christopher -X- _ O
D -X- _ O
Manning -X- _ O
. -X- _ O

2014 -X- _ O
. -X- _ O

GloVe -X- _ O
: -X- _ O
Global -X- _ O
vectors -X- _ O
for -X- _ O
word -X- _ O
representation -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
EMNLP -X- _ O
, -X- _ O
pages -X- _ O
Mohammad -X- _ O
Taher -X- _ O
Pilehvar -X- _ O
, -X- _ O
Jose -X- _ O
Camacho -X- _ O
- -X- _ O
Collados -X- _ O
, -X- _ O
Roberto -X- _ O
Navigli -X- _ O
, -X- _ O
and -X- _ O
Nigel -X- _ O
Collier -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Towards -X- _ O
a -X- _ O
Seamless -X- _ O
Integration -X- _ O
of -X- _ O
Word -X- _ O
Senses -X- _ O
into -X- _ O
Downstream -X- _ O
NLP -X- _ O
Applications -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
ACL -X- _ O
, -X- _ O
Vancouver -X- _ O
, -X- _ O
Canada -X- _ O
. -X- _ O

Adam -X- _ O
Poliak -X- _ O
, -X- _ O
Pushpendre -X- _ O
Rastogi -X- _ O
, -X- _ O
M. -X- _ O
Patrick -X- _ O
Martin -X- _ O
, -X- _ O
and -X- _ O
Benjamin -X- _ O
Van -X- _ O
Durme -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Efﬁcient -X- _ O
, -X- _ O
compositional -X- _ O
, -X- _ O
order -X- _ O
- -X- _ O
sensitive -X- _ O
n -X- _ O
- -X- _ O
gram -X- _ O
embeddings -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
EACL -X- _ O
. -X- _ O

Ivan -X- _ O
A -X- _ O
Sag -X- _ O
, -X- _ O
Timothy -X- _ O
Baldwin -X- _ O
, -X- _ O
Francis -X- _ O
Bond -X- _ O
, -X- _ O
Ann -X- _ O
Copestake -X- _ O
, -X- _ O
and -X- _ O
Dan -X- _ O
Flickinger -X- _ O
. -X- _ O

2002 -X- _ O
. -X- _ O

Multiword -X- _ O
expressions -X- _ O
: -X- _ O
A -X- _ O
pain -X- _ O
in -X- _ O
the -X- _ O
neck -X- _ O
for -X- _ O
nlp -X- _ O
. -X- _ O

In -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Intelligent -X- _ O
Text -X- _ O
Processing -X- _ O
and -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
pages -X- _ O
1–15 -X- _ O
. -X- _ O

Springer -X- _ O
. -X- _ O

Fabrizio -X- _ O
Sebastiani -X- _ O
. -X- _ O
2002 -X- _ O
. -X- _ O

Machine -X- _ O
learning -X- _ O
in -X- _ O
automated -X- _ O
text -X- _ O
categorization -X- _ O
. -X- _ O

ACM -X- _ O
computing -X- _ O
surveys -X- _ O
Richard -X- _ O
Socher -X- _ O
, -X- _ O
Alex -X- _ O
Perelygin -X- _ O
, -X- _ O
Jean -X- _ O
Wu -X- _ O
, -X- _ O
Jason -X- _ O
Chuang -X- _ O
, -X- _ O
Christopher -X- _ O
Manning -X- _ O
, -X- _ O
Andrew -X- _ O
Ng -X- _ O
, -X- _ O
and -X- _ O
Christopher -X- _ O
Potts -X- _ O
. -X- _ O
2013 -X- _ O
. -X- _ O

Parsing -X- _ O
With -X- _ O
Compositional -X- _ O
Vector -X- _ O
Grammars -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
EMNLP -X- _ O
. -X- _ O

Duyu -X- _ O
Tang -X- _ O
, -X- _ O
Bing -X- _ O
Qin -X- _ O
, -X- _ O
and -X- _ O
Ting -X- _ O
Liu -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

Document -X- _ O
modeling -X- _ O
with -X- _ O
gated -X- _ O
recurrent -X- _ O
neural -X- _ O
network -X- _ O
for -X- _ O
sentiment -X- _ O
classiﬁcation -X- _ O
. -X- _ O

In -X- _ O
EMNLP -X- _ O
, -X- _ O
pages -X- _ O
1422 -X- _ O
– -X- _ O
1432.Michal -X- _ O
Toman -X- _ O
, -X- _ O
Roman -X- _ O
Tesar -X- _ O
, -X- _ O
and -X- _ O
Karel -X- _ O
Jezek -X- _ O
. -X- _ O

2006 -X- _ O
. -X- _ O

Inﬂuence -X- _ O
of -X- _ O
word -X- _ O
normalization -X- _ O
on -X- _ O
text -X- _ O
classiﬁcation -X- _ O
. -X- _ O

Proceedings -X- _ O
of -X- _ O
InSciT -X- _ O
, -X- _ O
4:354–358 -X- _ O
. -X- _ O

Alper -X- _ O
Kursat -X- _ O
Uysal -X- _ O
and -X- _ O
Serkan -X- _ O
Gunal -X- _ O
. -X- _ O

2014 -X- _ O
. -X- _ O

The -X- _ O
impact -X- _ O
of -X- _ O
preprocessing -X- _ O
on -X- _ O
text -X- _ O
classiﬁcation -X- _ O
. -X- _ O

Information -X- _ O
Processing -X- _ O
& -X- _ O
Management -X- _ O
, -X- _ O
50 -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
:104–112 -X- _ O
. -X- _ O

Yijun -X- _ O
Xiao -X- _ O
and -X- _ O
Kyunghyun -X- _ O
Cho -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

Efﬁcient -X- _ O
character -X- _ O
- -X- _ O
level -X- _ O
document -X- _ O
classiﬁcation -X- _ O
by -X- _ O
combining -X- _ O
convolution -X- _ O
and -X- _ O
recurrent -X- _ O
layers -X- _ O
. -X- _ O

CoRR -X- _ O
, -X- _ O
Wenpeng -X- _ O
Yin -X- _ O
, -X- _ O
Katharina -X- _ O
Kann -X- _ O
, -X- _ O
Mo -X- _ O
Yu -X- _ O
, -X- _ O
and -X- _ O
Hinrich -X- _ O
Sch¨utze -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Comparative -X- _ O
study -X- _ O
of -X- _ O
cnn -X- _ B-MethodName
and -X- _ O
rnn -X- _ B-MethodName
for -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
. -X- _ O

arXiv -X- _ O
preprint -X- _ O
Wenpeng -X- _ O
Yin -X- _ O
and -X- _ O
Hinrich -X- _ O
Sch -X- _ O
¨utze -X- _ O
. -X- _ O

2014 -X- _ O
. -X- _ O

An -X- _ O
exploration -X- _ O
of -X- _ O
embeddings -X- _ O
for -X- _ O
generalized -X- _ O
phrases -X- _ O
. -X- _ O

In -X- _ O
ACL -X- _ O
( -X- _ O
Student -X- _ O
Research -X- _ O
Workshop -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
41–47 -X- _ O
. -X- _ O

Xiang -X- _ O
Zhang -X- _ O
and -X- _ O
Yann -X- _ O
LeCun -X- _ O
. -X- _ O

2017 -X- _ O
. -X- _ O

Which -X- _ O
encoding -X- _ O
is -X- _ O
the -X- _ O
best -X- _ O
for -X- _ O
text -X- _ O
classiﬁcation -X- _ O
in -X- _ O
chinese -X- _ O
, -X- _ O
english -X- _ O
, -X- _ O
japanese -X- _ O
and -X- _ O
korean -X- _ O
? -X- _ O

arXiv -X- _ O
preprint -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
2019 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
and -X- _ O
the -X- _ O
9th -X- _ O
International -X- _ O
Joint -X- _ O
Conference -X- _ O
on -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
pages -X- _ O
1–10 -X- _ O
, -X- _ O
Hong -X- _ O
Kong -X- _ O
, -X- _ O
China -X- _ O
, -X- _ O
November -X- _ O
3–7 -X- _ O
, -X- _ O
2019 -X- _ O
. -X- _ O

c -X- _ O

2019 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics1Attending -X- _ O
to -X- _ O
Future -X- _ O
Tokens -X- _ O
for -X- _ O
Bidirectional -X- _ O
Sequence -X- _ O
Generation -X- _ O
Carolin -X- _ O
Lawrence -X- _ O
andBhushan -X- _ O
Kotnis -X- _ O
andMathias -X- _ O
Niepert -X- _ O
NEC -X- _ O
Laboratories -X- _ O
Europe -X- _ O
{ -X- _ O
carolin.lawrence -X- _ O
, -X- _ O
bhushan.kotnis -X- _ O
, -X- _ O
mathias.niepert -X- _ O
} -X- _ O
@ -X- _ O
neclab.eu -X- _ O
Abstract -X- _ O
Neural -X- _ B-TaskName
sequence -X- _ I-TaskName
generation -X- _ I-TaskName
is -X- _ O
typically -X- _ O
performed -X- _ O
token -X- _ O
- -X- _ O
by -X- _ O
- -X- _ O
token -X- _ O
and -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
. -X- _ O

Whenever -X- _ O
a -X- _ O
token -X- _ O
is -X- _ O
generated -X- _ O
only -X- _ O
previously -X- _ O
produced -X- _ O
tokens -X- _ O
are -X- _ O
taken -X- _ O
into -X- _ O
consideration -X- _ O
. -X- _ O

In -X- _ O
contrast -X- _ O
, -X- _ O
for -X- _ O
problems -X- _ O
such -X- _ O
as -X- _ O
sequence -X- _ O
classiﬁcation -X- _ O
, -X- _ O
bidirectional -X- _ O
attention -X- _ O
, -X- _ O
which -X- _ O
takes -X- _ O
both -X- _ O
past -X- _ O
and -X- _ O
future -X- _ O
tokens -X- _ O
into -X- _ O
consideration -X- _ O
, -X- _ O
has -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
perform -X- _ O
much -X- _ O
better -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
to -X- _ O
make -X- _ O
the -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
process -X- _ O
bidirectional -X- _ O
by -X- _ O
employing -X- _ O
special -X- _ O
placeholder -X- _ O
tokens -X- _ O
. -X- _ O

Treated -X- _ O
as -X- _ O
a -X- _ O
node -X- _ O
in -X- _ O
a -X- _ O
fully -X- _ O
connected -X- _ O
graph -X- _ O
, -X- _ O
a -X- _ O
placeholder -X- _ O
token -X- _ O
can -X- _ O
take -X- _ O
past -X- _ O
and -X- _ O
future -X- _ O
tokens -X- _ O
into -X- _ O
consideration -X- _ O
when -X- _ O
generating -X- _ O
the -X- _ O
actual -X- _ O
output -X- _ O
token -X- _ O
. -X- _ O

We -X- _ O
verify -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
approach -X- _ O
experimentally -X- _ O
on -X- _ O
two -X- _ O
conversational -X- _ O
tasks -X- _ O
where -X- _ O
the -X- _ O
proposed -X- _ O
bidirectional -X- _ O
model -X- _ O
outperforms -X- _ O
competitive -X- _ O
baselines -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
. -X- _ O

1 -X- _ O
Introduction -X- _ O
When -X- _ O
generating -X- _ O
an -X- _ O
output -X- _ O
sequence -X- _ O
, -X- _ O
neural -X- _ O
network -X- _ O
models -X- _ O
typically -X- _ O
produce -X- _ O
one -X- _ O
token -X- _ O
at -X- _ O
a -X- _ O
time -X- _ O
. -X- _ O

At -X- _ O
each -X- _ O
generation -X- _ O
step -X- _ O
, -X- _ O
only -X- _ O
the -X- _ O
already -X- _ O
produced -X- _ O
sequence -X- _ O
is -X- _ O
taken -X- _ O
into -X- _ O
account -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
future -X- _ O
and -X- _ O
not -X- _ O
- -X- _ O
yet -X- _ O
- -X- _ O
produced -X- _ O
tokens -X- _ O
can -X- _ O
also -X- _ O
be -X- _ O
highly -X- _ O
relevant -X- _ O
when -X- _ O
choosing -X- _ O
the -X- _ O
current -X- _ O
token -X- _ O
. -X- _ O

The -X- _ O
importance -X- _ O
of -X- _ O
attending -X- _ O
to -X- _ O
both -X- _ O
past -X- _ O
and -X- _ O
future -X- _ O
tokens -X- _ O
is -X- _ O
apparent -X- _ O
in -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
architectures -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
module -X- _ O
of -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
network -X- _ O
treats -X- _ O
a -X- _ O
sequence -X- _ O
bidrectionally -X- _ O
as -X- _ O
a -X- _ O
fully -X- _ O
connected -X- _ O
graph -X- _ O
of -X- _ O
tokens -X- _ O
– -X- _ O
when -X- _ O
a -X- _ O
token -X- _ O
is -X- _ O
produced -X- _ O
all -X- _ O
other -X- _ O
tokens -X- _ O
are -X- _ O
taken -X- _ O
into -X- _ O
consideration -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
this -X- _ O
requires -X- _ O
the -X- _ O
entire -X- _ O
sequence -X- _ O
to -X- _ O
be -X- _ O
known -X- _ O
a -X- _ O
priori -X- _ O
and -X- _ O
when -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
is -X- _ O
used -X- _ O
for -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
, -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
process -X- _ O
only -X- _ O
includes -X- _ O
previously -X- _ O
produced -X- _ O
tokens -X- _ O
ter -X- _ O
alia -X- _ O
) -X- _ O
. -X- _ O

But -X- _ O
the -X- _ O
bidirectional -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
is -X- _ O
acrucial -X- _ O
property -X- _ O
of -X- _ O
the -X- _ O
highly -X- _ O
successful -X- _ O
language -X- _ O
model -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

During -X- _ O
the -X- _ O
pretraining -X- _ O
procedure -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
a -X- _ O
fraction -X- _ O
of -X- _ O
input -X- _ O
tokens -X- _ O
is -X- _ O
randomly -X- _ O
masked -X- _ O
out -X- _ O
and -X- _ O
the -X- _ O
training -X- _ O
objective -X- _ O
is -X- _ O
to -X- _ O
predict -X- _ O
these -X- _ O
masked -X- _ O
tokens -X- _ O
correctly -X- _ O
. -X- _ O

BERT -X- _ B-MethodName
can -X- _ O
then -X- _ O
be -X- _ O
ﬁne -X- _ O
- -X- _ O
tuned -X- _ O
for -X- _ O
various -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
. -X- _ O

Unfortunately -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
can -X- _ O
not -X- _ O
be -X- _ O
directly -X- _ O
used -X- _ O
for -X- _ O
sequence -X- _ O
generation -X- _ O
because -X- _ O
the -X- _ O
bidirectional -X- _ O
nature -X- _ O
of -X- _ O
the -X- _ O
approach -X- _ O
requires -X- _ O
the -X- _ O
entire -X- _ O
sequence -X- _ O
to -X- _ O
be -X- _ O
known -X- _ O
beforehand -X- _ O
. -X- _ O

Inspired -X- _ O
by -X- _ O
BERT -X- _ B-MethodName
’s -X- _ O
masking -X- _ O
- -X- _ O
based -X- _ O
objective -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
start -X- _ O
out -X- _ O
with -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
placeholder -X- _ O
tokens -X- _ O
which -X- _ O
are -X- _ O
iteratively -X- _ O
replaced -X- _ O
by -X- _ O
tokens -X- _ O
from -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
to -X- _ O
eventually -X- _ O
generate -X- _ O
the -X- _ O
full -X- _ O
output -X- _ O
sequence -X- _ O
. -X- _ O

For -X- _ O
an -X- _ O
example -X- _ O
see -X- _ O
Figure -X- _ O
1 -X- _ O
. -X- _ O

With -X- _ O
this -X- _ O
novel -X- _ O
model -X- _ O
component -X- _ O
, -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
of -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
can -X- _ O
take -X- _ O
both -X- _ O
past -X- _ O
and -X- _ O
future -X- _ O
tokens -X- _ O
into -X- _ O
consideration -X- _ O
, -X- _ O
leading -X- _ O
to -X- _ O
Bidirectional -X- _ B-MethodName
Sequence -X- _ I-MethodName
generati -X- _ I-MethodName
on -X- _ I-MethodName
( -X- _ O
BISON -X- _ B-MethodName
) -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
it -X- _ O
allows -X- _ O
us -X- _ O
to -X- _ O
directly -X- _ O
incorporate -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
model -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
, -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
for -X- _ O
the -X- _ O
ﬁrst -X- _ O
time -X- _ O
directly -X- _ O
ﬁnetune -X- _ O
it -X- _ O
for -X- _ O
sequence -X- _ O
generation -X- _ O
. -X- _ O

BISONmakes -X- _ B-MethodName
two -X- _ O
major -X- _ O
contributions -X- _ O
which -X- _ O
we -X- _ O
investigate -X- _ O
in -X- _ O
turn -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
we -X- _ O
explore -X- _ O
different -X- _ O
stochastic -X- _ O
placeholder -X- _ O
replacement -X- _ O
strategies -X- _ O
to -X- _ O
determine -X- _ O
, -X- _ O
at -X- _ O
training -X- _ O
time -X- _ O
, -X- _ O
where -X- _ O
to -X- _ O
position -X- _ O
the -X- _ O
placeholder -X- _ O
tokens -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
crucial -X- _ O
as -X- _ O
we -X- _ O
need -X- _ O
the -X- _ O
B -X- _ B-MethodName
ISONmodels -X- _ I-MethodName
to -X- _ O
be -X- _ O
exposed -X- _ O
to -X- _ O
a -X- _ O
large -X- _ O
number -X- _ O
of -X- _ O
heterogeneous -X- _ O
placeholder -X- _ O
conﬁgurations -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
we -X- _ O
explore -X- _ O
several -X- _ O
strategies -X- _ O
for -X- _ O
iteratively -X- _ O
generating -X- _ O
, -X- _ O
at -X- _ O
inference -X- _ O
time -X- _ O
, -X- _ O
a -X- _ O
complete -X- _ O
output -X- _ O
sequence -X- _ O
from -X- _ O
an -X- _ O
initial -X- _ O
sequence -X- _ O
of -X- _ O
placeholders -X- _ O
. -X- _ O

We -X- _ O
evaluate -X- _ O
our -X- _ O
bidirectional -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
approach -X- _ O
on -X- _ O
two -X- _ O
conversational -X- _ O
tasks -X- _ O
. -X- _ O

B -X- _ B-MethodName
ISONoutperforms -X- _ I-MethodName
both -X- _ O
competitive -X- _ O
baselines -X- _ O
and -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
neural -X- _ O
network -X- _ O
approaches -X- _ O
on -X- _ O
both -X- _ O
datasets -X- _ O
by -X- _ O
a -X- _ O
signiﬁcant -X- _ O
margin -X- _ O
. -X- _ O

2 -X- _ O
p -X- _ O
you -X- _ O
been -X- _ O
p -X- _ O
or -X- _ O
enrolled -X- _ O
in -X- _ O
an -X- _ O
p -X- _ O
degree -X- _ O
or -X- _ O
program -X- _ O
? -X- _ O

Figure -X- _ O
1 -X- _ O
: -X- _ O
Example -X- _ O
generation -X- _ O
, -X- _ O
going -X- _ O
from -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
the -X- _ O
placeholder -X- _ O
token -X- _ O
p -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
to -X- _ O
an -X- _ O
intermediate -X- _ O
representation -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
and -X- _ O
to -X- _ O
the -X- _ O
ﬁnal -X- _ O
output -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O

2 -X- _ O
Sequence -X- _ B-TaskName
Generation -X- _ I-TaskName
with -X- _ O
Transformers -X- _ B-MethodName
For -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
tasks -X- _ O
, -X- _ O
an -X- _ O
input -X- _ O
sequencex -X- _ O
= -X- _ O
x1 -X- _ O
; -X- _ O
x2 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
xjxjis -X- _ O
to -X- _ O
be -X- _ O
mapped -X- _ O
to -X- _ O
an -X- _ O
output -X- _ O
sequence -X- _ O
y -X- _ O
= -X- _ O
y1 -X- _ O
; -X- _ O
y2 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
yjyjby -X- _ O
some -X- _ O
modelwith -X- _ O
learnable -X- _ O
parameters -X- _ O
 -X- _ O
. -X- _ O

For -X- _ O
neural -X- _ O
models -X- _ O
, -X- _ O
this -X- _ O
is -X- _ O
typically -X- _ O
done -X- _ O
by -X- _ O
ﬁrst -X- _ O
encoding -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
xand -X- _ O
then -X- _ O
calling -X- _ O
a -X- _ O
decoder -X- _ O
ttimes -X- _ O
to -X- _ O
produce -X- _ O
a -X- _ O
sequence -X- _ O
ytoken -X- _ O
- -X- _ O
by -X- _ O
- -X- _ O
token -X- _ O
, -X- _ O
from -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
. -X- _ O

A -X- _ O
popular -X- _ O
choice -X- _ O
for -X- _ O
both -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
is -X- _ O
the -X- _ O
transformer -X- _ B-MethodName
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

It -X- _ O
takes -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
embedded -X- _ O
tokens -X- _ O
s -X- _ O
= -X- _ O
s1 -X- _ O
; -X- _ O
s2 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
sjsj -X- _ O
and -X- _ O
treats -X- _ O
it -X- _ O
as -X- _ O
a -X- _ O
fully -X- _ O
connected -X- _ O
graph -X- _ O
over -X- _ O
which -X- _ O
a -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
module -X- _ O
is -X- _ O
applied -X- _ O
: -X- _ O
for -X- _ O
each -X- _ O
tokenstin -X- _ O
the -X- _ O
sequence -X- _ O
it -X- _ O
assigns -X- _ O
a -X- _ O
probabilistic -X- _ O
attention -X- _ O
score -X- _ O
atto -X- _ O
every -X- _ O
other -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
sentence -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
full -X- _ O
mathematical -X- _ O
details -X- _ O
we -X- _ O
refer -X- _ O
the -X- _ O
reader -X- _ O
to -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Typically -X- _ O
a -X- _ O
transformer -X- _ B-MethodName
encoder -X- _ O
is -X- _ O
employed -X- _ O
to -X- _ O
encodex -X- _ O
, -X- _ O
whereas -X- _ O
a -X- _ O
transformer -X- _ B-MethodName
decoder -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
producey -X- _ O
. -X- _ O

In -X- _ O
contrast -X- _ O
to -X- _ O
the -X- _ O
encoder -X- _ O
, -X- _ O
the -X- _ O
decoder -X- _ O
at -X- _ O
time -X- _ O
step -X- _ O
tonly -X- _ O
has -X- _ O
access -X- _ O
to -X- _ O
previously -X- _ O
produced -X- _ O
tokens -X- _ O
s -X- _ O
< -X- _ O
t -X- _ O
= -X- _ O
s1 -X- _ O
; -X- _ O
s2 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
st 1 -X- _ O
. -X- _ O

Consequently -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
module -X- _ O
can -X- _ O
not -X- _ O
take -X- _ O
possible -X- _ O
future -X- _ O
tokens -X- _ O
into -X- _ O
account -X- _ O
when -X- _ O
making -X- _ O
its -X- _ O
decision -X- _ O
at -X- _ O
time -X- _ O
t. -X- _ O
Additionally -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
encoderdecoder -X- _ O
framework -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
disconnect -X- _ O
between -X- _ O
inputxand -X- _ O
output -X- _ O
ybecause -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
modules -X- _ O
are -X- _ O
applied -X- _ O
to -X- _ O
xandyin -X- _ O
isolation -X- _ O
before -X- _ O
they -X- _ O
are -X- _ O
combined -X- _ O
. -X- _ O

The -X- _ O
latter -X- _ O
weakness -X- _ O
has -X- _ O
been -X- _ O
overcome -X- _ O
in -X- _ O
recent -X- _ O
work -X- _ O
( -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Wolf -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
by -X- _ O
feeding -X- _ O
the -X- _ O
concatenation -X- _ O
s -X- _ O
= -X- _ O
xyto -X- _ O
a -X- _ O
transformer -X- _ B-MethodName
decoder -X- _ O
. -X- _ O

At -X- _ O
training -X- _ O
time -X- _ O
, -X- _ O
given -X- _ O
the -X- _ O
current -X- _ O
token -X- _ O
st -X- _ O
, -X- _ O
the -X- _ O
transformer -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
predict -X- _ B-TaskName
the -X- _ I-TaskName
next -X- _ I-TaskName
word -X- _ I-TaskName
st+1via -X- _ O
maximum -X- _ O
likelihood -X- _ O
estimation -X- _ O
. -X- _ O

At -X- _ O
test -X- _ O
time -X- _ O
, -X- _ O
the -X- _ O
transformer -X- _ B-MethodName
is -X- _ O
conditioned -X- _ O
on -X- _ O
xand -X- _ O
then -X- _ O
produces -X- _ O
the -X- _ O
outputytoken -X- _ O
- -X- _ O
by -X- _ O
- -X- _ O
token -X- _ O
. -X- _ O

But -X- _ O
because -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
a -X- _ O
transformer -X- _ B-MethodName
decoder -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
unable -X- _ O
to -X- _ O
take -X- _ O
possible -X- _ O
future -X- _ O
tokens -X- _ O
into -X- _ O
account.3 -X- _ O
Bidirectional -X- _ O
Sequence -X- _ B-TaskName
Generation -X- _ I-TaskName
During -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
, -X- _ O
we -X- _ O
want -X- _ O
to -X- _ O
take -X- _ O
both -X- _ O
past -X- _ O
and -X- _ O
future -X- _ O
tokens -X- _ O
into -X- _ O
account -X- _ O
. -X- _ O

More -X- _ O
formally -X- _ O
, -X- _ O
at -X- _ O
time -X- _ O
t -X- _ O
, -X- _ O
we -X- _ O
want -X- _ O
to -X- _ O
attend -X- _ O
to -X- _ O
both -X- _ O
s1 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
st 1as -X- _ O
well -X- _ O
asst+1 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
sjsj -X- _ O
. -X- _ O

To -X- _ O
do -X- _ O
this -X- _ O
, -X- _ O
we -X- _ O
give -X- _ O
the -X- _ O
sequence -X- _ O
s -X- _ O
= -X- _ O
xy -X- _ O
, -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
the -X- _ O
sequences -X- _ O
xandy -X- _ O
, -X- _ O
to -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
encoder -X- _ O
, -X- _ O
rather -X- _ O
than -X- _ O
a -X- _ O
decoder -X- _ O
. -X- _ O

Of -X- _ O
course -X- _ O
, -X- _ O
at -X- _ O
inference -X- _ O
time -X- _ O
yis -X- _ O
unknown -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
replace -X- _ O
each -X- _ O
token -X- _ O
yjwith -X- _ O
a -X- _ O
placeholder -X- _ O
token -X- _ O
p -X- _ O
. -X- _ O

Since -X- _ O
the -X- _ O
model -X- _ O
needs -X- _ O
to -X- _ O
be -X- _ O
exposed -X- _ O
to -X- _ O
heterogeneous -X- _ O
placeholder -X- _ O
token -X- _ O
conﬁgurations -X- _ O
during -X- _ O
training -X- _ O
time -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
a -X- _ O
placeholder -X- _ O
strategythat -X- _ O
replaces -X- _ O
some -X- _ O
tokens -X- _ O
yjwith -X- _ O
placeholder -X- _ O
tokens -X- _ O
pat -X- _ O
training -X- _ O
time -X- _ O
. -X- _ O

Hence -X- _ O
, -X- _ O
during -X- _ O
training -X- _ O
, -X- _ O
a -X- _ O
sequence -X- _ O
yis -X- _ O
replaced -X- _ O
by -X- _ O
a -X- _ O
sequence -X- _ O
p= -X- _ O
p1 -X- _ O
; -X- _ O
p2 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
pjyj -X- _ O
, -X- _ O
where -X- _ O
a -X- _ O
token -X- _ O
pjis -X- _ O
either -X- _ O
the -X- _ O
original -X- _ O
tokenyjor -X- _ O
the -X- _ O
placeholder -X- _ O
token -X- _ O
p -X- _ O
. -X- _ O

We -X- _ O
introduce -X- _ O
two -X- _ O
placeholder -X- _ O
strategies -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
section -X- _ O
. -X- _ O

At -X- _ O
inference -X- _ O
time -X- _ O
, -X- _ O
pcontains -X- _ O
only -X- _ O
placeholder -X- _ O
tokens -X- _ O
up -X- _ O
to -X- _ O
some -X- _ O
pre -X- _ O
- -X- _ O
determined -X- _ O
maximum -X- _ B-HyperparameterName
sequence -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
. -X- _ O

With -X- _ O
the -X- _ O
placeholder -X- _ O
strategy -X- _ O
in -X- _ O
place -X- _ O
, -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
encoder -X- _ O
is -X- _ O
given -X- _ O
the -X- _ O
sequence -X- _ O
s -X- _ O
= -X- _ O
xp -X- _ O
as -X- _ O
input -X- _ O
. -X- _ O

The -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
module -X- _ O
then -X- _ O
computes -X- _ O
hidden -X- _ O
representations -X- _ O
rtof -X- _ O
each -X- _ O
token -X- _ O
stby -X- _ O
attending -X- _ O
to -X- _ O
every -X- _ O
other -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
s -X- _ O
. -X- _ O

Because -X- _ O
the -X- _ O
output -X- _ O
sequence -X- _ O
is -X- _ O
already -X- _ O
present -X- _ O
in -X- _ O
the -X- _ O
form -X- _ O
of -X- _ O
placeholder -X- _ O
tokens -X- _ O
both -X- _ O
past -X- _ O
tokens -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
future -X- _ O
, -X- _ O
not -X- _ O
- -X- _ O
yet -X- _ O
- -X- _ O
produced -X- _ O
, -X- _ O
tokens -X- _ O
can -X- _ O
be -X- _ O
taken -X- _ O
into -X- _ O
consideration -X- _ O
for -X- _ O
every -X- _ O
token -X- _ O
st -X- _ O
. -X- _ O

Following -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
step -X- _ O
, -X- _ O
placeholder -X- _ O
tokens -X- _ O
are -X- _ O
converted -X- _ O
into -X- _ O
a -X- _ O
token -X- _ O
from -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
with -X- _ O
a -X- _ O
language -X- _ O
model -X- _ O
( -X- _ O
LM -X- _ O
) -X- _ O
classiﬁcation -X- _ O
layer -X- _ O
, -X- _ O
where -X- _ O
for -X- _ O
each -X- _ O
placeholder -X- _ O
pt -X- _ O
, -X- _ O
its -X- _ O
hidden -X- _ O
representation -X- _ O
rtis -X- _ O
mapped -X- _ O
to -X- _ O
a -X- _ O
distribution -X- _ O
dt -X- _ O
over -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
. -X- _ O

At -X- _ O
training -X- _ O
time -X- _ O
, -X- _ O
each -X- _ O
output -X- _ O
sequence -X- _ O
token -X- _ O
is -X- _ O
fed -X- _ O
the -X- _ O
gold -X- _ O
label -X- _ O
and -X- _ O
updates -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
are -X- _ O
performed -X- _ O
using -X- _ O
stochastic -X- _ O
gradient -X- _ O
descent -X- _ O
with -X- _ O
a -X- _ O
cross -X- _ O
- -X- _ O
entropy -X- _ O
loss -X- _ O
, -X- _ O
i.e -X- _ O
. -X- _ O

MMX -X- _ O
m=1jyjX -X- _ O
j=1log -X- _ O
( -X- _ O
pj -X- _ O
= -X- _ O
yjjs -X- _ O
) -X- _ O
; -X- _ O
whereMis -X- _ O
the -X- _ O
size -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
a -X- _ I-HyperparameterName
minibatch -X- _ I-HyperparameterName
. -X- _ O

At -X- _ O
inference -X- _ O
time -X- _ O
, -X- _ O
the -X- _ O
placeholder -X- _ O
tokens -X- _ O
can -X- _ O
be -X- _ O
replaced -X- _ O
iteratively -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
probability -X- _ O
distributiondtover -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
for -X- _ O
each -X- _ O
placeholder -X- _ O
pt -X- _ O
. -X- _ O

Different -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
strategies -X- _ O
are -X- _ O
outlined -X- _ O
in -X- _ O
Section -X- _ O
3.2 -X- _ O
. -X- _ O

3.1 -X- _ O
Placeholder -X- _ O
Replacement -X- _ O
Strategy -X- _ O
At -X- _ O
inference -X- _ O
time -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
sequence -X- _ O
starts -X- _ O
out -X- _ O
with -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
placeholder -X- _ O
tokens -X- _ O
. -X- _ O

To -X- _ O
introduce -X- _ O
this -X- _ O
notion -X- _ O
at -X- _ O
training -X- _ O
time -X- _ O
, -X- _ O
we -X- _ O
require -X- _ O
a -X- _ O
strategy -X- _ O
that -X- _ O
replaces -X- _ O
some -X- _ O
output -X- _ O
sequence -X- _ O
tokens -X- _ O
yj -X- _ O
with -X- _ O
the -X- _ O
placeholder -X- _ O
token -X- _ O
p -X- _ O
. -X- _ O

The -X- _ O
simplest -X- _ O
approach -X- _ O
would -X- _ O
be -X- _ O
to -X- _ O
replace -X- _ O
all -X- _ O
output -X- _ O
sequence -X- _ O
tokensyjwith -X- _ O
the -X- _ O
placeholder -X- _ O
token -X- _ O
p -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
with -X- _ O
this -X- _ O
approach -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
never -X- _ O
confronted -X- _ O
with -X- _ O
a -X- _ O
sequence -X- _ O
containing -X- _ O
a -X- _ O
mix -X- _ O
of -X- _ O
output -X- _ O
sequence -X- _ O
tokens -X- _ O
and -X- _ O
placeholder -X- _ O
tokens -X- _ O
. -X- _ O

Due -X- _ O
to -X- _ O
the -X- _ O
exponential -X- _ O
number -X- _ O
of -X- _ O
possible -X- _ O
replacement -X- _ O
conﬁgurations -X- _ O
per -X- _ O
given -X- _ O
token -X- _ O
sequence -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
probabilistic -X- _ O
generative -X- _ O
models -X- _ O
that -X- _ O
we -X- _ O
can -X- _ O
use -X- _ O
to -X- _ O
draw -X- _ O
diverse -X- _ O
sequence -X- _ O
replacements -X- _ O
. -X- _ O

To -X- _ O
this -X- _ O
end -X- _ O
, -X- _ O
we -X- _ O
model -X- _ O
the -X- _ O
decision -X- _ O
whether -X- _ O
to -X- _ O
use -X- _ O
placeholder -X- _ O
or -X- _ O
input -X- _ O
tokens -X- _ O
with -X- _ O
probabilistic -X- _ O
models -X- _ O
, -X- _ O
two -X- _ O
of -X- _ O
which -X- _ O
we -X- _ O
propose -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
. -X- _ O

Bernoulli -X- _ O
Random -X- _ O
Variables -X- _ O
( -X- _ O
RV -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
model -X- _ O
each -X- _ O
position -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
with -X- _ O
a -X- _ O
binary -X- _ O
random -X- _ O
variable -X- _ O
with -X- _ O
a -X- _ O
ﬁxed -X- _ O
mean -X- _ O
. -X- _ O

For -X- _ O
every -X- _ O
input -X- _ O
sequence -X- _ O
and -X- _ O
every -X- _ O
position -X- _ O
iin -X- _ O
the -X- _ O
sequence -X- _ O
, -X- _ O
we -X- _ O
draw -X- _ O
from -X- _ O
a -X- _ O
Bernoulli -X- _ O
variable -X- _ O
with -X- _ O
mean -X- _ O
to -X- _ O

decide -X- _ O
whether -X- _ O
to -X- _ O
use -X- _ O
yiorp -X- _ O
. -X- _ O

The -X- _ O
expected -X- _ O
number -X- _ O
of -X- _ O
placeholder -X- _ O
tokens -X- _ O
in -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
length -X- _ O
jyjisjyjand -X- _ O
the -X- _ O
variance -X- _ O
is -X- _ O
2 -X- _ O
= -X- _ O
jyj -X- _ O
( -X- _ O
1  -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
variance -X- _ O
of -X- _ O
this -X- _ O
strategy -X- _ O
is -X- _ O
determined -X- _ O
by -X- _ O
the -X- _ O
mean -X- _ O
and -X- _ O
, -X- _ O
therefore -X- _ O
, -X- _ O
the -X- _ O
probabilistic -X- _ O
model -X- _ O
has -X- _ O
one -X- _ O
tunable -X- _ O
parameter -X- _ O
. -X- _ O
Gaussian -X- _ O
Random -X- _ O
Variables -X- _ O
. -X- _ O

The -X- _ O
number -X- _ O
of -X- _ O
placeholder -X- _ O
tokens -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
pcan -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
drawn -X- _ O
from -X- _ O
an -X- _ O
unknown -X- _ O
optimal -X- _ O
Binomial -X- _ O
distribution -X- _ O
. -X- _ O

We -X- _ O
can -X- _ O
approximate -X- _ O
this -X- _ O
Binomial -X- _ O
with -X- _ O
a -X- _ O
normal -X- _ O
distribution -X- _ O
N -X- _ O
( -X- _ O
 -X- _ O
; -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
is -X- _ O
the -X- _ O
mean -X- _ O
and -X- _ O
the -X- _ O
standard -X- _ O
deviation -X- _ O
and -X- _ O
they -X- _ O
are -X- _ O
considered -X- _ O
hyperparameters -X- _ O
. -X- _ O

More -X- _ O
formally -X- _ O
, -X- _ O
for -X- _ O
every -X- _ O
input -X- _ O
sequence -X- _ O
, -X- _ O
we -X- _ O
draw -X- _ O
a -X- _ O
value -X- _ O
P -X- _ O
N -X- _ O
( -X- _ O
 -X- _ O
; -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O

Multiplied -X- _ O
with -X- _ O
the -X- _ O
sequencelengthjyj -X- _ O
, -X- _ O
the -X- _ O
nearest -X- _ O
integer -X- _ O
value -X- _ O
b -X- _ O
( -X- _ O
jyjP -X- _ O
) -X- _ O
eis -X- _ O
used -X- _ O
as -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
placeholder -X- _ O
tokens -X- _ O
for -X- _ O
the -X- _ O
given -X- _ O
sequence -X- _ O
. -X- _ O

The -X- _ O
positions -X- _ O
of -X- _ O
the -X- _ O
placeholder -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
are -X- _ O
then -X- _ O
determined -X- _ O
at -X- _ O
random -X- _ O
. -X- _ O

The -X- _ O
resulting -X- _ O
probabilistic -X- _ O
model -X- _ O
’s -X- _ O
two -X- _ O
parameters -X- _ O
( -X- _ O
mean -X- _ O
and -X- _ O
standard -X- _ O
deviation -X- _ O
 -X- _ O
) -X- _ O
are -X- _ O
treated -X- _ O
as -X- _ O
hyperparameters -X- _ O
and -X- _ O
are -X- _ O
not -X- _ O
updated -X- _ O
during -X- _ O
training -X- _ O
. -X- _ O

Being -X- _ O
able -X- _ O
to -X- _ O
tune -X- _ O
the -X- _ O
variance -X- _ O
parameter -X- _ O
independently -X- _ O
from -X- _ O
the -X- _ O
mean -X- _ O
parameter -X- _ O
might -X- _ O
provide -X- _ O
an -X- _ O
advantage -X- _ O
over -X- _ O
the -X- _ O
parameterization -X- _ O
with -X- _ O
Bernoulli -X- _ O
RVs -X- _ O
. -X- _ O

3.2 -X- _ O
Sequence -X- _ B-TaskName
Generation -X- _ I-TaskName
Strategies -X- _ O
Starting -X- _ O
with -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
placeholder -X- _ O
tokens -X- _ O
at -X- _ O
inference -X- _ O
time -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
possible -X- _ O
to -X- _ O
generate -X- _ O
output -X- _ O
token -X- _ O
sequences -X- _ O
in -X- _ O
arbitrary -X- _ O
order -X- _ O
. -X- _ O

We -X- _ O
experiment -X- _ O
with -X- _ O
the -X- _ O
following -X- _ O
strategies -X- _ O
. -X- _ O

The -X- _ O
distribution -X- _ O
in -X- _ O
all -X- _ O
of -X- _ O
these -X- _ O
strategies -X- _ O
are -X- _ O
the -X- _ O
distributions -X- _ O
dt -X- _ O
( -X- _ O
t= -X- _ O
1 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
n -X- _ O
) -X- _ O
for -X- _ O
the -X- _ O
placeholders -X- _ O
over -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
term -X- _ O
uncover -X- _ O
to -X- _ O
mean -X- _ O
that -X- _ O
an -X- _ O
output -X- _ O
token -X- _ O
is -X- _ O
generated -X- _ O
for -X- _ O
a -X- _ O
placeholder -X- _ O
token -X- _ O
. -X- _ O

One -X- _ O
- -X- _ O
step -X- _ O
greedy -X- _ O
. -X- _ O

In -X- _ O
a -X- _ O
single -X- _ O
time -X- _ O
step -X- _ O
, -X- _ O
all -X- _ O
placeholder -X- _ O
tokens -X- _ O
are -X- _ O
uncovered -X- _ O
simultaneously -X- _ O
by -X- _ O
picking -X- _ O
the -X- _ O
most -X- _ O
probable -X- _ O
token -X- _ O
from -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
for -X- _ O
each -X- _ O
placeholder -X- _ O
. -X- _ O

Highest -X- _ O
probability -X- _ O
. -X- _ O

Placeholders -X- _ O
are -X- _ O
replaced -X- _ O
iteratively -X- _ O
and -X- _ O
the -X- _ O
placeholder -X- _ O
to -X- _ O
be -X- _ O
uncovered -X- _ O
is -X- _ O
the -X- _ O
placeholder -X- _ O
that -X- _ O
assigns -X- _ O
the -X- _ O
highest -X- _ O
probability -X- _ O
to -X- _ O
a -X- _ O
token -X- _ O
from -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
, -X- _ O
indicating -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
the -X- _ O
most -X- _ O
sure -X- _ O
about -X- _ O
this -X- _ O
token -X- _ O
. -X- _ O

Lowest -X- _ O
entropy -X- _ O
. -X- _ O

Placeholders -X- _ O
are -X- _ O
replaced -X- _ O
iteratively -X- _ O
and -X- _ O
the -X- _ O
placeholder -X- _ O
to -X- _ O
be -X- _ O
uncovered -X- _ O
is -X- _ O
the -X- _ O
placeholder -X- _ O
that -X- _ O
exhibits -X- _ O
the -X- _ O
lowest -X- _ O
entropy -X- _ O
over -X- _ O
its -X- _ O
output -X- _ O
vocabulary -X- _ O
distribution -X- _ O
and -X- _ O
the -X- _ O
most -X- _ O
likely -X- _ O
token -X- _ O
at -X- _ O
this -X- _ O
position -X- _ O
is -X- _ O
chosen -X- _ O
. -X- _ O

Intuitively -X- _ O
, -X- _ O
the -X- _ O
lowest -X- _ O
entropy -X- _ O
indicates -X- _ O
the -X- _ O
position -X- _ O
where -X- _ O
the -X- _ O
uncertainty -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
decide -X- _ O
between -X- _ O
tokens -X- _ O
of -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
is -X- _ O
the -X- _ O
lowest -X- _ O
. -X- _ O

Left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
. -X- _ O

Placeholders -X- _ O
are -X- _ O
replaced -X- _ O
iteratively -X- _ O
, -X- _ O
moving -X- _ O
from -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
and -X- _ O
thus -X- _ O
mimicking -X- _ O
the -X- _ O
typical -X- _ O
writing -X- _ O
style -X- _ O
for -X- _ O
English -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
this -X- _ O
approach -X- _ O
still -X- _ O
differs -X- _ O
from -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
decoders -X- _ O
because -X- _ O
future -X- _ O
tokens -X- _ O
are -X- _ O
considered -X- _ O
via -X- _ O
the -X- _ O
placeholder -X- _ O
representations -X- _ O
. -X- _ O

No -X- _ O
look -X- _ O
ahead -X- _ O
. -X- _ O

To -X- _ O
test -X- _ O
whether -X- _ O
future -X- _ O
placeholders -X- _ O
hold -X- _ O
useful -X- _ O
information -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
an -X- _ O
adversarial -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
strategy -X- _ O
: -X- _ O

Again -X- _ O
we -X- _ O
iteratively -X- _ O
uncover -X- _ O
placeholders -X- _ O
from -X- _ O
left -X- _ O
- -X- _ O
toright -X- _ O
, -X- _ O
but -X- _ O
we -X- _ O
suppress -X- _ O
all -X- _ O
attention -X- _ O
ﬂows -X- _ O
from -X- _ O
future -X- _ O
placeholders -X- _ O
. -X- _ O

This -X- _ O
imitates -X- _ O
the -X- _ O
behaviour -X- _ O
of -X- _ O

4a -X- _ O
transformer -X- _ O
decoder -X- _ O
but -X- _ O
follows -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
predicting -X- _ O
a -X- _ O
token -X- _ O
on -X- _ O
a -X- _ O
placeholder -X- _ O
, -X- _ O
rather -X- _ O
than -X- _ O
predicting -X- _ O
the -X- _ O
next -X- _ O
word -X- _ O
as -X- _ O
is -X- _ O
typically -X- _ O
done -X- _ O
in -X- _ O
transformer -X- _ O
decoders -X- _ O
. -X- _ O

If -X- _ O
this -X- _ O
performs -X- _ O
worse -X- _ O
than -X- _ O
leftto -X- _ O
- -X- _ O
right -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
indeed -X- _ O
valuable -X- _ O
information -X- _ O
in -X- _ O
future -X- _ O
placeholder -X- _ O
tokens -X- _ O
. -X- _ O

4 -X- _ O
Experiments -X- _ O
We -X- _ O
conduct -X- _ O
a -X- _ O
series -X- _ O
of -X- _ O
experiments -X- _ O
to -X- _ O
explore -X- _ O
B -X- _ B-MethodName
ISON -X- _ I-MethodName
’s -X- _ O
behavior -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
we -X- _ O
want -X- _ O
to -X- _ O
compare -X- _ O
two -X- _ O
token -X- _ O
replacement -X- _ O
strategies -X- _ O
for -X- _ O
training -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
four -X- _ O
generation -X- _ O
strategies -X- _ O
for -X- _ O
inference -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
we -X- _ O
want -X- _ O
to -X- _ O
compare -X- _ O
B -X- _ B-MethodName
ISONto -X- _ I-MethodName
state -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
methods -X- _ O
and -X- _ O
investigate -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
its -X- _ O
ability -X- _ O
to -X- _ O
attend -X- _ O
to -X- _ O
future -X- _ O
tokens -X- _ O
. -X- _ O

4.1 -X- _ O
Datasets -X- _ O
We -X- _ O
run -X- _ O
experiments -X- _ O
on -X- _ O
the -X- _ O
two -X- _ O
following -X- _ O
conversational -X- _ O
datasets -X- _ O
. -X- _ O

Goal -X- _ O
- -X- _ O
oriented -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
( -X- _ O
Saeidi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

SHARC -X- _ B-DatasetName
is -X- _ O
a -X- _ O
dialogue -X- _ O
, -X- _ O
text -X- _ O
- -X- _ O
based -X- _ O
questionanswering -X- _ O
dataset -X- _ O
. -X- _ O

Unlike -X- _ O
many -X- _ O
popular -X- _ O
QA -X- _ O
datasets -X- _ O
, -X- _ O
answers -X- _ O
can -X- _ O
not -X- _ O
simply -X- _ O
be -X- _ O
extracted -X- _ O
from -X- _ O
the -X- _ O
text -X- _ O
. -X- _ O

Given -X- _ O
a -X- _ O
regulatory -X- _ O
text -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
a -X- _ O
text -X- _ O
from -X- _ O
the -X- _ O
UK -X- _ O
government -X- _ O
’s -X- _ O
website -X- _ O
, -X- _ O
and -X- _ O
a -X- _ O
user -X- _ O
scenario -X- _ O
with -X- _ O
corresponding -X- _ O
question -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
necessary -X- _ O
to -X- _ O
interpret -X- _ O
the -X- _ O
text -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
the -X- _ O
speciﬁc -X- _ O
user -X- _ O
’s -X- _ O
needs -X- _ O
. -X- _ O

Before -X- _ O
generating -X- _ O
its -X- _ O
ﬁnal -X- _ O
answer -X- _ O
, -X- _ O
a -X- _ O
system -X- _ O
may -X- _ O
generate -X- _ O
clariﬁcation -X- _ O
questions -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
the -X- _ O
system -X- _ O
decides -X- _ O
if -X- _ O
the -X- _ O
answer -X- _ O
to -X- _ O
the -X- _ O
user -X- _ O
’s -X- _ O
original -X- _ O
question -X- _ O
is -X- _ O
“ -X- _ O
Yes -X- _ O
” -X- _ O
, -X- _ O
“ -X- _ O
No -X- _ O
” -X- _ O
or -X- _ O
“ -X- _ O
Irrelevant -X- _ O
” -X- _ O
where -X- _ O
the -X- _ O
latter -X- _ O
means -X- _ O
the -X- _ O
question -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
answered -X- _ O
with -X- _ O
the -X- _ O
given -X- _ O
text -X- _ O
. -X- _ O

We -X- _ O
perform -X- _ O
the -X- _ O
evaluation -X- _ O
with -X- _ O
the -X- _ O
ofﬁcial -X- _ O
SHARC -X- _ B-DatasetName
script -X- _ O
. -X- _ O

For -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
generated -X- _ O
clariﬁcation -X- _ O
questions -X- _ O
, -X- _ O
it -X- _ O
computes -X- _ O
BLEU -X- _ B-MetricName
n -X- _ B-MetricName
- -X- _ I-MetricName
gram -X- _ I-MetricName
scores -X- _ O
forn= -X- _ O
1 -X- _ B-MetricValue
; -X- _ I-MetricValue
2 -X- _ I-MetricValue
; -X- _ I-MetricValue
3 -X- _ I-MetricValue
; -X- _ I-MetricValue
4using -X- _ I-MetricValue
a -X- _ O
set -X- _ O
of -X- _ O
clariﬁcation -X- _ O
question -X- _ O
in -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
gold -X- _ O
responses -X- _ O
. -X- _ O

In -X- _ O
each -X- _ O
step -X- _ O
of -X- _ O
the -X- _ O
conversation -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
under -X- _ O
evaluation -X- _ O
generates -X- _ O
an -X- _ O
output -X- _ O
token -X- _ O
sequence -X- _ O
. -X- _ O

This -X- _ O
output -X- _ O
is -X- _ O
automatically -X- _ O
assigned -X- _ O
to -X- _ O
the -X- _ O
category -X- _ O
“ -X- _ O
More -X- _ O
” -X- _ O
if -X- _ O
it -X- _ O
is -X- _ O
a -X- _ O
clariﬁcation -X- _ O
question -X- _ O
, -X- _ O
and -X- _ O
to -X- _ O
“ -X- _ O
Yes -X- _ O
” -X- _ O
, -X- _ O
“ -X- _ O
No -X- _ O
” -X- _ O
, -X- _ O
and -X- _ O
“ -X- _ O
Irrelevant -X- _ O
” -X- _ O
otherwise -X- _ O
. -X- _ O

Since -X- _ O
this -X- _ O
is -X- _ O
a -X- _ O
classiﬁcation -X- _ O
task -X- _ O
we -X- _ O
can -X- _ O
compute -X- _ O
micro -X- _ O
and -X- _ O
macro -X- _ O
accuracy -X- _ O
for -X- _ O
it -X- _ O
. -X- _ O

The -X- _ O
ﬁnal -X- _ O
model -X- _ O
is -X- _ O
chosen -X- _ O
using -X- _ O
the -X- _ O
highest -X- _ O
BLEU-4 -X- _ B-MetricName
score -X- _ O
on -X- _ O
the -X- _ O
development -X- _ O
set -X- _ O
. -X- _ O

The -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
dataset -X- _ O
has -X- _ O
a -X- _ O
hidden -X- _ O
test -X- _ O
set -X- _ O
and -X- _ O
, -X- _ O
therefore -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
feasible -X- _ O
to -X- _ O
evaluate -X- _ O
our -X- _ O
various -X- _ O
model -X- _ O
variants -X- _ O
. -X- _ O

Hence -X- _ O
, -X- _ O
we -X- _ O
take -X- _ O
30 -X- _ O
unique -X- _ O
rule -X- _ O
texts -X- _ O
and -X- _ O
their -X- _ O
corresponding -X- _ O
training -X- _ O
examples -X- _ O
from -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
. -X- _ O

This -X- _ O
leads -X- _ O
to -X- _ O
a -X- _ O
new -X- _ O
de -X- _ O
- -X- _ O
velopment -X- _ O
set -X- _ O
of -X- _ O
2,465 -X- _ O
instances -X- _ O
and -X- _ O
leaves -X- _ O
the -X- _ O
ofﬁcial -X- _ O
development -X- _ O
set -X- _ O
to -X- _ O
be -X- _ O
used -X- _ O
as -X- _ O
a -X- _ O
test -X- _ O
set -X- _ O
here -X- _ O
. -X- _ O

Finally -X- _ O
we -X- _ O
submitted -X- _ O
our -X- _ O
best -X- _ O
model -X- _ O
to -X- _ O
be -X- _ O
evaluated -X- _ O
on -X- _ O
the -X- _ O
hidden -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

Free -X- _ O
- -X- _ O
form -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

DAILY -X- _ B-DatasetName
DIALOG -X- _ I-DatasetName
is -X- _ O
a -X- _ O
dataset -X- _ O
of -X- _ O
written -X- _ O
conversations -X- _ O
occurring -X- _ O
in -X- _ O
daily -X- _ O
life -X- _ O
. -X- _ O

Following -X- _ O
the -X- _ O
authors -X- _ O
of -X- _ O
the -X- _ O
corpus -X- _ O
, -X- _ O
we -X- _ O
report -X- _ O
BLEU -X- _ B-MetricName
n -X- _ B-MetricName
- -X- _ I-MetricName
gram -X- _ I-MetricName
scores -X- _ O
for -X- _ O
n= -X- _ O
1 -X- _ B-MetricValue
; -X- _ I-MetricValue
2 -X- _ I-MetricValue
; -X- _ I-MetricValue
3 -X- _ I-MetricValue
; -X- _ I-MetricValue
4for -X- _ I-MetricValue
the -X- _ O
generated -X- _ O
output -X- _ O
sequences -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
the -X- _ O
given -X- _ O
gold -X- _ O
responses -X- _ O
. -X- _ O

We -X- _ O
tokenize -X- _ O
these -X- _ O
responses -X- _ O
equally -X- _ O
to -X- _ O
ensure -X- _ O
a -X- _ O
fair -X- _ O
comparison -X- _ O
. -X- _ O

4.2 -X- _ O
B -X- _ B-MethodName
ISONSettings -X- _ I-MethodName
We -X- _ O
implement -X- _ O
B -X- _ B-MethodName
ISONbased -X- _ I-MethodName
on -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
Pytorch -X- _ O
code1and -X- _ O
initialize -X- _ O
with -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
BERT -X- _ B-MethodName
-BASE -X- _ O
-UNCASED -X- _ O

( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Consequently -X- _ O
we -X- _ O
employ -X- _ O
the -X- _ O
same -X- _ O
model -X- _ O
architecture -X- _ O
and -X- _ O
tokenisation -X- _ O
as -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
resulting -X- _ O
in -X- _ O
a -X- _ O
model -X- _ O
with -X- _ O
about -X- _ O
110 -X- _ O
M -X- _ O
parameters -X- _ O
. -X- _ O

To -X- _ O
remain -X- _ O
compatible -X- _ O
with -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
, -X- _ O
we -X- _ O
prepend -X- _ O
each -X- _ O
sequence -X- _ O
with -X- _ O
a -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
and -X- _ O
place -X- _ O
a -X- _ O
[ -X- _ O
SEP -X- _ O
] -X- _ O
token -X- _ O
after -X- _ O
the -X- _ O
input -X- _ O
context -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
producing -X- _ O
a -X- _ O
second -X- _ O
[ -X- _ O
SEP -X- _ O
] -X- _ O
token -X- _ O
indicates -X- _ O
the -X- _ O
end -X- _ O
of -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
. -X- _ O

For -X- _ O
input -X- _ O
context -X- _ O
of -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
, -X- _ O
we -X- _ O
follow -X- _ O
Saeidi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O

and -X- _ O
use -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
question -X- _ O
, -X- _ O
rule -X- _ O
text -X- _ O
, -X- _ O
scenario -X- _ O
and -X- _ O
history -X- _ O
. -X- _ O

The -X- _ O
input -X- _ O
context -X- _ O
for -X- _ O
DAILY -X- _ B-DatasetName
DIALOG -X- _ I-DatasetName
is -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
all -X- _ O
previous -X- _ O
utterances -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
and -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
training -X- _ O
sets -X- _ O
we -X- _ O
train -X- _ O
for -X- _ O
20 -X- _ B-HyperparameterValue
and -X- _ O
40 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
, -X- _ O
respectively -X- _ O
, -X- _ O
which -X- _ O
equates -X- _ O
in -X- _ O
each -X- _ O
case -X- _ O
to -X- _ O
about -X- _ O
200kseen -X- _ O
examples -X- _ O
. -X- _ O

As -X- _ O
optimizer -X- _ O
we -X- _ O
used -X- _ O
A -X- _ O
DAM -X- _ O
( -X- _ O
Kingma -X- _ O
weight -X- _ B-HyperparameterName
decay -X- _ I-HyperparameterName
of -X- _ O
0:01and -X- _ B-HyperparameterValue
a -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
warm -X- _ I-HyperparameterName
- -X- _ I-HyperparameterName
up -X- _ I-HyperparameterName
over -X- _ O
the -X- _ O
ﬁrst -X- _ O
10 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
training -X- _ O
steps -X- _ O
. -X- _ O

As -X- _ O
learning -X- _ O
rates -X- _ O
we -X- _ O
consider -X- _ O
both -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
BERT -X- _ B-MethodName
1e-4and -X- _ B-HyperparameterValue
the -X- _ O
ﬁne -X- _ O
- -X- _ O
tuning -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
3e-5 -X- _ B-HyperparameterValue
. -X- _ O

On -X- _ O
preliminary -X- _ O
experiments -X- _ O
3e-5proved -X- _ O
to -X- _ O
be -X- _ O
best -X- _ O
for -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
, -X- _ O
whereas -X- _ O
it -X- _ O
is -X- _ O
1e-4for -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
. -X- _ O

We -X- _ O
set -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
to -X- _ O
15 -X- _ B-HyperparameterValue
. -X- _ O

Finally -X- _ O
, -X- _ O
the -X- _ O
maximum -X- _ O
sequence -X- _ B-HyperparameterName
generation -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
, -X- _ O
is -X- _ O
set -X- _ O
to -X- _ O
50 -X- _ B-HyperparameterValue
for -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
and -X- _ O
to -X- _ O
100 -X- _ B-HyperparameterValue
for -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
, -X- _ O
which -X- _ O
was -X- _ O
chosen -X- _ O
based -X- _ O
on -X- _ O
values -X- _ O
observed -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
maximum -X- _ B-HyperparameterName
sequence -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
of -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
is -X- _ O
512 -X- _ B-HyperparameterValue
, -X- _ O
longer -X- _ O
input -X- _ O
sequences -X- _ O
are -X- _ O
truncated -X- _ O
accordingly -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
main -X- _ O
results -X- _ O
, -X- _ O
we -X- _ O
employ -X- _ O
the -X- _ O
sequence -X- _ O
generation -X- _ O
strategy -X- _ O
left1https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
huggingface -X- _ O
/ -X- _ O
pytorch -X- _ O
- -X- _ O
pretrained -X- _ O
- -X- _ O
BERT -X- _ B-MethodName

5to -X- _ O
- -X- _ O
right -X- _ O
, -X- _ O
which -X- _ O
we -X- _ O
found -X- _ O
to -X- _ O
work -X- _ O
best -X- _ O
. -X- _ O

Later -X- _ O
on -X- _ O
we -X- _ O
also -X- _ O
report -X- _ O
results -X- _ O
for -X- _ O
the -X- _ O
other -X- _ O
strategies -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
Bernoulli -X- _ O
RV -X- _ O
approach -X- _ O
, -X- _ O
we -X- _ O
test -X- _ O
2 -X- _ O

[ -X- _ O
0:2 -X- _ O
; -X- _ O
0:8 -X- _ O
] -X- _ O
with -X- _ O
increments -X- _ O
of -X- _ O
0:1 -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
Gaussian -X- _ O
RV -X- _ O
approach -X- _ O
, -X- _ O
we -X- _ O
test -X- _ O
all -X- _ O
possible -X- _ O
combinations -X- _ O
for -X- _ O
the -X- _ O
following -X- _ O
hyperparmeters -X- _ O
=f0:4 -X- _ O
; -X- _ O
0:5 -X- _ O
; -X- _ O
0:6 -X- _ O
g -X- _ O
and=f0:3 -X- _ O
; -X- _ O
0:6 -X- _ O
; -X- _ O
0:9 -X- _ O
g -X- _ O
. -X- _ O

The -X- _ O
best -X- _ O
combination -X- _ O
on -X- _ O
performs -X- _ O
the -X- _ O
best -X- _ O
Bernoulli -X- _ O
approach -X- _ O
( -X- _ O
= -X- _ O
0:7 -X- _ O
) -X- _ O
by -X- _ O
3.4 -X- _ B-MetricValue
point -X- _ I-MetricValue
in -X- _ O
BLEU-4 -X- _ B-MetricName
score -X- _ O
. -X- _ O

Some -X- _ O
Bernoulli -X- _ O
experiments -X- _ O
in -X- _ O
fact -X- _ O
only -X- _ O
produced -X- _ O
a -X- _ O
very -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
clariﬁcation -X- _ O
question -X- _ O
, -X- _ O
e.g. -X- _ O
= -X- _ O
0:5only -X- _ O
generated -X- _ O
9 -X- _ O
clariﬁcation -X- _ O
questions -X- _ O
on -X- _ O
the -X- _ O
development -X- _ O
set -X- _ O
, -X- _ O
whereas -X- _ O
in -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
responses -X- _ O
846 -X- _ O
clariﬁcation -X- _ O
questions -X- _ O
occur -X- _ O
. -X- _ O

This -X- _ O
suggests -X- _ O
that -X- _ O
a -X- _ O
high -X- _ O
variance -X- _ O
is -X- _ O
important -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
Bernoulli -X- _ O
setups -X- _ O
all -X- _ O
have -X- _ O
a -X- _ O
variance -X- _ O
of -X- _ O
0:25or -X- _ O
lower -X- _ O
and -X- _ O
our -X- _ O
best -X- _ O
Gaussian -X- _ O
approach -X- _ O
has -X- _ O
a -X- _ O
variance -X- _ O
of -X- _ O
0:6 -X- _ O
. -X- _ O

We -X- _ O
directly -X- _ O
employ -X- _ O
the -X- _ O
Gaussian -X- _ O
distribution -X- _ O
with -X- _ O
= -X- _ O
0:5 -X- _ O
; -X- _ O
= -X- _ O

0:6on -X- _ O
the -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
task -X- _ O
. -X- _ O

4.3 -X- _ O
Baselines -X- _ O
To -X- _ O
measure -X- _ O
the -X- _ O
success -X- _ O
of -X- _ O
our -X- _ O
proposed -X- _ O
approach -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
the -X- _ O
following -X- _ O
three -X- _ O
baselines -X- _ O
. -X- _ O

Encoder -X- _ B-MethodName
- -X- _ I-MethodName
Decoder -X- _ I-MethodName
Transformer -X- _ I-MethodName
( -X- _ O
E -X- _ B-MethodName
& -X- _ I-MethodName
D -X- _ I-MethodName
) -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
our -X- _ O
bidirectional -X- _ O
encoder -X- _ O
to -X- _ O
a -X- _ O
standard -X- _ O
encoder -X- _ B-MethodName
- -X- _ I-MethodName
decoder -X- _ I-MethodName
Transformer -X- _ I-MethodName
where -X- _ O
the -X- _ O
decoder -X- _ O
only -X- _ O
has -X- _ O
access -X- _ O
to -X- _ O
tokens -X- _ O
produced -X- _ O
so -X- _ O
far -X- _ O
to -X- _ O
compute -X- _ O
its -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
implementation -X- _ O
of -X- _ O
OpenNMT -X- _ O
( -X- _ O
Klein -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
employ -X- _ O
the -X- _ O
parameters -X- _ O
suggested -X- _ O
by -X- _ O
them -X- _ O
, -X- _ O
but -X- _ O
adjust -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
to -X- _ O
0:1 -X- _ B-HyperparameterValue
, -X- _ O
which -X- _ O
we -X- _ O
found -X- _ O
to -X- _ O
work -X- _ O
better -X- _ O
for -X- _ O
both -X- _ O
datasets -X- _ O
. -X- _ O

Additionally -X- _ O
, -X- _ O
we -X- _ O
increased -X- _ O
the -X- _ O
word -X- _ O
and -X- _ O
hidden -X- _ O
dimension -X- _ O
size -X- _ O
to -X- _ O
768 -X- _ O
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
attention -X- _ O
heads -X- _ O
to -X- _ O
12 -X- _ O
to -X- _ O
match -X- _ O
the -X- _ O
capacity -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

Training -X- _ O
ran -X- _ O
for -X- _ O
50 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
. -X- _ O

Needing -X- _ O
both -X- _ O
an -X- _ O
encoder -X- _ O
and -X- _ O
a -X- _ O
decoder -X- _ O
, -X- _ O
this -X- _ O
leads -X- _ O
to -X- _ O
a -X- _ O
total -X- _ O
of -X- _ O
about -X- _ O
270 -X- _ O
M -X- _ O
parameters -X- _ O
. -X- _ O

Encoder -X- _ B-MethodName
- -X- _ I-MethodName
Decoder -X- _ I-MethodName
Transformer -X- _ I-MethodName
with -X- _ I-MethodName
BERT -X- _ I-MethodName
( -X- _ O
E -X- _ B-MethodName
& -X- _ I-MethodName
D+B -X- _ I-MethodName
) -X- _ O
. -X- _ O

The -X- _ O
power -X- _ O
of -X- _ O
our -X- _ O
bidirectional -X- _ O
decoder -X- _ O
stems -X- _ O
from -X- _ O
two -X- _ O
advantages -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
initialize -X- _ O
our -X- _ O
model -X- _ O
with -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
BERT -X- _ O
-BASE -X- _ O
UNCASED -X- _ O
model -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
the -X- _ O
decoding -X- _ O
process -X- _ O
is -X- _ O
bidirectional -X- _ O
. -X- _ O

It -X- _ O
would -X- _ O
be -X- _ O
possible -X- _ O
to -X- _ O
transfer -X- _ O
the -X- _ O
ﬁrst -X- _ O
advantage -X- _ O
to -X- _ O
an -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
framework -X- _ O
by -X- _ O
using -X- _ O
BERT -X- _ B-MethodName
embeddings -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
however -X- _ O
only -X- _ O
possible -X- _ O
for -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
, -X- _ O
because -X- _ O
the -X- _ O
bidirectionality -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
requires -X- _ O
the -X- _ O
entire -X- _ O
sequence -X- _ O
to -X- _ O
be -X- _ O
available -X- _ O
beforehand -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
we -X- _ O
modify -X- _ O
implementation -X- _ O
of -X- _ O
OpenNMT -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
as -X- _ O
the -X- _ O
encoder -X- _ O
. -X- _ O

The -X- _ O
weights -X- _ O
are -X- _ O
frozenModel -X- _ O
Micro -X- _ O
Acc -X- _ O
. -X- _ O

Macro -X- _ O
Table -X- _ O
1 -X- _ O
: -X- _ O
Results -X- _ O
on -X- _ O
the -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
test -X- _ O
set -X- _ O
, -X- _ O
averaged -X- _ O
over -X- _ O
3 -X- _ O
independent -X- _ O
runs -X- _ O
for -X- _ O
GPT2 -X- _ B-MethodName
and -X- _ O
B -X- _ B-MethodName
ISON -X- _ I-MethodName
, -X- _ O
reporting -X- _ O
micro -X- _ B-MetricName
accuracy -X- _ I-MetricName
and -X- _ O
macro -X- _ B-MetricName
accuracy -X- _ I-MetricName
in -X- _ O
terms -X- _ O
of -X- _ O
the -X- _ O
classiﬁcation -X- _ O
task -X- _ O
and -X- _ O
BLEU-1 -X- _ B-MetricName
and -X- _ O
BLEU-4 -X- _ B-MetricName
on -X- _ O
instances -X- _ O
for -X- _ O
which -X- _ O
a -X- _ O
clariﬁcation -X- _ O
question -X- _ O
was -X- _ O
generated -X- _ O
. -X- _ O

E -X- _ O
& -X- _ O
D -X- _ O
uses -X- _ O
no -X- _ O
language -X- _ O
model -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O

Model -X- _ O
Micro -X- _ O
Acc -X- _ O
. -X- _ O

Macro -X- _ O
Table -X- _ O
2 -X- _ O
: -X- _ O
Results -X- _ O
on -X- _ O
the -X- _ O
ofﬁcial -X- _ O
hidden -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
test -X- _ O
set -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
model -X- _ O
on -X- _ O
the -X- _ O
leaderboard -X- _ O
, -X- _ O
E3 -X- _ B-MethodName
( -X- _ O
Zhong -X- _ O
and -X- _ O
Zettlemoyer -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

when -X- _ O
training -X- _ O
the -X- _ O
decoder -X- _ O
, -X- _ O
which -X- _ O
produced -X- _ O
better -X- _ O
results -X- _ O
than -X- _ O
allowing -X- _ O
the -X- _ O
gradients -X- _ O
to -X- _ O
also -X- _ O
ﬂow -X- _ O
through -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
. -X- _ O

Again -X- _ O
, -X- _ O
with -X- _ O
both -X- _ O
an -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
, -X- _ O
this -X- _ O
leads -X- _ O
to -X- _ O
a -X- _ O
total -X- _ O
of -X- _ O
about -X- _ O
270 -X- _ O
M -X- _ O
parameters -X- _ O
. -X- _ O

GPT2 -X- _ B-MethodName
. -X- _ O

Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
present -X- _ O
a -X- _ O
transformer -X- _ B-MethodName
decoder -X- _ O
, -X- _ O
GPT2 -X- _ B-MethodName
, -X- _ O
trained -X- _ O
as -X- _ O
a -X- _ O
language -X- _ O
model -X- _ O
on -X- _ O
large -X- _ O
amounts -X- _ O
of -X- _ O
monolingual -X- _ O
text -X- _ O
. -X- _ O

Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
showed -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
possible -X- _ O
to -X- _ O
perform -X- _ O
various -X- _ O
tasks -X- _ O
in -X- _ O
a -X- _ O
zero -X- _ O
- -X- _ O
shot -X- _ O
setting -X- _ O
by -X- _ O
priming -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
with -X- _ O
an -X- _ O
input -X- _ O
and -X- _ O
letting -X- _ O
it -X- _ O
generate -X- _ O
further -X- _ O
words -X- _ O
greedily -X- _ O
. -X- _ O

This -X- _ O
setup -X- _ O
can -X- _ O
be -X- _ O
transferred -X- _ O
to -X- _ O
a -X- _ O
supervised -X- _ O
setting -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
ﬁne -X- _ O
- -X- _ O
tuned -X- _ O
to -X- _ O
a -X- _ O
dataset -X- _ O
by -X- _ O
using -X- _ O
maximum -X- _ O
likelihood -X- _ O
estimation -X- _ O
to -X- _ O
increase -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
gold -X- _ O
output -X- _ O
sequence -X- _ O
( -X- _ O
Wolf -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
starting -X- _ O
point -X- _ O
for -X- _ O
the -X- _ O
supervised -X- _ O
learning -X- _ O
, -X- _ O
we -X- _ O
initialize -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
model -X- _ O
GPT-2 -X- _ O
- -X- _ O
117 -X- _ O
M -X- _ O
released -X- _ O
by -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2019 -X- _ O
) -X- _ O
2 -X- _ O
and -X- _ O
then -X- _ O
ﬁne -X- _ O
- -X- _ O
tune -X- _ O
. -X- _ O

With -X- _ O
117 -X- _ O
M -X- _ O
parameters -X- _ O
, -X- _ O
this -X- _ O
model -X- _ O
is -X- _ O
comparable -X- _ O
to -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

Unlike -X- _ O
baseline -X- _ O
2 -X- _ O
, -X- _ O
this -X- _ O
setup -X- _ O
can -X- _ O
directly -X- _ O
employ -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
model -X- _ O
as -X- _ O
our -X- _ O
approach -X- _ O
can -X- _ O
, -X- _ O
but -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
bidirectional -X- _ O
. -X- _ O

2https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
openai -X- _ O
/ -X- _ O
gpt-2 -X- _ O

64.4 -X- _ O
Results -X- _ O
We -X- _ O
report -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
our -X- _ O
approach -X- _ O
, -X- _ O
the -X- _ O
various -X- _ O
baselines -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
previous -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
( -X- _ O
SOTA -X- _ O
) -X- _ O
scores -X- _ O
where -X- _ O
applicable -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
and -X- _ O
2 -X- _ O
for -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
and -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
for -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
. -X- _ O

On -X- _ O
the -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
dataset -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
very -X- _ O
poor -X- _ O
BLEU-4 -X- _ B-MetricName
performance -X- _ O
for -X- _ O
the -X- _ O
encoder -X- _ B-MethodName
- -X- _ I-MethodName
decoder -X- _ I-MethodName
Transformer -X- _ I-MethodName
( -X- _ O
E -X- _ B-MethodName
& -X- _ I-MethodName
D -X- _ I-MethodName
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
results -X- _ O
from -X- _ O
Saeidi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
who -X- _ O
could -X- _ O
not -X- _ O
get -X- _ O
a -X- _ O
LSTM -X- _ O
- -X- _ O
based -X- _ O
network -X- _ O
to -X- _ O
work -X- _ O
without -X- _ O
an -X- _ O
additional -X- _ O
classiﬁcation -X- _ O
head -X- _ O
. -X- _ O

Adding -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
E -X- _ B-MethodName
& -X- _ I-MethodName
D+B -X- _ I-MethodName
) -X- _ O
slightly -X- _ O
improves -X- _ O
performance -X- _ O
. -X- _ O

By -X- _ O
directly -X- _ O
leveraging -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
model -X- _ O
, -X- _ O
GPT2 -X- _ O
outperforms -X- _ O
the -X- _ O
previous -X- _ O
models -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
, -X- _ O
reaching -X- _ O
33.9 -X- _ B-MetricValue
% -X- _ I-MetricValue
on -X- _ O
BLEU-4 -X- _ B-MetricName
and -X- _ O
a -X- _ O
micro -X- _ B-MetricName
accuracy -X- _ I-MetricName
of -X- _ O
60.4 -X- _ B-MetricValue
% -X- _ I-MetricValue
. -X- _ O

B -X- _ B-MethodName
ISONis -X- _ I-MethodName
able -X- _ O
to -X- _ O
take -X- _ O
future -X- _ O
tokens -X- _ O
into -X- _ O
consideration -X- _ O
and -X- _ O
outperforms -X- _ O
GPT2 -X- _ B-MethodName
by -X- _ O
12.3 -X- _ B-MetricValue
percentage -X- _ I-MetricValue
points -X- _ I-MetricValue
in -X- _ O
BLEU-4 -X- _ B-MetricName
and -X- _ O
by -X- _ O
4.5 -X- _ B-MetricValue
points -X- _ I-MetricValue
in -X- _ O
micro -X- _ B-MetricName
accuracy -X- _ I-MetricName
. -X- _ O

We -X- _ O
submitted -X- _ O
the -X- _ O
best -X- _ O
B -X- _ B-MethodName
ISONmodel -X- _ I-MethodName
out -X- _ O
of -X- _ O
the -X- _ O
random -X- _ O
three -X- _ O
of -X- _ O
Table -X- _ O
1 -X- _ O
to -X- _ O
be -X- _ O
evaluated -X- _ O
on -X- _ O
the -X- _ O
hidden -X- _ O
test -X- _ O
set -X- _ O
and -X- _ O
report -X- _ O
results -X- _ O
in -X- _ O
comparison -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
model -X- _ O
on -X- _ O
the -X- _ O
leaderboard,3E3 -X- _ O
( -X- _ O
Zhong -X- _ O
and -X- _ O
Zettlemoyer -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
. -X- _ O

B -X- _ B-MethodName
ISONoutperforms -X- _ I-MethodName
E3 -X- _ B-MethodName
by -X- _ O
5.6 -X- _ B-MetricValue
BLEU-4 -X- _ B-MetricName
points -X- _ O
, -X- _ O
while -X- _ O
it -X- _ O
is -X- _ O
only -X- _ O
slightly -X- _ O
worse -X- _ O
than -X- _ O
E3 -X- _ B-MethodName
in -X- _ O
terms -X- _ O
of -X- _ O
accuracy -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
dataset -X- _ O
the -X- _ O
information -X- _ O
retrieval -X- _ O
- -X- _ O
based -X- _ O
method -X- _ O
( -X- _ O
IR -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
) -X- _ O
introduced -X- _ O
by -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
is -X- _ O
very -X- _ O
strong -X- _ O
and -X- _ O
outperforms -X- _ O
the -X- _ O
best -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
model -X- _ O
( -X- _ O
E2E -X- _ B-MethodName
) -X- _ O
( -X- _ O
Luo -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
by -X- _ O
over -X- _ O
16 -X- _ B-MetricValue
percentage -X- _ I-MetricValue
points -X- _ O
in -X- _ O
BLEU-4 -X- _ B-MetricName
. -X- _ O

The -X- _ O
best -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
model -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
LSTMs -X- _ O
and -X- _ O
Luo -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
report -X- _ O
performance -X- _ O
increases -X- _ O
when -X- _ O
adding -X- _ O
an -X- _ O
attention -X- _ O
module -X- _ O
to -X- _ O
their -X- _ O
setup -X- _ O
. -X- _ O

The -X- _ O
encoder -X- _ B-MethodName
- -X- _ I-MethodName
decoder -X- _ I-MethodName
transformer -X- _ I-MethodName
( -X- _ O
E -X- _ B-MethodName
& -X- _ I-MethodName
D -X- _ I-MethodName
) -X- _ O
outperforms -X- _ O
this -X- _ O
setup -X- _ O
by -X- _ O
over -X- _ O
2 -X- _ B-MetricValue
percentage -X- _ I-MetricValue
points -X- _ I-MetricValue
in -X- _ O
BLEU4 -X- _ B-MetricName
and -X- _ O
we -X- _ O
conjecture -X- _ O
that -X- _ O
this -X- _ O
is -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
transformer -X- _ B-MethodName
making -X- _ O
more -X- _ O
effective -X- _ O
use -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ O
principle -X- _ O
. -X- _ O

Adding -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
E -X- _ B-MethodName
& -X- _ I-MethodName
D+B -X- _ I-MethodName
) -X- _ O
does -X- _ O
not -X- _ O
help -X- _ O
much -X- _ O
for -X- _ O
this -X- _ O
dataset -X- _ O
. -X- _ O

But -X- _ O
again -X- _ O
we -X- _ O
observe -X- _ O
a -X- _ O
large -X- _ O
increase -X- _ O
of -X- _ O
performance -X- _ O
when -X- _ O
directly -X- _ O
employing -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
models -X- _ O
. -X- _ O

GPT2 -X- _ B-MethodName
performs -X- _ O
on -X- _ O
par -X- _ O
with -X- _ O
the -X- _ O
IR -X- _ O
SOTA -X- _ O
, -X- _ O
achieving -X- _ O
a -X- _ O
BLEU-4 -X- _ B-MetricName
score -X- _ O
of -X- _ O
19.4 -X- _ B-MetricValue
% -X- _ I-MetricValue
. -X- _ O

Again -X- _ O
, -X- _ O
B -X- _ B-MethodName
ISONcan -X- _ I-MethodName
outperform -X- _ O
GPT2 -X- _ B-MethodName
, -X- _ O
here -X- _ O
with -X- _ O
a -X- _ O
difference -X- _ O
of -X- _ O
6.2 -X- _ B-MetricValue
points -X- _ I-MetricValue
in -X- _ O
BLEU-4 -X- _ B-MetricName
and -X- _ O
even -X- _ O
larger -X- _ O
increases -X- _ O
in -X- _ O
the -X- _ O
other -X- _ O
scores -X- _ O
. -X- _ O

Effect -X- _ O
of -X- _ O
bidirectionality -X- _ O
. -X- _ O

To -X- _ O
investigate -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
beneﬁts -X- _ O
from -X- _ O
bidirectionality -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
a -X- _ O
setup -X- _ O
where -X- _ O
B -X- _ B-MethodName
ISONisn’t -X- _ I-MethodName
allowed -X- _ O
to -X- _ O
attend -X- _ O
3https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
sharc-data.github.io -X- _ O
/ -X- _ O
leaderboard.html -X- _ O
, -X- _ O
19 -X- _ O
August -X- _ O
2019Model -X- _ O
B-1 -X- _ O
B-2 -X- _ O
B-3 -X- _ O
B-4DAILY -X- _ O
DIALOGIR -X- _ O
- -X- _ O
25.8 -X- _ O
20.4 -X- _ O
19.4 -X- _ O
Table -X- _ O
3 -X- _ O
: -X- _ O
BLEU -X- _ B-MetricName
n -X- _ B-MetricName
- -X- _ I-MetricName
gram -X- _ I-MetricName
scores -X- _ O
for -X- _ O
n= -X- _ O
1 -X- _ B-MetricValue
; -X- _ I-MetricValue
2 -X- _ I-MetricValue
; -X- _ I-MetricValue
3 -X- _ I-MetricValue
; -X- _ I-MetricValue
4on -X- _ I-MetricValue
the -X- _ O
DailyDialog -X- _ B-DatasetName
test -X- _ O
set -X- _ O
, -X- _ O
averaged -X- _ O
over -X- _ O
3 -X- _ O
independent -X- _ O
runs -X- _ O
for -X- _ O
GPT2 -X- _ B-MethodName
and -X- _ O
B -X- _ B-MethodName
ISON -X- _ I-MethodName
. -X- _ O

Models -X- _ O
before -X- _ O
the -X- _ O
line -X- _ O
do -X- _ O
not -X- _ O
make -X- _ O
use -X- _ O
of -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
model -X- _ O
. -X- _ O

IR -X- _ O
( -X- _ O
SOTA -X- _ O
) -X- _ O
are -X- _ O
, -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
the -X- _ O
best -X- _ O
previously -X- _ O
published -X- _ O
scores -X- _ O
for -X- _ O
information -X- _ O
retrieval -X- _ O
and -X- _ O
end -X- _ O
- -X- _ O
toend -X- _ O
approaches -X- _ O
. -X- _ O

Model -X- _ O
Micro -X- _ O
Acc -X- _ O
. -X- _ O

Macro -X- _ O
Table -X- _ O
4 -X- _ O
: -X- _ O
Comparison -X- _ O
of -X- _ O
B -X- _ B-MethodName
ISONto -X- _ I-MethodName
a -X- _ O
setup -X- _ O
where -X- _ O
B -X- _ B-MethodName
ISONisn’t -X- _ I-MethodName
allowed -X- _ O
to -X- _ O
attend -X- _ O
to -X- _ O
future -X- _ O
tokens -X- _ O
, -X- _ O
i.e. -X- _ O
past -X- _ O
only -X- _ O
, -X- _ O
for -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
and -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
( -X- _ O
DD -X- _ B-DatasetName
) -X- _ O
. -X- _ O

to -X- _ O
future -X- _ O
tokens -X- _ O
during -X- _ O
prediction -X- _ O
( -X- _ O
see -X- _ O
Table -X- _ O
4 -X- _ O
) -X- _ O
. -X- _ O

It -X- _ O
causes -X- _ O
a -X- _ O
drop -X- _ O
in -X- _ O
BLEU-4 -X- _ B-MetricName
performance -X- _ O
of -X- _ O
about -X- _ O
25 -X- _ B-MetricValue
points -X- _ I-MetricValue
on -X- _ O
the -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
dataset -X- _ O
and -X- _ O
a -X- _ O
drop -X- _ O
of -X- _ O
10 -X- _ B-MetricValue
points -X- _ I-MetricValue
on -X- _ O
the -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
dataset -X- _ O
. -X- _ O

This -X- _ O
showcases -X- _ O
that -X- _ O
B -X- _ B-MethodName
ISONduring -X- _ I-MethodName
training -X- _ O
has -X- _ O
learnt -X- _ O
to -X- _ O
rely -X- _ O
on -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
attend -X- _ O
to -X- _ O
future -X- _ O
tokens -X- _ O
. -X- _ O

Effect -X- _ O
of -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
are -X- _ O
curious -X- _ O
how -X- _ O
big -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
model -X- _ O
is -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
instead -X- _ O
of -X- _ O
starting -X- _ O
with -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
-BASE -X- _ O
UNCASED -X- _ O
weights -X- _ O
, -X- _ O
we -X- _ O
initialize -X- _ O
B -X- _ B-MethodName
ISONwith -X- _ I-MethodName
random -X- _ O
weights -X- _ O
drawn -X- _ O
from -X- _ O
a -X- _ O
normal -X- _ O
distribution -X- _ O
with -X- _ O
mean -X- _ B-HyperparameterName
0.0 -X- _ B-HyperparameterValue
and -X- _ O
standard -X- _ B-HyperparameterName
deviation -X- _ I-HyperparameterName
of -X- _ O
0.02 -X- _ B-HyperparameterValue
. -X- _ O

Results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
5 -X- _ O
for -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
and -X- _ O
DAILY -X- _ B-DatasetName
DIALOG -X- _ I-DatasetName
. -X- _ O

Even -X- _ O
without -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
model -X- _ O
, -X- _ O
our -X- _ O
approach -X- _ O
can -X- _ O
outperform -X- _ O
the -X- _ O
standard -X- _ O
encoder -X- _ B-MethodName
- -X- _ I-MethodName
decoder -X- _ I-MethodName
transformer -X- _ I-MethodName
framework -X- _ O
( -X- _ O
E -X- _ B-MethodName
& -X- _ I-MethodName
D -X- _ I-MethodName
) -X- _ O
on -X- _ O
both -X- _ O
datasets -X- _ O
, -X- _ O
although -X- _ O
we -X- _ O
had -X- _ O
to -X- _ O
increase -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
epochs -X- _ I-HyperparameterName
for -X- _ O
the -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
dataset -X- _ O
to -X- _ O
40 -X- _ B-HyperparameterValue
. -X- _ O

On -X- _ O
the -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
task -X- _ O
, -X- _ O
we -X- _ O
are -X- _ O
even -X- _ O
able -X- _ O
to -X- _ O
outperform -X- _ O
GPT2 -X- _ B-MethodName
. -X- _ O

This -X- _ O
demonstrates -X- _ O

Acc -X- _ O
. -X- _ O

Macro -X- _ O
Table -X- _ O
5 -X- _ O
: -X- _ O
Best -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
models -X- _ O
that -X- _ O
do -X- _ O
not -X- _ O
use -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
model -X- _ O
in -X- _ O
comparison -X- _ O
with -X- _ O
B -X- _ B-MethodName
ISON -X- _ I-MethodName
that -X- _ O
uses -X- _ O
randomly -X- _ O
initialized -X- _ O
weights -X- _ O
for -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
and -X- _ O
DAILY -X- _ B-DatasetName
DIALOG -X- _ I-DatasetName
( -X- _ O
DD -X- _ B-DatasetName
) -X- _ O
, -X- _ O
averaged -X- _ O
over -X- _ O
3 -X- _ O
runs -X- _ O
. -X- _ O

Strategy -X- _ O
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
one -X- _ O
step -X- _ O
greedy -X- _ O
22.9 -X- _ O
9.3 -X- _ O
lowest -X- _ O
entropy -X- _ O
40.3 -X- _ O
16.8 -X- _ O
highest -X- _ O
probability -X- _ O
50.9 -X- _ O
16.4 -X- _ O
Table -X- _ O
6 -X- _ O
: -X- _ O
BLEU-4 -X- _ B-MetricName
using -X- _ O
various -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
strategies -X- _ O
for -X- _ O
B -X- _ B-MethodName
ISONon -X- _ I-MethodName
S -X- _ B-DatasetName
HARC -X- _ I-DatasetName
and -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
. -X- _ O

the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
approach -X- _ O
in -X- _ O
itself -X- _ O
, -X- _ O
free -X- _ O
of -X- _ O
any -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
model -X- _ O
. -X- _ O

Effect -X- _ O
of -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
strategies -X- _ O
. -X- _ O

We -X- _ O
present -X- _ O
the -X- _ O
different -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
strategies -X- _ O
in -X- _ O
Table -X- _ O
6 -X- _ O
. -X- _ O

The -X- _ O
best -X- _ O
overall -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
strategy -X- _ O
is -X- _ O
to -X- _ O
predict -X- _ O
from -X- _ O
left -X- _ O
to -X- _ O
right -X- _ O
which -X- _ O
achieves -X- _ O
good -X- _ O
results -X- _ O
on -X- _ O
both -X- _ O
datasets -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
SHARC -X- _ B-DatasetName
dataset -X- _ O
the -X- _ O
highest -X- _ O
probability -X- _ O
approach -X- _ O
performs -X- _ O
better -X- _ O
than -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
on -X- _ O
DAILY -X- _ B-DatasetName
DIALOG -X- _ I-DatasetName
this -X- _ O
approach -X- _ O
is -X- _ O
not -X- _ O
as -X- _ O
successful -X- _ O
. -X- _ O

This -X- _ O
suggests -X- _ O
that -X- _ O
it -X- _ O
might -X- _ O
be -X- _ O
worth -X- _ O
selecting -X- _ O
the -X- _ O
best -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
strategy -X- _ O
for -X- _ O
each -X- _ O
dataset -X- _ O
individually -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
we -X- _ O
hypothesize -X- _ O
that -X- _ O
leftto -X- _ O
- -X- _ O
right -X- _ O
works -X- _ O
consistently -X- _ O
well -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
left -X- _ O
- -X- _ O
toright -X- _ O
nature -X- _ O
of -X- _ O
the -X- _ O
English -X- _ O
language -X- _ O
. -X- _ O

A -X- _ O
brief -X- _ O
experiment -X- _ O
with -X- _ O
a -X- _ O
right -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
left -X- _ O
strategy -X- _ O
gave -X- _ O
poor -X- _ O
results -X- _ O
. -X- _ O

5 -X- _ O
Analysis -X- _ O
We -X- _ O
believe -X- _ O
that -X- _ O
the -X- _ O
placeholders -X- _ O
capture -X- _ O
sequential -X- _ O
information -X- _ O
present -X- _ O
in -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
learned -X- _ O
during -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O

After -X- _ O
running -X- _ O
a -X- _ O
transformer -X- _ B-MethodName
encoder -X- _ O
where -X- _ O
each -X- _ O
position -X- _ O
can -X- _ O
attend -X- _ O
to -X- _ O
every -X- _ O
other -X- _ O
position -X- _ O
, -X- _ O
a -X- _ O
placeholder -X- _ O
token -X- _ O
will -X- _ O
have -X- _ O
a -X- _ O
probability -X- _ O
distribution -X- _ O
over -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
and -X- _ O
this -X- _ O
distribution -X- _ O
is -X- _ O
informed -X- _ O
by -X- _ O
all -X- _ O
other -X- _ O
tokens -X- _ O
in -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
a -X- _ O
place -X- _ O
- -X- _ O
Dataset -X- _ O

1 -X- _ O

2 -X- _ O

3 -X- _ O
Table -X- _ O
7 -X- _ O
: -X- _ O
Average -X- _ O
attention -X- _ O
weights -X- _ O
and -X- _ O
standard -X- _ O
deviation -X- _ O
when -X- _ O
predicting -X- _ O
from -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
on -X- _ O
both -X- _ O
SHARC -X- _ B-DatasetName
and -X- _ O
D -X- _ B-DatasetName
AILY -X- _ I-DatasetName
DIALOG -X- _ I-DatasetName
( -X- _ O
DD -X- _ B-DatasetName
) -X- _ O
for -X- _ O
different -X- _ O
parts -X- _ O
of -X- _ O
the -X- _ O
sequence -X- _ O
, -X- _ O
where -X- _ O

1is -X- _ O
for -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
x -X- _ O
, -X- _ O

2= -X- _ O

2is -X- _ O
for -X- _ O
the -X- _ O
already -X- _ O
produced -X- _ O
sequence -X- _ O
yand -X- _ O

3= -X- _ O

3is -X- _ O
for -X- _ O
the -X- _ O
sequence -X- _ O
of -X- _ O
remaining -X- _ O
placeholder -X- _ O
tokensp -X- _ O
. -X- _ O

kare -X- _ O
the -X- _ O
normalized -X- _ O
attention -X- _ O
weights -X- _ O
across -X- _ O
all -X- _ O
three -X- _ O
parts -X- _ O
, -X- _ O
whereas -X- _ O
 -X- _ O

knormalizes -X- _ O
over -X- _ O
the -X- _ O
second -X- _ O
and -X- _ O
third -X- _ O
part -X- _ O
. -X- _ O

holder -X- _ O
could -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
a -X- _ O
mixture -X- _ O
of -X- _ O
tokens -X- _ O
with -X- _ O
varying -X- _ O
probabilities -X- _ O
. -X- _ O

As -X- _ O
placeholders -X- _ O
are -X- _ O
subsequently -X- _ O
uncovered -X- _ O
, -X- _ O
the -X- _ O
other -X- _ O
placeholders -X- _ O
can -X- _ O
update -X- _ O
their -X- _ O
distribution -X- _ O
by -X- _ O
taking -X- _ O
the -X- _ O
newly -X- _ O
revealed -X- _ O
token -X- _ O
into -X- _ O
consideration -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
, -X- _ O
for -X- _ O
the -X- _ O
sentence“is -X- _ O
the -X- _ O
animal -X- _ O
an -X- _ O
endangered -X- _ O
animal -X- _ O
? -X- _ O
” -X- _ O
, -X- _ O
while -X- _ O
generating -X- _ O
“ -X- _ O
endangered -X- _ O
” -X- _ O
, -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
head -X- _ O
pays -X- _ O
attention -X- _ O
to -X- _ O
the -X- _ O
next -X- _ O
placeholder -X- _ O
token -X- _ O
, -X- _ O
which -X- _ O
in -X- _ O
the -X- _ O
next -X- _ O
step -X- _ O
is -X- _ O
revealed -X- _ O
to -X- _ O
be -X- _ O
“ -X- _ O
animal -X- _ O
” -X- _ O
. -X- _ O

While -X- _ O
producing -X- _ O
“ -X- _ O
endangered -X- _ O
” -X- _ O
, -X- _ O
the -X- _ O
distribution -X- _ O
for -X- _ O
the -X- _ O
next -X- _ O
position -X- _ O
already -X- _ O
placed -X- _ O
a -X- _ O
high -X- _ O
probability -X- _ O
on -X- _ O
“ -X- _ O
animal -X- _ O
” -X- _ O
, -X- _ O
thus -X- _ O
the -X- _ O
current -X- _ O
token -X- _ O
can -X- _ O
take -X- _ O
this -X- _ O
into -X- _ O
consideration -X- _ O
and -X- _ O
produces -X- _ O
“ -X- _ O
endangered -X- _ O
” -X- _ O
. -X- _ O

Further -X- _ O
heat -X- _ O
maps -X- _ O
demonstrating -X- _ O
this -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
the -X- _ O
appendix -X- _ O
. -X- _ O

The -X- _ O
quantify -X- _ O
this -X- _ O
intuition -X- _ O
, -X- _ O
we -X- _ O
measure -X- _ O
the -X- _ O
average -X- _ O
attention -X- _ O
score -X- _ O
on -X- _ O
various -X- _ O
parts -X- _ O
of -X- _ O
the -X- _ O
sequence -X- _ O
. -X- _ O

For -X- _ O
this -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
prediction -X- _ O
strategy -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
at -X- _ O
time -X- _ O
t -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
decompose -X- _ O
our -X- _ O
sequence -X- _ O
into -X- _ O
three -X- _ O
parts -X- _ O
: -X- _ O
s -X- _ O
= -X- _ O
xyp -X- _ O
, -X- _ O
where -X- _ O
xis -X- _ O
the -X- _ O
input -X- _ O
, -X- _ O
ythe -X- _ O
already -X- _ O
produced -X- _ O
sequence -X- _ O
andpthe -X- _ O
remaining -X- _ O
sequence -X- _ O
of -X- _ O
placeholder -X- _ O
tokens -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
attention -X- _ O
head -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
decompose -X- _ O
the -X- _ O
attention -X- _ O
probabilities -X- _ O
into -X- _ O
the -X- _ O
three -X- _ O
parts -X- _ O
, -X- _ O
1 -X- _ O
. -X- _ O
attention -X- _ O
on -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
, -X- _ O
2 -X- _ O
. -X- _ O
attention -X- _ O
on -X- _ O
the -X- _ O
current -X- _ O
word -X- _ O
and -X- _ O
already -X- _ O
generated -X- _ O
words -X- _ O
( -X- _ O
left -X- _ O
of -X- _ O
the -X- _ O
current -X- _ O
word -X- _ O
) -X- _ O
, -X- _ O
3 -X- _ O
. -X- _ O
attention -X- _ O
on -X- _ O
words -X- _ O
that -X- _ O
are -X- _ O
yet -X- _ O
to -X- _ O
be -X- _ O
generated -X- _ O
( -X- _ O
right -X- _ O
of -X- _ O
the -X- _ O
current -X- _ O
word -X- _ O
) -X- _ O
. -X- _ O

Figure -X- _ O
2 -X- _ O
: -X- _ O
Heat -X- _ O
map -X- _ O
( -X- _ O
darker -X- _ O
hues -X- _ O
indicate -X- _ O
higher -X- _ O
attention -X- _ O
) -X- _ O
that -X- _ O
shows -X- _ O
an -X- _ O
example -X- _ O
of -X- _ O
where -X- _ O
an -X- _ O
attention -X- _ O
head -X- _ O
looks -X- _ O
into -X- _ O
the -X- _ O
future -X- _ O
while -X- _ O
generating -X- _ O
from -X- _ O
left -X- _ O
to -X- _ O
right -X- _ O
. -X- _ O

Each -X- _ O
row -X- _ O
shows -X- _ O
the -X- _ O
attention -X- _ O
over -X- _ O
the -X- _ O
output -X- _ O
sequence -X- _ O
for -X- _ O
this -X- _ O
row -X- _ O
’s -X- _ O
placeholder -X- _ O
token -X- _ O
at -X- _ O
that -X- _ O
point -X- _ O
in -X- _ O
time -X- _ O
. -X- _ O

Word -X- _ O
in -X- _ O
previous -X- _ O
rows -X- _ O
have -X- _ O
been -X- _ O
produced -X- _ O
already -X- _ O
, -X- _ O
whereas -X- _ O
words -X- _ O
of -X- _ O
later -X- _ O
rows -X- _ O
still -X- _ O
hold -X- _ O
placeholder -X- _ O
tokens -X- _ O
. -X- _ O

Thus -X- _ O
the -X- _ O
upper -X- _ O
triangle -X- _ O
of -X- _ O
the -X- _ O
matrix -X- _ O
shows -X- _ O
the -X- _ O
attention -X- _ O
that -X- _ O
is -X- _ O
paid -X- _ O
to -X- _ O
future -X- _ O
tokens -X- _ O
. -X- _ O

The -X- _ O
red -X- _ O
square -X- _ O
shows -X- _ O
that -X- _ O
while -X- _ O
generating -X- _ O
the -X- _ O
token -X- _ O
“ -X- _ O
endangered -X- _ O
” -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
head -X- _ O
already -X- _ O
takes -X- _ O
the -X- _ O
next -X- _ O
placeholder -X- _ O
into -X- _ O
account -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
revealed -X- _ O
to -X- _ O
be -X- _ O
“ -X- _ O
animal -X- _ O
” -X- _ O
in -X- _ O
the -X- _ O
next -X- _ O
step -X- _ O
. -X- _ O

Best -X- _ O
viewed -X- _ O
in -X- _ O
color -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
mathematically -X- _ O
expressed -X- _ O
as -X- _ O
at -X- _ O
= -X- _ O
a0 -X- _ O
: -X- _ O
jxjajxj+1 -X- _ O
: -X- _ O
jxj+tajxj+t+1 -X- _ O
; -X- _ O
jsj -X- _ O
; -X- _ O
wherejsjis -X- _ O
the -X- _ O
maximum -X- _ O
possible -X- _ O
sequence -X- _ O
length -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
part -X- _ O
we -X- _ O
can -X- _ O
calculate -X- _ O
an -X- _ O
average -X- _ O
leading -X- _ O
to -X- _ O
three -X- _ O
values -X- _ O
, -X- _ O
a1 -X- _ O
t -X- _ O
; -X- _ O
a2 -X- _ O
tanda3 -X- _ O
t. -X- _ O
Averaged -X- _ O
over -X- _ O
all -X- _ O
Tgeneration -X- _ O
time -X- _ O
steps -X- _ O
and -X- _ O
allDdata -X- _ O
points -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
derive -X- _ O
a -X- _ O
score -X- _ O
for -X- _ O
each -X- _ O
partk -X- _ O
; -X- _ O
k= -X- _ O
1 -X- _ O
; -X- _ O
2 -X- _ O
; -X- _ O
3and -X- _ O
each -X- _ O
attention -X- _ O
head -X- _ O
h -X- _ O
: -X- _ O


k -X- _ O
D1 -X- _ O
TDX -X- _ O
d=1TX -X- _ O
t=1ak -X- _ O
d -X- _ O
; -X- _ O
t -X- _ O

Note -X- _ O
that -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
attention -X- _ O
heads -X- _ O
in -X- _ O
the -X- _ O
ﬁnal -X- _ O
BERT -X- _ B-MethodName
self -X- _ O
- -X- _ O
attention -X- _ O
layer -X- _ O
. -X- _ O

Averaging -X- _ O
over -X- _ O
all -X- _ O
H -X- _ O
attention -X- _ O
heads -X- _ O
, -X- _ O

k=1 -X- _ O
HPH -X- _ O
h -X- _ O
, -X- _ O
leads -X- _ O
to -X- _ O
the -X- _ O
results -X- _ O
reported -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
for -X- _ O
both -X- _ O
datasets -X- _ O
. -X- _ O

Unsurprisingly -X- _ O
, -X- _ O
we -X- _ O
ﬁnd -X- _ O
that -X- _ O
with -X- _ O
scores -X- _ O
of -X- _ O
over -X- _ O
90 -X- _ O
% -X- _ O
for -X- _ O
both -X- _ O
datasets -X- _ O
the -X- _ O
majority -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ O
is -X- _ O
focused -X- _ O
on -X- _ O
the -X- _ O
ﬁrst -X- _ O
part -X- _ O
, -X- _ O
i.e. -X- _ O
the -X- _ O
conditioning -X- _ O
input -X- _ O
x -X- _ O
( -X- _ O
see -X- _ O

1 -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
remaining -X- _ O
attention -X- _ O
is -X- _ O
split -X- _ O
between -X- _ O
the -X- _ O
already -X- _ O
produced -X- _ O
sequence -X- _ O
( -X- _ O

2 -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
future -X- _ O
tokens -X- _ O
( -X- _ O

3 -X- _ O
) -X- _ O
.To -X- _ O
directly -X- _ O
compare -X- _ O
the -X- _ O
relationship -X- _ O
within -X- _ O
the -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
, -X- _ O
we -X- _ O
re -X- _ O
- -X- _ O
normalize -X- _ O
over -X- _ O

2and -X- _ O

3 -X- _ O
, -X- _ O
leading -X- _ O
to -X- _ O
new -X- _ O
values -X- _ O
 -X- _ O

2and -X- _ O

3 -X- _ O
( -X- _ O
see -X- _ O
Table -X- _ O
7 -X- _ O
) -X- _ O
. -X- _ O

Here -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
the -X- _ O
past -X- _ O
, -X- _ O
already -X- _ O
produced -X- _ O
tokens -X- _ O
are -X- _ O
about -X- _ O
twice -X- _ O
as -X- _ O
important -X- _ O
as -X- _ O
the -X- _ O
future -X- _ O
, -X- _ O
not -X- _ O
- -X- _ O
yet -X- _ O
- -X- _ O
produced -X- _ O
tokens -X- _ O
. -X- _ O

But -X- _ O
with -X- _ O
scores -X- _ O
of -X- _ O
just -X- _ O
under -X- _ O
30 -X- _ O
% -X- _ O
on -X- _ O
both -X- _ O
datasets -X- _ O
, -X- _ O
we -X- _ O
see -X- _ O
that -X- _ O
a -X- _ O
substantial -X- _ O
amount -X- _ O
of -X- _ O
attention -X- _ O
is -X- _ O
also -X- _ O
focused -X- _ O
on -X- _ O
the -X- _ O
future -X- _ O
, -X- _ O
not -X- _ O
- -X- _ O
yet -X- _ O
- -X- _ O
produced -X- _ O
tokens -X- _ O
. -X- _ O

Interestingly -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
standard -X- _ O
deviation -X- _ O
of -X- _ O
about -X- _ O
14 -X- _ O
% -X- _ O
, -X- _ O
the -X- _ O
values -X- _ O
of -X- _ O
 -X- _ O

2and -X- _ O

3vary -X- _ O
strongly -X- _ O
across -X- _ O
the -X- _ O
different -X- _ O
attention -X- _ O
heads -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
on -X- _ O
the -X- _ O
SHARC -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
we -X- _ O
ﬁnd -X- _ O
one -X- _ O
attention -X- _ O
head -X- _ O
where -X- _ O
only -X- _ O
about -X- _ O
9 -X- _ O
% -X- _ O
is -X- _ O
focused -X- _ O
on -X- _ O
the -X- _ O
future -X- _ O
and -X- _ O
another -X- _ O
where -X- _ O
it -X- _ O
is -X- _ O
about -X- _ O
64 -X- _ O
% -X- _ O
and -X- _ O
thus -X- _ O
this -X- _ O
attention -X- _ O
head -X- _ O
pays -X- _ O
more -X- _ O
attention -X- _ O
to -X- _ O
the -X- _ O
future -X- _ O
than -X- _ O
the -X- _ O
past -X- _ O
. -X- _ O

A -X- _ O
graphical -X- _ O
overview -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
the -X- _ O
appendix -X- _ O
for -X- _ O
both -X- _ O
datasets -X- _ O
. -X- _ O

6 -X- _ O
Related -X- _ O
Work -X- _ O
Transformers -X- _ B-MethodName
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
model -X- _ O
sequences -X- _ O
as -X- _ O
fully -X- _ O
connected -X- _ O
graphs -X- _ O
and -X- _ O
apply -X- _ O
a -X- _ O
bidirectional -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
module -X- _ O
where -X- _ O
every -X- _ O
token -X- _ O
can -X- _ O
attend -X- _ O
to -X- _ O
every -X- _ O
other -X- _ O
token -X- _ O
. -X- _ O

Because -X- _ O
of -X- _ O
this -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
is -X- _ O
not -X- _ O
restricted -X- _ O
to -X- _ O
sequential -X- _ O
orderings -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
; -X- _ O
inter -X- _ O
alia -X- _ O
still -X- _ O
restrict -X- _ O
themselves -X- _ O
to -X- _ O
producing -X- _ O
tokens -X- _ O
from -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
and -X- _ O
only -X- _ O
allow -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
decoder -X- _ O
to -X- _ O
attend -X- _ O
to -X- _ O
previously -X- _ O
produced -X- _ O
tokens -X- _ O
. -X- _ O

Recently -X- _ O
, -X- _ O
several -X- _ O
attempts -X- _ O
have -X- _ O
been -X- _ O
made -X- _ O
to -X- _ O
lift -X- _ O
the -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
restriction -X- _ O
in -X- _ O
Transformer -X- _ B-MethodName
or -X- _ O
LSTM -X- _ B-MethodName
- -X- _ O
based -X- _ O
models -X- _ O
( -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Stern -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
in -X- _ O
those -X- _ O
approaches -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
possible -X- _ O
to -X- _ O
attend -X- _ O
to -X- _ O
future -X- _ O
, -X- _ O
not -X- _ O
- -X- _ O
yet -X- _ O
- -X- _ O
produced -X- _ O
tokens -X- _ O
. -X- _ O

Concurrently -X- _ O
to -X- _ O
our -X- _ O
work -X- _ O
, -X- _ O
( -X- _ O
Ghazvininejad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
proposed -X- _ O
a -X- _ O
similar -X- _ O
placeholder -X- _ O
strategy -X- _ O
approach -X- _ O
for -X- _ O
generating -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
machine -X- _ O
translation -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
they -X- _ O
employ -X- _ O
an -X- _ O
encoderdecoder -X- _ O
framework -X- _ O
, -X- _ O
whereas -X- _ O
we -X- _ O
only -X- _ O
require -X- _ O
an -X- _ O
encoder -X- _ O
, -X- _ O
which -X- _ O
more -X- _ O
closely -X- _ O
links -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
via -X- _ O
a -X- _ O
single -X- _ O
shared -X- _ O
attention -X- _ O
module -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
they -X- _ O
only -X- _ O
consider -X- _ O
uniform -X- _ O
sampling -X- _ O
of -X- _ O
placeholders -X- _ O
whereas -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
the -X- _ O
higher -X- _ O
variance -X- _ O
, -X- _ O
which -X- _ O
we -X- _ O
can -X- _ O
control -X- _ O
with -X- _ O
the -X- _ O
Gaussian -X- _ O
random -X- _ O
variable -X- _ O
approach -X- _ O
, -X- _ O
leads -X- _ O
to -X- _ O
better -X- _ O
results -X- _ O
. -X- _ O

Bidirectionality -X- _ O
is -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
crucial -X- _ O
ingredients -X- _ O
in -X- _ O
the -X- _ O
success -X- _ O
of -X- _ O
the -X- _ O
recently -X- _ O
proposed -X- _ O
unsupervised -X- _ O
language -X- _ O
model -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
this -X- _ O
, -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
propose -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
encoder -X- _ O
to -X- _ O
take -X- _ O
full -X- _ O
advantage -X- _ O
of -X- _ O
the -X- _ O
bidi- -X- _ O

9rectional -X- _ O
nature -X- _ O
of -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
. -X- _ O

Their -X- _ O
resulting -X- _ O
model -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
can -X- _ O
directly -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
various -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
but -X- _ O
not -X- _ O
to -X- _ O
sequence -X- _ O
generation -X- _ O
tasks -X- _ O
. -X- _ O

Our -X- _ O
approach -X- _ O
shows -X- _ O
how -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
encoder -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
for -X- _ O
sequence -X- _ O
generation -X- _ O
and -X- _ O
this -X- _ O
allows -X- _ O
us -X- _ O
to -X- _ O
directly -X- _ O
incorporate -X- _ O
BERT -X- _ B-MethodName
into -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

GPT -X- _ B-MethodName
( -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
GPT2 -X- _ B-MethodName
( -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
are -X- _ O
both -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
that -X- _ O
use -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
decoder -X- _ O
instead -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
only -X- _ O
attend -X- _ O
to -X- _ O
already -X- _ O
produced -X- _ O
tokens -X- _ O
. -X- _ O

For -X- _ O
dialogue -X- _ O
, -X- _ O
the -X- _ O
GPT -X- _ B-MethodName
model -X- _ O
has -X- _ O
been -X- _ O
ﬁne -X- _ O
- -X- _ O
tuned -X- _ O
for -X- _ O
the -X- _ O
chit -X- _ O
- -X- _ O
chat -X- _ O
dataset -X- _ O
PersonaChat -X- _ B-DatasetName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
by -X- _ O
Wolf -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

While -X- _ O
GPT -X- _ B-MethodName
and -X- _ O
GPT2 -X- _ B-MethodName
can -X- _ O
immediately -X- _ O
be -X- _ O
used -X- _ O
as -X- _ O
a -X- _ O
sequence -X- _ O
generators -X- _ O
, -X- _ O
these -X- _ O
models -X- _ O
do -X- _ O
not -X- _ O
offer -X- _ O
bidirectionality -X- _ O
and -X- _ O
they -X- _ O
can -X- _ O
not -X- _ O
attend -X- _ O
to -X- _ O
not -X- _ O
- -X- _ O
yet -X- _ O
- -X- _ O
produced -X- _ O
tokens -X- _ O
. -X- _ O

Our -X- _ O
bidirectional -X- _ O
encoder -X- _ O
for -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
can -X- _ O
combine -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
both -X- _ O
worlds -X- _ O
. -X- _ O

7 -X- _ O
Conclusion -X- _ O
We -X- _ O
introduced -X- _ O
bidirectional -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
by -X- _ O
employing -X- _ O
placeholders -X- _ O
in -X- _ O
the -X- _ O
output -X- _ O
sequence -X- _ O
. -X- _ O

These -X- _ O
placeholder -X- _ O
tokens -X- _ O
are -X- _ O
subsequently -X- _ O
replaced -X- _ O
by -X- _ O
tokens -X- _ O
of -X- _ O
the -X- _ O
output -X- _ O
vocabulary -X- _ O
. -X- _ O

Crucially -X- _ O
, -X- _ O
this -X- _ O
allows -X- _ O
a -X- _ O
transformer -X- _ B-MethodName
encoder -X- _ O
to -X- _ O
attend -X- _ O
to -X- _ O
both -X- _ O
past -X- _ O
and -X- _ O
future -X- _ O
, -X- _ O
not -X- _ O
- -X- _ O
yet -X- _ O
- -X- _ O
produced -X- _ O
token -X- _ O
. -X- _ O

Simply -X- _ O
masking -X- _ O
all -X- _ O
placeholder -X- _ O
tokens -X- _ O
is -X- _ O
not -X- _ O
feasible -X- _ O
. -X- _ O

Instead -X- _ O
we -X- _ O
investigated -X- _ O
two -X- _ O
placeholder -X- _ O
strategies -X- _ O
, -X- _ O
based -X- _ O
on -X- _ O
Bernoulli -X- _ O
and -X- _ O
Gaussian -X- _ O
random -X- _ O
variables -X- _ O
. -X- _ O

At -X- _ O
prediction -X- _ O
time -X- _ O
, -X- _ O
our -X- _ O
approach -X- _ O
is -X- _ O
not -X- _ O
restricted -X- _ O
to -X- _ O
produce -X- _ O
the -X- _ O
output -X- _ O
sequence -X- _ O
from -X- _ O
left -X- _ O
to -X- _ O
right -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
this -X- _ O
strategy -X- _ O
proved -X- _ O
to -X- _ O
produce -X- _ O
most -X- _ O
consistent -X- _ O
results -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

Our -X- _ O
approach -X- _ O
outperforms -X- _ O
previous -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
approaches -X- _ O
that -X- _ O
do -X- _ O
not -X- _ O
make -X- _ O
use -X- _ O
of -X- _ O
any -X- _ O
pretrained -X- _ O
language -X- _ O
models -X- _ O
. -X- _ O

In -X- _ O
conjunction -X- _ O
with -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
model -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
our -X- _ O
bidirectional -X- _ O
sequence -X- _ O
generation -X- _ O
approach -X- _ O
allows -X- _ O
us -X- _ O
to -X- _ O
achieve -X- _ O
new -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
art -X- _ O
results -X- _ O
on -X- _ O
both -X- _ O
conversational -X- _ B-TaskName
tasks -X- _ I-TaskName
. -X- _ O

In -X- _ O
the -X- _ O
future -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
apply -X- _ O
our -X- _ O
approach -X- _ O
to -X- _ O
other -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
tasks -X- _ O
. -X- _ O

Additionally -X- _ O
, -X- _ O
we -X- _ O
wonder -X- _ O
if -X- _ O
a -X- _ O
further -X- _ O
performance -X- _ O
increase -X- _ O
could -X- _ O
be -X- _ O
achieved -X- _ O
if -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
would -X- _ O
employ -X- _ O
our -X- _ O
placeholder -X- _ O
strategy -X- _ O
. -X- _ O

References -X- _ O
Jacob -X- _ O
Devlin -X- _ O
, -X- _ O
Ming -X- _ O
- -X- _ O
Wei -X- _ O
Chang -X- _ O
, -X- _ O
Kenton -X- _ O
Lee -X- _ O
, -X- _ O
and -X- _ O
Kristina -X- _ O
Toutanova -X- _ O
. -X- _ O

2018 -X- _ O
. -X- _ O

BERT -X- _ B-MethodName
: -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
of -X- _ O
deep -X- _ O
bidirectional -X- _ O
transformers -X- _ O
for -X- _ O
language -X- _ O
understanding -X- _ O
. -X- _ O

ArXiv -X- _ O
e -X- _ O
- -X- _ O
prints -X- _ O
, -X- _ O
1810.04805.Marjan -X- _ O
Ghazvininejad -X- _ O
, -X- _ O
Omer -X- _ O
Levy -X- _ O
, -X- _ O
Yinhan -X- _ O
Liu -X- _ O
, -X- _ O
and -X- _ O
Luke -X- _ O
Zettlemoyer -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Constant -X- _ O
- -X- _ O
Time -X- _ O
Machine -X- _ O
Translation -X- _ O
with -X- _ O
Conditional -X- _ O
Masked -X- _ O
Language -X- _ O
Models -X- _ O
. -X- _ O

arXiv:1904.09324 -X- _ O

[ -X- _ O
cs -X- _ O
, -X- _ O
stat -X- _ O
] -X- _ O
. -X- _ O

ArXiv -X- _ O
: -X- _ O
Jiatao -X- _ O
Gu -X- _ O
, -X- _ O
Qi -X- _ O
Liu -X- _ O
, -X- _ O
and -X- _ O
Kyunghyun -X- _ O
Cho -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Insertion -X- _ O
- -X- _ O
based -X- _ O
decoding -X- _ O
with -X- _ O
automatically -X- _ O
inferred -X- _ O
generation -X- _ O
order -X- _ O
. -X- _ O

CoRR -X- _ O
, -X- _ O
abs -X- _ O
/ -X- _ O
1902.01370 -X- _ O
. -X- _ O

Diederick -X- _ O
P -X- _ O
Kingma -X- _ O
and -X- _ O
Jimmy -X- _ O
Ba -X- _ O
. -X- _ O
2015 -X- _ O
. -X- _ O

Adam -X- _ O
: -X- _ O
A -X- _ O
method -X- _ O
for -X- _ O
stochastic -X- _ O
optimization -X- _ O
. -X- _ O

In -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Learning -X- _ O
Representations -X- _ O
( -X- _ O
ICLR -X- _ O
) -X- _ O
, -X- _ O
San -X- _ O
Diego -X- _ O
, -X- _ O
CA -X- _ O
, -X- _ O
USA -X- _ O
. -X- _ O

G. -X- _ O
Klein -X- _ O
, -X- _ O
Y -X- _ O
. -X- _ O

Kim -X- _ O
, -X- _ O
Y -X- _ O
. -X- _ O

Deng -X- _ O
, -X- _ O
J. -X- _ O
Senellart -X- _ O
, -X- _ O
and -X- _ O
A. -X- _ O
M. -X- _ O
Rush -X- _ O
. -X- _ O

2017 -X- _ O
. -X- _ O

OpenNMT -X- _ O
: -X- _ O
Open -X- _ O
- -X- _ O
Source -X- _ O
Toolkit -X- _ O
for -X- _ O
Neural -X- _ O
Machine -X- _ O
Translation -X- _ O
. -X- _ O

ArXiv -X- _ O
e -X- _ O
- -X- _ O
prints -X- _ O
, -X- _ O
Yanran -X- _ O
Li -X- _ O
, -X- _ O
Hui -X- _ O
Su -X- _ O
, -X- _ O
Xiaoyu -X- _ O
Shen -X- _ O
, -X- _ O
Wenjie -X- _ O
Li -X- _ O
, -X- _ O
Ziqiang -X- _ O
Cao -X- _ O
, -X- _ O
and -X- _ O
Shuzi -X- _ O
Niu -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Dailydialog -X- _ O
: -X- _ O

A -X- _ O
manually -X- _ O
labelled -X- _ O
multi -X- _ O
- -X- _ O
turn -X- _ O
dialogue -X- _ O
dataset -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
Eighth -X- _ O
International -X- _ O
Joint -X- _ O
Conference -X- _ O
on -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
IJCNLP -X- _ O
) -X- _ O
, -X- _ O
Taipei -X- _ O
, -X- _ O
Taiwan -X- _ O
. -X- _ O

Liangchen -X- _ O
Luo -X- _ O
, -X- _ O
Jingjing -X- _ O
Xu -X- _ O
, -X- _ O
Junyang -X- _ O
Lin -X- _ O
, -X- _ O
Qi -X- _ O
Zeng -X- _ O
, -X- _ O
and -X- _ O
Xu -X- _ O
Sun -X- _ O
. -X- _ O

2018 -X- _ O
. -X- _ O

An -X- _ O
auto -X- _ O
- -X- _ O
encoder -X- _ O
matching -X- _ O
model -X- _ O
for -X- _ O
learning -X- _ O
utterance -X- _ O
- -X- _ O
level -X- _ O
semantic -X- _ O
dependency -X- _ O
in -X- _ O
dialogue -X- _ B-TaskName
generation -X- _ I-TaskName
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2018 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
EMNLP -X- _ O
) -X- _ O
. -X- _ O

Alec -X- _ O
Radford -X- _ O
, -X- _ O
Karthik -X- _ O
Narasimhan -X- _ O
, -X- _ O
Tim -X- _ O
Salimans -X- _ O
, -X- _ O
and -X- _ O
Ilya -X- _ O
Sutskever -X- _ O
. -X- _ O
2018 -X- _ O
. -X- _ O

Improving -X- _ O
Language -X- _ O
Understanding -X- _ O
by -X- _ O
Generative -X- _ O
Pre -X- _ O
- -X- _ O
Training -X- _ O
. -X- _ O

Technical -X- _ O
Report -X- _ O
Technical -X- _ O
report -X- _ O
, -X- _ O
OpenAI -X- _ O
. -X- _ O

Alec -X- _ O
Radford -X- _ O
, -X- _ O
Jeffrey -X- _ O
Wu -X- _ O
, -X- _ O
Rewon -X- _ O
Child -X- _ O
, -X- _ O
David -X- _ O
Luan -X- _ O
, -X- _ O
and -X- _ O
Dario -X- _ O
Amodei -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Language -X- _ O
Models -X- _ O
are -X- _ O
Unsupervised -X- _ O
Multitask -X- _ O
Learners -X- _ O
. -X- _ O

Technical -X- _ O
report -X- _ O
, -X- _ O
OpenAI -X- _ O
. -X- _ O

Marzieh -X- _ O
Saeidi -X- _ O
, -X- _ O
Max -X- _ O
Bartolo -X- _ O
, -X- _ O
Patrick -X- _ O
Lewis -X- _ O
, -X- _ O
Sameer -X- _ O
Singh -X- _ O
, -X- _ O
Tim -X- _ O
Rocktäschel -X- _ O
, -X- _ O
Mike -X- _ O
Sheldon -X- _ O
, -X- _ O
Guillaume -X- _ O
Bouchard -X- _ O
, -X- _ O
and -X- _ O
Sebastian -X- _ O
Riedel -X- _ O
. -X- _ O
2018 -X- _ O
. -X- _ O

Interpretation -X- _ O
of -X- _ O
natural -X- _ O
language -X- _ O
rules -X- _ O
in -X- _ O
conversational -X- _ O
machine -X- _ O
reading -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2018 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
EMNLP -X- _ O
) -X- _ O
. -X- _ O

Mitchell -X- _ O
Stern -X- _ O
, -X- _ O
William -X- _ O
Chan -X- _ O
, -X- _ O
Jamie -X- _ O
Kiros -X- _ O
, -X- _ O
and -X- _ O
Jakob -X- _ O
Uszkoreit -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Insertion -X- _ O
transformer -X- _ O
: -X- _ O
Flexible -X- _ O
sequence -X- _ B-TaskName
generation -X- _ I-TaskName
via -X- _ O
insertion -X- _ O
operations -X- _ O
. -X- _ O

CoRR -X- _ O
, -X- _ O
Ashish -X- _ O
Vaswani -X- _ O
, -X- _ O
Noam -X- _ O
Shazeer -X- _ O
, -X- _ O
Niki -X- _ O
Parmar -X- _ O
, -X- _ O
Jakob -X- _ O
Uszkoreit -X- _ O
, -X- _ O
Llion -X- _ O
Jones -X- _ O
, -X- _ O
Aidan -X- _ O
N -X- _ O
Gomez -X- _ O
, -X- _ O
Ł -X- _ O
ukasz -X- _ O
Kaiser -X- _ O
, -X- _ O
and -X- _ O
Illia -X- _ O
Polosukhin -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Attention -X- _ O
is -X- _ O
All -X- _ O
you -X- _ O
Need -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
30 -X- _ O
( -X- _ O
NIPS -X- _ O
) -X- _ O
. -X- _ O

Sean -X- _ O
Welleck -X- _ O
, -X- _ O
Kianté -X- _ O
Brantley -X- _ O
, -X- _ O
Hal -X- _ O
Daumé -X- _ O
III -X- _ O
, -X- _ O
and -X- _ O
Kyunghyun -X- _ O
Cho -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Non -X- _ B-TaskName
- -X- _ I-TaskName
monotonic -X- _ I-TaskName
sequential -X- _ I-TaskName
text -X- _ I-TaskName
generation -X- _ I-TaskName
. -X- _ O

CoRR -X- _ O
, -X- _ O
abs -X- _ O
/ -X- _ O
1902.02192 -X- _ O
. -X- _ O

10Thomas -X- _ O
Wolf -X- _ O
, -X- _ O
Victor -X- _ O
Sanh -X- _ O
, -X- _ O
Julien -X- _ O
Chaumond -X- _ O
, -X- _ O
and -X- _ O
Clement -X- _ O
Delangue -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

TransferTransfo -X- _ O
: -X- _ O
A -X- _ O
Transfer -X- _ O
Learning -X- _ O
Approach -X- _ O
for -X- _ O
Neural -X- _ O
Network -X- _ O
Based -X- _ O
Conversational -X- _ O
Agents -X- _ O
. -X- _ O

ArXiv -X- _ O
e -X- _ O
- -X- _ O
prints -X- _ O
, -X- _ O
Saizheng -X- _ O
Zhang -X- _ O
, -X- _ O
Emily -X- _ O
Dinan -X- _ O
, -X- _ O
Jack -X- _ O
Urbanek -X- _ O
, -X- _ O
Arthur -X- _ O
Szlam -X- _ O
, -X- _ O
Douwe -X- _ O
Kiela -X- _ O
, -X- _ O
and -X- _ O
Jason -X- _ O
Weston -X- _ O
. -X- _ O

2018 -X- _ O
. -X- _ O

Personalizing -X- _ O
dialogue -X- _ O
agents -X- _ O
: -X- _ O
I -X- _ O
have -X- _ O
a -X- _ O
dog -X- _ O
, -X- _ O
do -X- _ O
you -X- _ O
have -X- _ O
pets -X- _ O
too -X- _ O
? -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
56th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
( -X- _ O
ACL -X- _ O
) -X- _ O
, -X- _ O
Melbourne -X- _ O
, -X- _ O
Australia -X- _ O
. -X- _ O

Victor -X- _ O
Zhong -X- _ O
and -X- _ O
Luke -X- _ O
Zettlemoyer -X- _ O
. -X- _ O

2019 -X- _ O
. -X- _ O

E3 -X- _ O
: -X- _ O
Entailment -X- _ O
- -X- _ O
driven -X- _ O
extracting -X- _ O
and -X- _ O
editing -X- _ O
for -X- _ O
conversational -X- _ O
machine -X- _ O
reading -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
57th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
Florence -X- _ O
, -X- _ O
Italy -X- _ O
. -X- _ O

Long -X- _ O
Zhou -X- _ O
, -X- _ O
Jiajun -X- _ O
Zhang -X- _ O
, -X- _ O
and -X- _ O
Chengqing -X- _ O
Zong -X- _ O
. -X- _ O

2019 -X- _ O
. -X- _ O

Synchronous -X- _ O
bidirectional -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
. -X- _ O

Transactions -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
7:91–105 -X- _ O
. -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
2020 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
pages -X- _ O
1–23 -X- _ O
, -X- _ O
November -X- _ O
16–20 -X- _ O
, -X- _ O
2020 -X- _ O
. -X- _ O

c -X- _ O

2020 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics1Detecting -X- _ O

Attackable -X- _ O
Sentences -X- _ O
in -X- _ O
Arguments -X- _ O
Yohan -X- _ O
Jo1Seojin -X- _ O
Bang1Emaad -X- _ O
Manzoor2Eduard -X- _ O
Hovy1Chris -X- _ O
Reed3 -X- _ O
fyohanj -X- _ O
, -X- _ O
seojinb -X- _ O
g -X- _ O
@ -X- _ O
cs.cmu.edu -X- _ O
, -X- _ O
femaad -X- _ O
, -X- _ O
hovyg -X- _ O
@ -X- _ O
cmu.edu -X- _ O
c.a.reed -X- _ O
@ -X- _ O
dundee.ac.uk -X- _ O

Abstract -X- _ O
Finding -X- _ O
attackable -X- _ O
sentences -X- _ O
in -X- _ O
an -X- _ O
argument -X- _ O
is -X- _ O
the -X- _ O
ﬁrst -X- _ O
step -X- _ O
toward -X- _ O
successful -X- _ O
refutation -X- _ O
in -X- _ O
argumentation -X- _ O
. -X- _ O

We -X- _ O
present -X- _ O
a -X- _ O
ﬁrst -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
analysis -X- _ B-TaskName
of -X- _ I-TaskName
sentence -X- _ I-TaskName
attackability -X- _ I-TaskName
in -X- _ O
online -X- _ O
arguments -X- _ O
. -X- _ O

We -X- _ O
analyze -X- _ O
driving -X- _ O
reasons -X- _ O
for -X- _ O
attacks -X- _ O
in -X- _ O
argumentation -X- _ O
and -X- _ O
identify -X- _ O
relevant -X- _ O
characteristics -X- _ O
of -X- _ O
sentences -X- _ O
. -X- _ O

We -X- _ O
demonstrate -X- _ O
that -X- _ O
a -X- _ O
sentence -X- _ O
’s -X- _ O
attackability -X- _ O
is -X- _ O
associated -X- _ O
with -X- _ O
many -X- _ O
of -X- _ O
these -X- _ O
characteristics -X- _ O
regarding -X- _ O
the -X- _ O
sentence -X- _ O
’s -X- _ O
content -X- _ O
, -X- _ O
proposition -X- _ O
types -X- _ O
, -X- _ O
and -X- _ O
tone -X- _ O
, -X- _ O
and -X- _ O
that -X- _ O
an -X- _ O
external -X- _ O
knowledge -X- _ O
source -X- _ O
can -X- _ O
provide -X- _ O
useful -X- _ O
information -X- _ O
about -X- _ O
attackability -X- _ O
. -X- _ O

Building -X- _ O
on -X- _ O
these -X- _ O
ﬁndings -X- _ O
, -X- _ O
we -X- _ O
demonstrate -X- _ O
that -X- _ O
machine -X- _ O
learning -X- _ O
models -X- _ O
can -X- _ O
automatically -X- _ O
detect -X- _ B-TaskName
attackable -X- _ I-TaskName
sentences -X- _ I-TaskName
in -X- _ O
arguments -X- _ O
, -X- _ O
signiﬁcantly -X- _ O
better -X- _ O
than -X- _ O
several -X- _ O
baselines -X- _ O
and -X- _ O
comparably -X- _ O
well -X- _ O
to -X- _ O
laypeople.1 -X- _ O
1 -X- _ O
Introduction -X- _ O
Effectively -X- _ O
refuting -X- _ O
an -X- _ O
argument -X- _ O
is -X- _ O
an -X- _ O
important -X- _ O
skill -X- _ O
in -X- _ O
persuasion -X- _ O
dialogue -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
ﬁrst -X- _ O
step -X- _ O
is -X- _ O
to -X- _ O
ﬁnd -X- _ O
appropriate -X- _ O
points -X- _ O
to -X- _ O
attack -X- _ O
in -X- _ O
the -X- _ O
argument -X- _ O
. -X- _ O

Prior -X- _ O
work -X- _ O
in -X- _ O
NLP -X- _ O
has -X- _ O
studied -X- _ O
argument -X- _ O
quality -X- _ O
( -X- _ O
Wachsmuth -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017a -X- _ O
; -X- _ O
Habernal -X- _ O
and -X- _ O
Gurevych -X- _ O
, -X- _ O
2016a -X- _ O
) -X- _ O
and -X- _ O
counterargument -X- _ O
generation -X- _ O
( -X- _ O
Hua -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Wachsmuth -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

But -X- _ O
these -X- _ O
studies -X- _ O
mainly -X- _ O
concern -X- _ O
an -X- _ O
argument -X- _ O
’s -X- _ O
overall -X- _ O
quality -X- _ O
and -X- _ O
making -X- _ O
counterarguments -X- _ O
toward -X- _ O
the -X- _ O
main -X- _ O
claim -X- _ O
, -X- _ O
without -X- _ O
investigating -X- _ O
what -X- _ O
parts -X- _ O
of -X- _ O
an -X- _ O
argument -X- _ O
are -X- _ O
attackable -X- _ O
for -X- _ O
successful -X- _ O
persuasion -X- _ O
. -X- _ O

Nevertheless -X- _ O
, -X- _ O
attacking -X- _ O
speciﬁc -X- _ O
points -X- _ O
of -X- _ O
an -X- _ O
argument -X- _ O
is -X- _ O
common -X- _ O
and -X- _ O
effective -X- _ O
; -X- _ O
in -X- _ O
our -X- _ O
data -X- _ O
of -X- _ O
online -X- _ O
discussions -X- _ O
, -X- _ O
challengers -X- _ O
who -X- _ O
successfully -X- _ O
change -X- _ O
the -X- _ O
original -X- _ O
poster -X- _ O
’s -X- _ O
view -X- _ O
are -X- _ O
1.5 -X- _ O
times -X- _ O
more -X- _ O
likely -X- _ O
to -X- _ O
quote -X- _ O
speciﬁc -X- _ O
sentences -X- _ O
of -X- _ O
the -X- _ O
argument -X- _ O
for -X- _ O
attacks -X- _ O
than -X- _ O
unsuccessful -X- _ O
challengers -X- _ O
( -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
examine -X- _ O
how -X- _ O
to -X- _ O
computationally -X- _ O
com -X- _ O
/ -X- _ O
yohanjo -X- _ O
/ -X- _ O
emnlp20_arg_attack -X- _ O
2 -X- _ O
> -X- _ O
A -X- _ O
society -X- _ O
where -X- _ O
everyone -X- _ O
is -X- _ O
equal -X- _ O
seems -X- _ O
great -X- _ O
to -X- _ O
me -X- _ O

That -X- _ O
's -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
big -X- _ O
problems -X- _ O
with -X- _ O
communism -X- _ O
- -X- _ O
what -X- _ O
is -X- _ O
equality -X- _ O
? -X- _ O

Is -X- _ O
everyone -X- _ O
equal -X- _ O
? -X- _ O

[ -X- _ O
... -X- _ O
] -X- _ O
> -X- _ O
it -X- _ O
removes -X- _ O
some -X- _ O
of -X- _ O
the -X- _ O
basic -X- _ O
faults -X- _ O
in -X- _ O
society -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
poverty -X- _ O
, -X- _ O
homelessness -X- _ O
, -X- _ O
joblessness -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
touching -X- _ O
on -X- _ O
moral -X- _ O
values -X- _ O
such -X- _ O
as -X- _ O
greed -X- _ O
, -X- _ O
and -X- _ O
envy -X- _ O

Yes -X- _ O
there -X- _ O
are -X- _ O
problems -X- _ O
within -X- _ O
society -X- _ O
but -X- _ O
this -X- _ O
does -X- _ O
n't -X- _ O
mean -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
fault -X- _ O
with -X- _ O
society -X- _ O
. -X- _ O

[ -X- _ O
... -X- _ O
] -X- _ O
> -X- _ O
I -X- _ O
believe -X- _ O
a -X- _ O
proper -X- _ O
Communist -X- _ O
society -X- _ O
( -X- _ O
I.E. -X- _ O
one -X- _ O
that -X- _ O
is -X- _ O
not -X- _ O
a -X- _ O
dictatorship -X- _ O
like -X- _ O
Joseph -X- _ O
Stalin -X- _ O
or -X- _ O
Fidel -X- _ O
Castro -X- _ O
) -X- _ O
furthermore -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
unlikely -X- _ O
we -X- _ O
could -X- _ O
ever -X- _ O
get -X- _ O
a -X- _ O
true -X- _ O
communist -X- _ O
society -X- _ O
due -X- _ O
to -X- _ O
human -X- _ O
nature -X- _ O
. -X- _ O

[ -X- _ O
... -X- _ O
] -X- _ O
OP -X- _ O
: -X- _ O
I -X- _ O
believe -X- _ O
that -X- _ O
Communism -X- _ O
is -X- _ O
not -X- _ O
as -X- _ O
bad -X- _ O
as -X- _ O
everyone -X- _ O
saysFigure -X- _ O
1 -X- _ O
: -X- _ O
A -X- _ O
comment -X- _ O
to -X- _ O
a -X- _ O
post -X- _ O
entitled -X- _ O
“ -X- _ O
I -X- _ O
believe -X- _ O
that -X- _ O
Communism -X- _ O
is -X- _ O
not -X- _ O
as -X- _ O
bad -X- _ O
as -X- _ O
everyone -X- _ O
says -X- _ O
” -X- _ O
. -X- _ O

It -X- _ O
quotes -X- _ O
and -X- _ O
attacks -X- _ O
some -X- _ O
sentences -X- _ O
in -X- _ O
the -X- _ O
post -X- _ O
( -X- _ O
red -X- _ O
with -X- _ O
“ -X- _ O
> -X- _ O
” -X- _ O
) -X- _ O
detect -X- _ B-TaskName
attackable -X- _ I-TaskName
sentences -X- _ I-TaskName
in -X- _ I-TaskName
arguments -X- _ I-TaskName
. -X- _ O

This -X- _ O
attackability -X- _ O
information -X- _ O
would -X- _ O
help -X- _ O
people -X- _ O
make -X- _ O
persuasive -X- _ O
refutations -X- _ O
and -X- _ O
strengthen -X- _ O
an -X- _ O
argument -X- _ O
by -X- _ O
solidifying -X- _ O
potentially -X- _ O
attackable -X- _ O
points -X- _ O
. -X- _ O

To -X- _ O
examine -X- _ B-TaskName
the -X- _ I-TaskName
characteristics -X- _ I-TaskName
of -X- _ I-TaskName
attackable -X- _ I-TaskName
sentences -X- _ I-TaskName
in -X- _ I-TaskName
an -X- _ I-TaskName
argument -X- _ I-TaskName
, -X- _ O
we -X- _ O
ﬁrst -X- _ O
conduct -X- _ O
a -X- _ O
qualitative -X- _ O
analysis -X- _ O
of -X- _ O
reasons -X- _ O
for -X- _ O
attacks -X- _ O
in -X- _ O
online -X- _ O
arguments -X- _ O
. -X- _ O

Our -X- _ O
data -X- _ O
comes -X- _ O
from -X- _ O
discussions -X- _ O
in -X- _ O
the -X- _ O
ChangeMyView -X- _ O
( -X- _ O
CMV -X- _ O
) -X- _ O
forum -X- _ O
on -X- _ O
Reddit -X- _ O
. -X- _ O

In -X- _ O
CMV -X- _ O
, -X- _ O
users -X- _ O
challenge -X- _ O
the -X- _ O
viewpoints -X- _ O
of -X- _ O
original -X- _ O
posters -X- _ O
( -X- _ O
OPs -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
those -X- _ O
who -X- _ O
succeed -X- _ O
receive -X- _ O
a -X- _ O
from -X- _ O
the -X- _ O
OPs -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
setting -X- _ O
, -X- _ O
sentences -X- _ O
that -X- _ O
are -X- _ O
attacked -X- _ O
and -X- _ O
lead -X- _ O
to -X- _ O
the -X- _ O
OP -X- _ O
’s -X- _ O
view -X- _ O
change -X- _ O
are -X- _ O
considered -X- _ O
“ -X- _ O
attackable -X- _ O
” -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
targets -X- _ O
that -X- _ O
are -X- _ O
worth -X- _ O
attacking -X- _ O
. -X- _ O

Admittedly -X- _ O
, -X- _ O
persuasion -X- _ O
has -X- _ O
to -X- _ O
do -X- _ O
with -X- _ O
“ -X- _ O
how -X- _ O
” -X- _ O
to -X- _ O
attack -X- _ O
as -X- _ O
well -X- _ O
, -X- _ O
but -X- _ O
this -X- _ O
is -X- _ O
beyond -X- _ O
the -X- _ O
scope -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
. -X- _ O

We -X- _ O
only -X- _ O
focus -X- _ O
on -X- _ O
choosing -X- _ O
proper -X- _ O
sentences -X- _ O
to -X- _ O
attack -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
a -X- _ O
prerequisite -X- _ O
for -X- _ O
effective -X- _ O
persuasion -X- _ O
. -X- _ O

This -X- _ O
analysis -X- _ O
of -X- _ O
reasons -X- _ O
for -X- _ O
attacks -X- _ O
, -X- _ O
along -X- _ O
with -X- _ O
argumentation -X- _ O
theory -X- _ O
and -X- _ O
discourse -X- _ O
studies -X- _ O
, -X- _ O
provide -X- _ O
insights -X- _ O
into -X- _ O
what -X- _ O
characteristics -X- _ O
of -X- _ O
sentences -X- _ O
are -X- _ O
relevant -X- _ O
to -X- _ O
attackability -X- _ O
. -X- _ O

Informed -X- _ O
by -X- _ O
these -X- _ O
insights -X- _ O
, -X- _ O
we -X- _ O
extract -X- _ O
features -X- _ O
that -X- _ O
represent -X- _ O
relevant -X- _ O
sentence -X- _ O
characteristics -X- _ O
, -X- _ O
clustered -X- _ O
into -X- _ O
four -X- _ O
categories -X- _ O
: -X- _ O
content -X- _ O
, -X- _ O
external -X- _ O
knowledge -X- _ O
, -X- _ O
proposition -X- _ O

2types -X- _ O
, -X- _ O
and -X- _ O
tone -X- _ O
. -X- _ O

We -X- _ O
demonstrate -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
individual -X- _ O
features -X- _ O
on -X- _ O
sentence -X- _ O
attackability -X- _ O
, -X- _ O
in -X- _ O
regard -X- _ O
to -X- _ O
whether -X- _ O
a -X- _ O
sentence -X- _ O
would -X- _ O
be -X- _ O
attacked -X- _ O
and -X- _ O
whether -X- _ O
a -X- _ O
sentence -X- _ O
would -X- _ O
be -X- _ O
attacked -X- _ O
successfully -X- _ O
. -X- _ O

Building -X- _ O
on -X- _ O
these -X- _ O
ﬁndings -X- _ O
, -X- _ O
we -X- _ O
examine -X- _ O
the -X- _ O
efﬁcacy -X- _ O
of -X- _ O
machine -X- _ O
learning -X- _ O
models -X- _ O
in -X- _ O
detecting -X- _ B-TaskName
attackable -X- _ I-TaskName
sentences -X- _ I-TaskName
in -X- _ O
arguments -X- _ O
. -X- _ O

We -X- _ O
demonstrate -X- _ O
that -X- _ O
their -X- _ O
decisions -X- _ O
match -X- _ O
the -X- _ O
gold -X- _ O
standard -X- _ O
signiﬁcantly -X- _ O
better -X- _ O
than -X- _ O
several -X- _ O
baselines -X- _ O
and -X- _ O
comparably -X- _ O
well -X- _ O
to -X- _ O
laypeople -X- _ O
. -X- _ O

To -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
this -X- _ O
work -X- _ O
is -X- _ O
the -X- _ O
ﬁrst -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
analysis -X- _ B-TaskName
of -X- _ I-TaskName
sentence -X- _ I-TaskName
attackability -X- _ I-TaskName
in -X- _ I-TaskName
arguments -X- _ I-TaskName
. -X- _ O

Our -X- _ O
contributions -X- _ O
are -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
We -X- _ O
introduce -X- _ O
the -X- _ O
problem -X- _ O
of -X- _ O
detecting -X- _ B-TaskName
attackable -X- _ I-TaskName
sentences -X- _ I-TaskName
in -X- _ O
arguments -X- _ O
and -X- _ O
release -X- _ O
the -X- _ O
processed -X- _ O
data -X- _ O
from -X- _ O
online -X- _ O
discussions -X- _ O
and -X- _ O
the -X- _ O
external -X- _ O
knowledge -X- _ O
source -X- _ O
we -X- _ O
used -X- _ O
. -X- _ O

We -X- _ O
analyze -X- _ O
driving -X- _ O
reasons -X- _ O
for -X- _ O
attacks -X- _ O
in -X- _ O
arguments -X- _ O
and -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
sentence -X- _ O
characteristics -X- _ O
on -X- _ O
a -X- _ O
sentence -X- _ O
’s -X- _ O
attackability -X- _ O
. -X- _ O

We -X- _ O
demonstrate -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
machine -X- _ O
learning -X- _ O
models -X- _ O
for -X- _ O
detecting -X- _ B-TaskName
attackable -X- _ I-TaskName
sentences -X- _ I-TaskName
, -X- _ O
setting -X- _ O
a -X- _ O
baseline -X- _ O
for -X- _ O
this -X- _ O
challenging -X- _ O
task -X- _ O
and -X- _ O
suggesting -X- _ O
future -X- _ O
directions -X- _ O
. -X- _ O

2 -X- _ O
Background -X- _ O
The -X- _ O
strength -X- _ O
of -X- _ O
an -X- _ O
argument -X- _ O
is -X- _ O
a -X- _ O
long -X- _ O
- -X- _ O
studied -X- _ O
topic -X- _ O
, -X- _ O
dating -X- _ O
back -X- _ O
to -X- _ O
Aristotle -X- _ O
( -X- _ O
2007 -X- _ O
) -X- _ O
, -X- _ O
who -X- _ O
suggested -X- _ O
three -X- _ O
aspects -X- _ O
of -X- _ O
argument -X- _ O
persuasiveness -X- _ O
: -X- _ O
ethos -X- _ O
( -X- _ O
the -X- _ O
arguer -X- _ O
’s -X- _ O
credibility -X- _ O
) -X- _ O
, -X- _ O
logos -X- _ O
( -X- _ O
logic -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
pathos -X- _ O
( -X- _ O
appeal -X- _ O
to -X- _ O
the -X- _ O
hearer -X- _ O
’s -X- _ O
emotion -X- _ O
) -X- _ O
. -X- _ O

More -X- _ O
recently -X- _ O
, -X- _ O
Wachsmuth -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2017b -X- _ O
) -X- _ O
summarized -X- _ O
various -X- _ O
aspects -X- _ O
of -X- _ O
argument -X- _ O
quality -X- _ O
studied -X- _ O
in -X- _ O
argumentation -X- _ O
theory -X- _ O
and -X- _ O
NLP -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
clarity -X- _ O
, -X- _ O
relevance -X- _ O
, -X- _ O
and -X- _ O
arrangement -X- _ O
. -X- _ O

Some -X- _ O
research -X- _ O
took -X- _ O
empirical -X- _ O
approaches -X- _ O
and -X- _ O
collected -X- _ O
argument -X- _ O
evaluation -X- _ O
criteria -X- _ O
from -X- _ O
human -X- _ O
evaluators -X- _ O
( -X- _ O
Habernal -X- _ O
and -X- _ O
Gurevych -X- _ O
, -X- _ O
2016a -X- _ O
; -X- _ O
Wachsmuth -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017a -X- _ O
) -X- _ O
. -X- _ O

By -X- _ O
adopting -X- _ O
some -X- _ O
of -X- _ O
these -X- _ O
aspects -X- _ O
, -X- _ O
computational -X- _ O
models -X- _ O
have -X- _ O
been -X- _ O
proposed -X- _ O
to -X- _ O
automatically -X- _ O
evaluate -X- _ O
argument -X- _ O
quality -X- _ O
in -X- _ O
various -X- _ O
settings -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
essays -X- _ O
( -X- _ O
Ke -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
online -X- _ O
comments -X- _ O
( -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
pairwise -X- _ O
ranking -X- _ O
( -X- _ O
Habernal -X- _ O
and -X- _ O
Gurevych -X- _ O
, -X- _ O
2016b -X- _ O
) -X- _ O
. -X- _ O

While -X- _ O
these -X- _ O
taxonomies -X- _ O
help -X- _ O
understand -X- _ O
and -X- _ O
evaluate -X- _ B-TaskName
the -X- _ I-TaskName
quality -X- _ I-TaskName
of -X- _ I-TaskName
an -X- _ I-TaskName
argument -X- _ I-TaskName
as -X- _ O
a -X- _ O
whole -X- _ O
, -X- _ O
little -X- _ O
empirical -X- _ O
analysis -X- _ O
has -X- _ O
been -X- _ O
done -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
what -X- _ O
to -X- _ O
attack -X- _ O
in -X- _ O
an -X- _ O
argument -X- _ O
to -X- _ O
persuade -X- _ O
the -X- _ O
arguer -X- _ O
. -X- _ O

What -X- _ O
can -X- _ O
be -X- _ O
attacked -X- _ O
in -X- _ O
an -X- _ O
argument -X- _ O
has -X- _ O
been -X- _ O
studied -X- _ O
more -X- _ O
in -X- _ O
argumentation -X- _ O
theory -X- _ O
. -X- _ O

Particularly -X- _ O
, -X- _ O
Walton -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2008 -X- _ O
) -X- _ O
present -X- _ O
argumentation -X- _ O
schemes -X- _ O
and -X- _ O
critical -X- _ O
questions -X- _ O
( -X- _ O
CQs -X- _ O
) -X- _ O
. -X- _ O

Argument -X- _ O
schemesare -X- _ O
reasoning -X- _ O
types -X- _ O
commonly -X- _ O
used -X- _ O
in -X- _ O
daily -X- _ O
argumentation -X- _ O
. -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
the -X- _ O
scheme -X- _ O
of -X- _ O
argument -X- _ O
from -X- _ O
cause -X- _ O
to -X- _ O
effect -X- _ O
has -X- _ O
the -X- _ O
conclusion -X- _ O
“ -X- _ O
Bwill -X- _ O
occur -X- _ O
” -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
premise -X- _ O
“ -X- _ O
if -X- _ O
Aoccurs -X- _ O
, -X- _ O
Bwill -X- _ O
occur -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
case -X- _ O
, -X- _ O
Aoccurs -X- _ O
” -X- _ O
. -X- _ O

Each -X- _ O
scheme -X- _ O
is -X- _ O
associated -X- _ O
with -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
CQs -X- _ O
for -X- _ O
judging -X- _ O
the -X- _ O
argument -X- _ O
to -X- _ O
be -X- _ O
good -X- _ O
or -X- _ O
fallacious -X- _ O
. -X- _ O

CQs -X- _ O
for -X- _ O
the -X- _ O
above -X- _ O
scheme -X- _ O
include -X- _ O
“ -X- _ O
How -X- _ O
strong -X- _ O
is -X- _ O
the -X- _ O
causal -X- _ O
generalization -X- _ O
? -X- _ O
” -X- _ O
and -X- _ O
“ -X- _ O
Are -X- _ O
there -X- _ O
other -X- _ O
factors -X- _ O
that -X- _ O
interfere -X- _ O
with -X- _ O
the -X- _ O
causal -X- _ O
effect -X- _ O
? -X- _ O
” -X- _ O

Unlike -X- _ O
the -X- _ O
general -X- _ O
argument -X- _ O
quality -X- _ O
described -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
paragraph -X- _ O
, -X- _ O
CQs -X- _ O
serve -X- _ O
as -X- _ O
an -X- _ O
evaluation -X- _ O
tool -X- _ O
that -X- _ O
specify -X- _ O
local -X- _ O
attackable -X- _ O
points -X- _ O
in -X- _ O
an -X- _ O
argument -X- _ O
. -X- _ O

They -X- _ O
have -X- _ O
been -X- _ O
adopted -X- _ O
for -X- _ O
grading -X- _ O
essays -X- _ O
( -X- _ O
Song -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
teaching -X- _ O
argumentation -X- _ O
skills -X- _ O
( -X- _ O
Nussbaum -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Some -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
characteristics -X- _ O
in -X- _ O
our -X- _ O
work -X- _ O
are -X- _ O
informed -X- _ O
by -X- _ O
argumentation -X- _ O
schemes -X- _ O
and -X- _ O
CQs -X- _ O
. -X- _ O

NLP -X- _ O
researchers -X- _ O
have -X- _ O
widely -X- _ O
studied -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
counterarguments -X- _ O
on -X- _ O
persuasion -X- _ O
( -X- _ O
Tan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Cano -X- _ O
- -X- _ O
Basave -X- _ O
and -X- _ O
He -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Wei -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
how -X- _ O
to -X- _ O
generate -X- _ O
counterarguments -X- _ O
( -X- _ O
Hua -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Wachsmuth -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Most -X- _ O
of -X- _ O
the -X- _ O
work -X- _ O
focuses -X- _ O
on -X- _ O
the -X- _ O
characteristics -X- _ O
of -X- _ O
counterarguments -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
topics -X- _ O
and -X- _ O
styles -X- _ O
, -X- _ O
without -X- _ O
consideration -X- _ O
of -X- _ O
what -X- _ O
points -X- _ O
to -X- _ O
attack -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
some -X- _ O
studies -X- _ O
aimed -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
salience -X- _ O
of -X- _ O
individual -X- _ O
sentences -X- _ O
in -X- _ O
attacked -X- _ O
arguments -X- _ O
by -X- _ O
paying -X- _ O
different -X- _ O
degrees -X- _ O
of -X- _ O
attention -X- _ O
to -X- _ O
sentences -X- _ O
using -X- _ O
attention -X- _ O
mechanism -X- _ O
( -X- _ O
Jo -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Ji -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

While -X- _ O
their -X- _ O
approaches -X- _ O
helped -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
success -X- _ O
of -X- _ O
persuasion -X- _ O
, -X- _ O
it -X- _ O
was -X- _ O
difﬁcult -X- _ O
to -X- _ O
interpret -X- _ O
what -X- _ O
constitute -X- _ O
the -X- _ O
salience -X- _ O
or -X- _ O
attackability -X- _ O
of -X- _ O
sentences -X- _ O
. -X- _ O

To -X- _ O
address -X- _ O
this -X- _ O
limitation -X- _ O
, -X- _ O
we -X- _ O
quantify -X- _ O
and -X- _ O
analyze -X- _ O
the -X- _ O
characteristics -X- _ O
of -X- _ O
sentences -X- _ O
that -X- _ O
are -X- _ O
attacked -X- _ O
and -X- _ O
lead -X- _ O
to -X- _ O
the -X- _ O
arguer -X- _ O
’s -X- _ O
view -X- _ O
change -X- _ O
. -X- _ O

3 -X- _ O
Data -X- _ O
Here -X- _ O
we -X- _ O
describe -X- _ O
how -X- _ O
we -X- _ O
collected -X- _ O
and -X- _ O
labeled -X- _ O
our -X- _ O
data -X- _ O
. -X- _ O

3.1 -X- _ O
Data -X- _ O
Collection -X- _ O
We -X- _ O
use -X- _ O
online -X- _ O
discussions -X- _ O
from -X- _ O
the -X- _ O
ChangeMyView -X- _ O
( -X- _ O
CMV -X- _ O
) -X- _ O
subreddit2 -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
forum -X- _ O
, -X- _ O
users -X- _ O
post -X- _ O
their -X- _ O
views -X- _ O
on -X- _ O
various -X- _ O
issues -X- _ O
and -X- _ O
invite -X- _ O
other -X- _ O
users -X- _ O
to -X- _ O
challenge -X- _ O
their -X- _ O
views -X- _ O
. -X- _ O

If -X- _ O
a -X- _ O
comment -X- _ O
changes -X- _ O
the -X- _ O
original -X- _ O
poster -X- _ O
( -X- _ O
OP -X- _ O
) -X- _ O
’s -X- _ O
view -X- _ O
, -X- _ O
the -X- _ O
OP -X- _ O
acknowledges -X- _ O
it -X- _ O
by -X- _ O
replying -X- _ O
to -X- _ O
the -X- _ O
comment -X- _ O
with -X- _ O
asymbol -X- _ O
. -X- _ O

The -X- _ O
high -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
discussions -X- _ O
in -X- _ O
this -X- _ O
forum -X- _ O
is -X- _ O
maintained -X- _ O
through -X- _ O
several -X- _ O
mod2https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
www.reddit.com -X- _ O
/ -X- _ O
r -X- _ O
/ -X- _ O
changemyview -X- _ O

3eration -X- _ O
rules -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
minimum -X- _ O
length -X- _ O
of -X- _ O
an -X- _ O
original -X- _ O
post -X- _ O
and -X- _ O
the -X- _ O
maximum -X- _ O
response -X- _ O
time -X- _ O
of -X- _ O
OPs -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
CMV -X- _ O
discussions -X- _ O
have -X- _ O
been -X- _ O
used -X- _ O
in -X- _ O
many -X- _ O
NLP -X- _ O
studies -X- _ O
( -X- _ O
Chakrabarty -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
We -X- _ O
scraped -X- _ O
CMV -X- _ O
posts -X- _ O
and -X- _ O
comments -X- _ O
written -X- _ O
between -X- _ O
January -X- _ O
1 -X- _ O
, -X- _ O
2014 -X- _ O
and -X- _ O
September -X- _ O
30 -X- _ O
, -X- _ O
2019 -X- _ O
, -X- _ O
using -X- _ O
the -X- _ O
Pushshift -X- _ O
API -X- _ O
. -X- _ O

We -X- _ O
split -X- _ O
them -X- _ O
into -X- _ O
a -X- _ O
dev -X- _ O
set -X- _ O
( -X- _ O
Jan -X- _ O
2014 -X- _ O
– -X- _ O
Jan -X- _ O
2018 -X- _ O
for -X- _ O
training -X- _ O
and -X- _ O
Feb -X- _ O
2018 -X- _ O
– -X- _ O
Nov -X- _ O
2018 -X- _ O
for -X- _ O
validation -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
test -X- _ O
set -X- _ O
( -X- _ O
Dec -X- _ O
2018 -X- _ O
– -X- _ O
Sep -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
ratio -X- _ O
of -X- _ O
6:2:2 -X- _ O
. -X- _ O

We -X- _ O
split -X- _ O
the -X- _ O
data -X- _ O
by -X- _ O
time -X- _ O
to -X- _ O
measure -X- _ O
our -X- _ O
models -X- _ O
’ -X- _ O
generality -X- _ O
to -X- _ O
unseen -X- _ O
subjects -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
characteristics -X- _ O
of -X- _ O
arguments -X- _ O
vary -X- _ O
across -X- _ O
different -X- _ O
issues -X- _ O
, -X- _ O
we -X- _ O
categorized -X- _ O
the -X- _ O
posts -X- _ O
into -X- _ O
domains -X- _ O
using -X- _ O
LDA -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
post -X- _ O
, -X- _ O
we -X- _ O
chose -X- _ O
as -X- _ O
its -X- _ O
domain -X- _ O
the -X- _ O
topic -X- _ O
that -X- _ O
has -X- _ O
the -X- _ O
highest -X- _ O
standard -X- _ O
score -X- _ O
; -X- _ O
topics -X- _ O
comprising -X- _ O
common -X- _ O
words -X- _ O
were -X- _ O
excluded -X- _ O
. -X- _ O

We -X- _ O
tried -X- _ O
different -X- _ O
numbers -X- _ O
of -X- _ O
topics -X- _ O
( -X- _ O
25 -X- _ O
, -X- _ O
30 -X- _ O
, -X- _ O
35 -X- _ O
, -X- _ O
40 -X- _ O
) -X- _ O
and -X- _ O
ﬁnalized -X- _ O
on -X- _ O
40 -X- _ O
, -X- _ O
as -X- _ O
it -X- _ O
achieves -X- _ O
the -X- _ O
lowest -X- _ O
perplexity -X- _ O
. -X- _ O

This -X- _ O
process -X- _ O
resulted -X- _ O
in -X- _ O
30 -X- _ O
domains -X- _ O
( -X- _ O
excluding -X- _ O
common -X- _ O
- -X- _ O
word -X- _ O
topics -X- _ O
) -X- _ O
: -X- _ O
media -X- _ O
, -X- _ O
abortion -X- _ O
, -X- _ O
sex -X- _ O
, -X- _ O
election -X- _ O
, -X- _ O
Reddit -X- _ O
, -X- _ O
human -X- _ O
economy -X- _ O
, -X- _ O
gender -X- _ O
, -X- _ O
race -X- _ O
, -X- _ O
family -X- _ O
, -X- _ O
life -X- _ O
, -X- _ O
crime -X- _ O
, -X- _ O
relationship -X- _ O
, -X- _ O
movie -X- _ O
, -X- _ O
world -X- _ O
, -X- _ O
game -X- _ O
, -X- _ O
tax -X- _ O
, -X- _ O
law -X- _ O
, -X- _ O
money -X- _ O
, -X- _ O
drug -X- _ O
, -X- _ O
war -X- _ O
, -X- _ O
religion -X- _ O
, -X- _ O
job -X- _ O
, -X- _ O
food -X- _ O
, -X- _ O
power -X- _ O
, -X- _ O
school -X- _ O
, -X- _ O
college -X- _ O
, -X- _ O
music -X- _ O
, -X- _ O
gun -X- _ O
, -X- _ O
and -X- _ O
Jewish -X- _ O
( -X- _ O
from -X- _ O
most -X- _ O
frequent -X- _ O
to -X- _ O
least -X- _ O
, -X- _ O
ranging -X- _ O
5 -X- _ O
% -X- _ O
–2 -X- _ O
% -X- _ O
) -X- _ O
. -X- _ O

3.2 -X- _ O
Labeling -X- _ O
Attackability -X- _ O
Since -X- _ O
we -X- _ O
are -X- _ O
interested -X- _ O
in -X- _ O
which -X- _ O
parts -X- _ O
of -X- _ O
a -X- _ O
post -X- _ O
are -X- _ O
attacked -X- _ O
by -X- _ O
comments -X- _ O
and -X- _ O
whether -X- _ O
the -X- _ O
attacks -X- _ O
lead -X- _ O
to -X- _ O
successful -X- _ O
view -X- _ O
changes -X- _ O
, -X- _ O
our -X- _ O
goal -X- _ O
here -X- _ O
is -X- _ O
to -X- _ O
label -X- _ O
each -X- _ O
sentence -X- _ O
in -X- _ O
a -X- _ O
post -X- _ O
as -X- _ O
successfully -X- _ O
attacked -X- _ O
, -X- _ O
unsuccessfully -X- _ O
attacked -X- _ O
, -X- _ O
orunattacked -X- _ O
. -X- _ O

We -X- _ O
only -X- _ O
consider -X- _ O
comments -X- _ O
directly -X- _ O
replying -X- _ O
to -X- _ O
each -X- _ O
post -X- _ O
( -X- _ O
toplevel -X- _ O
comments -X- _ O
) -X- _ O
, -X- _ O
as -X- _ O
lower -X- _ O
- -X- _ O
level -X- _ O
comments -X- _ O
usually -X- _ O
address -X- _ O
the -X- _ O
same -X- _ O
points -X- _ O
as -X- _ O
their -X- _ O
parent -X- _ O
comments -X- _ O
( -X- _ O
as -X- _ O
will -X- _ O
be -X- _ O
validated -X- _ O
at -X- _ O
the -X- _ O
end -X- _ O
of -X- _ O
the -X- _ O
section -X- _ O
) -X- _ O
. -X- _ O

Attacked -X- _ O
vs. -X- _ O
Unattacked -X- _ O
: -X- _ O
Some -X- _ O
comments -X- _ O
use -X- _ O
direct -X- _ O
quotes -X- _ O
with -X- _ O
the -X- _ O
> -X- _ O
symbol -X- _ O
to -X- _ O
address -X- _ O
speciﬁc -X- _ O
sentences -X- _ O
of -X- _ O
the -X- _ O
post -X- _ O
( -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O

Each -X- _ O
quote -X- _ O
is -X- _ O
matched -X- _ O
with -X- _ O
the -X- _ O
longest -X- _ O
sequence -X- _ O
of -X- _ O
sentences -X- _ O
in -X- _ O
the -X- _ O
post -X- _ O
using -X- _ O
the -X- _ O
Levenshtein -X- _ O
edit -X- _ O
distance -X- _ O
( -X- _ O
allowing -X- _ O
a -X- _ O
distance -X- _ O
of -X- _ O
2 -X- _ O
characters -X- _ O
for -X- _ O
typos -X- _ O
) -X- _ O
. -X- _ O

A -X- _ O
matched -X- _ O
text -X- _ O
span -X- _ O
should -X- _ O
contain -X- _ O
at -X- _ O
least -X- _ O
one -X- _ O
word -X- _ O
and -X- _ O
four -X- _ O
characters -X- _ O
, -X- _ O
and -X- _ O
cover -X- _ O
at -X- _ O
least -X- _ O
80 -X- _ O
% -X- _ O
of -X- _ O
the -X- _ O
quote -X- _ O
to -X- _ O
exclude -X- _ O
cases -X- _ O
where -X- _ O
the -X- _ O
> -X- _ O
symbol -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
quote -X- _ O
external -X- _ O
content -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
98 -X- _ O
% -X- _ O
of -X- _ O
the -X- _ O
matched -X- _ O
spans -X- _ O
cover -X- _ O
the -X- _ O
corresponding -X- _ O
quotes -X- _ O
entirely -X- _ O
. -X- _ O

Additionally -X- _ O
, -X- _ O
a -X- _ O
sentence -X- _ O
in -X- _ O
the -X- _ O
post -X- _ O
is -X- _ O
considered -X- _ O
tobe -X- _ O
quoted -X- _ O
if -X- _ O
at -X- _ O
least -X- _ O
four -X- _ O
non -X- _ O
- -X- _ O
stopwords -X- _ O
appear -X- _ O
in -X- _ O
a -X- _ O
comment -X- _ O
’s -X- _ O
sentence -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
: -X- _ O
Post -X- _ O
: -X- _ O
... -X- _ O

If -X- _ O
you -X- _ O
do -X- _ O
something -X- _ O
, -X- _ O
you -X- _ O
should -X- _ O
be -X- _ O
prepared -X- _ O
to -X- _ O
accept -X- _ O
the -X- _ O
consequences -X- _ O
. -X- _ O

... -X- _ O

Comment -X- _ O
: -X- _ O
... -X- _ O

I -X- _ O
guess -X- _ O
my -X- _ O
point -X- _ O
is -X- _ O
, -X- _ O
even -X- _ O
if -X- _ O
you -X- _ O
do -X- _ O
believe -X- _ O
that -X- _ O
“ -X- _ O
If -X- _ O
you -X- _ O
do -X- _ O
something -X- _ O
, -X- _ O
you -X- _ O
should -X- _ O
be -X- _ O
prepared -X- _ O
to -X- _ O
accept -X- _ O
the -X- _ O
consequences -X- _ O
, -X- _ O
” -X- _ O
you -X- _ O
can -X- _ O
still -X- _ O
feel -X- _ O
bad -X- _ O
for -X- _ O
the -X- _ O
victims -X- _ O
. -X- _ O

... -X- _ O

We -X- _ O
considered -X- _ O
manually -X- _ O
annotating -X- _ O
attacked -X- _ O
sentences -X- _ O
too -X- _ O
, -X- _ O
but -X- _ O
it -X- _ O
turned -X- _ O
out -X- _ O
to -X- _ O
be -X- _ O
extremely -X- _ O
timeconsuming -X- _ O
and -X- _ O
subjective -X- _ O
( -X- _ O
Appendix -X- _ O
A -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
tried -X- _ O
to -X- _ O
automate -X- _ O
it -X- _ O
using -X- _ O
heuristics -X- _ O
( -X- _ O
word -X- _ O
overlap -X- _ O
and -X- _ O
vector -X- _ O
embeddings -X- _ O
) -X- _ O
, -X- _ O
but -X- _ O
precision -X- _ O
severely -X- _ O
deteriorated -X- _ O
. -X- _ O

As -X- _ O
we -X- _ O
value -X- _ O
the -X- _ O
precision -X- _ O
of -X- _ O
labels -X- _ O
over -X- _ O
recall -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
use -X- _ O
the -X- _ O
method -X- _ O
described -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
paragraph -X- _ O
. -X- _ O

Chakrabarty -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
used -X- _ O
the -X- _ O
same -X- _ O
method -X- _ O
to -X- _ O
collect -X- _ O
attack -X- _ O
relations -X- _ O
in -X- _ O
CMV -X- _ O
. -X- _ O

Successfully -X- _ O
vs. -X- _ O
Unsuccessfully -X- _ O
Attacked -X- _ O
: -X- _ O
After -X- _ O
each -X- _ O
sentence -X- _ O
in -X- _ O
a -X- _ O
post -X- _ O
is -X- _ O
labeled -X- _ O
as -X- _ O
attacked -X- _ O
or -X- _ O
not -X- _ O
, -X- _ O
each -X- _ O
attacked -X- _ O
sentence -X- _ O
is -X- _ O
further -X- _ O
labeled -X- _ O
as -X- _ O
successfully -X- _ O
attacked -X- _ O
if -X- _ O
any -X- _ O
of -X- _ O
the -X- _ O
comments -X- _ O
that -X- _ O
attack -X- _ O
it -X- _ O
, -X- _ O
or -X- _ O
their -X- _ O
lower -X- _ O
- -X- _ O
level -X- _ O
comments -X- _ O
win -X- _ O
a -X- _ O
 -X- _ O
. -X- _ O

We -X- _ O
post -X- _ O
- -X- _ O
process -X- _ O
the -X- _ O
resulting -X- _ O
labels -X- _ O
to -X- _ O
increase -X- _ O
their -X- _ O
validity -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
as -X- _ O
a -X- _ O
challenger -X- _ O
and -X- _ O
the -X- _ O
OP -X- _ O
have -X- _ O
discussion -X- _ O
down -X- _ O
the -X- _ O
comment -X- _ O
thread -X- _ O
, -X- _ O
the -X- _ O
challenger -X- _ O
might -X- _ O
attack -X- _ O
different -X- _ O
sentences -X- _ O
than -X- _ O
the -X- _ O
originally -X- _ O
attacked -X- _ O
ones -X- _ O
and -X- _ O
change -X- _ O
the -X- _ O
OP -X- _ O
’s -X- _ O
view -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
case -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
ambiguous -X- _ O
which -X- _ O
sentences -X- _ O
contribute -X- _ O
to -X- _ O
the -X- _ O
view -X- _ O
change -X- _ O
. -X- _ O

Hence -X- _ O
, -X- _ O
we -X- _ O
extract -X- _ O
quotes -X- _ O
from -X- _ O
all -X- _ O
lower -X- _ O
- -X- _ O
level -X- _ O
comments -X- _ O
of -X- _ O
-winning -X- _ O
challengers -X- _ O
, -X- _ O
and -X- _ O
if -X- _ O
any -X- _ O
of -X- _ O
the -X- _ O
quotes -X- _ O
attack -X- _ O
new -X- _ O
sentences -X- _ O
, -X- _ O
this -X- _ O
challenger -X- _ O
’s -X- _ O
attacks -X- _ O
are -X- _ O
excluded -X- _ O
from -X- _ O
the -X- _ O
labeling -X- _ O
of -X- _ O
successfully -X- _ O
attacked -X- _ O
. -X- _ O

This -X- _ O
case -X- _ O
is -X- _ O
not -X- _ O
common -X- _ O
, -X- _ O
however -X- _ O
( -X- _ O
0.2 -X- _ O
% -X- _ O
) -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
if -X- _ O
a -X- _ O
comment -X- _ O
attacks -X- _ O
many -X- _ O
sentences -X- _ O
in -X- _ O
the -X- _ O
post -X- _ O
and -X- _ O
change -X- _ O
the -X- _ O
OP -X- _ O
’s -X- _ O
view -X- _ O
, -X- _ O
some -X- _ O
of -X- _ O
them -X- _ O
may -X- _ O
not -X- _ O
contribute -X- _ O
to -X- _ O
the -X- _ O
view -X- _ O
change -X- _ O
but -X- _ O
are -X- _ O
still -X- _ O
labeled -X- _ O
as -X- _ O
successfully -X- _ O
attacked -X- _ O
. -X- _ O

To -X- _ O
reduce -X- _ O
this -X- _ O
noise -X- _ O
, -X- _ O
comments -X- _ O
that -X- _ O
have -X- _ O
more -X- _ O
than -X- _ O
three -X- _ O
quotes -X- _ O
are -X- _ O
excluded -X- _ O
from -X- _ O
the -X- _ O
labeling -X- _ O
of -X- _ O
successfully -X- _ O
attacked3 -X- _ O
. -X- _ O

This -X- _ O
amounts -X- _ O
to -X- _ O
12 -X- _ O
% -X- _ O
of -X- _ O
top -X- _ O
- -X- _ O
level -X- _ O
comments -X- _ O
( -X- _ O
63 -X- _ O
% -X- _ O
of -X- _ O
comments -X- _ O
have -X- _ O
only -X- _ O
one -X- _ O
quote -X- _ O
, -X- _ O
17 -X- _ O
% -X- _ O
two -X- _ O
quotes -X- _ O
, -X- _ O
and -X- _ O
8 -X- _ O
% -X- _ O
three -X- _ O
quotes -X- _ O
) -X- _ O
. -X- _ O

Lastly -X- _ O
, -X- _ O
we -X- _ O
veriﬁed -X- _ O
if -X- _ O
quoted -X- _ O
sentences -X- _ O
are -X- _ O
actually -X- _ O
attacked -X- _ O
. -X- _ O

We -X- _ O
randomly -X- _ O
selected -X- _ O
500 -X- _ O
comments -X- _ O
and -X- _ O
checked -X- _ O
if -X- _ O
each -X- _ O
quoted -X- _ O
sentence -X- _ O
is -X- _ O
purely -X- _ O
agreed -X- _ O
with -X- _ O
without -X- _ O
any -X- _ O
opposition -X- _ O
, -X- _ O
challenge -X- _ O
, -X- _ O
or -X- _ O
question -X- _ O
. -X- _ O

This -X- _ O
case -X- _ O
was -X- _ O
rare -X- _ O
( -X- _ O
0.4 -X- _ O
% -X- _ O
) -X- _ O
4 -X- _ O
, -X- _ O
so -X- _ O
we -X- _ O
do -X- _ O
signals -X- _ O
for -X- _ O
successful -X- _ O
attacks -X- _ O
than -X- _ O
without -X- _ O
this -X- _ O
process -X- _ O
. -X- _ O

comments -X- _ O
( -X- _ O
0.2 -X- _ O
% -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
author -X- _ O
agreed -X- _ O
with -X- _ O
4 -X- _ O
quoted -X- _ O
sentences -X- _ O
. -X- _ O

In -X- _ O
CMV -X- _ O
, -X- _ O
challengers -X- _ O
do -X- _ O
use -X- _ O
concessions -X- _ O
but -X- _ O
hardly -X- _ O

Table -X- _ O
1 -X- _ O
: -X- _ O
Data -X- _ O
statistics -X- _ O
. -X- _ O

“ -X- _ O
Attacked -X- _ O
” -X- _ O
contains -X- _ O
posts -X- _ O
with -X- _ O
at -X- _ O
least -X- _ O
one -X- _ O
attacked -X- _ O
sentence -X- _ O
. -X- _ O

“ -X- _ O
Successful -X- _ O
” -X- _ O
contains -X- _ O
posts -X- _ O
with -X- _ O
at -X- _ O
least -X- _ O
one -X- _ O
successfully -X- _ O
attacked -X- _ O
sentence -X- _ O
. -X- _ O

R1 -X- _ O
Sis -X- _ O
true -X- _ O
but -X- _ O
does -X- _ O
not -X- _ O
support -X- _ O
the -X- _ O
main -X- _ O
claim -X- _ O
( -X- _ O
19 -X- _ O
% -X- _ O
) -X- _ O
R2 -X- _ O
Smisses -X- _ O
cases -X- _ O
suggesting -X- _ O
opposite -X- _ O
judgment -X- _ O
( -X- _ O
18 -X- _ O
% -X- _ O
) -X- _ O
R3 -X- _ O
Shas -X- _ O
exceptions -X- _ O
( -X- _ O
17 -X- _ O
% -X- _ O
) -X- _ O

R4 -X- _ O

Sis -X- _ O
false -X- _ O
( -X- _ O
12 -X- _ O
% -X- _ O
) -X- _ O
R5 -X- _ O
Smisses -X- _ O
nuanced -X- _ O
distinctions -X- _ O
of -X- _ O
a -X- _ O
concept -X- _ O
( -X- _ O
8 -X- _ O
% -X- _ O
) -X- _ O
R6 -X- _ O
Sis -X- _ O
unlikely -X- _ O
to -X- _ O
happen -X- _ O
( -X- _ O
6 -X- _ O
% -X- _ O
) -X- _ O
R7 -X- _ O
Shas -X- _ O
no -X- _ O
evidence -X- _ O
( -X- _ O
6 -X- _ O
% -X- _ O
) -X- _ O
R8 -X- _ O
Suses -X- _ O
an -X- _ O
invalid -X- _ O
assumption -X- _ O
or -X- _ O
hypothetical -X- _ O
( -X- _ O
4 -X- _ O
% -X- _ O
) -X- _ O
R9 -X- _ O
Scontradicts -X- _ O
statements -X- _ O
in -X- _ O
the -X- _ O
argument -X- _ O
( -X- _ O
4 -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
Rationales -X- _ O
for -X- _ O
attacking -X- _ O
a -X- _ O
sentence -X- _ O
( -X- _ O
S -X- _ O
) -X- _ O
. -X- _ O

F1 -X- _ O
Personal -X- _ O
opinion -X- _ O
( -X- _ O
28 -X- _ O
% -X- _ O
) -X- _ O
F2 -X- _ O
Invalid -X- _ O
hypothetical -X- _ O
( -X- _ O
26 -X- _ O
% -X- _ O
) -X- _ O
F3 -X- _ O
Invalid -X- _ O
generalization -X- _ O
( -X- _ O
13 -X- _ O
% -X- _ O
) -X- _ O
F4 -X- _ O

No -X- _ O
evidence -X- _ O
( -X- _ O
11 -X- _ O
% -X- _ O
) -X- _ O
F5 -X- _ O
Absolute -X- _ O
statement -X- _ O
( -X- _ O
7 -X- _ O
% -X- _ O
) -X- _ O

F6 -X- _ O
Concession -X- _ O
( -X- _ O
5 -X- _ O
% -X- _ O
) -X- _ O
F7 -X- _ O
Restrictive -X- _ O
qualiﬁer -X- _ O
( -X- _ O
5 -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
Motivating -X- _ O
factors -X- _ O
for -X- _ O
attacks -X- _ O
. -X- _ O

Table -X- _ O
2 -X- _ O
: -X- _ O
Rationales -X- _ O
and -X- _ O
motivating -X- _ O
factors -X- _ O
for -X- _ O
attacks -X- _ O
. -X- _ O

not -X- _ O
further -X- _ O
process -X- _ O
this -X- _ O
case -X- _ O
. -X- _ O

Table -X- _ O
1 -X- _ O
shows -X- _ O
some -X- _ O
statistics -X- _ O
of -X- _ O
the -X- _ O
ﬁnal -X- _ O
data -X- _ O
. -X- _ O

4 -X- _ O
Quantifying -X- _ O
Sentence -X- _ O
Characteristics -X- _ O
As -X- _ O
the -X- _ O
ﬁrst -X- _ O
step -X- _ O
for -X- _ O
analyzing -X- _ O
the -X- _ O
characteristics -X- _ O
of -X- _ O
attackable -X- _ O
sentences -X- _ O
, -X- _ O
we -X- _ O
examine -X- _ O
driving -X- _ O
reasons -X- _ O
for -X- _ O
attacks -X- _ O
and -X- _ O
quantify -X- _ O
relevant -X- _ O
characteristics -X- _ O
. -X- _ O

4.1 -X- _ O
Rationales -X- _ O
and -X- _ O
Motivation -X- _ O
for -X- _ O
Attacks -X- _ O
To -X- _ O
analyze -X- _ O
rationales -X- _ O
for -X- _ O
attacks -X- _ O
, -X- _ O
two -X- _ O
authors -X- _ O
examined -X- _ O
quotes -X- _ O
and -X- _ O
rebuttals -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
( -X- _ O
one -X- _ O
successful -X- _ O
and -X- _ O
one -X- _ O
unsuccessful -X- _ O
comment -X- _ O
for -X- _ O
each -X- _ O
post -X- _ O
) -X- _ O
. -X- _ O

From -X- _ O
156 -X- _ O
attacks -X- _ O
, -X- _ O
we -X- _ O
identiﬁed -X- _ O
10 -X- _ O
main -X- _ O
rationales -X- _ O
( -X- _ O
Table -X- _ O
2a -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
ﬁner -X- _ O
- -X- _ O
grained -X- _ O
than -X- _ O
the -X- _ O
refutation -X- _ O
reasons -X- _ O
in -X- _ O
prior -X- _ O
work -X- _ O
( -X- _ O
Wei -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
most -X- _ O
common -X- _ O
rationale -X- _ O
is -X- _ O
that -X- _ O
the -X- _ O
sentence -X- _ O
is -X- _ O
factually -X- _ O
correct -X- _ O
but -X- _ O
is -X- _ O
irrelevant -X- _ O
to -X- _ O
the -X- _ O
main -X- _ O
claim -X- _ O
( -X- _ O
19 -X- _ O
% -X- _ O
) -X- _ O
. -X- _ O

Counterexample -X- _ O
- -X- _ O
related -X- _ O
rationales -X- _ O
are -X- _ O
also -X- _ O
common -X- _ O
: -X- _ O
the -X- _ O
sentence -X- _ O
misses -X- _ O
an -X- _ O
example -X- _ O
suggestquote -X- _ O
the -X- _ O
OP -X- _ O
’s -X- _ O
sentences -X- _ O
just -X- _ O
to -X- _ O
agree.ing -X- _ O
the -X- _ O
opposite -X- _ O
judgment -X- _ O
to -X- _ O
the -X- _ O
sentence -X- _ O
’s -X- _ O
own -X- _ O
( -X- _ O
18 -X- _ O
% -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
sentence -X- _ O
has -X- _ O
exceptions -X- _ O
( -X- _ O
17 -X- _ O
% -X- _ O
) -X- _ O
. -X- _ O

This -X- _ O
analysis -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
polished -X- _ O
rebuttals -X- _ O
, -X- _ O
which -X- _ O
mostly -X- _ O
emphasize -X- _ O
logical -X- _ O
aspects -X- _ O
, -X- _ O
and -X- _ O
can -X- _ O
not -X- _ O
fully -X- _ O
capture -X- _ O
other -X- _ O
factors -X- _ O
that -X- _ O
motivate -X- _ O
attacks -X- _ O
. -X- _ O

Hence -X- _ O
, -X- _ O
we -X- _ O
conducted -X- _ O
a -X- _ O
complementary -X- _ O
analysis -X- _ O
, -X- _ O
where -X- _ O
an -X- _ O
undergraduate -X- _ O
student -X- _ O
chose -X- _ O
three -X- _ O
sentences -X- _ O
to -X- _ O
attack -X- _ O
for -X- _ O
each -X- _ O
of -X- _ O
50 -X- _ O
posts -X- _ O
and -X- _ O
speciﬁed -X- _ O
the -X- _ O
reasons -X- _ O
in -X- _ O
their -X- _ O
own -X- _ O
terms -X- _ O
( -X- _ O
Table -X- _ O
2b -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
most -X- _ O
common -X- _ O
factor -X- _ O
is -X- _ O
that -X- _ O
the -X- _ O
sentence -X- _ O
is -X- _ O
only -X- _ O
a -X- _ O
personal -X- _ O
opinion -X- _ O
( -X- _ O
28 -X- _ O
% -X- _ O
) -X- _ O
. -X- _ O

Invalid -X- _ O
hypotheticals -X- _ O
are -X- _ O
also -X- _ O
a -X- _ O
common -X- _ O
factor -X- _ O
( -X- _ O
26 -X- _ O
% -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
tone -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
motivates -X- _ O
attacks -X- _ O
as -X- _ O
well -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
generalization -X- _ O
( -X- _ O
13 -X- _ O
% -X- _ O
) -X- _ O
, -X- _ O
absoluteness -X- _ O
( -X- _ O
7 -X- _ O
% -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
concession -X- _ O
( -X- _ O
5 -X- _ O
% -X- _ O
) -X- _ O
. -X- _ O

4.2 -X- _ O
Feature -X- _ O
Extraction -X- _ O
Based -X- _ O
on -X- _ O
these -X- _ O
analyses -X- _ O
, -X- _ O
we -X- _ O
cluster -X- _ O
various -X- _ O
sentence -X- _ O
characteristics -X- _ O
into -X- _ O
four -X- _ O
categories -X- _ O
— -X- _ O
content -X- _ O
, -X- _ O
external -X- _ O
knowledge -X- _ O
, -X- _ O
proposition -X- _ O
types -X- _ O
, -X- _ O
and -X- _ O
tone.5 -X- _ O
4.2.1 -X- _ O
Content -X- _ O
Content -X- _ O
and -X- _ O
logic -X- _ O
play -X- _ O
the -X- _ O
most -X- _ O
important -X- _ O
role -X- _ O
in -X- _ O
CMV -X- _ O
discussions -X- _ O
. -X- _ O

We -X- _ O
extract -X- _ O
the -X- _ O
content -X- _ O
of -X- _ O
each -X- _ O
sentence -X- _ O
at -X- _ O
two -X- _ O
levels -X- _ O
: -X- _ O
TFIDF -X- _ O
- -X- _ O
weighted -X- _ O
n -X- _ O
- -X- _ O
grams -X- _ O
( -X- _ O
n= -X- _ O
1 -X- _ O
; -X- _ O
2 -X- _ O
; -X- _ O
3 -X- _ O
) -X- _ O
and -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
topics -X- _ O
. -X- _ O

Each -X- _ O
sentence -X- _ O
is -X- _ O
assigned -X- _ O
one -X- _ O
topic -X- _ O
using -X- _ O
Sentence -X- _ O
LDA -X- _ O
( -X- _ O
Jo -X- _ O
and -X- _ O
Oh -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
train -X- _ O
a -X- _ O
model -X- _ O
on -X- _ O
posts -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
and -X- _ O
apply -X- _ O
it -X- _ O
to -X- _ O
all -X- _ O
posts -X- _ O
, -X- _ O
exploring -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
topics2f10 -X- _ O
; -X- _ O
50 -X- _ O
; -X- _ O
100g.6 -X- _ O
4.2.2 -X- _ O
External -X- _ O
Knowledge -X- _ O
External -X- _ O
knowledge -X- _ O
sources -X- _ O
may -X- _ O
provide -X- _ O
information -X- _ O
as -X- _ O
to -X- _ O
how -X- _ O
truthful -X- _ O
or -X- _ O
convincing -X- _ O
a -X- _ O
sentence -X- _ O
is -X- _ O
As -X- _ O
our -X- _ O
knowledge -X- _ O
source -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
kialo.com -X- _ O
— -X- _ O
a -X- _ O
collaborative -X- _ O
argument -X- _ O
platform -X- _ O
over -X- _ O
more -X- _ O
than -X- _ O
1.4 -X- _ O
K -X- _ O
issues -X- _ O
. -X- _ O

Each -X- _ O
issue -X- _ O
has -X- _ O
a -X- _ O
main -X- _ O
statement -X- _ O
, -X- _ O
and -X- _ O
users -X- _ O
can -X- _ O
respond -X- _ O
to -X- _ O
any -X- _ O
existing -X- _ O
statement -X- _ O
with -X- _ O
pro -X- _ O
/ -X- _ O
con -X- _ O
statements -X- _ O
( -X- _ O
1 -X- _ O
- -X- _ O
2 -X- _ O
sentences -X- _ O
) -X- _ O
, -X- _ O
building -X- _ O
an -X- _ O
argumentation -X- _ O
tree -X- _ O
. -X- _ O

Kialo -X- _ O
has -X- _ O
advantages -X- _ O
over -X- _ O
structured -X- _ O
knowledge -X- _ O
bases -X- _ O
and -X- _ O
Wikipedia -X- _ O
in -X- _ O
that -X- _ O
it -X- _ O
includes -X- _ O
many -X- _ O
debatable -X- _ O
statements -X- _ O
; -X- _ O
many -X- _ O
attacked -X- _ O
sentences -X- _ O
are -X- _ O
subjective -X- _ O
judgments -X- _ O
( -X- _ O
x4.1 -X- _ O
) -X- _ O
, -X- _ O
so -X- _ O
factbased -X- _ O
knowledge -X- _ O
sources -X- _ O
may -X- _ O
have -X- _ O
limited -X- _ O
utility -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
each -X- _ O
statement -X- _ O
in -X- _ O
Kialo -X- _ O
has -X- _ O
pro -X- _ O
/ -X- _ O
con -X- _ O
counts -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
reﬂect -X- _ O
the -X- _ O
convincingness -X- _ O
of -X- _ O
the -X- _ O
statement -X- _ O
. -X- _ O

We -X- _ O
scraped -X- _ O
1,417 -X- _ O
argumentation -X- _ O
trees -X- _ O
and -X- _ O
130 -X- _ O
K -X- _ O
statements -X- _ O
( -X- _ O
written -X- _ O
until -X- _ O
Oct -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

to -X- _ O
operationalize -X- _ O
reliably -X- _ O
using -X- _ O
the -X- _ O
current -X- _ O
NLP -X- _ O
technology -X- _ O
and -X- _ O
thus -X- _ O
are -X- _ O
not -X- _ O
included -X- _ O
in -X- _ O
our -X- _ O
features -X- _ O
. -X- _ O

SLING -X- _ O
( -X- _ O
Ringgaard -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
but -X- _ O
they -X- _ O
were -X- _ O
not -X- _ O
helpful -X- _ O
. -X- _ O

lar -X- _ O
statements -X- _ O
in -X- _ O
Kialo -X- _ O
that -X- _ O
have -X- _ O
at -X- _ O
least -X- _ O
5 -X- _ O
common -X- _ O
words7and -X- _ O
compute -X- _ O
the -X- _ O
following -X- _ O
three -X- _ O
features -X- _ O
. -X- _ O

Frequency -X- _ O
is -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
retrieved -X- _ O
statements -X- _ O
; -X- _ O
sentences -X- _ O
that -X- _ O
are -X- _ O
not -X- _ O
suitable -X- _ O
for -X- _ O
argumentation -X- _ O
are -X- _ O
unlikely -X- _ O
to -X- _ O
appear -X- _ O
in -X- _ O
Kialo -X- _ O
. -X- _ O

This -X- _ O
feature -X- _ O
is -X- _ O
computed -X- _ O
as -X- _ O
log2 -X- _ O
( -X- _ O
N+ -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
whereN -X- _ O
is -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
retrieved -X- _ O
statements -X- _ O
. -X- _ O

Attractiveness -X- _ O
is -X- _ O
the -X- _ O
average -X- _ O
number -X- _ O
of -X- _ O
responses -X- _ O
for -X- _ O
the -X- _ O
matched -X- _ O
statements -X- _ O
, -X- _ O
reﬂecting -X- _ O
how -X- _ O
debatable -X- _ O
the -X- _ O
sentence -X- _ O
is -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
computed -X- _ O
as -X- _ O
log2 -X- _ O
( -X- _ O
M+ -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
whereM=1 -X- _ O
NPN -X- _ O
i=1RiandRiis -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
responses -X- _ O
for -X- _ O
the -X- _ O
ith -X- _ O
retrieved -X- _ O
statement -X- _ O
. -X- _ O

Lastly -X- _ O
, -X- _ O
extremeness -X- _ O
is1 -X- _ O
NPN -X- _ O
i=1jPi Nij -X- _ O
, -X- _ O
wherePiand -X- _ O
Niare -X- _ O
the -X- _ O
proportions -X- _ O
( -X- _ O
between -X- _ O
0 -X- _ O
and -X- _ O
1 -X- _ O
) -X- _ O
of -X- _ O
pro -X- _ O
responses -X- _ O
and -X- _ O
con -X- _ O
responses -X- _ O
for -X- _ O
the -X- _ O
ith -X- _ O
retrieved -X- _ O
statement -X- _ O
. -X- _ O

A -X- _ O
sentence -X- _ O
that -X- _ O
most -X- _ O
people -X- _ O
would -X- _ O
see -X- _ O
ﬂawed -X- _ O
would -X- _ O
have -X- _ O
a -X- _ O
high -X- _ O
extremeness -X- _ O
value -X- _ O
. -X- _ O

4.2.3 -X- _ O
Proposition -X- _ O
Types -X- _ O
Sentences -X- _ O
convey -X- _ O
different -X- _ O
types -X- _ O
of -X- _ O
propositions -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
predictions -X- _ O
and -X- _ O
hypotheticals -X- _ O
. -X- _ O

No -X- _ O
proposition -X- _ O
types -X- _ O
are -X- _ O
fallacious -X- _ O
by -X- _ O
nature -X- _ O
, -X- _ O
but -X- _ O
some -X- _ O
of -X- _ O
them -X- _ O
may -X- _ O
make -X- _ O
it -X- _ O
harder -X- _ O
to -X- _ O
generate -X- _ O
a -X- _ O
sound -X- _ O
argument -X- _ O
. -X- _ O

They -X- _ O
also -X- _ O
communicate -X- _ O
different -X- _ O
moods -X- _ O
, -X- _ O
causing -X- _ O
the -X- _ O
hearer -X- _ O
to -X- _ O
react -X- _ O
differently -X- _ O
. -X- _ O

We -X- _ O
extract -X- _ O
13 -X- _ O
binary -X- _ O
features -X- _ O
for -X- _ O
proposition -X- _ O
types -X- _ O
. -X- _ O

They -X- _ O
are -X- _ O
all -X- _ O
based -X- _ O
on -X- _ O
lexicons -X- _ O
and -X- _ O
regular -X- _ O
expressions -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
available -X- _ O
in -X- _ O
Appendix -X- _ O
C. -X- _ O
Questions -X- _ O
express -X- _ O
the -X- _ O
intent -X- _ O
of -X- _ O
information -X- _ O
seeking -X- _ O
. -X- _ O

Depending -X- _ O
on -X- _ O
the -X- _ O
form -X- _ O
, -X- _ O
we -X- _ O
deﬁne -X- _ O
three -X- _ O
features -X- _ O
: -X- _ O
confusion -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
I -X- _ O
do -X- _ O
n’t -X- _ O
understand -X- _ O
) -X- _ O
, -X- _ O
why -X- _ O
/ -X- _ O
how -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
why -X- _ O
... -X- _ O
? -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
other -X- _ O
. -X- _ O

Normative -X- _ O
sentences -X- _ O
suggest -X- _ O
that -X- _ O
an -X- _ O
action -X- _ O
be -X- _ O
carried -X- _ O
out -X- _ O
. -X- _ O

Due -X- _ O
to -X- _ O
their -X- _ O
imperative -X- _ O
mood -X- _ O
, -X- _ O
they -X- _ O
can -X- _ O
sound -X- _ O
face -X- _ O
- -X- _ O
threatening -X- _ O
and -X- _ O
thus -X- _ O
attract -X- _ O
attacks -X- _ O
. -X- _ O

Prediction -X- _ O
sentences -X- _ O
predict -X- _ O
a -X- _ O
future -X- _ O
event -X- _ O
. -X- _ O

They -X- _ O
can -X- _ O
be -X- _ O
attacked -X- _ O
with -X- _ O
reasons -X- _ O
why -X- _ O
the -X- _ O
prediction -X- _ O
is -X- _ O
unlikely -X- _ O
( -X- _ O
Table -X- _ O
2a -X- _ O
- -X- _ O
R6 -X- _ O
) -X- _ O
, -X- _ O
as -X- _ O
in -X- _ O
critical -X- _ O
questions -X- _ O
for -X- _ O
argument -X- _ O
from -X- _ O
cause -X- _ O
to -X- _ O
effect -X- _ O
( -X- _ O
Walton -X- _ O
Hypothetical -X- _ O
sentences -X- _ O
may -X- _ O
make -X- _ O
implausible -X- _ O
assumptions -X- _ O
( -X- _ O
Table -X- _ O
2a -X- _ O
- -X- _ O
R8 -X- _ O
and -X- _ O
Table -X- _ O
2b -X- _ O
- -X- _ O
F2 -X- _ O
) -X- _ O
or -X- _ O
restrict -X- _ O
the -X- _ O
applicability -X- _ O
of -X- _ O
the -X- _ O
argument -X- _ O
too -X- _ O
much -X- _ O
( -X- _ O
Table -X- _ O
2b -X- _ O
- -X- _ O
F7 -X- _ O
) -X- _ O
. -X- _ O

Citation -X- _ O
often -X- _ O
strengthens -X- _ O
a -X- _ O
claim -X- _ O
using -X- _ O
authority -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
credibility -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
could -X- _ O
be -X- _ O
attacked -X- _ O
( -X- _ O
Walton -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
. -X- _ O

Comparison -X- _ O
may -X- _ O
reﬂect -X- _ O
personal -X- _ O
preferences -X- _ O
that -X- _ O
are -X- _ O
vulnerable -X- _ O
to -X- _ O
attacks -X- _ O
( -X- _ O
Table -X- _ O
2b -X- _ O
- -X- _ O
F1 -X- _ O
) -X- _ O
. -X- _ O

knowledge -X- _ O
representation -X- _ O
did -X- _ O
not -X- _ O
help -X- _ O
( -X- _ O
Appendix -X- _ O
B -X- _ O
) -X- _ O
.Examples -X- _ O
in -X- _ O
a -X- _ O
sentence -X- _ O
may -X- _ O
be -X- _ O
attacked -X- _ O
for -X- _ O
their -X- _ O
invalidity -X- _ O
( -X- _ O
Walton -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
or -X- _ O
counterexamples -X- _ O
( -X- _ O
Table -X- _ O
2a -X- _ O
- -X- _ O
R3 -X- _ O
) -X- _ O
. -X- _ O

Deﬁnitions -X- _ O
form -X- _ O
a -X- _ O
ground -X- _ O
for -X- _ O
arguments -X- _ O
, -X- _ O
and -X- _ O
challengers -X- _ O
could -X- _ O
undermine -X- _ O
an -X- _ O
argument -X- _ O
by -X- _ O
attacking -X- _ O
this -X- _ O
basis -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
Table -X- _ O
2a -X- _ O
- -X- _ O
R5 -X- _ O
) -X- _ O
. -X- _ O

Personal -X- _ O
stories -X- _ O
are -X- _ O
the -X- _ O
arguer -X- _ O
’s -X- _ O
experiences -X- _ O
, -X- _ O
whose -X- _ O
validity -X- _ O
is -X- _ O
difﬁcult -X- _ O
to -X- _ O
refute -X- _ O
. -X- _ O

A -X- _ O
sentence -X- _ O
with -X- _ O
a -X- _ O
personal -X- _ O
story -X- _ O
has -X- _ O
subject -X- _ O
Iand -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
epistemic -X- _ O
verb -X- _ O
; -X- _ O
or -X- _ O
it -X- _ O
has -X- _ O
mymodifying -X- _ O
non -X- _ O
- -X- _ O
epistemic -X- _ O
nouns -X- _ O
. -X- _ O

Inclusive -X- _ O
sentences -X- _ O
that -X- _ O
mention -X- _ O
youandweengage -X- _ O
the -X- _ O
hearer -X- _ O
into -X- _ O
the -X- _ O
discourse -X- _ O
( -X- _ O
Hyland -X- _ O
, -X- _ O
2005 -X- _ O
) -X- _ O
, -X- _ O
making -X- _ O
the -X- _ O
argument -X- _ O
more -X- _ O
vulnerable -X- _ O
to -X- _ O
attacks -X- _ O
. -X- _ O

Challengers -X- _ O
are -X- _ O
inﬂuenced -X- _ O
by -X- _ O
the -X- _ O
tone -X- _ O
of -X- _ O
an -X- _ O
argument -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
subjectiveness -X- _ O
, -X- _ O
absoluteness -X- _ O
, -X- _ O
or -X- _ O
conﬁdence -X- _ O
( -X- _ O
Table -X- _ O
2b -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
extract -X- _ O
8 -X- _ O
features -X- _ O
for -X- _ O
the -X- _ O
tone -X- _ O
of -X- _ O
sentences -X- _ O
. -X- _ O

Subjectivity -X- _ O
comprises -X- _ O
judgments -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
often -X- _ O
attacked -X- _ O
due -X- _ O
to -X- _ O
counterexamples -X- _ O
( -X- _ O
Table -X- _ O
2a -X- _ O
- -X- _ O
R2 -X- _ O
) -X- _ O
or -X- _ O
their -X- _ O
arbitrariness -X- _ O
( -X- _ O
Table -X- _ O
2b -X- _ O
- -X- _ O
F1 -X- _ O
, -X- _ O
Walton -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2008 -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
subjectivity -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
is -X- _ O
the -X- _ O
average -X- _ O
subjectivity -X- _ O
score -X- _ O
of -X- _ O
words -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
Subjectivity -X- _ O
Lexicon -X- _ O
( -X- _ O
Wilson -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2005 -X- _ O
) -X- _ O
( -X- _ O
non -X- _ O
- -X- _ O
neutral -X- _ O
words -X- _ O
of -X- _ O
“ -X- _ O
weaksubj -X- _ O
” -X- _ O
= -X- _ O
0.5 -X- _ O
and -X- _ O
“ -X- _ O
strongsubj -X- _ O
” -X- _ O
= -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O

Concreteness -X- _ O
is -X- _ O
the -X- _ O
inverse -X- _ O
of -X- _ O
abstract -X- _ O
diction -X- _ O
, -X- _ O
whose -X- _ O
meaning -X- _ O
depends -X- _ O
on -X- _ O
subjective -X- _ O
perceptions -X- _ O
and -X- _ O
experiences -X- _ O
. -X- _ O

The -X- _ O
concreteness -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
is -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
the -X- _ O
standardized -X- _ O
word -X- _ O
scores -X- _ O
based -X- _ O
on -X- _ O
Brysbaert -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
’s -X- _ O
concreteness -X- _ O
lexicon -X- _ O
. -X- _ O

Qualiﬁcation -X- _ O
expresses -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
generality -X- _ O
of -X- _ O
a -X- _ O
claim -X- _ O
, -X- _ O
where -X- _ O
absolute -X- _ O
statements -X- _ O
can -X- _ O
motivate -X- _ O
attacks -X- _ O
( -X- _ O
Table -X- _ O
2b -X- _ O
- -X- _ O
R3 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
qualiﬁcation -X- _ O
score -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
is -X- _ O
the -X- _ O
average -X- _ O
word -X- _ O
score -X- _ O
based -X- _ O
on -X- _ O
our -X- _ O
lexicon -X- _ O
of -X- _ O
qualiﬁers -X- _ O
and -X- _ O
generality -X- _ O
words -X- _ O
. -X- _ O

Hedging -X- _ O
can -X- _ O
sound -X- _ O
unconvincing -X- _ O
( -X- _ O
Durik -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
and -X- _ O
motivate -X- _ O
attacks -X- _ O
. -X- _ O

A -X- _ O
sentence -X- _ O
’s -X- _ O
hedging -X- _ O
score -X- _ O
is -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
word -X- _ O
scores -X- _ O
based -X- _ O
on -X- _ O
our -X- _ O
lexicon -X- _ O
of -X- _ O
downtoners -X- _ O
and -X- _ O
boosters -X- _ O
. -X- _ O

Sentiment -X- _ O
represents -X- _ O
the -X- _ O
valence -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
. -X- _ O

Polar -X- _ O
judgments -X- _ O
may -X- _ O
attract -X- _ O
more -X- _ O
attacks -X- _ O
than -X- _ O
neutral -X- _ O
statements -X- _ O
. -X- _ O

We -X- _ O
calculate -X- _ O
the -X- _ O
sentiment -X- _ O
of -X- _ O
each -X- _ O
sentence -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
data -X- _ O
of -X- _ O
SemEval -X- _ B-DatasetName
2017 -X- _ I-DatasetName
Task -X- _ O
4 -X- _ O
( -X- _ O
Rosenthal -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Sentiment -X- _ O
score -X- _ O
is -X- _ O
a -X- _ O
continuous -X- _ O
value -X- _ O
ranging -X- _ O
between -X- _ O
-1 -X- _ O
( -X- _ O
negative -X- _ O
) -X- _ O
and -X- _ O
+1 -X- _ O
( -X- _ O
positive -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
sentiment -X- _ O
categories -X- _ O
are -X- _ O
nominal -X- _ O
( -X- _ O
positive -X- _ O
, -X- _ O
neutral -X- _ O
, -X- _ O
and -X- _ O
negative -X- _ O
) -X- _ O
8 -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
compute -X- _ O
the -X- _ O
than -X- _ O
the -X- _ O
winner -X- _ O
team -X- _ O
’s -X- _ O
performance -X- _ O
of -X- _ O
0.681 -X- _ O
. -X- _ O

6scores -X- _ O
of -X- _ O
arousal -X- _ O
( -X- _ O
intensity -X- _ O
) -X- _ O
and -X- _ O
dominance -X- _ O
( -X- _ O
control -X- _ O
) -X- _ O
as -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
the -X- _ O
standardized -X- _ O
word -X- _ O
scores -X- _ O
based -X- _ O
on -X- _ O
Warriner -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2013 -X- _ O
) -X- _ O
’s -X- _ O
lexicon -X- _ O
. -X- _ O

5 -X- _ O
Task -X- _ O
1 -X- _ O
: -X- _ O
Attackability -X- _ O
Characteristics -X- _ O
One -X- _ O
of -X- _ O
our -X- _ O
goals -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
is -X- _ O
to -X- _ O
analyze -X- _ O
what -X- _ O
characteristics -X- _ O
of -X- _ O
sentences -X- _ O
are -X- _ O
associated -X- _ O
with -X- _ O
a -X- _ O
sentence -X- _ O
’s -X- _ O
attackability -X- _ O
. -X- _ O

Hence -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
measure -X- _ O
the -X- _ O
effect -X- _ O
size -X- _ O
and -X- _ O
statistical -X- _ O
signiﬁcance -X- _ O
of -X- _ O
each -X- _ O
feature -X- _ O
toward -X- _ O
two -X- _ O
labels -X- _ O
: -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
whether -X- _ O
a -X- _ O
sentence -X- _ O
is -X- _ O
attacked -X- _ O
or -X- _ O
not -X- _ O
, -X- _ O
using -X- _ O
the -X- _ O
dev -X- _ O
set -X- _ O
of -X- _ O
the -X- _ O
“ -X- _ O
Attacked -X- _ O
” -X- _ O
dataset -X- _ O
( -X- _ O
N=553,635 -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
whether -X- _ O
a -X- _ O
sentence -X- _ O
is -X- _ O
attacked -X- _ O
successfully -X- _ O
or -X- _ O
unsuccessfully -X- _ O
, -X- _ O
using -X- _ O
all -X- _ O
attacked -X- _ O
sentences -X- _ O
( -X- _ O
N=159,417 -X- _ O
) -X- _ O
.9Since -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
characteristics -X- _ O
may -X- _ O
depend -X- _ O
on -X- _ O
the -X- _ O
issue -X- _ O
being -X- _ O
discussed -X- _ O
, -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
each -X- _ O
feature -X- _ O
is -X- _ O
estimated -X- _ O
conditioned -X- _ O
on -X- _ O
the -X- _ O
domain -X- _ O
of -X- _ O
each -X- _ O
post -X- _ O
using -X- _ O
a -X- _ O
logistic -X- _ O
regression -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
statistical -X- _ O
signiﬁcance -X- _ O
of -X- _ O
the -X- _ O
effect -X- _ O
is -X- _ O
assessed -X- _ O
using -X- _ O
the -X- _ O
Wald -X- _ O
test -X- _ O
. -X- _ O

For -X- _ O
interpretation -X- _ O
purposes -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
odds -X- _ B-MetricName
ratio -X- _ I-MetricName
( -X- _ O
OR -X- _ B-MetricName
) -X- _ O
—the -X- _ O
exponent -X- _ O
of -X- _ O
the -X- _ O
effect -X- _ O
size.10 -X- _ O
5.1 -X- _ O
Content -X- _ O
Attacked -X- _ O
sentences -X- _ O
tend -X- _ O
to -X- _ O
mention -X- _ O
big -X- _ O
issues -X- _ O
like -X- _ O
gender -X- _ O
, -X- _ O
race -X- _ O
, -X- _ O
and -X- _ O
health -X- _ O
as -X- _ O
revealed -X- _ O
in -X- _ O
topics -X- _ O
47 -X- _ O
, -X- _ O
8 -X- _ O
, -X- _ O
and -X- _ O
6 -X- _ O
( -X- _ O
Table -X- _ O
3 -X- _ O
) -X- _ O
and -X- _ O
n -X- _ O
- -X- _ O
grams -X- _ O
life -X- _ O
, -X- _ O
weapons -X- _ O
, -X- _ O
women -X- _ O
, -X- _ O
society -X- _ O
, -X- _ O
and -X- _ O
men -X- _ O
( -X- _ O
Table -X- _ O
7 -X- _ O
in -X- _ O
Appendix -X- _ O
E -X- _ O
) -X- _ O
. -X- _ O

These -X- _ O
issues -X- _ O
are -X- _ O
also -X- _ O
positively -X- _ O
correlated -X- _ O
with -X- _ O
successful -X- _ O
attacks -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
mentioning -X- _ O
relatively -X- _ O
personal -X- _ O
issues -X- _ O
( -X- _ O
tv -X- _ O
, -X- _ O
friends -X- _ O
, -X- _ O
topic -X- _ O
38 -X- _ O
) -X- _ O
seems -X- _ O
negatively -X- _ O
correlated -X- _ O
with -X- _ O
successful -X- _ O
attacks -X- _ O
. -X- _ O

So -X- _ O
do -X- _ O
forum -X- _ O
- -X- _ O
speciﬁc -X- _ O
messages -X- _ O
( -X- _ O
cmv -X- _ O
, -X- _ O
thank -X- _ O
, -X- _ O
topic -X- _ O
4 -X- _ O
) -X- _ O
. -X- _ O

Attacking -X- _ O
seemingly -X- _ O
evidenced -X- _ O
sentences -X- _ O
appears -X- _ O
to -X- _ O
be -X- _ O
effective -X- _ O
for -X- _ O
persuasion -X- _ O
when -X- _ O
properly -X- _ O
done -X- _ O
. -X- _ O

Successfully -X- _ O
attacked -X- _ O
sentences -X- _ O
are -X- _ O
likely -X- _ O
to -X- _ O
mention -X- _ O
speciﬁc -X- _ O
data -X- _ O
( -X- _ O
data -X- _ O
, -X- _ O
% -X- _ O
) -X- _ O
and -X- _ O
be -X- _ O
the -X- _ O
OP -X- _ O
’s -X- _ O
speciﬁc -X- _ O
reasons -X- _ O
under -X- _ O
bullet -X- _ O
points -X- _ O
( -X- _ O
2.and3 -X- _ O
. -X- _ O
) -X- _ O
. -X- _ O

n -X- _ O
- -X- _ O
grams -X- _ O
capture -X- _ O
various -X- _ O
characteristics -X- _ O
that -X- _ O
are -X- _ O
vulnerable -X- _ O
to -X- _ O
attacks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
uncertainty -X- _ O
and -X- _ O
absoluteness -X- _ O
( -X- _ O
i -X- _ O
believe -X- _ O
, -X- _ O
never -X- _ O
) -X- _ O
, -X- _ O
hypotheticals -X- _ O
( -X- _ O
if -X- _ O
i -X- _ O
) -X- _ O
, -X- _ O
questions -X- _ O
( -X- _ O
? -X- _ O
, -X- _ O
why -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
norms -X- _ O
( -X- _ O
should -X- _ O
) -X- _ O
. -X- _ O

prediction -X- _ O
setting -X- _ O
provides -X- _ O
an -X- _ O
incomplete -X- _ O
picture -X- _ O
of -X- _ O
the -X- _ O
roles -X- _ O
of -X- _ O
the -X- _ O
characteristics -X- _ O
. -X- _ O

Some -X- _ O
features -X- _ O
may -X- _ O
not -X- _ O
have -X- _ O
drastic -X- _ O
contribution -X- _ O
to -X- _ O
prediction -X- _ O
due -X- _ O
to -X- _ O
their -X- _ O
infrequency -X- _ O
, -X- _ O
although -X- _ O
they -X- _ O
may -X- _ O
have -X- _ O
signiﬁcant -X- _ O
effects -X- _ O
on -X- _ O
attackability -X- _ O
. -X- _ O

10Odds -X- _ O
are -X- _ O
the -X- _ O
ratio -X- _ O
of -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
being -X- _ O
( -X- _ O
successfully -X- _ O
) -X- _ O
attacked -X- _ O
to -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
being -X- _ O
not -X- _ O
( -X- _ O
successfully -X- _ O
) -X- _ O
attacked -X- _ O
; -X- _ O
OR -X- _ O
is -X- _ O
the -X- _ O
ratio -X- _ O
of -X- _ O
odds -X- _ O

when -X- _ O
the -X- _ O
value -X- _ O
of -X- _ O
the -X- _ O
characteristic -X- _ O
increases -X- _ O
by -X- _ O
one -X- _ O
unit -X- _ O
( -X- _ O
Appendix -X- _ O
D -X- _ O
) -X- _ O
.Feature -X- _ O
Attacked -X- _ O
SuccessfulContentTopic47 -X- _ O
: -X- _ O
Gendery1.37 -X- _ O
( -X- _ O
* -X- _ O
* -X- _ O
* -X- _ O
) -X- _ O
1.34 -X- _ O
( -X- _ O
* -X- _ O
* -X- _ O
* -X- _ O
) -X- _ O
Kialo -X- _ O
Extremeness -X- _ O
1.51 -X- _ O
( -X- _ O
* -X- _ O
* -X- _ O
* -X- _ O
) -X- _ O
1.19 -X- _ O
( -X- _ O
* -X- _ O
* -X- _ O
* -X- _ O
) -X- _ O
Proposition -X- _ O
TypesQuestion -X- _ O
- -X- _ O
Confusiony0.97 -X- _ O
( -X- _ O
) -X- _ O
1.29 -X- _ O
( -X- _ O
* -X- _ O
) -X- _ O
Table -X- _ O
3 -X- _ O
: -X- _ O
Odds -X- _ O
ratio -X- _ O
( -X- _ O
OR -X- _ O
) -X- _ O
and -X- _ O
statistical -X- _ O
signiﬁcance -X- _ O
of -X- _ O
features -X- _ O
. -X- _ O

An -X- _ O
effect -X- _ O
is -X- _ O
positive -X- _ O
( -X- _ O
blue -X- _ O
) -X- _ O
if -X- _ O
OR -X- _ B-MetricName
> -X- _ O
1 -X- _ O
and -X- _ O
negative -X- _ O
( -X- _ O
red -X- _ O
) -X- _ O
if -X- _ O
OR -X- _ B-MetricName
< -X- _ O
1 -X- _ O
. -X- _ O

( -X- _ O
y -X- _ O
: -X- _ O
binary -X- _ O
, -X- _ O
z -X- _ O
: -X- _ O
standardized -X- _ O
/ -X- _ O
5.2 -X- _ O
External -X- _ O
Knowledge -X- _ O

The -X- _ O
Kialo -X- _ O
- -X- _ O
based -X- _ O
knowledge -X- _ O
features -X- _ O
provide -X- _ O
significant -X- _ O
information -X- _ O
about -X- _ O
whether -X- _ O
a -X- _ O
sentence -X- _ O
would -X- _ O
be -X- _ O
attacked -X- _ O
successfully -X- _ O
( -X- _ O
Table -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
frequency -X- _ O
of -X- _ O
matched -X- _ O
statements -X- _ O
in -X- _ O
Kialo -X- _ O
increases -X- _ O
twice -X- _ O
, -X- _ O
the -X- _ O
odds -X- _ O
for -X- _ O
successful -X- _ O
attack -X- _ O
increase -X- _ O
by -X- _ O
7 -X- _ O
% -X- _ O
. -X- _ O

As -X- _ O
an -X- _ O
example -X- _ O
, -X- _ O
the -X- _ O
following -X- _ O
attacked -X- _ O
sentence -X- _ O
has -X- _ O
18 -X- _ O
matched -X- _ O
statements -X- _ O
in -X- _ O
Kialo -X- _ O
. -X- _ O

I -X- _ O
feel -X- _ O
like -X- _ O
it -X- _ O
is -X- _ O
a -X- _ O
parents -X- _ O
right -X- _ O
and -X- _ O
responsibility -X- _ O
to -X- _ O
make -X- _ O
important -X- _ O
decisions -X- _ O
for -X- _ O
their -X- _ O
child -X- _ O
. -X- _ O

The -X- _ O
attractiveness -X- _ O
feature -X- _ O
has -X- _ O
a -X- _ O
stronger -X- _ O
effect -X- _ O
; -X- _ O
as -X- _ O
matched -X- _ O
statements -X- _ O
have -X- _ O
twice -X- _ O
more -X- _ O
responses -X- _ O
, -X- _ O
the -X- _ O
odds -X- _ O
for -X- _ O
successful -X- _ O
attack -X- _ O
increase -X- _ O
by -X- _ O
18 -X- _ O
% -X- _ O
, -X- _ O
probably -X- _ O
due -X- _ O
to -X- _ O
higher -X- _ O
debatability -X- _ O
. -X- _ O

A -X- _ O
sentence -X- _ O
being -X- _ O
completely -X- _ O
extreme -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
matched -X- _ O
sentences -X- _ O
have -X- _ O
only -X- _ O
pro -X- _ O
or -X- _ O
con -X- _ O
responses -X- _ O
) -X- _ O
increases -X- _ O
the -X- _ O
odds -X- _ O
for -X- _ O
successful -X- _ O
attack -X- _ O
by -X- _ O
19 -X- _ O
% -X- _ O
. -X- _ O

As -X- _ O
expected -X- _ O
, -X- _ O
the -X- _ O
argumentative -X- _ O
nature -X- _ O
of -X- _ O
Kialo -X- _ O

7allows -X- _ O
its -X- _ O
statements -X- _ O
to -X- _ O
match -X- _ O
many -X- _ O
subjective -X- _ O
sentences -X- _ O
in -X- _ O
CMV -X- _ O
and -X- _ O
serves -X- _ O
as -X- _ O
an -X- _ O
effective -X- _ O
information -X- _ O
source -X- _ O
for -X- _ O
a -X- _ O
sentence -X- _ O
’s -X- _ O
attackability -X- _ O
. -X- _ O

5.3 -X- _ O
Proposition -X- _ O
Types -X- _ O
Questions -X- _ O
, -X- _ O
especially -X- _ O
why -X- _ O
/ -X- _ O
how -X- _ O
, -X- _ O
are -X- _ O
effective -X- _ O
targets -X- _ O
for -X- _ O
successful -X- _ O
attack -X- _ O
( -X- _ O
Table -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O

Although -X- _ O
challengers -X- _ O
do -X- _ O
not -X- _ O
pay -X- _ O
special -X- _ O
attention -X- _ O
to -X- _ O
expressions -X- _ O
of -X- _ O
confusion -X- _ O
( -X- _ O
see -X- _ O
column -X- _ O
“ -X- _ O
Attacked -X- _ O
” -X- _ O
) -X- _ O
, -X- _ O
they -X- _ O
are -X- _ O
positively -X- _ O
correlated -X- _ O
with -X- _ O
successful -X- _ O
attack -X- _ O
( -X- _ O
OR=1.29 -X- _ O
) -X- _ O
. -X- _ O

Citations -X- _ O
are -X- _ O
often -X- _ O
used -X- _ O
to -X- _ O
back -X- _ O
up -X- _ O
an -X- _ O
argument -X- _ O
and -X- _ O
have -X- _ O
a -X- _ O
low -X- _ O
chance -X- _ O
of -X- _ O
being -X- _ O
attacked -X- _ O
, -X- _ O
reducing -X- _ O
the -X- _ O
odds -X- _ O
by -X- _ O
half -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
properly -X- _ O
attacking -X- _ O
citations -X- _ O
signiﬁcantly -X- _ O
increases -X- _ O
the -X- _ O
odds -X- _ O
for -X- _ O
successful -X- _ O
attack -X- _ O
by -X- _ O
17 -X- _ O
% -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
personal -X- _ O
stories -X- _ O
have -X- _ O
a -X- _ O
low -X- _ O
chance -X- _ O
of -X- _ O
being -X- _ O
attacked -X- _ O
and -X- _ O
deﬁnitions -X- _ O
do -X- _ O
not -X- _ O
attract -X- _ O
challengers -X- _ O
’ -X- _ O
attacks -X- _ O
, -X- _ O
but -X- _ O
attacking -X- _ O
them -X- _ O
is -X- _ O
found -X- _ O
to -X- _ O
be -X- _ O
effective -X- _ O
for -X- _ O
successful -X- _ O
persuasion -X- _ O
. -X- _ O

All -X- _ O
other -X- _ O
features -X- _ O
for -X- _ O
proposition -X- _ O
types -X- _ O
have -X- _ O
signiﬁcantly -X- _ O
positive -X- _ O
effects -X- _ O
on -X- _ O
being -X- _ O
attacked -X- _ O
( -X- _ O
OR=1.18–1.29 -X- _ B-MetricName
) -X- _ O
, -X- _ O
but -X- _ O
only -X- _ O
normative -X- _ O
and -X- _ O
example -X- _ O
sentences -X- _ O
are -X- _ O
correlated -X- _ O
with -X- _ O
successful -X- _ O
attack -X- _ O
. -X- _ O

5.4 -X- _ O
Tone -X- _ O
Successfully -X- _ O
attacked -X- _ O
sentences -X- _ O
tend -X- _ O
to -X- _ O
have -X- _ O
lower -X- _ O
subjectivity -X- _ O
and -X- _ O
arousal -X- _ O
( -X- _ O
Table -X- _ O
3 -X- _ O
) -X- _ O
, -X- _ O
in -X- _ O
line -X- _ O
with -X- _ O
the -X- _ O
previous -X- _ O
observation -X- _ O
that -X- _ O
they -X- _ O
are -X- _ O
more -X- _ O
data- -X- _ O
and -X- _ O
reference -X- _ O
- -X- _ O
based -X- _ O
than -X- _ O
unsuccessfully -X- _ O
attacked -X- _ O
sentences -X- _ O
. -X- _ O

In -X- _ O
contrast -X- _ O
, -X- _ O
sentences -X- _ O
about -X- _ O
concrete -X- _ O
concepts -X- _ O
are -X- _ O
found -X- _ O
to -X- _ O
be -X- _ O
less -X- _ O
attackable -X- _ O
. -X- _ O

Uncertainty -X- _ O
( -X- _ O
high -X- _ O
hedging -X- _ O
) -X- _ O
and -X- _ O
absoluteness -X- _ O
( -X- _ O
low -X- _ O
qualiﬁcation -X- _ O
) -X- _ O
both -X- _ O
increase -X- _ O
the -X- _ O
chance -X- _ O
of -X- _ O
attacks -X- _ O
, -X- _ O
which -X- _ O
aligns -X- _ O
with -X- _ O
the -X- _ O
motivating -X- _ O
factors -X- _ O
for -X- _ O
attacks -X- _ O
( -X- _ O
Table -X- _ O
2b -X- _ O
) -X- _ O
, -X- _ O
while -X- _ O
only -X- _ O
hedges -X- _ O
are -X- _ O
positively -X- _ O
correlated -X- _ O
with -X- _ O
successful -X- _ O
attacks -X- _ O
, -X- _ O
implying -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
addressing -X- _ O
the -X- _ O
arguer -X- _ O
’s -X- _ O
uncertainty -X- _ O
. -X- _ O

Negative -X- _ O
sentences -X- _ O
with -X- _ O
high -X- _ O
arousal -X- _ O
and -X- _ O
dominance -X- _ O
have -X- _ O
a -X- _ O
high -X- _ O
chance -X- _ O
of -X- _ O
being -X- _ O
attacked -X- _ O
, -X- _ O
but -X- _ O
most -X- _ O
of -X- _ O
these -X- _ O
characteristics -X- _ O
have -X- _ O
either -X- _ O
no -X- _ O
or -X- _ O
negative -X- _ O
effects -X- _ O
on -X- _ O
successful -X- _ O
attacks -X- _ O
. -X- _ O

5.5 -X- _ O
Discussion -X- _ O
We -X- _ O
have -X- _ O
found -X- _ O
some -X- _ O
evidence -X- _ O
that -X- _ O
, -X- _ O
somewhat -X- _ O
counter -X- _ O
- -X- _ O
intuitively -X- _ O
, -X- _ O
seemingly -X- _ O
evidenced -X- _ O
sentences -X- _ O
are -X- _ O
more -X- _ O
effective -X- _ O
to -X- _ O
attack -X- _ O
. -X- _ O

Such -X- _ O
sentences -X- _ O
use -X- _ O
speciﬁc -X- _ O
data -X- _ O
( -X- _ O
data -X- _ O
, -X- _ O
% -X- _ O
) -X- _ O
, -X- _ O
citations -X- _ O
, -X- _ O
and -X- _ O
deﬁnitions -X- _ O
. -X- _ O

Although -X- _ O
attacking -X- _ O
these -X- _ O
sentences -X- _ O
may -X- _ O
require -X- _ O
even -X- _ O
stronger -X- _ O
evidence -X- _ O
and -X- _ O
deeper -X- _ O
knowledge -X- _ O
, -X- _ O
arguers -X- _ O
seem -X- _ O
to -X- _ O
change -X- _ O
their -X- _ O
viewpoints -X- _ O
when -X- _ O
a -X- _ O
fact -X- _ O
they -X- _ O
believe -X- _ O
with -X- _ O
evidence -X- _ O
is -X- _ O
undermined -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
it -X- _ O
seems -X- _ O
very -X- _ O
important -X- _ O
and -X- _ O
effective -X- _ O
toidentify -X- _ O
and -X- _ O
address -X- _ O
what -X- _ O
the -X- _ O
arguer -X- _ O
is -X- _ O
confused -X- _ O
( -X- _ O
confusion -X- _ O
) -X- _ O
or -X- _ O
uncertain -X- _ O
( -X- _ O
hedges -X- _ O
) -X- _ O
about -X- _ O
. -X- _ O

Our -X- _ O
analysis -X- _ O
also -X- _ O
reveals -X- _ O
some -X- _ O
discrepancies -X- _ O
between -X- _ O
the -X- _ O
characteristics -X- _ O
of -X- _ O
sentences -X- _ O
that -X- _ O
challengers -X- _ O
commonly -X- _ O
think -X- _ O
are -X- _ O
attackable -X- _ O
and -X- _ O
those -X- _ O
that -X- _ O
are -X- _ O
indeed -X- _ O
attackable -X- _ O
. -X- _ O

Challengers -X- _ O
are -X- _ O
often -X- _ O
attracted -X- _ O
to -X- _ O
subjective -X- _ O
and -X- _ O
negative -X- _ O
sentences -X- _ O
with -X- _ O
high -X- _ O
arousal -X- _ O
, -X- _ O
but -X- _ O
successfully -X- _ O
attacked -X- _ O
sentences -X- _ O
have -X- _ O
rather -X- _ O
lower -X- _ O
subjectivity -X- _ O
and -X- _ O
arousal -X- _ O
, -X- _ O
and -X- _ O
have -X- _ O
no -X- _ O
difference -X- _ O
in -X- _ O
negativity -X- _ O
compared -X- _ O
to -X- _ O
unsuccessfully -X- _ O
attacked -X- _ O
sentences -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
challengers -X- _ O
pay -X- _ O
less -X- _ O
attention -X- _ O
to -X- _ O
personal -X- _ O
stories -X- _ O
, -X- _ O
while -X- _ O
successful -X- _ O
attacks -X- _ O
address -X- _ O
personal -X- _ O
stories -X- _ O
more -X- _ O
often -X- _ O
. -X- _ O

6 -X- _ O
Task -X- _ O
2 -X- _ O
: -X- _ O
Attackability -X- _ B-TaskName
Prediction -X- _ I-TaskName
Now -X- _ O
we -X- _ O
examine -X- _ O
how -X- _ O
well -X- _ O
computational -X- _ O
models -X- _ O
can -X- _ O
detect -X- _ B-TaskName
attackable -X- _ I-TaskName
sentences -X- _ I-TaskName
in -X- _ I-TaskName
arguments -X- _ I-TaskName
. -X- _ O

6.1 -X- _ O
Problem -X- _ O
Formulation -X- _ O
This -X- _ O
task -X- _ O
is -X- _ O
cast -X- _ O
as -X- _ O
ranking -X- _ O
sentences -X- _ O
in -X- _ O
each -X- _ O
post -X- _ O
by -X- _ O
their -X- _ O
attackability -X- _ O
scores -X- _ O
predicted -X- _ O
by -X- _ O
a -X- _ O
regression -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
consider -X- _ O
two -X- _ O
types -X- _ O
of -X- _ O
attackability -X- _ O
: -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
whether -X- _ O
a -X- _ O
sentence -X- _ O
will -X- _ O
be -X- _ O
attacked -X- _ O
or -X- _ O
not -X- _ O
, -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
whether -X- _ O
a -X- _ O
sentence -X- _ O
will -X- _ O
be -X- _ O
successfully -X- _ O
attacked -X- _ O
or -X- _ O
not -X- _ O
( -X- _ O
attacked -X- _ O
unsuccessfully -X- _ O
+ -X- _ O
unattacked -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
both -X- _ O
settings -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
posts -X- _ O
that -X- _ O
have -X- _ O
at -X- _ O
least -X- _ O
one -X- _ O
sentence -X- _ O
with -X- _ O
the -X- _ O
positive -X- _ O
label -X- _ O
( -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
three -X- _ O
evaluation -X- _ O
metrics -X- _ O
. -X- _ O

P -X- _ B-MetricName
@ -X- _ I-MetricName
1 -X- _ I-MetricName
is -X- _ O
the -X- _ O
precision -X- _ O
of -X- _ O
the -X- _ O
ﬁrst -X- _ O
ranked -X- _ O
sentence -X- _ O
, -X- _ O
measuring -X- _ O
the -X- _ O
model -X- _ O
’s -X- _ O
accuracy -X- _ O
when -X- _ O
choosing -X- _ O
one -X- _ O
sentence -X- _ O
to -X- _ O
attack -X- _ O
for -X- _ O
each -X- _ O
post -X- _ O
. -X- _ O

Less -X- _ O
strictly -X- _ O
, -X- _ O
A -X- _ B-MetricName
@ -X- _ I-MetricName
3 -X- _ I-MetricName
gives -X- _ O
a -X- _ O
score -X- _ O
of -X- _ O
1 -X- _ O
if -X- _ O
any -X- _ O
of -X- _ O
the -X- _ O
top -X- _ O
3 -X- _ O
sentences -X- _ O
is -X- _ O
a -X- _ O
positive -X- _ O
instance -X- _ O
and -X- _ O
0 -X- _ O
otherwise -X- _ O
. -X- _ O

AUC -X- _ B-MetricName
measures -X- _ O
individual -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
accuracy -X- _ O
— -X- _ O
how -X- _ O
likely -X- _ O
positive -X- _ O
sentences -X- _ O
are -X- _ O
assigned -X- _ O
higher -X- _ O
probabilities -X- _ O
. -X- _ O

6.2 -X- _ O
Comparison -X- _ O
Models -X- _ O
For -X- _ O
machine -X- _ O
learning -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
explore -X- _ O
two -X- _ O
logistic -X- _ B-MethodName
regression -X- _ I-MethodName
models -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
positive -X- _ O
label -X- _ O
for -X- _ O
each -X- _ O
sentence -X- _ O
, -X- _ O
which -X- _ O
becomes -X- _ O
the -X- _ O
sentence -X- _ O
’s -X- _ O
attackability -X- _ O
score -X- _ O
.LRis -X- _ B-MethodName

a -X- _ O
basic -X- _ O
logistic -X- _ B-MethodName
regression -X- _ I-MethodName
with -X- _ O
our -X- _ O
features11 -X- _ O
( -X- _ O
Section -X- _ O
4 -X- _ O
) -X- _ O
and -X- _ O
binary -X- _ O
variables -X- _ O
for -X- _ O
domains -X- _ O
. -X- _ O

We -X- _ O
explored -X- _ O
feature -X- _ O
selection -X- _ O
using -X- _ O
L1 -X- _ O
- -X- _ O
norm -X- _ O
and -X- _ O
regularization -X- _ O
using -X- _ O
L2 -X- _ O
- -X- _ O
norm.12BERT -X- _ B-MethodName
is -X- _ O
logistic -X- _ B-MethodName
regression -X- _ I-MethodName
where -X- _ O
our -X- _ O
features -X- _ O
are -X- _ O
replaced -X- _ O
with -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
embedding -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Contextualized -X- _ O
BERT -X- _ B-MethodName
embeddings -X- _ O
have -X- _ O
achieved -X- _ O
11We -X- _ O
tried -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
topics -X- _ O
2f10 -X- _ O
, -X- _ O
50 -X- _ O
, -X- _ O
100 -X- _ O
g -X- _ O
, -X- _ O
and -X- _ O
50 -X- _ O
has -X- _ O
the -X- _ O
best -X- _ O
AUC -X- _ B-MetricName
on -X- _ O
the -X- _ O
val -X- _ O
set -X- _ O
for -X- _ O
both -X- _ O
prediction -X- _ O
settings -X- _ O
. -X- _ O

12We -X- _ O
also -X- _ O
tried -X- _ O
a -X- _ O
multilayer -X- _ B-MethodName
perceptron -X- _ I-MethodName
to -X- _ O
model -X- _ O
feature -X- _ O
interactions -X- _ O
, -X- _ O
but -X- _ O
it -X- _ O
consistently -X- _ O
performed -X- _ O
worse -X- _ O
than -X- _ O
LR -X- _ B-MethodName
. -X- _ O

Table -X- _ O
4 -X- _ O
: -X- _ O
Prediction -X- _ O
accuracy -X- _ O
. -X- _ O

All -X- _ O
LR -X- _ B-MethodName
/ -X- _ O
BERT -X- _ B-MethodName
scores -X- _ O
( -X- _ O
rows -X- _ O
3–8 -X- _ O
) -X- _ O
have -X- _ O
standard -X- _ O
deviations -X- _ O
between -X- _ O
0.1 -X- _ O
and -X- _ O
1.1 -X- _ O
, -X- _ O
signiﬁcantly -X- _ O
outperforming -X- _ O
“ -X- _ O
Length”.yThe -X- _ O
average -X- _ O
bootstrap -X- _ O
accuracy -X- _ B-MetricName
after -X- _ O
resampling -X- _ B-HyperparameterName
100 -X- _ B-HyperparameterValue
K -X- _ I-HyperparameterValue
times -X- _ O
with -X- _ O
sample -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
200 -X- _ B-HyperparameterValue
— -X- _ O
the -X- _ O
standard -X- _ O
deviations -X- _ O
of -X- _ O
P -X- _ B-MetricName
@ -X- _ I-MetricName
1 -X- _ I-MetricName
and -X- _ O
A -X- _ B-MetricName
@ -X- _ I-MetricName
3 -X- _ I-MetricName
range -X- _ O
between -X- _ O
2.1 -X- _ B-MetricValue
and -X- _ O
3.5 -X- _ B-MetricValue
. -X- _ O

state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
performance -X- _ O
in -X- _ O
many -X- _ O
NLP -X- _ O
tasks -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
pretrained -X- _ O
, -X- _ O
uncased -X- _ O
base -X- _ O
model -X- _ O
from -X- _ O
Hugging -X- _ O
Face -X- _ O
( -X- _ O
Wolf -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
ﬁne -X- _ O
- -X- _ O
tune -X- _ O
it -X- _ O
during -X- _ O
training.13 -X- _ O

We -X- _ O
explore -X- _ O
two -X- _ O
baseline -X- _ O
models -X- _ O
. -X- _ O

Random -X- _ O
is -X- _ O
to -X- _ O
rank -X- _ O
sentences -X- _ O
randomly -X- _ O
. -X- _ O

Length -X- _ O
is -X- _ O
to -X- _ O
rank -X- _ O
sentences -X- _ O
from -X- _ O
longest -X- _ O
to -X- _ O
shortest -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
intuition -X- _ O
that -X- _ O
longer -X- _ O
sentences -X- _ O
may -X- _ O
contain -X- _ O
more -X- _ O
information -X- _ O
and -X- _ O
thus -X- _ O
more -X- _ O
content -X- _ O
to -X- _ O
attack -X- _ O
as -X- _ O
well -X- _ O
. -X- _ O

Lastly -X- _ O
, -X- _ O
we -X- _ O
estimate -X- _ O
laypeople -X- _ O
’s -X- _ O
performance -X- _ O
on -X- _ O
this -X- _ O
task -X- _ O
. -X- _ O

Three -X- _ O
undergraduate -X- _ O
students -X- _ O
each -X- _ O
read -X- _ O
100 -X- _ O
posts -X- _ O
and -X- _ O
rank -X- _ O
three -X- _ O
sentences -X- _ O
to -X- _ O
attack -X- _ O
for -X- _ O
each -X- _ O
post -X- _ O
. -X- _ O

Posts -X- _ O
that -X- _ O
have -X- _ O
at -X- _ O
least -X- _ O
one -X- _ O
positive -X- _ O
instance -X- _ O
are -X- _ O
randomly -X- _ O
selected -X- _ O
from -X- _ O
the -X- _ O
test -X- _ O
set.14 -X- _ O
6.3 -X- _ O
Results -X- _ O
All -X- _ O
computational -X- _ O
models -X- _ O
were -X- _ O
run -X- _ O
10 -X- _ O
times -X- _ O
, -X- _ O
and -X- _ O
their -X- _ O
average -X- _ O
accuracy -X- _ O
is -X- _ O
reported -X- _ O
in -X- _ O
Table -X- _ O
4 -X- _ O
. -X- _ O

Both -X- _ O
the -X- _ O
LR -X- _ B-MethodName
and -X- _ O
BERT -X- _ B-MethodName
models -X- _ O
signiﬁcantly -X- _ O
outperform -X- _ O
the -X- _ O
baselines -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
performs -X- _ O
best -X- _ O
. -X- _ O

For -X- _ O
predicting -X- _ B-TaskName
attacked -X- _ I-TaskName
sentences -X- _ I-TaskName
, -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
’s -X- _ O
top -X- _ B-MetricName
1 -X- _ I-MetricName
decisions -X- _ I-MetricName
match -X- _ O
the -X- _ O
gold -X- _ O
standard -X- _ O
50 -X- _ O
% -X- _ O
of -X- _ O
the -X- _ O
time -X- _ O
; -X- _ O
its -X- _ O
decisions -X- _ O
match -X- _ O
78 -X- _ O
% -X- _ O
of -X- _ O
the -X- _ O
time -X- _ O
when -X- _ O
three -X- _ O
sentences -X- _ O
are -X- _ O
chosen -X- _ O
. -X- _ O

Predicting -X- _ B-TaskName
successfully -X- _ I-TaskName
attacked -X- _ I-TaskName
sentences -X- _ I-TaskName
is -X- _ O
harder -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
performance -X- _ O
gap -X- _ O
between -X- _ O
our -X- _ O
models -X- _ O
and -X- _ O
the -X- _ O
baselines -X- _ O
gets -X- _ O
larger -X- _ O
. -X- _ O

The -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
’s -X- _ O
top -X- _ B-MetricName
1 -X- _ I-MetricName
decisions -X- _ I-MetricName
match -X- _ O
the -X- _ O
gold -X- _ O
standard -X- _ O
28 -X- _ O
% -X- _ O
of -X- _ O
the -X- _ O
time -X- _ O
— -X- _ O
a -X- _ O
27 -X- _ O
% -X- _ O
and -X- _ O
10 -X- _ O
% -X- _ O
boost -X- _ O
from -X- _ O
random -X- _ O
and -X- _ O
length -X- _ O
- -X- _ O
based -X- _ O
performance -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

13Details -X- _ O
for -X- _ O
reproducibility -X- _ O
are -X- _ O
in -X- _ O
Appendix -X- _ O
F. -X- _ O
14We -X- _ O
were -X- _ O
interested -X- _ O
in -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
young -X- _ O
adults -X- _ O
who -X- _ O
are -X- _ O
academically -X- _ O
active -X- _ O
and -X- _ O
have -X- _ O
a -X- _ O
moderate -X- _ O
level -X- _ O
of -X- _ O
life -X- _ O
experience -X- _ O
. -X- _ O

Their -X- _ O
performance -X- _ O
may -X- _ O
not -X- _ O
represent -X- _ O
the -X- _ O
general -X- _ O
population -X- _ O
, -X- _ O
though -X- _ O
. -X- _ O

To -X- _ O
examine -X- _ O
the -X- _ O
contribution -X- _ O
of -X- _ O
each -X- _ O
feature -X- _ O
category -X- _ O
, -X- _ O
we -X- _ O
did -X- _ O
ablation -X- _ O
tests -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
best -X- _ O
performing -X- _ O
LR -X- _ B-MethodName
model -X- _ O
( -X- _ O
Table -X- _ O
4 -X- _ O
rows -X- _ O
4–7 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
two -X- _ O
prediction -X- _ O
settings -X- _ O
show -X- _ O
similar -X- _ O
tendencies -X- _ O
. -X- _ O

Regarding -X- _ O
P -X- _ B-MetricName
@ -X- _ I-MetricName
1 -X- _ I-MetricName
for -X- _ O
successful -X- _ O
attack -X- _ O
, -X- _ O
content -X- _ O
has -X- _ O
the -X- _ O
highest -X- _ O
contribution -X- _ O
, -X- _ O
followed -X- _ O
by -X- _ O
knowledge -X- _ O
, -X- _ O
proposition -X- _ O
types -X- _ O
, -X- _ O
and -X- _ O
tone -X- _ O
. -X- _ O

This -X- _ O
result -X- _ O
reafﬁrms -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
content -X- _ O
for -X- _ O
a -X- _ O
sentence -X- _ O
’s -X- _ O
attackability -X- _ O
. -X- _ O

But -X- _ O
the -X- _ O
other -X- _ O
features -X- _ O
still -X- _ O
have -X- _ O
signiﬁcant -X- _ O
contribution -X- _ O
, -X- _ O
yielding -X- _ O
higher -X- _ O
P -X- _ B-MetricName
@ -X- _ I-MetricName
1 -X- _ I-MetricName
and -X- _ O
AUC -X- _ B-MetricName
( -X- _ O
Table -X- _ O
4 -X- _ O
row -X- _ O
4 -X- _ O
) -X- _ O
than -X- _ O
the -X- _ O
baselines -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
worth -X- _ O
noting -X- _ O
that -X- _ O
our -X- _ O
features -X- _ O
, -X- _ O
despite -X- _ O
the -X- _ O
lower -X- _ O
accuracy -X- _ O
than -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
, -X- _ O
are -X- _ O
clearly -X- _ O
informative -X- _ O
of -X- _ O
attackability -X- _ O
prediction -X- _ O
as -X- _ O
Table -X- _ O
4 -X- _ O
row -X- _ O
3 -X- _ O
shows -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
since -X- _ O
they -X- _ O
directly -X- _ O
operationalize -X- _ O
the -X- _ O
sentence -X- _ O
characteristics -X- _ O
we -X- _ O
compiled -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
pretty -X- _ O
transparent -X- _ O
that -X- _ O
they -X- _ O
capture -X- _ O
relevant -X- _ O
information -X- _ O
that -X- _ O
contributes -X- _ O
to -X- _ O
sentence -X- _ O
attackability -X- _ O
and -X- _ O
help -X- _ O
us -X- _ O
better -X- _ O
understand -X- _ O
what -X- _ O
characteristics -X- _ O
have -X- _ O
positive -X- _ O
and -X- _ O
negative -X- _ O
signals -X- _ O
for -X- _ O
sentence -X- _ O
attackability -X- _ O
. -X- _ O

We -X- _ O
speculate -X- _ O
that -X- _ O
transformer -X- _ O
models -X- _ O
like -X- _ O
BERT -X- _ B-MethodName
are -X- _ O
capable -X- _ O
of -X- _ O
encoding -X- _ O
these -X- _ O
characteristics -X- _ O
more -X- _ O
sophisticatedly -X- _ O
and -X- _ O
may -X- _ O
include -X- _ O
some -X- _ O
additional -X- _ O
information -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
lexical -X- _ O
patterns -X- _ O
, -X- _ O
leading -X- _ O
to -X- _ O
higher -X- _ O
accuracy -X- _ O
. -X- _ O

But -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
less -X- _ O
clear -X- _ O
exactly -X- _ O
what -X- _ O
they -X- _ O
capture -X- _ O
and -X- _ O
whether -X- _ O
they -X- _ O
capture -X- _ O
relevant -X- _ O
information -X- _ O
or -X- _ O
irrelevant -X- _ O
statistics -X- _ O
, -X- _ O
as -X- _ O
is -X- _ O
often -X- _ O
the -X- _ O
case -X- _ O
in -X- _ O
computational -X- _ O
argumentation -X- _ O
( -X- _ O
Niven -X- _ O
and -X- _ O
Kao -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

Figure -X- _ O
2 -X- _ O
illustrates -X- _ O
how -X- _ O
LR -X- _ B-MethodName
allows -X- _ O
us -X- _ O
to -X- _ O
interpret -X- _ O
the -X- _ O
contribution -X- _ O
of -X- _ O
different -X- _ O
features -X- _ O
to -X- _ O
attackability -X- _ O
, -X- _ O
by -X- _ O
visualizing -X- _ O
a -X- _ O
post -X- _ O
with -X- _ O
important -X- _ O
features -X- _ O
highlighted -X- _ O
. -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
external -X- _ O
knowledge -X- _ O
plays -X- _ O
a -X- _ O
crucial -X- _ O
role -X- _ O
in -X- _ O
this -X- _ O
post -X- _ O
; -X- _ O
all -X- _ O
successfully -X- _ O
attacked -X- _ O
sentences -X- _ O
match -X- _ O
substantially -X- _ O
more -X- _ O
Kialo -X- _ O
statements -X- _ O
than -X- _ O
other -X- _ O
sentences -X- _ O
. -X- _ O

The -X- _ O
attackability -X- _ O
scores -X- _ O
of -X- _ O
these -X- _ O
sentences -X- _ O
are -X- _ O
also -X- _ O
increased -X- _ O
by -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
hypotheticals -X- _ O
and -X- _ O
certain -X- _ O
n -X- _ O
- -X- _ O
grams -X- _ O
like -X- _ O
could -X- _ O
. -X- _ O

These -X- _ O
features -X- _ O
align -X- _ O
well -X- _ O
with -X- _ O
the -X- _ O
actual -X- _ O
attacks -X- _ O
by -X- _ O
successful -X- _ O
challengers -X- _ O
. -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
they -X- _ O
pointed -X- _ O
out -X- _ O
that -X- _ O
the -X- _ O
expulsion -X- _ O
of -X- _ O
Russian -X- _ O
diplomats -X- _ O
( -X- _ O
sentence -X- _ O
2 -X- _ O
) -X- _ O
is -X- _ O
not -X- _ O
an -X- _ O
aggressive -X- _ O
reaction -X- _ O
because -X- _ O
the -X- _ O
diplomats -X- _ O
can -X- _ O
be -X- _ O
simply -X- _ O
replaced -X- _ O
with -X- _ O
new -X- _ O
ones -X- _ O
. -X- _ O

Kialo -X- _ O
has -X- _ O
a -X- _ O
discussion -X- _ O
on -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
the -X- _ O
U.S. -X- _ O
and -X- _ O
Russia -X- _ O
, -X- _ O
and -X- _ O
one -X- _ O
statement -X- _ O
puts -X- _ O
forward -X- _ O
exactly -X- _ O
the -X- _ O
same -X- _ O
point -X- _ O
that -X- _ O
the -X- _ O
expulsion -X- _ O
was -X- _ O
a -X- _ O
forceful -X- _ O
- -X- _ O
looking -X- _ O
but -X- _ O
indeed -X- _ O
a -X- _ O
nice -X- _ O
gesture -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
a -X- _ O
successful -X- _ O
challenger -X- _ O
pointed -X- _ O
out -X- _ O
the -X- _ O
consistent -X- _ O
attitude -X- _ O
of -X- _ O
the -X- _ O
U.S. -X- _ O
toward -X- _ O
regime -X- _ O
change -X- _ O
in -X- _ O
North -X- _ O
Korea -X- _ O
( -X- _ O
sentence -X- _ O
3 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
North -X- _ O
Korean -X- _ O
regime -X- _ O
is -X- _ O
a -X- _ O
controversial -X- _ O
topic -X- _ O
in -X- _ O
Kialo -X- _ O
. -X- _ O

Lastly -X- _ O
, -X- _ O

So -X- _ O
let -X- _ O
's -X- _ O
say -X- _ O
I -X- _ O
'm -X- _ O
good -X- _ O
friends -X- _ O
with -X- _ O
Amanda -X- _ O
and -X- _ O
Bailey -X- _ O
. -X- _ O

I -X- _ O
' -X- _ O
m -X- _ O
compatible -X- _ O
with -X- _ O
both -X- _ O
of -X- _ O
them -X- _ O
on -X- _ O
a -X- _ O
platonic -X- _ O
level -X- _ O
, -X- _ O
but -X- _ O
I -X- _ O
only -X- _ O
take -X- _ O
a -X- _ O
romantic -X- _ O
interest -X- _ O
in -X- _ O
Bailey -X- _ O
because -X- _ O
she -X- _ O
's -X- _ O
( -X- _ O
physically -X- _ O
) -X- _ O
my -X- _ O
type -X- _ O
. -X- _ O

Not -X- _ O
to -X- _ O
say -X- _ O
that -X- _ O
Amanda -X- _ O
is -X- _ O
ugly -X- _ O
, -X- _ O
just -X- _ O
that -X- _ O
I -X- _ O
'm -X- _ O
not -X- _ O
really -X- _ O
into -X- _ O
her -X- _ O
body -X- _ O
structure -X- _ O
. -X- _ O

Another -X- _ O
piece -X- _ O
of -X- _ O
evidence -X- _ O
to -X- _ O
support -X- _ O
this -X- _ O
is -X- _ O
when -X- _ O
you -X- _ O
feel -X- _ O
attracted -X- _ O
to -X- _ O
a -X- _ O
complete -X- _ O
stranger -X- _ O
, -X- _ O
because -X- _ O
of -X- _ O
their -X- _ O
physical -X- _ O
appearance -X- _ O
. -X- _ O

You -X- _ O
know -X- _ O
absolutely -X- _ O
nothing -X- _ O
about -X- _ O
them -X- _ O
yet -X- _ O
, -X- _ O
you -X- _ O
could -X- _ O
envision -X- _ O
a -X- _ O
happy -X- _ O
relationship -X- _ O
with -X- _ O
them -X- _ O
just -X- _ O
from -X- _ O
their -X- _ O
looks -X- _ O
. -X- _ O

I -X- _ O
feel -X- _ O
this -X- _ O
way -X- _ O
because -X- _ O
many -X- _ O
times -X- _ O
when -X- _ O
I -X- _ O
'm -X- _ O
hanging -X- _ O
out -X- _ O
with -X- _ O
my -X- _ O
friends -X- _ O
( -X- _ O
of -X- _ O
both -X- _ O
genders -X- _ O
) -X- _ O

I -X- _ O
think -X- _ O
to -X- _ O
myself -X- _ O
" -X- _ O
wow -X- _ O
we -X- _ O
'd -X- _ O
make -X- _ O
such -X- _ O
a -X- _ O
good -X- _ O
couple -X- _ O
" -X- _ O
but -X- _ O
even -X- _ O
so -X- _ O
do -X- _ O
n't -X- _ O
feel -X- _ O
the -X- _ O
desire -X- _ O
to -X- _ O
enter -X- _ O
a -X- _ O
relationship -X- _ O
with -X- _ O
them -X- _ O
. -X- _ O

Topic15 -X- _ O
( -X- _ O
-0.18 -X- _ O
) -X- _ O

Personal -X- _ O
( -X- _ O
-0.20 -X- _ O
) -X- _ O
Erroneous -X- _ O
1 -X- _ O
( -X- _ O
t3_87as7t -X- _ O
) -X- _ O
Topic15 -X- _ O
( -X- _ O
-0.18 -X- _ O
) -X- _ O
Personal -X- _ O
( -X- _ O
-0.20 -X- _ O
) -X- _ O
Topic15 -X- _ O
( -X- _ O
-0.18 -X- _ O
) -X- _ O
Personal -X- _ O
( -X- _ O
-0.20 -X- _ O
) -X- _ O
Use -X- _ O
of -X- _ O
" -X- _ O
You -X- _ O
" -X- _ O
( -X- _ O
-0.15 -X- _ O
) -X- _ O

Topic15 -X- _ O
( -X- _ O
-0.18 -X- _ O
) -X- _ O
Use -X- _ O
of -X- _ O
" -X- _ O
You -X- _ O
" -X- _ O
( -X- _ O
-0.15 -X- _ O
) -X- _ O

Topic15 -X- _ O
( -X- _ O
-0.18 -X- _ O
) -X- _ O
KialoFreq -X- _ O
( -X- _ O
0.23 -X- _ O
) -X- _ O

Topic15 -X- _ O
( -X- _ O
-0.18 -X- _ O
) -X- _ O
Use -X- _ O
of -X- _ O
" -X- _ O
We -X- _ O
" -X- _ O
( -X- _ O
-0.19 -X- _ O
) -X- _ O
Personal -X- _ O
( -X- _ O
-0.20 -X- _ O
) -X- _ O
I -X- _ O
realize -X- _ O
I -X- _ O
have -X- _ O
a -X- _ O
bias -X- _ O
because -X- _ O
I -X- _ O
grew -X- _ O
up -X- _ O
in -X- _ O
a -X- _ O
big -X- _ O
city -X- _ O
in -X- _ O
Canada -X- _ O
and -X- _ O
not -X- _ O
a -X- _ O
single -X- _ O
person -X- _ O
I -X- _ O
knew -X- _ O
owned -X- _ O
a -X- _ O
gun -X- _ O
and -X- _ O
most -X- _ O
law -X- _ O
enforcement -X- _ O
ofﬁcers -X- _ O

I -X- _ O
saw -X- _ O
on -X- _ O
the -X- _ O
street -X- _ O
also -X- _ O
did -X- _ O
n't -X- _ O
carry -X- _ O
guns -X- _ O
and -X- _ O
I -X- _ O
perceive -X- _ O
Canada -X- _ O
to -X- _ O
generally -X- _ O
be -X- _ O
safer -X- _ O
than -X- _ O
the -X- _ O
open -X- _ O
carry -X- _ O
US -X- _ O
state -X- _ O
that -X- _ O
I -X- _ O
now -X- _ O
live -X- _ O
in -X- _ O
. -X- _ O

I -X- _ O
see -X- _ O
zero -X- _ O
reason -X- _ O
to -X- _ O
own -X- _ O
a -X- _ O
gun -X- _ O
, -X- _ O
not -X- _ O
even -X- _ O
for -X- _ O
hunting -X- _ O
. -X- _ O

I -X- _ O
think -X- _ O
hunters -X- _ O
should -X- _ O
use -X- _ O
bows -X- _ O
and -X- _ O
arrows -X- _ O
. -X- _ O

I -X- _ O
admit -X- _ O
I -X- _ O
've -X- _ O
never -X- _ O
been -X- _ O
hunting -X- _ O
myself -X- _ O
. -X- _ O

I -X- _ O
believe -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
guns -X- _ O
in -X- _ O
society -X- _ O
makes -X- _ O
society -X- _ O
less -X- _ O
safe -X- _ O
and -X- _ O
we -X- _ O
would -X- _ O
all -X- _ O
be -X- _ O
safer -X- _ O
if -X- _ O
there -X- _ O
were -X- _ O
fewer -X- _ O
of -X- _ O
them -X- _ O
and -X- _ O
they -X- _ O
were -X- _ O
far -X- _ O
more -X- _ O
difﬁcult -X- _ O
and -X- _ O
expensive -X- _ O
to -X- _ O
buy -X- _ O
on -X- _ O
the -X- _ O
black -X- _ O
market -X- _ O
rather -X- _ O
than -X- _ O
being -X- _ O
able -X- _ O
to -X- _ O
pick -X- _ O
one -X- _ O
up -X- _ O
easily -X- _ O
at -X- _ O
a -X- _ O
gun -X- _ O
show -X- _ O
parking -X- _ O
lot -X- _ O
using -X- _ O
cash -X- _ O
and -X- _ O
with -X- _ O
no -X- _ O
background -X- _ O
check -X- _ O
. -X- _ O

I -X- _ O
know -X- _ O
that -X- _ O
violence -X- _ O
can -X- _ O
be -X- _ O
committed -X- _ O
with -X- _ O
other -X- _ O
weapons -X- _ O
such -X- _ O
as -X- _ O
knives -X- _ O
or -X- _ O
running -X- _ O
someone -X- _ O
over -X- _ O
with -X- _ O
a -X- _ O
car -X- _ O
. -X- _ O

But -X- _ O
we -X- _ O
have -X- _ O
laws -X- _ O
about -X- _ O
who -X- _ O
can -X- _ O
drive -X- _ O
a -X- _ O
car -X- _ O
and -X- _ O
it -X- _ O
's -X- _ O
actually -X- _ O
more -X- _ O
difﬁcult -X- _ O
to -X- _ O
kill -X- _ O
people -X- _ O
with -X- _ O
such -X- _ O
things -X- _ O
and -X- _ O
l -X- _ O
e -X- _ O
s -X- _ O
s -X- _ O
efﬁcient -X- _ O
. -X- _ O

KialoFreq -X- _ O
( -X- _ O
0.78 -X- _ O
) -X- _ O
Comparison -X- _ O
( -X- _ O
0.20 -X- _ O
) -X- _ O

Topic43 -X- _ O
( -X- _ O
0.19 -X- _ O
) -X- _ O
KialoAttr -X- _ O
( -X- _ O
0.13 -X- _ O
) -X- _ O
Use -X- _ O
of -X- _ O
" -X- _ O
We -X- _ O
" -X- _ O
( -X- _ O
-0.19 -X- _ O
) -X- _ O
Personal -X- _ O
( -X- _ O
-0.20 -X- _ O
) -X- _ O
Erroneous -X- _ O
2 -X- _ O
( -X- _ O
t3_95wq12 -X- _ O
) -X- _ O
Topic43 -X- _ O
( -X- _ O
0.19 -X- _ O
) -X- _ O
Topic43 -X- _ O
( -X- _ O
0.19 -X- _ O
) -X- _ O
Normative -X- _ O
( -X- _ O
0.18 -X- _ O
) -X- _ O
Topic43 -X- _ O
( -X- _ O
0.19 -X- _ O
) -X- _ O

Personal -X- _ O
( -X- _ O
-0.20 -X- _ O
) -X- _ O
KialoFreq -X- _ O
( -X- _ O
0.75 -X- _ O
) -X- _ O
Comparison -X- _ O
( -X- _ O
0.20 -X- _ O
) -X- _ O
Topic43 -X- _ O
( -X- _ O
0.19 -X- _ O
) -X- _ O
Prediction -X- _ O
( -X- _ O
0.12 -X- _ O
) -X- _ O
KialoAttr -X- _ O
( -X- _ O
0.06 -X- _ O
) -X- _ O
KialoExtr -X- _ O
( -X- _ O
-0.12 -X- _ O
) -X- _ O
Use -X- _ O
of -X- _ O
" -X- _ O
We -X- _ O
" -X- _ O
( -X- _ O
-0.19 -X- _ O
) -X- _ O
Topic43 -X- _ O
( -X- _ O
0.19 -X- _ O
) -X- _ O
Example -X- _ O
( -X- _ O
0.11 -X- _ O
) -X- _ O
KialoFreq -X- _ O
( -X- _ O
0.36 -X- _ O
) -X- _ O
Topic32 -X- _ O
( -X- _ O
0.11 -X- _ O
) -X- _ O
Use -X- _ O
of -X- _ O
" -X- _ O
We -X- _ O
" -X- _ O
( -X- _ O
-0.19 -X- _ O
) -X- _ O
Figure -X- _ O
2 -X- _ O
: -X- _ O
Prediction -X- _ O
visualization -X- _ O
. -X- _ O

Background -X- _ O
color -X- _ O
indicates -X- _ O
predicted -X- _ O
attackability -X- _ O
( -X- _ O
blue -X- _ O
: -X- _ O
high -X- _ O
, -X- _ O
red -X- _ O
: -X- _ O
low -X- _ O
) -X- _ O
. -X- _ O

Successfully -X- _ O
attacked -X- _ O
sentences -X- _ O
are -X- _ O
underlined -X- _ O
. -X- _ O

Features -X- _ O
with -X- _ O
high -X- _ O
/ -X- _ O
low -X- _ O
weights -X- _ O
are -X- _ O
indicated -X- _ O
with -X- _ O
blue -X- _ O
/ -X- _ O
red -X- _ O
. -X- _ O

one -X- _ O
successful -X- _ O
challenger -X- _ O
attacked -X- _ O
the -X- _ O
hypothetical -X- _ O
outcomes -X- _ O
in -X- _ O
sentences -X- _ O
4 -X- _ O
and -X- _ O
5 -X- _ O
, -X- _ O
pointing -X- _ O
out -X- _ O
that -X- _ O
those -X- _ O
outcomes -X- _ O
are -X- _ O
not -X- _ O
plausible -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
LR -X- _ B-MethodName
model -X- _ O
also -X- _ O
captures -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
hypothetical -X- _ O
and -X- _ O
the -X- _ O
word -X- _ O
could -X- _ O
as -X- _ O
highly -X- _ O
indicative -X- _ O
of -X- _ O
attackability -X- _ O
. -X- _ O

More -X- _ O
successful -X- _ O
and -X- _ O
erroneous -X- _ O
cases -X- _ O
are -X- _ O
in -X- _ O
Appendix -X- _ O
H. -X- _ O
Laypeople -X- _ O
perform -X- _ O
signiﬁcantly -X- _ O
better -X- _ O
than -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
for -X- _ O
predicting -X- _ O
attacked -X- _ O
sentences -X- _ O
, -X- _ O
but -X- _ O
only -X- _ O
comparably -X- _ O
well -X- _ O
for -X- _ O
successfully -X- _ O
attacked -X- _ O
sentences -X- _ O
( -X- _ O
Table -X- _ O
4 -X- _ O
row -X- _ O
9 -X- _ O
) -X- _ O
. -X- _ O

Persuasive -X- _ O
argumentation -X- _ O
in -X- _ O
CMV -X- _ O
requires -X- _ O
substantial -X- _ O
domain -X- _ O
knowledge -X- _ O
, -X- _ O
but -X- _ O
laypeople -X- _ O
do -X- _ O
not -X- _ O
have -X- _ O
such -X- _ O
expertise -X- _ O
for -X- _ O
many -X- _ O
domains -X- _ O
. -X- _ O

The -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
seems -X- _ O
to -X- _ O
take -X- _ O
advantage -X- _ O
of -X- _ O
the -X- _ O
large -X- _ O
data -X- _ O
and -X- _ O
encodes -X- _ O
useful -X- _ O
linguistic -X- _ O
patterns -X- _ O
that -X- _ O
are -X- _ O
predictive -X- _ O
of -X- _ O
attackability -X- _ O
. -X- _ O

A -X- _ O
similar -X- _ O
tendency -X- _ O
has -X- _ O
been -X- _ O
observed -X- _ O
in -X- _ O
predicting -X- _ O
persuasive -X- _ O
refutation -X- _ O
( -X- _ O
Guo -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
a -X- _ O
machine -X- _ O
- -X- _ O
learned -X- _ O
model -X- _ O
outperformed -X- _ O
laypeople -X- _ O
. -X- _ O

Nevertheless -X- _ O
, -X- _ O
in -X- _ O
our -X- _ O
task -X- _ O
, -X- _ O
the -X- _ O
humans -X- _ O
and -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
seem -X- _ O
to -X- _ O
make -X- _ O
similar -X- _ O
decisions -X- _ O
; -X- _ O
the -X- _ O
association -X- _ O
between -X- _ O
their -X- _ O
choices -X- _ O
of -X- _ O
sentences -X- _ O
is -X- _ O
high -X- _ O
, -X- _ O
with -X- _ O
odds -X- _ B-MetricName
ratios -X- _ I-MetricName
ranging -X- _ O
between -X- _ O
3.43 -X- _ B-MetricValue
( -X- _ O
top -X- _ O
1 -X- _ O
) -X- _ O
and -X- _ O
3.33 -X- _ B-MetricValue
( -X- _ O
top -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O

Interestingly -X- _ O
, -X- _ O
the -X- _ O
LR -X- _ B-MethodName
model -X- _ O
has -X- _ O
a -X- _ O
low -X- _ O
association -X- _ O
with -X- _ O
the -X- _ O
human -X- _ O
decisions -X- _ O
for -X- _ O
top -X- _ O
1 -X- _ O
( -X- _ O
OR=2.65 -X- _ B-MetricName
) -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
association -X- _ O
exceeds -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
for -X- _ O
top -X- _ O
3 -X- _ O
( -X- _ O
OR=3.69 -X- _ B-MetricName
) -X- _ O
. -X- _ O

It -X- _ O
would -X- _ O
be -X- _ O
interesting -X- _ O
to -X- _ O
further -X- _ O
examine -X- _ O
the -X- _ O
similarities -X- _ O
and -X- _ O
differencesin -X- _ O
how -X- _ O
humans -X- _ O
and -X- _ O
machines -X- _ O
choose -X- _ O
sentences -X- _ O
to -X- _ O
attack -X- _ O
. -X- _ O

7 -X- _ O
Conclusion -X- _ O
We -X- _ O
studied -X- _ O
how -X- _ O
to -X- _ O
detect -X- _ B-TaskName
attackable -X- _ I-TaskName
sentences -X- _ I-TaskName
in -X- _ I-TaskName
arguments -X- _ I-TaskName
for -X- _ O
successful -X- _ O
persuasion -X- _ O
. -X- _ O

Using -X- _ O
online -X- _ O
arguments -X- _ O
, -X- _ O
we -X- _ O
demonstrated -X- _ O
that -X- _ O
a -X- _ O
sentence -X- _ O
’s -X- _ O
attackability -X- _ O
is -X- _ O
associated -X- _ O
with -X- _ O
many -X- _ O
of -X- _ O
its -X- _ O
characteristics -X- _ O
regarding -X- _ O
its -X- _ O
content -X- _ O
, -X- _ O
proposition -X- _ O
types -X- _ O
, -X- _ O
and -X- _ O
tone -X- _ O
, -X- _ O
and -X- _ O
that -X- _ O
Kialo -X- _ O
provides -X- _ O
useful -X- _ O
information -X- _ O
about -X- _ O
attackability -X- _ O
. -X- _ O

Based -X- _ O
on -X- _ O
these -X- _ O
ﬁndings -X- _ O
we -X- _ O
demonstrated -X- _ O
that -X- _ O
machine -X- _ O
learning -X- _ O
models -X- _ O
can -X- _ O
automatically -X- _ O
detect -X- _ B-TaskName
attackable -X- _ I-TaskName
sentences -X- _ I-TaskName
, -X- _ O
comparably -X- _ O
well -X- _ O
to -X- _ O
laypeople -X- _ O
. -X- _ O

Our -X- _ O
work -X- _ O
contributes -X- _ O
a -X- _ O
new -X- _ O
application -X- _ O
to -X- _ O
the -X- _ O
growing -X- _ O
literature -X- _ O
on -X- _ O
causal -X- _ O
inference -X- _ O
from -X- _ O
text -X- _ O
( -X- _ O
Egami -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
setting -X- _ O
of -X- _ O
“ -X- _ O
text -X- _ O
as -X- _ O
a -X- _ O
treatment -X- _ O
” -X- _ O
. -X- _ O

Speciﬁcally -X- _ O
, -X- _ O
our -X- _ O
ﬁndings -X- _ O
in -X- _ O
Section -X- _ O
5 -X- _ O
pave -X- _ O
the -X- _ O
way -X- _ O
towards -X- _ O
answering -X- _ O
the -X- _ O
causal -X- _ O
question -X- _ O
: -X- _ O
would -X- _ O
attacking -X- _ O
a -X- _ O
certain -X- _ O
type -X- _ O
of -X- _ O
sentence -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
questions -X- _ O
or -X- _ O
expressions -X- _ O
of -X- _ O
confusion -X- _ O
) -X- _ O
in -X- _ O
an -X- _ O
argument -X- _ O
increase -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
persuading -X- _ O
the -X- _ O
opinion -X- _ O
holder -X- _ O
? -X- _ O

While -X- _ O
our -X- _ O
ﬁndings -X- _ O
suggest -X- _ O
initial -X- _ O
hypotheses -X- _ O
about -X- _ O
the -X- _ O
characteristics -X- _ O
of -X- _ O
sentences -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
successfully -X- _ O
attacked -X- _ O
, -X- _ O
establishing -X- _ O
causality -X- _ O
in -X- _ O
a -X- _ O
credible -X- _ O
manner -X- _ O
would -X- _ O
require -X- _ O
addressing -X- _ O
confounders -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
challenger -X- _ O
’s -X- _ O
reputation -X- _ O
( -X- _ O
Manzoor -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
persuasive -X- _ O
skill -X- _ O
reﬂected -X- _ O
in -X- _ O
their -X- _ O
attack -X- _ O
( -X- _ O
Tan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
leave -X- _ O
this -X- _ O
analysis -X- _ O
to -X- _ O
future -X- _ O
work -X- _ O
. -X- _ O

Our -X- _ O
work -X- _ O
could -X- _ O
be -X- _ O
improved -X- _ O
also -X- _ O
by -X- _ O
including -X- _ O
discourse -X- _ O
properties -X- _ O
( -X- _ O
coherence -X- _ O
, -X- _ O
cohesiveness -X- _ O
) -X- _ O
. -X- _ O

Further -X- _ O
, -X- _ O
argumentation -X- _ O
structure -X- _ O
( -X- _ O
support -X- _ O
relations -X- _ O
between -X- _ O
sentences -X- _ O
or -X- _ O
lack -X- _ O
thereof -X- _ O
) -X- _ O
might -X- _ O
provide -X- _ O
useful -X- _ O
information -X- _ O
about -X- _ O
each -X- _ O
sentence -X- _ O
’s -X- _ O
attackability -X- _ O
. -X- _ O

Acknowledgments -X- _ O
This -X- _ O
research -X- _ O
was -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
Kwanjeong -X- _ O
Educational -X- _ O
Foundation -X- _ O
. -X- _ O

References -X- _ O
Aristotle -X- _ O
. -X- _ O

2007 -X- _ O
. -X- _ O

On -X- _ O
Rhetoric -X- _ O
. -X- _ O

BERT -X- _ O
: -X- _ O
Pre -X- _ O
- -X- _ O
training -X- _ O
of -X- _ O
Deep -X- _ O
Bidirectional -X- _ O
Transformers -X- _ O
for -X- _ O
Language -X- _ O
Understanding -X- _ O
. -X- _ O

Journal -X- _ O
of -X- _ O
Language -X- _ O
and -X- _ O
Social -X- _ O
Psychology -X- _ O
, -X- _ O
Naoki -X- _ O
Egami -X- _ O
, -X- _ O
Christian -X- _ O
J -X- _ O
Fong -X- _ O
, -X- _ O
Justin -X- _ O
Grimmer -X- _ O
, -X- _ O
Margaret -X- _ O
E -X- _ O
Roberts -X- _ O
, -X- _ O
and -X- _ O
Brandon -X- _ O
M -X- _ O
Stewart -X- _ O
. -X- _ O

B.1 -X- _ O
UKP -X- _ O
Sentence -X- _ O
Embedding -X- _ O
- -X- _ O
Based -X- _ O
Retrieval -X- _ O
We -X- _ O
measured -X- _ O
the -X- _ O
similarity -X- _ O
between -X- _ O
CMV -X- _ O
sentences -X- _ O
and -X- _ O
Kialo -X- _ O
statements -X- _ O
using -X- _ O
the -X- _ O
UKP -X- _ O
sentence -X- _ O
embedding -X- _ O
— -X- _ O
BERT -X- _ O
embeddings -X- _ O
ﬁne -X- _ O
- -X- _ O
tuned -X- _ O
to -X- _ O
measure -X- _ O
argument -X- _ O
similarity -X- _ O
( -X- _ O
Reimers -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

Speciﬁcally -X- _ O
, -X- _ O
the -X- _ O
authors -X- _ O
provide -X- _ O
pretrained -X- _ O
embeddings -X- _ O
constructed -X- _ O
by -X- _ O
appending -X- _ O
a -X- _ O
ﬁnal -X- _ O
softmax -X- _ O
layer -X- _ O
to -X- _ O
BERT -X- _ O
to -X- _ O
predict -X- _ O
a -X- _ O
numerical -X- _ O
dissimilarity -X- _ O
score -X- _ O
between -X- _ O
0 -X- _ O
and -X- _ O
1 -X- _ O
for -X- _ O
each -X- _ O
sentence -X- _ O
pair -X- _ O
in -X- _ O
the -X- _ O
UKP -X- _ O
ASPECT -X- _ O
corpus -X- _ O
. -X- _ O

19F -X- _ O
Reproducibility -X- _ O
Checklist -X- _ O
Criterion -X- _ O
LR -X- _ O
BERT -X- _ O
Computing -X- _ O
infrastructure -X- _ O
Intel -X- _ O
( -X- _ O
R -X- _ O
) -X- _ O
Core -X- _ O
( -X- _ O
TM -X- _ O
) -X- _ O
i7 -X- _ O
- -X- _ O
3770 -X- _ O
K -X- _ O
CPU -X- _ O
@ -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
2021 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
pages -X- _ O
1–14 -X- _ O
November -X- _ O
7–11 -X- _ O
, -X- _ O
2021 -X- _ O
. -X- _ O

c -X- _ O

2021 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics1AligNART -X- _ O
: -X- _ O
Non -X- _ O
- -X- _ O
autoregressive -X- _ O
Neural -X- _ O
Machine -X- _ O
Translation -X- _ O
by -X- _ O
Jointly -X- _ O
Learning -X- _ O
to -X- _ O
Estimate -X- _ O
Alignment -X- _ O
and -X- _ O
Translate -X- _ O
Jongyoon -X- _ O
Song1 -X- _ O
; -X- _ O
2Sungwon -X- _ O
Kim1 -X- _ O
{ -X- _ O
coms1580 -X- _ O
, -X- _ O
ksw0306 -X- _ O
, -X- _ O
sryoon -X- _ O
} -X- _ O
@ -X- _ O
snu.ac.krSungroh -X- _ O
Yoon1 -X- _ O
; -X- _ O
3y -X- _ O
Abstract -X- _ O
Non -X- _ O
- -X- _ O
autoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
( -X- _ O
NART -X- _ B-TaskName
) -X- _ O
models -X- _ O
suffer -X- _ O
from -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
modality -X- _ O
problem -X- _ O
which -X- _ O
causes -X- _ O
translation -X- _ O
inconsistency -X- _ O
such -X- _ O
as -X- _ O
token -X- _ O
repetition -X- _ O
. -X- _ O

Most -X- _ O
recent -X- _ O
approaches -X- _ O
have -X- _ O
attempted -X- _ O
to -X- _ O
solve -X- _ O
this -X- _ O
problem -X- _ O
by -X- _ O
implicitly -X- _ O
modeling -X- _ O
dependencies -X- _ O
between -X- _ O
outputs -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
AligNART -X- _ B-MethodName
, -X- _ O
which -X- _ O
leverages -X- _ O
full -X- _ O
alignment -X- _ O
information -X- _ O
to -X- _ O
explicitly -X- _ O
reduce -X- _ O
the -X- _ O
modality -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
distribution -X- _ O
. -X- _ O

AligNART -X- _ B-MethodName
divides -X- _ O
the -X- _ O
machine -X- _ O
translation -X- _ O
task -X- _ O
into -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
alignment -X- _ O
estimation -X- _ O
and -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
translation -X- _ O
with -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
, -X- _ O
guiding -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
focus -X- _ O
on -X- _ O
simpliﬁed -X- _ O
one -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
translation -X- _ O
. -X- _ O

To -X- _ O
alleviate -X- _ O
the -X- _ O
alignment -X- _ O
estimation -X- _ O
problem -X- _ O
, -X- _ O
we -X- _ O
further -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
alignment -X- _ O
decomposition -X- _ O
method -X- _ O
. -X- _ O

Our -X- _ O
experiments -X- _ O
show -X- _ O
that -X- _ O
AligNART -X- _ B-MethodName
outperforms -X- _ O
previous -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
that -X- _ O
focus -X- _ O
on -X- _ O
explicit -X- _ O
modality -X- _ O
reduction -X- _ O
on -X- _ O
WMT14 -X- _ B-TaskName
En -X- _ O
$ -X- _ O
De -X- _ O
and -X- _ O
WMT16 -X- _ B-TaskName
Ro -X- _ O
! -X- _ O

En -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
AligNART -X- _ B-MethodName
achieves -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
comparable -X- _ O
to -X- _ O
those -X- _ O
of -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
connectionist -X- _ O
temporal -X- _ O
classiﬁcation -X- _ O
based -X- _ O
models -X- _ O
on -X- _ O
WMT14 -X- _ B-TaskName
En -X- _ O
$ -X- _ O
De -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
observe -X- _ O
that -X- _ O
AligNART -X- _ B-MethodName
effectively -X- _ O
addresses -X- _ O
the -X- _ O
token -X- _ O
repetition -X- _ O
problem -X- _ O
even -X- _ O
without -X- _ O
sequence -X- _ O
- -X- _ O
level -X- _ O
knowledge -X- _ O
distillation -X- _ O
. -X- _ O

1 -X- _ O
Introduction -X- _ O
In -X- _ O
the -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
( -X- _ O
NMT -X- _ B-TaskName
) -X- _ O
domain -X- _ O
, -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
NMT -X- _ B-TaskName
( -X- _ O
NART -X- _ B-MethodName
) -X- _ O
models -X- _ O
( -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
have -X- _ O
been -X- _ O
proposed -X- _ O
to -X- _ O
alleviate -X- _ O
the -X- _ O
low -X- _ O
translation -X- _ O
speeds -X- _ O
of -X- _ O
autoregressive -X- _ O
NMT -X- _ B-TaskName
( -X- _ O
ART -X- _ B-MethodName
) -X- _ O
models -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
these -X- _ O
models -X- _ O
suffer -X- _ O
from -X- _ O
degenerated -X- _ O
translation -X- _ O
quality -X- _ O
( -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Sun -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

To -X- _ O
improve -X- _ O
the -X- _ O
translation -X- _ O
quality -X- _ O
of -X- _ O
NART -X- _ B-MethodName
, -X- _ O
several -X- _ O
studies -X- _ O
on -X- _ O
NART -X- _ B-MethodName
iteratively -X- _ O
reﬁne -X- _ O
decoded -X- _ O
outputs -X- _ O
with -X- _ O
minimal -X- _ O
iterations -X- _ O
( -X- _ O
Ghazvininejad -X- _ O
Guo -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Saharia -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
; -X- _ O
other -X- _ O
recent -X- _ O
This -X- _ O
work -X- _ O
was -X- _ O
done -X- _ O
during -X- _ O
an -X- _ O
internship -X- _ O
at -X- _ O
Kakao -X- _ O
Enterprise -X- _ O
. -X- _ O

yCorresponding -X- _ O
author.works -X- _ O
target -X- _ O
to -X- _ O
improve -X- _ O
NART -X- _ B-MethodName
without -X- _ O
iteration -X- _ O
One -X- _ O
of -X- _ O
the -X- _ O
signiﬁcant -X- _ O
limitations -X- _ O
of -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
is -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
modality -X- _ O
problem -X- _ O
. -X- _ O

This -X- _ O
problem -X- _ O
originates -X- _ O
from -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
the -X- _ O
models -X- _ O
should -X- _ O
maximize -X- _ O
the -X- _ O
probabilities -X- _ O
of -X- _ O
multiple -X- _ O
targets -X- _ O
without -X- _ O
considering -X- _ O
conditional -X- _ O
dependencies -X- _ O
between -X- _ O
target -X- _ O
tokens -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
in -X- _ O
English -X- _ O
- -X- _ O
toGerman -X- _ O
translation -X- _ O
, -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
" -X- _ O
Thank -X- _ O
you -X- _ O
very -X- _ O
much -X- _ O
. -X- _ O
" -X- _ O

can -X- _ O
be -X- _ O
translated -X- _ O
to -X- _ O
" -X- _ O
Danke -X- _ O
schön -X- _ O
. -X- _ O
" -X- _ O

or -X- _ O
" -X- _ O
Vielen -X- _ O
Dank -X- _ O
. -X- _ O
" -X- _ O
. -X- _ O

Under -X- _ O
the -X- _ O
conditional -X- _ O
independence -X- _ O
assumption -X- _ O
, -X- _ O
the -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
are -X- _ O
likely -X- _ O
to -X- _ O
generate -X- _ O
improper -X- _ O
translations -X- _ O
such -X- _ O
as -X- _ O
" -X- _ O
Danke -X- _ O
Dank -X- _ O
. -X- _ O
" -X- _ O
or -X- _ O
" -X- _ O
Vielen -X- _ O
schön -X- _ O
. -X- _ O
" -X- _ O

( -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
same -X- _ O
reason -X- _ O
, -X- _ O
other -X- _ O
inconsistency -X- _ O
problems -X- _ O
such -X- _ O
as -X- _ O
token -X- _ O
repetition -X- _ O
or -X- _ O
omission -X- _ O
occur -X- _ O
frequently -X- _ O
in -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
( -X- _ O
Gu -X- _ O
and -X- _ O
Kong -X- _ O
, -X- _ O
There -X- _ O
are -X- _ O
two -X- _ O
main -X- _ O
methods -X- _ O
for -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
to -X- _ O
address -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
modality -X- _ O
problem -X- _ O
. -X- _ O

Some -X- _ O
works -X- _ O
focus -X- _ O
on -X- _ O
an -X- _ O
implicit -X- _ O
modeling -X- _ O
of -X- _ O
the -X- _ O
dependencies -X- _ O
between -X- _ O
the -X- _ O
target -X- _ O
tokens -X- _ O
( -X- _ O
Gu -X- _ O
and -X- _ O
Kong -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
Ghazvininejad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
Saharia -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Gu -X- _ O
and -X- _ O
Kong -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
modify -X- _ O
the -X- _ O
objective -X- _ O
function -X- _ O
based -X- _ O
on -X- _ O
dynamic -X- _ O
programming -X- _ O
, -X- _ O
whereas -X- _ O
Qian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
provide -X- _ O
target -X- _ O
tokens -X- _ O
to -X- _ O
the -X- _ O
decoder -X- _ O
during -X- _ O
training -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
other -X- _ O
works -X- _ O
focus -X- _ O
on -X- _ O
an -X- _ O
explicit -X- _ O
reduction -X- _ O
of -X- _ O
the -X- _ O
modality -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
distribution -X- _ O
by -X- _ O
utilizing -X- _ O
external -X- _ O
source -X- _ O
or -X- _ O
target -X- _ O
sentence -X- _ O
information -X- _ O
rather -X- _ O
than -X- _ O
modifying -X- _ O
the -X- _ O
objective -X- _ O
function -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
Akoury -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
use -X- _ O
syntactic -X- _ O
or -X- _ O
semantic -X- _ O
inand -X- _ O
Ran -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
use -X- _ O
the -X- _ O
alignment -X- _ O
information -X- _ O
between -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
tokens -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
previous -X- _ O
explicit -X- _ O
modality -X- _ O
reduction -X- _ O
methods -X- _ O
show -X- _ O
suboptimal -X- _ O
performance -X- _ O
. -X- _ O

tract -X- _ O
fertility -X- _ O
( -X- _ O
Brown -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
1993 -X- _ O
) -X- _ O
and -X- _ O
ordering -X- _ O

2information -X- _ O
in -X- _ O
word -X- _ O
alignments -X- _ O
, -X- _ O
which -X- _ O
enables -X- _ O
the -X- _ O
modeling -X- _ O
of -X- _ O
several -X- _ O
types -X- _ O
of -X- _ O
mappings -X- _ O
except -X- _ O
for -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
and -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
many -X- _ O
cases -X- _ O
. -X- _ O

We -X- _ O
hypothesize -X- _ O
that -X- _ O
leveraging -X- _ O
entire -X- _ O
mappings -X- _ O
signiﬁcantly -X- _ O
reduces -X- _ O
the -X- _ O
modality -X- _ O
and -X- _ O
is -X- _ O
the -X- _ O
key -X- _ O
to -X- _ O
performance -X- _ O
improvement -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
AligNART -X- _ B-MethodName
, -X- _ O
a -X- _ O
noniterative -X- _ O
NART -X- _ B-MethodName
model -X- _ O
that -X- _ O
mitigates -X- _ O
the -X- _ O
multimodality -X- _ O
problem -X- _ O
by -X- _ O
utilizing -X- _ O
complete -X- _ O
information -X- _ O
in -X- _ O
word -X- _ O
alignments -X- _ O
. -X- _ O

AligNART -X- _ B-MethodName
divides -X- _ O
the -X- _ O
machine -X- _ O
translation -X- _ O
task -X- _ O
into -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
alignment -X- _ O
estimation -X- _ O
and -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
translation -X- _ O
under -X- _ O
the -X- _ O
given -X- _ O
alignments -X- _ O
. -X- _ O

Modeling -X- _ O
all -X- _ O
the -X- _ O
type -X- _ O
of -X- _ O
mapping -X- _ O
guides -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
more -X- _ O
close -X- _ O
to -X- _ O
one -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
translation -X- _ O
. -X- _ O

In -X- _ O
AligNART -X- _ B-MethodName
, -X- _ O
a -X- _ O
module -X- _ O
called -X- _ O
Aligner -X- _ O
is -X- _ O
simply -X- _ O
augmented -X- _ O
to -X- _ O
NAT -X- _ B-MethodName
( -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
which -X- _ O
estimates -X- _ O
alignments -X- _ O
to -X- _ O
generate -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
challenging -X- _ O
to -X- _ O
estimate -X- _ O
the -X- _ O
complex -X- _ O
alignment -X- _ O
information -X- _ O
using -X- _ O
only -X- _ O
source -X- _ O
sentence -X- _ O
during -X- _ O
inference -X- _ O
. -X- _ O

Speciﬁcally -X- _ O
, -X- _ O
Aligner -X- _ O
should -X- _ O
simultaneously -X- _ O
predict -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
target -X- _ O
tokens -X- _ O
corresponding -X- _ O
to -X- _ O
each -X- _ O
source -X- _ O
token -X- _ O
and -X- _ O
their -X- _ O
mapping -X- _ O
. -X- _ O

To -X- _ O
overcome -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
we -X- _ O
further -X- _ O
propose -X- _ O
alignment -X- _ O
decomposition -X- _ O
which -X- _ O
factorizes -X- _ O
the -X- _ O
alignment -X- _ O
process -X- _ O
into -X- _ O
three -X- _ O
subprocesses -X- _ O
: -X- _ O
duplication -X- _ O
, -X- _ O
permutation -X- _ O
, -X- _ O
and -X- _ O
grouping -X- _ O
. -X- _ O

Each -X- _ O
sub -X- _ O
- -X- _ O
process -X- _ O
corresponds -X- _ O
to -X- _ O
much -X- _ O
feasible -X- _ O
sub -X- _ O
- -X- _ O
problems -X- _ O
: -X- _ O
one -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
many -X- _ O
mapping -X- _ O
, -X- _ O
ordering -X- _ O
, -X- _ O
and -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
mapping -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

Our -X- _ O
experimental -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
AligNART -X- _ B-MethodName
outperforms -X- _ O
previous -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
of -X- _ O
explicit -X- _ O
modality -X- _ O
reduction -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
$ -X- _ O
De -X- _ O
and -X- _ O
WMT16 -X- _ B-DatasetName
Ro -X- _ O
! -X- _ O
En -X- _ O
. -X- _ O

AligNART -X- _ B-MethodName
achieves -X- _ O
performance -X- _ O
comparable -X- _ O
to -X- _ O
that -X- _ O
of -X- _ O
the -X- _ O
recent -X- _ O
stateof -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
model -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
$ -X- _ O
De -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
the -X- _ O
modality -X- _ O
reduction -X- _ O
in -X- _ O
AligNART -X- _ B-MethodName
addresses -X- _ O
the -X- _ O
token -X- _ O
repetition -X- _ O
issue -X- _ O
even -X- _ O
without -X- _ O
sequence -X- _ O
- -X- _ O
level -X- _ O
knowledge -X- _ O
distillation -X- _ O
( -X- _ O
Kim -X- _ O
and -X- _ O
Rush -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
conduct -X- _ O
quantitative -X- _ O
and -X- _ O
qualitative -X- _ O
analyses -X- _ O
on -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
alignment -X- _ O
decomposition -X- _ O
. -X- _ O

2 -X- _ O
Background -X- _ O
Given -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
x -X- _ O
= -X- _ O
fx1 -X- _ O
; -X- _ O
x2 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
x -X- _ O
Mgand -X- _ O
its -X- _ O
translation -X- _ O
y -X- _ O
= -X- _ O
fy1 -X- _ O
; -X- _ O
y2 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
y -X- _ O
Ng -X- _ O
, -X- _ O
ART -X- _ O
models -X- _ O
with -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
architecture -X- _ O
are -X- _ O
trained -X- _ O
with -X- _ O
chained -X- _ O
target -X- _ O
distributions -X- _ O
and -X- _ O
infer -X- _ O
the -X- _ O
target -X- _ O
sentence -X- _ O
autoregressively -X- _ O
: -X- _ O
p -X- _ O
( -X- _ O
yjx -X- _ O
) -X- _ O
= -X- _ O
NY -X- _ O
n=1p -X- _ O
( -X- _ O
ynjy -X- _ O
< -X- _ O
n -X- _ O
; -X- _ O
x -X- _ O
) -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
At -X- _ O
each -X- _ O
decoding -X- _ O
position -X- _ O
n -X- _ O
, -X- _ O
the -X- _ O
decoder -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
conditioned -X- _ O
with -X- _ O
previous -X- _ O
target -X- _ O
tokens -X- _ O
y -X- _ O
< -X- _ O
n -X- _ O
= -X- _ O
fy1 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
y -X- _ O
n 1 -X- _ O
g -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
key -X- _ O
factor -X- _ O
of -X- _ O
performance -X- _ O
in -X- _ O
ART -X- _ O
models -X- _ O
. -X- _ O

Previous -X- _ O
target -X- _ O
tokens -X- _ O
reduce -X- _ O
the -X- _ O
target -X- _ O
distribution -X- _ O
modality -X- _ O
and -X- _ O
provide -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
target -X- _ O
sentence -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
autoregressive -X- _ O
decoding -X- _ O
scheme -X- _ O
enforces -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
iterate -X- _ O
Ntimes -X- _ O
to -X- _ O
complete -X- _ O
the -X- _ O
translation -X- _ O
and -X- _ O
increases -X- _ O
the -X- _ O
translation -X- _ O
time -X- _ O
linearly -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
sentence -X- _ O
. -X- _ O

Non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
( -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Sun -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Sun -X- _ O
and -X- _ O
Yang -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
assume -X- _ O
conditional -X- _ O
independence -X- _ O
between -X- _ O
the -X- _ O
target -X- _ O
tokens -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
translation -X- _ O
speed -X- _ O
: -X- _ O
p -X- _ O
( -X- _ O
yjx -X- _ O
) -X- _ O
= -X- _ O
p -X- _ O
( -X- _ O
Njx -X- _ O
) -X- _ O
NY -X- _ O
whereNis -X- _ O

the -X- _ O
predicted -X- _ O
target -X- _ O
length -X- _ O
to -X- _ O
parallelize -X- _ O
the -X- _ O
decoding -X- _ O
process -X- _ O
. -X- _ O

Non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
provide -X- _ O
only -X- _ O
the -X- _ O
length -X- _ O
information -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
sentence -X- _ O
to -X- _ O
the -X- _ O
decoder -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
insufﬁcient -X- _ O
to -X- _ O
address -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
modality -X- _ O
problem -X- _ O
. -X- _ O

3 -X- _ O
AligNART -X- _ B-MethodName
3.1 -X- _ O
Model -X- _ O
Overview -X- _ O
Given -X- _ O
the -X- _ O
word -X- _ O
alignments -X- _ O
between -X- _ O
the -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
sentences -X- _ O
A2f0 -X- _ O
; -X- _ O
1gNM -X- _ O
, -X- _ O
we -X- _ O
factorize -X- _ O
the -X- _ O
task -X- _ O
into -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
alignment -X- _ O
estimation -X- _ O
and -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
translation -X- _ O
with -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
p -X- _ O
( -X- _ O
yjx -X- _ O
) -X- _ O
= -X- _ O
p -X- _ O
( -X- _ O
Ajx -X- _ O
) -X- _ O
NY -X- _ O
whereMandNare -X- _ O
the -X- _ O
lengths -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
sentences -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

Although -X- _ O
we -X- _ O
can -X- _ O
also -X- _ O
modify -X- _ O
the -X- _ O
negative -X- _ O
log -X- _ O
- -X- _ O
likelihood -X- _ O
loss -X- _ O
to -X- _ O
model -X- _ O
dependencies -X- _ O
between -X- _ O
outputs -X- _ O
such -X- _ O
as -X- _ O
connectionist -X- _ B-MetricName
temporal -X- _ I-MetricName
classiﬁcation -X- _ I-MetricName
( -X- _ I-MetricName
CTC -X- _ I-MetricName
) -X- _ I-MetricName
loss -X- _ I-MetricName
( -X- _ O
Graves -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
the -X- _ O
introduction -X- _ O
of -X- _ O
alignment -X- _ O
as -X- _ O
additional -X- _ O
information -X- _ O
. -X- _ O

AligNART -X- _ B-MethodName
is -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
architecture -X- _ O
, -X- _ O
with -X- _ O
an -X- _ O
alignment -X- _ O
estimation -X- _ O
module -X- _ O
called -X- _ O
Aligner -X- _ O
as -X- _ O
depicted -X- _ O
in -X- _ O
Figure -X- _ O
1a -X- _ O
. -X- _ O

The -X- _ O
encoder -X- _ O
maps -X- _ O
the -X- _ O
embedding -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
tokens -X- _ O
into -X- _ O
hidden -X- _ O
representations -X- _ O
h -X- _ O
= -X- _ O
fh1 -X- _ O
; -X- _ O
h2 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
h -X- _ O
Mg -X- _ O
. -X- _ O

Aligner -X- _ O
constructs -X- _ O
the -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
d= -X- _ O
fd1 -X- _ O
; -X- _ O
d2 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
d -X- _ O
Ngas -X- _ O
follows -X- _ O
: -X- _ O
dn=1 -X- _ O
rnMX -X- _ O

EncoderAligner -X- _ O
𝒙𝒙Decoder𝒚𝒚 -X- _ O
Attention -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O

Model -X- _ O
overview -X- _ O
of -X- _ O
AligNART -X- _ B-MethodName
Duplication -X- _ O
Predictor𝒅𝒅′ -X- _ O
Permutation -X- _ O
PredictorGrouping -X- _ O
Predictor𝒅𝒅 -X- _ O
𝑷𝑷𝑮𝑮𝑮𝑮×𝟏𝟏 -X- _ O
/ -X- _ O
𝒓𝒓𝒏𝒏 -X- _ O

( -X- _ O
b -X- _ O
) -X- _ O
Alignment -X- _ O
decomposition -X- _ O
and -X- _ O
sub -X- _ O
- -X- _ O
processes -X- _ O
of -X- _ O
Aligner -X- _ O
Figure -X- _ O
1 -X- _ O
: -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
Given -X- _ O
the -X- _ O
encoder -X- _ O
outputs -X- _ O
h -X- _ O
, -X- _ O
Aligner -X- _ O
returns -X- _ O
aligned -X- _ O
encoder -X- _ O
outputs -X- _ O
d -X- _ O
. -X- _ O

The -X- _ O
decoder -X- _ O
then -X- _ O
translates -X- _ O
the -X- _ O
aligned -X- _ O
inputs -X- _ O
to -X- _ O
y. -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
The -X- _ O
dotted -X- _ O
lines -X- _ O
indicate -X- _ O
the -X- _ O
alignment -X- _ O
decomposition -X- _ O
. -X- _ O

During -X- _ O
training -X- _ O
, -X- _ O
predictors -X- _ O
are -X- _ O
trained -X- _ O
with -X- _ O
the -X- _ O
decomposed -X- _ O
matrices -X- _ O
D -X- _ O
, -X- _ O
P -X- _ O
, -X- _ O
and -X- _ O
G -X- _ O
, -X- _ O
and -X- _ O
align -X- _ O
inputs -X- _ O
using -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
as -X- _ O
indicated -X- _ O
by -X- _ O
the -X- _ O
solid -X- _ O
lines -X- _ O
. -X- _ O

During -X- _ O
inference -X- _ O
, -X- _ O
predictors -X- _ O
align -X- _ O
inputs -X- _ O
using -X- _ O
the -X- _ O
estimated -X- _ O
matrices -X- _ O
as -X- _ O
indicated -X- _ O
by -X- _ O
the -X- _ O
dashed -X- _ O
lines -X- _ O
. -X- _ O

wherernis -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
non -X- _ O
- -X- _ O
zero -X- _ O
elements -X- _ O
in -X- _ O
then -X- _ O
- -X- _ O
th -X- _ O
row -X- _ O
ofA -X- _ O
. -X- _ O

Given -X- _ O
the -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
, -X- _ O
the -X- _ O
decoder -X- _ O
is -X- _ O
guided -X- _ O
to -X- _ O
focus -X- _ O
on -X- _ O
a -X- _ O
one -X- _ O
- -X- _ O
toone -X- _ O
translation -X- _ O
from -X- _ O
dntoyn -X- _ O
. -X- _ O

One -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
mapping -X- _ O
signiﬁcantly -X- _ O
reduces -X- _ O
the -X- _ O
modality -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
distribution -X- _ O
. -X- _ O

The -X- _ O
key -X- _ O
component -X- _ O
of -X- _ O
AligNART -X- _ B-MethodName
, -X- _ O
Aligner -X- _ O
, -X- _ O
models -X- _ O
a -X- _ O
conditional -X- _ O
distribution -X- _ O
of -X- _ O
alignments -X- _ O
Agiven -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
xduring -X- _ O
training -X- _ O
, -X- _ O
and -X- _ O
aligns -X- _ O
encoder -X- _ O
outputs -X- _ O
using -X- _ O
the -X- _ O
estimated -X- _ O
alignments -X- _ O
during -X- _ O
inference -X- _ O
, -X- _ O
as -X- _ O
depicted -X- _ O
in -X- _ O
Figure -X- _ O
1b -X- _ O
. -X- _ O

The -X- _ O
ground -X- _ O
truth -X- _ O
of -X- _ O
the -X- _ O
alignments -X- _ O
is -X- _ O
extracted -X- _ O
using -X- _ O
an -X- _ O
external -X- _ O
word -X- _ O
alignment -X- _ O
tool -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
alignment -X- _ O
estimation -X- _ O
given -X- _ O
only -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
is -X- _ O
challenging -X- _ O
since -X- _ O
the -X- _ O
alignment -X- _ O
consists -X- _ O
of -X- _ O
two -X- _ O
components -X- _ O
related -X- _ O
with -X- _ O
target -X- _ O
tokens -X- _ O
: -X- _ O
•The -X- _ O
number -X- _ O
of -X- _ O
target -X- _ O
tokens -X- _ O
that -X- _ O
correspond -X- _ O
to -X- _ O
each -X- _ O
encoder -X- _ O
output -X- _ O
hm -X- _ O
. -X- _ O

•The -X- _ O
positions -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
tokens -X- _ O
to -X- _ O
which -X- _ O
hmcorresponds -X- _ O
. -X- _ O

The -X- _ O
Aligner -X- _ O
decomposes -X- _ O
the -X- _ O
alignment -X- _ O
for -X- _ O
effective -X- _ O
estimation -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
described -X- _ O
in -X- _ O
Section -X- _ O
3.2 -X- _ O
. -X- _ O

3.2 -X- _ O
Aligner -X- _ O
To -X- _ O
alleviate -X- _ O
the -X- _ O
alignment -X- _ O
estimation -X- _ O
problem -X- _ O
, -X- _ O
we -X- _ O
start -X- _ O
by -X- _ O
factorizing -X- _ O
the -X- _ O
alignment -X- _ O
process -X- _ O
as -X- _ O
shownin -X- _ O
Figure -X- _ O
1b -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
we -X- _ O
copy -X- _ O
each -X- _ O
encoder -X- _ O
output -X- _ O
hmby -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
target -X- _ O
tokens -X- _ O
mapped -X- _ O
to -X- _ O
hm -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
denoted -X- _ O
as -X- _ O
cm -X- _ O
= -X- _ O
P -X- _ O
nAn -X- _ O
; -X- _ O
m -X- _ O
. -X- _ O

Given -X- _ O
the -X- _ O
duplicated -X- _ O
encoder -X- _ O
outputs -X- _ O
h0 -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
positions -X- _ O
of -X- _ O
target -X- _ O
tokens -X- _ O
to -X- _ O
which -X- _ O
each -X- _ O
element -X- _ O
inh0is -X- _ O
mapped -X- _ O
. -X- _ O

We -X- _ O
further -X- _ O
decompose -X- _ O
the -X- _ O
remaining -X- _ O
prediction -X- _ O
process -X- _ O
into -X- _ O
permutation -X- _ O
andgrouping -X- _ O
, -X- _ O
since -X- _ O
noniterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
have -X- _ O
no -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
target -X- _ O
length -X- _ O
Nduring -X- _ O
inference -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
permutation -X- _ O
process -X- _ O
, -X- _ O
h0is -X- _ O
re -X- _ O
- -X- _ O
ordered -X- _ O
into -X- _ O
d0such -X- _ O
that -X- _ O
elements -X- _ O
corresponding -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
target -X- _ O
token -X- _ O
are -X- _ O
placed -X- _ O
adjacent -X- _ O
to -X- _ O
each -X- _ O
other -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
grouping -X- _ O
process -X- _ O
, -X- _ O
each -X- _ O
element -X- _ O
in -X- _ O
d0is -X- _ O
clustered -X- _ O
into -X- _ O
Ngroups -X- _ O
by -X- _ O
predicting -X- _ O
whether -X- _ O
each -X- _ O
element -X- _ O
is -X- _ O
mapped -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
target -X- _ O
token -X- _ O
as -X- _ O
the -X- _ O
previous -X- _ O
element -X- _ O
.rn -X- _ O

= -X- _ O
P -X- _ O
mAn -X- _ O
; -X- _ O
mdenotes -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
elements -X- _ O
in -X- _ O
the -X- _ O
n -X- _ O
- -X- _ O
th -X- _ O
group -X- _ O
which -X- _ O
is -X- _ O
equivalent -X- _ O
to -X- _ O
rn -X- _ O
in -X- _ O
Equation -X- _ O
4 -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
derive -X- _ O
the -X- _ O
decoder -X- _ O
inputsdin -X- _ O
Equation -X- _ O
4 -X- _ O
by -X- _ O
averaging -X- _ O
the -X- _ O
elements -X- _ O
in -X- _ O
each -X- _ O
group -X- _ O
in -X- _ O
d0 -X- _ O
. -X- _ O

In -X- _ O
summary -X- _ O
, -X- _ O
we -X- _ O
decompose -X- _ O
the -X- _ O
alignment -X- _ O
estimation -X- _ O
task -X- _ O
into -X- _ O
three -X- _ O
sequential -X- _ O
sub -X- _ O
- -X- _ O
tasks -X- _ O
: -X- _ O
duplication -X- _ O
, -X- _ O
permutation -X- _ O
, -X- _ O
and -X- _ O
grouping -X- _ O
. -X- _ O

3.2.1 -X- _ O
Alignment -X- _ O
Decomposition -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1b -X- _ O
, -X- _ O
we -X- _ O
factorize -X- _ O
the -X- _ O
alignment -X- _ O
matrix -X- _ O
Ainto -X- _ O
duplication -X- _ O
, -X- _ O
permutation -X- _ O
, -X- _ O
and -X- _ O

4grouping -X- _ O
matrices -X- _ O
that -X- _ O
correspond -X- _ O
to -X- _ O
each -X- _ O
prodenotes -X- _ O
the -X- _ O
duplicated -X- _ O
encoder -X- _ O
outputs -X- _ O
where -X- _ O
hi -X- _ O
; -X- _ O
jis -X- _ O
thej -X- _ O
- -X- _ O
th -X- _ O
copied -X- _ O
element -X- _ O
of -X- _ O
hi -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
the -X- _ O
permuted -X- _ O
encoder -X- _ O
outputs -X- _ O
where -X- _ O
di -X- _ O
; -X- _ O
jis -X- _ O
thejth -X- _ O
element -X- _ O
in -X- _ O
the -X- _ O
i -X- _ O
- -X- _ O
th -X- _ O
group -X- _ O
. -X- _ O

The -X- _ O
number -X- _ O
of -X- _ O
nonzero -X- _ O
elements -X- _ O
in -X- _ O
the -X- _ O
alignment -X- _ O
matrix -X- _ O
is -X- _ O
deﬁned -X- _ O
as -X- _ O
L -X- _ O
= -X- _ O
P -X- _ O
mcm -X- _ O
= -X- _ O
P -X- _ O
nrn -X- _ O
. -X- _ O

Duplication -X- _ O
Matrix -X- _ O
Aligner -X- _ O
copies -X- _ O
hmbycm -X- _ O
to -X- _ O
construct -X- _ O
the -X- _ O
duplicated -X- _ O
encoder -X- _ O
outputs -X- _ O

h0with -X- _ O
a -X- _ O
duplication -X- _ O
matrix -X- _ O
D2f0 -X- _ O
; -X- _ O
1gLM -X- _ O
. -X- _ O

LetCm -X- _ O
= -X- _ O
Pm -X- _ O
i=1ciandC0= -X- _ O
0 -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
deﬁne -X- _ O
Dusing -X- _ O
cmas -X- _ O
follows -X- _ O
: -X- _ O
Dl -X- _ O
; -X- _ O
m= -X- _ O
( -X- _ O
1ifCm 1 -X- _ O
< -X- _ O
lCm -X- _ O

We -X- _ O
indexh0by -X- _ O
the -X- _ O
following -X- _ O
rule -X- _ O
: -X- _ O
•For -X- _ O
anyhm -X- _ O
; -X- _ O
iandhm -X- _ O
; -X- _ O
j -X- _ O
( -X- _ O
i -X- _ O
< -X- _ O
j -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
matched -X- _ O
to -X- _ O
dxi -X- _ O
; -X- _ O
yianddxj -X- _ O
; -X- _ O
yj -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
xixjandyiyj -X- _ O
. -X- _ O

The -X- _ O
duplication -X- _ O
matrix -X- _ O
Dcontains -X- _ O
similar -X- _ O
information -X- _ O
to -X- _ O
fertility -X- _ O
( -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Permutation -X- _ O
Matrix -X- _ O
Aligner -X- _ O
re -X- _ O
- -X- _ O
orders -X- _ O
h0to -X- _ O
constructd0with -X- _ O
a -X- _ O
permutation -X- _ O
matrix -X- _ O
P2 -X- _ O
f0 -X- _ O
; -X- _ O
1gLL. -X- _ O
Since -X- _ O
all -X- _ O
the -X- _ O
indexed -X- _ O
elements -X- _ O
in -X- _ O
h0 -X- _ O
andd0are -X- _ O
distinct -X- _ O
, -X- _ O
the -X- _ O
permutation -X- _ O
matrix -X- _ O
Pis -X- _ O
uniquely -X- _ O
deﬁned -X- _ O
. -X- _ O

Grouping -X- _ O
Matrix -X- _ O
Aligner -X- _ O
ﬁnally -X- _ O
aggregates -X- _ O
d0 -X- _ O
to -X- _ O
construct -X- _ O
d -X- _ O
, -X- _ O
the -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
grouping -X- _ O
matrix -X- _ O
G2 -X- _ O
f0 -X- _ O
; -X- _ O
1gNL. -X- _ O
LetRn -X- _ O
= -X- _ O
Pn -X- _ O
i=1riandR0= -X- _ O
0 -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
Gcan -X- _ O
be -X- _ O
deﬁned -X- _ O
using -X- _ O
rnas -X- _ O
follows -X- _ O
: -X- _ O
Gn -X- _ O
; -X- _ O
l= -X- _ O
( -X- _ O
1ifRn 1 -X- _ O
< -X- _ O
lRn -X- _ O

We -X- _ O
indexd0by -X- _ O
the -X- _ O
following -X- _ O
rule -X- _ O
: -X- _ O
•For -X- _ O
anydn -X- _ O
; -X- _ O
ianddn -X- _ O
; -X- _ O
j -X- _ O
( -X- _ O
i -X- _ O
< -X- _ O
j -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
matched -X- _ O
to -X- _ O
hxi -X- _ O
; -X- _ O
yiandhxj -X- _ O
; -X- _ O
yj -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
xixjandyiyj -X- _ O
. -X- _ O

We -X- _ O
can -X- _ O
derive -X- _ O
the -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
by -X- _ O
separately -X- _ O
estimating -X- _ O
the -X- _ O
decomposed -X- _ O
matrices -X- _ O
D -X- _ O
, -X- _ O
P -X- _ O
, -X- _ O
andG -X- _ O
, -X- _ O
which -X- _ O
approximately -X- _ O
correspond -X- _ O
to -X- _ O
one -X- _ O
- -X- _ O
tomany -X- _ O
mapping -X- _ O
, -X- _ O
ordering -X- _ O
, -X- _ O
and -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
mapping -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

The -X- _ O
decomposed -X- _ O
matrices -X- _ O
have -X- _ O
an -X- _ O
easily -X- _ O
predictable -X- _ O
form -X- _ O
while -X- _ O
recovering -X- _ O
the -X- _ O
complete -X- _ O
alignment -X- _ O
matrix.3.2.2 -X- _ O
Training -X- _ O
Aligner -X- _ O
consists -X- _ O
of -X- _ O
three -X- _ O
prediction -X- _ O
sub -X- _ O
- -X- _ O
modules -X- _ O
: -X- _ O
duplication -X- _ O
, -X- _ O
permutation -X- _ O
, -X- _ O
and -X- _ O
grouping -X- _ O
predictors -X- _ O
. -X- _ O

Each -X- _ O
of -X- _ O
them -X- _ O
estimates -X- _ O
the -X- _ O
decomposed -X- _ O
alignment -X- _ O
matrix -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
The -X- _ O
duplication -X- _ O
predictor -X- _ O
learns -X- _ O
to -X- _ O
classify -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
copies -X- _ O
of -X- _ O
hm -X- _ O
. -X- _ O

The -X- _ O
duplication -X- _ O
loss -X- _ O
is -X- _ O
deﬁned -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
MMX -X- _ O
wherepmis -X- _ O
the -X- _ O
predicted -X- _ O
probability -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
duplication -X- _ O
at -X- _ O
the -X- _ O
position -X- _ O
m. -X- _ O
To -X- _ O
discriminate -X- _ O
copied -X- _ O
elements -X- _ O
in -X- _ O
h0 -X- _ O
, -X- _ O
we -X- _ O
add -X- _ O
copy -X- _ O
position -X- _ O
embedding -X- _ O
tofhm -X- _ O
; -X- _ O
1 -X- _ O
; -X- _ O
: -X- _ O
: -X- _ O
: -X- _ O
; -X- _ O
h -X- _ O
m -X- _ O
; -X- _ O
cmgfor -X- _ O
the -X- _ O
next -X- _ O
two -X- _ O
predictors -X- _ O
. -X- _ O

The -X- _ O
permutation -X- _ O
predictor -X- _ O
takes -X- _ O
the -X- _ O
duplicated -X- _ O
encoder -X- _ O
outputs -X- _ O
h0as -X- _ O
inputs -X- _ O
. -X- _ O

We -X- _ O
simplify -X- _ O
the -X- _ O
permutation -X- _ O
prediction -X- _ O
problem -X- _ O
into -X- _ O
a -X- _ O
classiﬁcation -X- _ O
of -X- _ O
the -X- _ O
re -X- _ O
- -X- _ O
ordered -X- _ O
position -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
permutation -X- _ O
loss -X- _ O
, -X- _ O
we -X- _ O
minimize -X- _ O
the -X- _ O
KL -X- _ O
- -X- _ O
divergence -X- _ O
between -X- _ O
the -X- _ O
predictionPpredand -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
PGT -X- _ O
. -X- _ O

LX -X- _ O
iX -X- _ O
jPGT -X- _ O
i -X- _ O
; -X- _ O
jlogPpred -X- _ O
Given -X- _ O
the -X- _ O
permuted -X- _ O
encoder -X- _ O
outputs -X- _ O
, -X- _ O
the -X- _ O
grouping -X- _ O
predictor -X- _ O
conducts -X- _ O
a -X- _ O
binary -X- _ O
classiﬁcation -X- _ O
task -X- _ O
of -X- _ O
whetherd0 -X- _ O
lis -X- _ O
assigned -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
group -X- _ O
as -X- _ O
d0 -X- _ O
Let -X- _ O
the -X- _ O
label -X- _ O
at -X- _ O
the -X- _ O
position -X- _ O
lbegl -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
we -X- _ O
deﬁne -X- _ O
glfromGas -X- _ O
follows -X- _ O
: -X- _ O
gl= -X- _ O
( -X- _ O
1ifG -X- _ O
; -X- _ O
l -X- _ O
= -X- _ O
G -X- _ O
; -X- _ O
l 1andl -X- _ O
> -X- _ O
1 -X- _ O

The -X- _ O
grouping -X- _ O
loss -X- _ O
is -X- _ O
deﬁned -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

LLX -X- _ O
whereplis -X- _ O
the -X- _ O
predicted -X- _ O
probability -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
grouping -X- _ O
predictor -X- _ O
at -X- _ O
position -X- _ O
l. -X- _ O
Our -X- _ O
ﬁnal -X- _ O
loss -X- _ O
function -X- _ O
is -X- _ O
deﬁned -X- _ O
as -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
the -X- _ O
negative -X- _ O
log -X- _ O
- -X- _ O
likelihood -X- _ O
based -X- _ O
translation -X- _ O
loss -X- _ O
LTand -X- _ O
alignment -X- _ O
loss -X- _ O
LA -X- _ O
: -X- _ O
where -X- _ O
we -X- _ O
set -X- _ O

= -X- _ O

= -X- _ O

= -X- _ O
0:5for -X- _ O
all -X- _ O
the -X- _ O
experiments -X- _ O
. -X- _ O

53.2.3 -X- _ O
Inference -X- _ O
During -X- _ O
inference -X- _ O
, -X- _ O
Aligner -X- _ O
sequentially -X- _ O
predicts -X- _ O
the -X- _ O
duplication -X- _ O
, -X- _ O
permutation -X- _ O
, -X- _ O
and -X- _ O
grouping -X- _ O
matrices -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
das -X- _ O
depicted -X- _ O
in -X- _ O
Figure -X- _ O
1b -X- _ O
. -X- _ O

The -X- _ O
duplication -X- _ O
predictor -X- _ O
in -X- _ O
Aligner -X- _ O
infers -X- _ O
^cmat -X- _ O
each -X- _ O
position -X- _ O
m -X- _ O
; -X- _ O
then -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
directly -X- _ O
construct -X- _ O
a -X- _ O
duplication -X- _ O
matrix -X- _ O
^Dusing -X- _ O
Equation -X- _ O
5 -X- _ O
. -X- _ O

The -X- _ O
permutation -X- _ O
predictor -X- _ O
predicts -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
position -X- _ O
Ppred -X- _ O
. -X- _ O

We -X- _ O
obtain -X- _ O
a -X- _ O
permutation -X- _ O
matrix -X- _ O
^Pthat -X- _ O
minimizes -X- _ O
the -X- _ O
KL -X- _ O
- -X- _ O
divergence -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
^P= -X- _ O
arg -X- _ O
min -X- _ O
P -X- _ O
( -X- _ O
 X -X- _ O
iX -X- _ O
jPi -X- _ O
; -X- _ O
jlogPpred -X- _ O

We -X- _ O
utilize -X- _ O
the -X- _ O
linear -X- _ O
sum -X- _ O
assignment -X- _ O
problem -X- _ O
solver -X- _ O
provided -X- _ O
by -X- _ O
Jones -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2001 -X- _ O
) -X- _ O
to -X- _ O
ﬁnd -X- _ O
^P -X- _ O
. -X- _ O

The -X- _ O
grouping -X- _ O
predictor -X- _ O
infers -X- _ O
the -X- _ O
binary -X- _ O
predictions -X- _ O
^glfrom -X- _ O
the -X- _ O
permuted -X- _ O
encoder -X- _ O
outputs -X- _ O
. -X- _ O

We -X- _ O
construct -X- _ O
a -X- _ O
grouping -X- _ O
matrix -X- _ O
^Gusing -X- _ O
^gland -X- _ O
Equations -X- _ O
6 -X- _ O
and -X- _ O
10 -X- _ O
. -X- _ O

With -X- _ O
a -X- _ O
predicted -X- _ O
alignment -X- _ O
matrix -X- _ O
^A=^G^P^D -X- _ O
, -X- _ O
Aligner -X- _ O
constructs -X- _ O
the -X- _ O
decoder -X- _ O
inputs -X- _ O
using -X- _ O
Equation -X- _ O
4 -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
decoder -X- _ O
performs -X- _ O
translation -X- _ O
from -X- _ O
the -X- _ O
aligned -X- _ O
inputs -X- _ O
. -X- _ O

3.2.4 -X- _ O
Decoding -X- _ O
Strategies -X- _ O
For -X- _ O
the -X- _ O
re -X- _ O
- -X- _ O
scoring -X- _ O
based -X- _ O
decoding -X- _ O
method -X- _ O
, -X- _ O
we -X- _ O
select -X- _ O
candidates -X- _ O
of -X- _ O
alignments -X- _ O
using -X- _ O
the -X- _ O
predicted -X- _ O
distributions -X- _ O
in -X- _ O
the -X- _ O
duplication -X- _ O
and -X- _ O
grouping -X- _ O
predictors -X- _ O
. -X- _ O

We -X- _ O
identify -X- _ O
m0positions -X- _ O
in -X- _ O
the -X- _ O
outputs -X- _ O
of -X- _ O
the -X- _ O
duplication -X- _ O
predictor -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
predicted -X- _ O
class -X- _ O
is -X- _ O
low -X- _ O
. -X- _ O

We -X- _ O
then -X- _ O
construct -X- _ O
a -X- _ O
2m0candidate -X- _ O
pool -X- _ O
where -X- _ O
the -X- _ O
predictions -X- _ O
in -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
m0positions -X- _ O
are -X- _ O
replaced -X- _ O
with -X- _ O
the -X- _ O
second -X- _ O
probable -X- _ O
class -X- _ O
. -X- _ O

Next -X- _ O
, -X- _ O
we -X- _ O
identify -X- _ O
the -X- _ O
top- -X- _ O
acandidates -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
joint -X- _ O
probabilities -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
we -X- _ O
construct -X- _ O
a -X- _ O
2l0 -X- _ O
- -X- _ O
candidate -X- _ O
pool -X- _ O
and -X- _ O
identify -X- _ O
bcandidates -X- _ O
in -X- _ O
the -X- _ O
grouping -X- _ O
predictor -X- _ O
for -X- _ O
the -X- _ O
acandidates -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
rank -X- _ O
abtranslations -X- _ O
for -X- _ O
the -X- _ O
alignments -X- _ O
candidates -X- _ O
using -X- _ O
a -X- _ O
teacher -X- _ O
ART -X- _ O
model -X- _ O
and -X- _ O
select -X- _ O
the -X- _ O
best -X- _ O
translation -X- _ O
among -X- _ O
them -X- _ O
. -X- _ O

3.3 -X- _ O
Architecture -X- _ O
of -X- _ O
AligNART -X- _ B-MethodName
We -X- _ O
use -X- _ O
the -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
( -X- _ I-MethodName
12 -X- _ I-MethodName
- -X- _ I-MethodName
1 -X- _ I-MethodName
for -X- _ I-MethodName
short -X- _ I-MethodName
) -X- _ I-MethodName
Transformer -X- _ I-MethodName
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
architecture -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
12layer -X- _ O
encoder -X- _ O
and -X- _ O
1 -X- _ O
- -X- _ O
layer -X- _ O
decoder -X- _ O
) -X- _ O
proposed -X- _ O
by -X- _ O
Kasai -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020b -X- _ O
) -X- _ O
for -X- _ O
two -X- _ O
reasons -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
a -X- _ O
deeper -X- _ O
encoder -X- _ O
assists -X- _ O
Aligner -X- _ O
to -X- _ O
increase -X- _ O
the -X- _ O
estimation -X- _ O
accuracy -X- _ O
of -X- _ O
the -X- _ O
alignment -X- _ O
matrix -X- _ O
during -X- _ O
inference -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
the -X- _ O
deep -X- _ O
- -X- _ O
shallow -X- _ O
architecture -X- _ O
improves -X- _ O
the -X- _ O
inference -X- _ O
speed -X- _ O
since -X- _ O
the -X- _ O
encoder -X- _ O
layer -X- _ O
has -X- _ O
no -X- _ O
cross -X- _ O
- -X- _ O
attention -X- _ O
module -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
decoderlayer -X- _ O
. -X- _ O

The -X- _ O
architecture -X- _ O
of -X- _ O
the -X- _ O
duplication -X- _ O
, -X- _ O
permutation -X- _ O
, -X- _ O
and -X- _ O
grouping -X- _ O
predictor -X- _ O
is -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
. -X- _ O

3.4 -X- _ O
Alignment -X- _ O
Score -X- _ O
Filtering -X- _ O
Some -X- _ O
alignment -X- _ O
tools -X- _ O
such -X- _ O
as -X- _ O
GIZA++ -X- _ O
( -X- _ O
Och -X- _ O
and -X- _ O
Ney -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
provide -X- _ O
an -X- _ O
alignment -X- _ O
score -X- _ O
for -X- _ O
each -X- _ O
sentence -X- _ O
pair -X- _ O
as -X- _ O
a -X- _ O
default -X- _ O
. -X- _ O

Samples -X- _ O
with -X- _ O
low -X- _ O
alignment -X- _ O
scores -X- _ O
are -X- _ O
more -X- _ O
likely -X- _ O
to -X- _ O
contain -X- _ O
noise -X- _ O
caused -X- _ O
by -X- _ O
sentence -X- _ O
pairs -X- _ O
or -X- _ O
alignment -X- _ O
tools -X- _ O
. -X- _ O

For -X- _ O
GIZA++ -X- _ O
, -X- _ O
we -X- _ O
ﬁlter -X- _ O
out -X- _ O
a -X- _ O
ﬁxed -X- _ O
portion -X- _ O
of -X- _ O
samples -X- _ O
with -X- _ O
low -X- _ O
alignment -X- _ O
scores -X- _ O
to -X- _ O
ease -X- _ O
the -X- _ O
alignment -X- _ O
estimation -X- _ O
. -X- _ O

Since -X- _ O
the -X- _ O
pair -X- _ O
of -X- _ O
long -X- _ O
sentences -X- _ O
tends -X- _ O
to -X- _ O
be -X- _ O
aligned -X- _ O
with -X- _ O
a -X- _ O
low -X- _ O
score -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
the -X- _ O
same -X- _ O
ﬁltering -X- _ O
portion -X- _ O
for -X- _ O
each -X- _ O
target -X- _ O
sentence -X- _ O
length -X- _ O
. -X- _ O

4 -X- _ O
Experimental -X- _ O
Setups -X- _ O
4.1 -X- _ O
Datasets -X- _ O
and -X- _ O
Preprocessing -X- _ O
We -X- _ O
evaluate -X- _ O
our -X- _ O
method -X- _ O
on -X- _ O
two -X- _ O
translation -X- _ O
datasets -X- _ O
: -X- _ O
WMT14 -X- _ B-DatasetName
English -X- _ O
- -X- _ O
German -X- _ O
( -X- _ O
En -X- _ O
- -X- _ O
De -X- _ O
) -X- _ O
and -X- _ O
WMT16 -X- _ B-DatasetName
English -X- _ O
- -X- _ O
Romanian -X- _ O
( -X- _ O
En -X- _ O
- -X- _ O
Ro -X- _ O
) -X- _ O
. -X- _ O

WMT14 -X- _ B-DatasetName
EnDe -X- _ O
/ -X- _ O
WMT16 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
Ro -X- _ O
datasets -X- _ O
contain -X- _ O
4.5M -X- _ O
/ -X- _ O
610 -X- _ O
K -X- _ O
training -X- _ O
pairs -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

For -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
dataset -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
preprocessing -X- _ O
pipelines -X- _ O
provided -X- _ O
by -X- _ O
fairseq1 -X- _ O
( -X- _ O
Ott -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
WMT16 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
Ro -X- _ O
dataset -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
preprocessed -X- _ O
corpus -X- _ O
provided -X- _ O
by -X- _ O
Lee -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Preprocessed -X- _ O
datasets -X- _ O
share -X- _ O
a -X- _ O
vocabulary -X- _ O
dictionary -X- _ O
between -X- _ O
the -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
languages -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
fast -X- _ O
align -X- _ O
( -X- _ O
FA -X- _ O
) -X- _ O
( -X- _ O
Dyer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
and -X- _ O
GIZA++ -X- _ O
( -X- _ O
GZ -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
known -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
accurate -X- _ O
than -X- _ O
fast -X- _ O
align -X- _ O
, -X- _ O
as -X- _ O
word -X- _ O
alignment -X- _ O
tools -X- _ O
. -X- _ O

All -X- _ O
the -X- _ O
corpus -X- _ O
are -X- _ O
passed -X- _ O
to -X- _ O
the -X- _ O
alignment -X- _ O
tools -X- _ O
at -X- _ O
the -X- _ O
subword -X- _ O
- -X- _ O
level -X- _ O
. -X- _ O

We -X- _ O
ﬁlter -X- _ O
out -X- _ O
samples -X- _ O
where -X- _ O
the -X- _ O
maximum -X- _ O
number -X- _ O
of -X- _ O
duplications -X- _ O
exceed -X- _ O
16 -X- _ O
. -X- _ O

We -X- _ O
explain -X- _ O
the -X- _ O
details -X- _ O
of -X- _ O
the -X- _ O
alignment -X- _ O
processing -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
sequence -X- _ O
- -X- _ O
level -X- _ O
knowledge -X- _ O
distillation -X- _ O
method -X- _ O
( -X- _ O
KD -X- _ O
) -X- _ O
for -X- _ O
the -X- _ O
distillation -X- _ O
set -X- _ O
. -X- _ O

Transformer -X- _ O
ART -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
to -X- _ O
generate -X- _ O
the -X- _ O
distillation -X- _ O
set -X- _ O
for -X- _ O
each -X- _ O
translation -X- _ O
direction -X- _ O
. -X- _ O

4.2 -X- _ O
Models -X- _ O
and -X- _ O
Baselines -X- _ O
We -X- _ O
compare -X- _ O
our -X- _ O
model -X- _ O
with -X- _ O
several -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
baselines -X- _ O
, -X- _ O
and -X- _ O
divide -X- _ O
the -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
into -X- _ O
two -X- _ O
types -X- _ O
as -X- _ O
aforementioned -X- _ O
: -X- _ O
implicit -X- _ O
dependency -X- _ O
modeling -X- _ O
andexplicit -X- _ O
modality -X- _ O
reduction -X- _ O
( -X- _ O
see -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
train -X- _ O
the -X- _ O
ART -X- _ B-MethodName
models -X- _ O
and -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
NAT -X- _ I-MethodName
for -X- _ O
the -X- _ O
analysis -X- _ O
. -X- _ O

Our -X- _ O
models -X- _ O
are -X- _ O
implemented -X- _ O
based -X- _ O
on -X- _ O
fairseq -X- _ O
. -X- _ O

1https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
pytorch -X- _ O
/ -X- _ O
fairseq -X- _ O

Models -X- _ O
En -X- _ O
! -X- _ O
De -X- _ O
! -X- _ O
Time -X- _ O
Speedup -X- _ O
En -X- _ O
! -X- _ O
Ro -X- _ O
! -X- _ O
Time -X- _ O
Speedup -X- _ O
Autoregressive -X- _ O
Models -X- _ O
Non -X- _ O
- -X- _ O
iterative -X- _ O
Non -X- _ O
- -X- _ O
autoregressive -X- _ O
Models -X- _ O
( -X- _ O
implicit -X- _ O
dependency -X- _ O
modeling -X- _ O
) -X- _ O
Non -X- _ O
- -X- _ O
iterative -X- _ O
Non -X- _ O
- -X- _ O
autoregressive -X- _ O
Models -X- _ O
( -X- _ O
explicit -X- _ O
modality -X- _ O
reduction -X- _ O
) -X- _ O
Table -X- _ O
1 -X- _ O
: -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
and -X- _ O
inference -X- _ O
speed -X- _ O
of -X- _ O
baselines -X- _ O
and -X- _ O
our -X- _ O
model -X- _ O
on -X- _ O
four -X- _ O
translation -X- _ O
tasks -X- _ O
. -X- _ O

Time -X- _ O
is -X- _ O
an -X- _ O
average -X- _ O
sentence -X- _ O
- -X- _ O
wise -X- _ O
latency -X- _ O
in -X- _ O
milliseconds -X- _ O
. -X- _ O

Speedup -X- _ O
is -X- _ O
a -X- _ O
relative -X- _ O
speedup -X- _ O
ratio -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
ART -X- _ I-MethodName
model -X- _ O
with -X- _ O
beam -X- _ B-HyperparameterName
width -X- _ I-HyperparameterName
5 -X- _ B-HyperparameterValue
. -X- _ O

AligNART -X- _ B-MethodName
is -X- _ O
implemented -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
Transformer -X- _ I-MethodName
architecture -X- _ O
. -X- _ O

We -X- _ O
set -X- _ O
dmodel -X- _ B-HyperparameterName
= -X- _ O
dhidden -X- _ B-HyperparameterName
to -X- _ O
512=2048 -X- _ B-HyperparameterValue
and -X- _ O
the -X- _ O
dropout -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
to -X- _ O
0.3 -X- _ B-HyperparameterValue
. -X- _ O

The -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
heads -X- _ I-HyperparameterName
in -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
attention -X- _ O
modules -X- _ O
is -X- _ O
8 -X- _ B-HyperparameterValue
except -X- _ O
for -X- _ O
the -X- _ O
last -X- _ O
attention -X- _ O
module -X- _ O
of -X- _ O
the -X- _ O
permutation -X- _ O
predictor -X- _ O
which -X- _ O
is -X- _ O
1 -X- _ B-HyperparameterValue
. -X- _ O

We -X- _ O
set -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
to -X- _ O
approximately -X- _ O
64 -X- _ B-HyperparameterValue
K -X- _ I-HyperparameterValue
tokens -X- _ O
for -X- _ O
all -X- _ O
the -X- _ O
models -X- _ O
we -X- _ O
implement -X- _ O
. -X- _ O

All -X- _ O
these -X- _ O
models -X- _ O
we -X- _ O
implement -X- _ O
are -X- _ O
trained -X- _ O
for -X- _ O
300 -X- _ O
K -X- _ O
= -X- _ O
50 -X- _ O
K -X- _ O
steps -X- _ O
on -X- _ O
EnDe -X- _ O
= -X- _ O
En -X- _ O
- -X- _ O
Ro -X- _ O
datasets -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

For -X- _ O
AligNART -X- _ B-MethodName
, -X- _ O
we -X- _ O
average -X- _ O
5 -X- _ O
checkpoints -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
validation -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
in -X- _ O
the -X- _ O
20 -X- _ O
latest -X- _ O
checkpoints -X- _ O
. -X- _ O

For -X- _ O
optimization -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
Adam -X- _ O
optimizer -X- _ O
= -X- _ O
10 8 -X- _ O
. -X- _ O

The -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
scheduling -X- _ O
follows -X- _ O
that -X- _ O
of -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
starting -X- _ O
from -X- _ O
10 7 -X- _ B-HyperparameterValue
and -X- _ O
warms -X- _ O
up -X- _ O
to -X- _ O
5e-4 -X- _ B-HyperparameterValue
in -X- _ O
10 -X- _ O
K -X- _ O
steps -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
label -X- _ O
smoothing -X- _ O
technique -X- _ O
with -X- _ O
ls= -X- _ O
0:1for -X- _ O
the -X- _ O
target -X- _ O
token -X- _ O
distribution -X- _ O
and -X- _ O
each -X- _ O
row -X- _ O
of -X- _ O
permutation -X- _ O
matrix -X- _ O
. -X- _ O

The -X- _ O
translation -X- _ O
latency -X- _ O
is -X- _ O
measured -X- _ O
on -X- _ O
an -X- _ O
NVIDIA -X- _ O
Tesla -X- _ O
V100 -X- _ O
GPU -X- _ O
. -X- _ O

5 -X- _ O
Results -X- _ O
5.1 -X- _ O
Main -X- _ O
Results -X- _ O
Table -X- _ O
1 -X- _ O
shows -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
, -X- _ O
translation -X- _ O
latency -X- _ O
and -X- _ O
speedup -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
and -X- _ O
WMT16 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
WMT16 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
Ro -X- _ O
Models -X- _ O
En -X- _ O
! -X- _ O
De -X- _ O
! -X- _ O

En -X- _ O
! -X- _ O
Ro -X- _ O
! -X- _ O
Table -X- _ O
2 -X- _ O
: -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
of -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
with -X- _ O
re -X- _ O
- -X- _ O
scoring -X- _ O
decoding -X- _ O
scheme -X- _ O
of -X- _ O
ncandidates -X- _ O
. -X- _ O

Ro -X- _ O
. -X- _ O

In -X- _ O
explicit -X- _ O
modality -X- _ O
reduction -X- _ O
, -X- _ O
AligNART -X- _ B-MethodName
( -X- _ O
FA -X- _ O
) -X- _ O
achieves -X- _ O
higher -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
than -X- _ O
Distortion -X- _ B-MethodName
and -X- _ O
ReorderNAT -X- _ B-MethodName
, -X- _ O
which -X- _ O
utilize -X- _ O
the -X- _ O
same -X- _ O
alignment -X- _ O
tool -X- _ O
, -X- _ O
since -X- _ O
we -X- _ O
leverage -X- _ O
the -X- _ O
entire -X- _ O
alignment -X- _ O
information -X- _ O
rather -X- _ O
than -X- _ O
partial -X- _ O
information -X- _ O
such -X- _ O
as -X- _ O
fertility -X- _ O
or -X- _ O
ordering -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
AligNART -X- _ B-MethodName
( -X- _ O
GZ -X- _ O
) -X- _ O
signiﬁcantly -X- _ O
outperforms -X- _ O
previous -X- _ O
models -X- _ O
for -X- _ O
explicit -X- _ O
modality -X- _ O
reduction -X- _ O
except -X- _ O
for -X- _ O
SNAT -X- _ B-MethodName
on -X- _ O
En -X- _ O
! -X- _ O
Ro -X- _ O
. -X- _ O

In -X- _ O
implicit -X- _ O
dependency -X- _ O
modeling -X- _ O
, -X- _ O
AligNART -X- _ B-MethodName
( -X- _ O
GZ -X- _ O
) -X- _ O
outperforms -X- _ O
Imputer -X- _ O
and -X- _ O
shows -X- _ O
performance -X- _ O
comparable -X- _ O
to -X- _ O
that -X- _ O
of -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
CTC -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
model -X- _ I-MethodName
on -X- _ O
En -X- _ O
$ -X- _ O
De -X- _ O
by -X- _ O
simply -X- _ O
augmenting -X- _ O
Aligner -X- _ O
module -X- _ O
to -X- _ O
deep -X- _ O
- -X- _ O
shallow -X- _ O
NAT -X- _ B-MethodName
. -X- _ O

In -X- _ O
this -X- _ O
study -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
introducing -X- _ O
complete -X- _ O
information -X- _ O
in -X- _ O
word -X- _ O
alignments -X- _ O
; -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
modify -X- _ O
the -X- _ O
objective -X- _ O
function -X- _ O
, -X- _ O

Models -X- _ O
D -X- _ O
P -X- _ O
G -X- _ O
D -X- _ O
P -X- _ O
G -X- _ O
Table -X- _ O
3 -X- _ O
: -X- _ O
Duplication -X- _ O
( -X- _ O
D -X- _ O
) -X- _ O
, -X- _ O
permutation -X- _ O
( -X- _ O
P -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
grouping -X- _ O
( -X- _ O
G -X- _ O
) -X- _ O
accuracy -X- _ O
of -X- _ O
Aligner -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
validation -X- _ O
set -X- _ O
. -X- _ O

Accuracy -X- _ O
on -X- _ O
raw -X- _ O
and -X- _ O
distilled -X- _ O
datasets -X- _ O
are -X- _ O
written -X- _ O
on -X- _ O
the -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
of -X- _ O
slash -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

Source -X- _ O
Denken -X- _ O
Sie -X- _ O
, -X- _ O
dass -X- _ O
die -X- _ O
Medien -X- _ O
zu -X- _ O
viel -X- _ O
vom -X- _ O
PS -X- _ O
_ -X- _ O
G -X- _ O
erwarten -X- _ O
? -X- _ O

Reference -X- _ O
Do -X- _ O
you -X- _ O
think -X- _ O
the -X- _ O
media -X- _ O
expect -X- _ O
too -X- _ O
much -X- _ O
of -X- _ O
PS -X- _ O
_ -X- _ O
G -X- _ O
? -X- _ O

NAT -X- _ B-MethodName
( -X- _ O
12 -X- _ O
- -X- _ O
1 -X- _ O
) -X- _ O
Do -X- _ O
you -X- _ O
think -X- _ O
that -X- _ O
the -X- _ O
expect -X- _ O
expect -X- _ O
much -X- _ O
from -X- _ O
the -X- _ O
PS -X- _ O
_ -X- _ O
G -X- _ O
? -X- _ O

OursDuplication -X- _ O
Denken -X- _ O
Denken -X- _ O
Sie -X- _ O
, -X- _ O
dass -X- _ O
die -X- _ O
Medien -X- _ O
zu -X- _ O
viel -X- _ O
vom -X- _ O
vom -X- _ O
PSG -X- _ O
PSG -X- _ O
erwarten -X- _ O
? -X- _ O

Permutation -X- _ O
Denken -X- _ O
Sie -X- _ O
Denken -X- _ O
, -X- _ O
dass -X- _ O
die -X- _ O
Medien -X- _ O
erwarten -X- _ O
zu -X- _ O
viel -X- _ O
vom -X- _ O
vom -X- _ O
PSG -X- _ O
PSG -X- _ O
? -X- _ O

Grouping -X- _ O
Denken -X- _ O
Sie -X- _ O
Denken -X- _ O
, -X- _ O
dass -X- _ O
die -X- _ O
Medien -X- _ O
erwarten -X- _ O
zu -X- _ O
viel -X- _ O
vom -X- _ O
vom -X- _ O
PSG -X- _ O
PSG -X- _ O
? -X- _ O

Output -X- _ O
Do -X- _ O
you -X- _ O
think -X- _ O
that -X- _ O
the -X- _ O
media -X- _ O
expect -X- _ O
too -X- _ O
much -X- _ O
from -X- _ O
thePS_G -X- _ O
? -X- _ O

Table -X- _ O
4 -X- _ O
: -X- _ O
Visualized -X- _ O
translation -X- _ O
example -X- _ O
of -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
NAT -X- _ I-MethodName
and -X- _ O
AligNART -X- _ B-MethodName
( -X- _ O
FA -X- _ O
) -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
De -X- _ O
! -X- _ O

En -X- _ O
validation -X- _ O
set -X- _ O
. -X- _ O

" -X- _ O
_ -X- _ O
" -X- _ O
stands -X- _ O
for -X- _ O
subword -X- _ O
tokenization -X- _ O
. -X- _ O

Highlighted -X- _ O
tokens -X- _ O
in -X- _ O
duplication -X- _ O
, -X- _ O
permutation -X- _ O
, -X- _ O
and -X- _ O
grouping -X- _ O
processes -X- _ O
are -X- _ O
modiﬁed -X- _ O
by -X- _ O
the -X- _ O
each -X- _ O
module -X- _ O
of -X- _ O
Aligner -X- _ O
. -X- _ O

Highlighted -X- _ O
tokens -X- _ O
in -X- _ O
output -X- _ O
correspond -X- _ O
to -X- _ O
the -X- _ O
tokens -X- _ O
highlighted -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
colors -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
processes -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
Aligner -X- _ O
ﬁrst -X- _ O
applies -X- _ O
mean -X- _ O
pooling -X- _ O
to -X- _ O
convert -X- _ O
subword -X- _ O
- -X- _ O
level -X- _ O
encoder -X- _ O
outputs -X- _ O
into -X- _ O
word -X- _ O
- -X- _ O
level -X- _ O
, -X- _ O
as -X- _ O
explained -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
. -X- _ O

fast -X- _ O
align -X- _ O
GIZA++ -X- _ O
Models -X- _ O
En -X- _ O
! -X- _ O
De -X- _ O
! -X- _ O
En -X- _ O
! -X- _ O
De -X- _ O
! -X- _ O
Table -X- _ O
5 -X- _ O
: -X- _ O
BLEU -X- _ B-DatasetName
scores -X- _ O
of -X- _ O
Aligner -X- _ O
ablation -X- _ O
study -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

which -X- _ O
can -X- _ O
be -X- _ O
explored -X- _ O
in -X- _ O
the -X- _ O
future -X- _ O
work -X- _ O
. -X- _ O

Table -X- _ O
2 -X- _ O
shows -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
with -X- _ O
re -X- _ O
- -X- _ O
scoring -X- _ O
decoding -X- _ O
strategies -X- _ O
of -X- _ O
the -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
modfor -X- _ O
8 -X- _ O
candidates -X- _ O
. -X- _ O

AligNART -X- _ B-MethodName
outperforms -X- _ O
the -X- _ O
baselines -X- _ O
on -X- _ O
En -X- _ O
! -X- _ O
De -X- _ O
and -X- _ O
Ro -X- _ O
! -X- _ O
En -X- _ O
, -X- _ O
and -X- _ O
shows -X- _ O
performance -X- _ O
similar -X- _ O
to -X- _ O
that -X- _ O
of -X- _ O
GLAT -X- _ B-MethodName
on -X- _ O
De -X- _ O
! -X- _ O

En -X- _ O
. -X- _ O

In -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
for -X- _ O
explicit -X- _ O
modality -X- _ O
reduction -X- _ O
, -X- _ O
AligNART -X- _ B-MethodName
shows -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
on -X- _ O
En -X- _ O
$ -X- _ O
De -X- _ O
and -X- _ O
Ro -X- _ O
! -X- _ O
En -X- _ O
. -X- _ O

5.2 -X- _ O
Analysis -X- _ O
of -X- _ O
Aligner -X- _ O
Components -X- _ O
In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
investigate -X- _ O
the -X- _ O
accuracy -X- _ O
, -X- _ O
example -X- _ O
, -X- _ O
and -X- _ O
ablation -X- _ O
results -X- _ O
of -X- _ O
Aligner -X- _ O
components -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
, -X- _ O
4 -X- _ O
, -X- _ O
and -X- _ O
5 -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
we -X- _ O
partially -X- _ O
provide -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
D -X- _ O
or -X- _ O
P -X- _ O
matrices -X- _ O
during -X- _ O
the -X- _ O
accuracy -X- _ O
measurement -X- _ O
. -X- _ O

Knowledge -X- _ O
Distillation -X- _ O
In -X- _ O
Table -X- _ O
3 -X- _ O
, -X- _ O
a -X- _ O
comparison -X- _ O
of -X- _ O
accuracy -X- _ O
between -X- _ O
raw -X- _ O
and -X- _ O
distilled -X- _ O
datasets -X- _ O
shows -X- _ O
that -X- _ O
KD -X- _ O
signiﬁcantly -X- _ O
decreases -X- _ O
multi -X- _ O
- -X- _ O
modality -X- _ O
of -X- _ O
each -X- _ O
component -X- _ O
. -X- _ O

After -X- _ O
KD -X- _ O
, -X- _ O
AligNART -X- _ B-MethodName
shows -X- _ O
marginally -X- _ O
reduced -X- _ O
accuracy -X- _ B-MetricName
on -X- _ O
the -X- _ O
raw -X- _ O
dataset -X- _ O
, -X- _ O
but -X- _ O
high -X- _ O
prediction -X- _ O
accuracy -X- _ B-MetricName
in -X- _ O
each -X- _ O
component -X- _ O
on -X- _ O
the -X- _ O
distillation -X- _ O
set -X- _ O
, -X- _ O
resulting -X- _ O
in -X- _ O
increased -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
. -X- _ O

Alignment -X- _ O
Tool -X- _ O
Before -X- _ O
KD -X- _ O
, -X- _ O
AligNART -X- _ B-MethodName
using -X- _ O
fast -X- _ O
align -X- _ O
and -X- _ O
GIZA++ -X- _ O
have -X- _ O
accuracy -X- _ O
bottlenecks -X- _ O
in -X- _ O
permutation -X- _ O
and -X- _ O
duplication -X- _ O
predictors -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
imply -X- _ O
that -X- _ O
the -X- _ O
alignment -X- _ O
tools -X- _ O
have -X- _ O
different -X- _ O
degrees -X- _ O
of -X- _ O
multimodality -X- _ O
on -X- _ O
the -X- _ O
D -X- _ O
, -X- _ O
P -X- _ O
, -X- _ O
and -X- _ O
G -X- _ O
matrices -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
explored -X- _ O
in -X- _ O
the -X- _ O
future -X- _ O
work -X- _ O
. -X- _ O

Qualitative -X- _ O
Study -X- _ O
Table -X- _ O
4 -X- _ O
shows -X- _ O
an -X- _ O
example -X- _ O
of -X- _ O
addressing -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
modality -X- _ O
problem -X- _ O
. -X- _ O

Deepshallow -X- _ B-MethodName
NAT -X- _ I-MethodName
monotonically -X- _ O
copies -X- _ O
the -X- _ O
encoder -X- _ O
outputs -X- _ O
and -X- _ O
suffers -X- _ O
from -X- _ O
repetition -X- _ O
and -X- _ O
omission -X- _ O
problems -X- _ O
. -X- _ O

AligNART -X- _ B-MethodName
( -X- _ O
FA -X- _ O
) -X- _ O
does -X- _ O
not -X- _ O
show -X- _ O
the -X- _ O
inconsistency -X- _ O
problems -X- _ O
thanks -X- _ O
to -X- _ O
the -X- _ O
well -X- _ O
- -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
, -X- _ O
which -X- _ O
signiﬁcantly -X- _ O
reduces -X- _ O
the -X- _ O
modality -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
distribution -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
conducted -X- _ O
a -X- _ O
case -X- _ O
study -X- _ O
on -X- _ O
predicted -X- _ O
alignments -X- _ O
and -X- _ O
their -X- _ O
translations -X- _ O
during -X- _ O
re -X- _ O
- -X- _ O
scoring -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
. -X- _ O

Ablation -X- _ O
Study -X- _ O
We -X- _ O
conduct -X- _ O
an -X- _ O
analysis -X- _ O
of -X- _ O
alignment -X- _ O
estimation -X- _ O
by -X- _ O
ablating -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
predictors -X- _ O

En -X- _ O
! -X- _ O
De -X- _ O
! -X- _ O
Imputer -X- _ B-MethodName
( -X- _ O
CTC -X- _ B-MethodName
, -X- _ O
w -X- _ O
/ -X- _ O
o -X- _ O
KD -X- _ O
) -X- _ O
15.6 -X- _ O
Table -X- _ O
6 -X- _ O
: -X- _ O
BLEU -X- _ O
scores -X- _ O
of -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
models -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
test -X- _ O
set -X- _ O
, -X- _ O
with -X- _ O
or -X- _ O
without -X- _ O
KD -X- _ O
. -X- _ O

during -X- _ O
inference -X- _ O
. -X- _ O

We -X- _ O
ablate -X- _ O
each -X- _ O
module -X- _ O
in -X- _ O
Aligner -X- _ O
by -X- _ O
replacing -X- _ O
the -X- _ O
predicted -X- _ O
matrix -X- _ O
with -X- _ O
an -X- _ O
identical -X- _ O
matrixI -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
5 -X- _ O
indicate -X- _ O
that -X- _ O
each -X- _ O
module -X- _ O
in -X- _ O
Aligner -X- _ O
properly -X- _ O
estimates -X- _ O
the -X- _ O
decomposed -X- _ O
information -X- _ O
in -X- _ O
word -X- _ O
alignments -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
an -X- _ O
exception -X- _ O
in -X- _ O
GIZA++ -X- _ O
where -X- _ O
many -X- _ O
- -X- _ O
toone -X- _ O
mapping -X- _ O
does -X- _ O
not -X- _ O
exist -X- _ O
, -X- _ O
resulting -X- _ O
in -X- _ O
performance -X- _ O
equal -X- _ O
to -X- _ O
that -X- _ O
without -X- _ O
the -X- _ O
grouping -X- _ O
predictor -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
AligNART -X- _ B-MethodName
achieves -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
comparable -X- _ O
to -X- _ O
those -X- _ O
of -X- _ O
CTC -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
models -X- _ I-MethodName
on -X- _ O
En -X- _ O
$ -X- _ O
De -X- _ O
even -X- _ O
with -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
word -X- _ O
alignments -X- _ O
of -X- _ O
partial -X- _ O
information -X- _ O
. -X- _ O

5.3 -X- _ O
Analysis -X- _ O
of -X- _ O
Modality -X- _ O
Reduction -X- _ O
Effects -X- _ O
To -X- _ O
evaluate -X- _ O
the -X- _ O
modality -X- _ O
reduction -X- _ O
effects -X- _ O
of -X- _ O
AligNART -X- _ B-MethodName
, -X- _ O
we -X- _ O
conducted -X- _ O
experiments -X- _ O
on -X- _ O
two -X- _ O
aspects -X- _ O
: -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
and -X- _ O
token -X- _ B-MetricName
repetition -X- _ I-MetricName
ratio -X- _ I-MetricName
. -X- _ O

Table -X- _ O
6 -X- _ O
shows -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
. -X- _ O

For -X- _ O
En -X- _ O
! -X- _ O
De -X- _ O
, -X- _ O
AligNART -X- _ B-MethodName
using -X- _ O
fast -X- _ O
align -X- _ O
without -X- _ O
KD -X- _ O
achieves -X- _ O
higher -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
than -X- _ O
previous -X- _ O
models -X- _ O
without -X- _ O
KD -X- _ O
and -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
NAT -X- _ I-MethodName
with -X- _ O
KD -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
indicate -X- _ O
that -X- _ O
our -X- _ O
method -X- _ O
is -X- _ O
effective -X- _ O
even -X- _ O
without -X- _ O
KD -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
known -X- _ O
to -X- _ O
decrease -X- _ O
data -X- _ O
complexity -X- _ O
( -X- _ O
Zhou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020a -X- _ O
) -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
alignments -X- _ O
from -X- _ O
GIZA++ -X- _ O
without -X- _ O
KD -X- _ O
are -X- _ O
more -X- _ O
complex -X- _ O
for -X- _ O
AligNART -X- _ B-MethodName
to -X- _ O
learn -X- _ O
, -X- _ O
resulting -X- _ O
in -X- _ O
lower -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
than -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
NAT -X- _ I-MethodName
with -X- _ O
KD -X- _ O
. -X- _ O

Ghazvininejad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
measured -X- _ O
the -X- _ O
token -X- _ B-MetricName
repetition -X- _ I-MetricName
ratio -X- _ I-MetricName
as -X- _ O
a -X- _ O
proxy -X- _ O
for -X- _ O
measuring -X- _ O
multimodality -X- _ O
. -X- _ O

The -X- _ O
token -X- _ B-MetricName
repetition -X- _ I-MetricName
ratio -X- _ I-MetricName
represents -X- _ O
the -X- _ O
degree -X- _ O
of -X- _ O
the -X- _ O
inconsistency -X- _ O
problem -X- _ O
. -X- _ O

In -X- _ O
Table -X- _ O
7 -X- _ O
, -X- _ O
the -X- _ O
token -X- _ B-MetricName
repetition -X- _ I-MetricName
ratio -X- _ I-MetricName
of -X- _ O
AligNART -X- _ B-MethodName
is -X- _ O
less -X- _ O
than -X- _ O
that -X- _ O
of -X- _ O
the -X- _ O
CMLM -X- _ B-MethodName
- -X- _ I-MethodName
base -X- _ I-MethodName
( -X- _ O
Ghazvininejad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
of -X- _ O
5 -X- _ O
iterations -X- _ O
, -X- _ O
AXE -X- _ B-MethodName
, -X- _ O
and -X- _ O
GLAT -X- _ B-MethodName
. -X- _ O

We -X- _ O
also -X- _ O
observe -X- _ O
that -X- _ O
the -X- _ O
decline -X- _ O
in -X- _ O
the -X- _ O
token -X- _ B-MetricName
repetition -X- _ I-MetricName
ratio -X- _ I-MetricName
from -X- _ O
Aligner -X- _ O
is -X- _ O
signiﬁcantly -X- _ O
larger -X- _ O
than -X- _ O
that -X- _ O
from -X- _ O
KD -X- _ O
. -X- _ O

Combined -X- _ O
with -X- _ O
the -X- _ O
results -X- _ O
from -X- _ O
Table -X- _ O
6 -X- _ O
, -X- _ O
alignmentWMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
En -X- _ O
! -X- _ O
De -X- _ O
! -X- _ O
CMLM -X- _ B-MethodName
- -X- _ I-MethodName
base -X- _ I-MethodName
( -X- _ O
5 -X- _ O
iterations -X- _ O
) -X- _ O
0.72 -X- _ O
% -X- _ O
Table -X- _ O
7 -X- _ O
: -X- _ O
Token -X- _ B-MetricName
repetition -X- _ I-MetricName
ratio -X- _ I-MetricName
of -X- _ O
NART -X- _ B-MethodName
models -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
En -X- _ O
! -X- _ O
De -X- _ O
! -X- _ O
- -X- _ O
Cross -X- _ O
attention -X- _ O
17.2 -X- _ O
21.9 -X- _ O
- -X- _ O
Cross -X- _ O
attention -X- _ O
26.1 -X- _ O
29.9 -X- _ O
Table -X- _ O
8 -X- _ O
: -X- _ O
Ablation -X- _ O
results -X- _ O
of -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
NAT -X- _ I-MethodName
and -X- _ O
AligNART -X- _ B-MethodName
( -X- _ O
GZ -X- _ O
) -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

information -X- _ O
adequately -X- _ O
alleviates -X- _ O
the -X- _ O
token -X- _ O
repetition -X- _ O
issue -X- _ O
even -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
where -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
is -X- _ O
lower -X- _ O
than -X- _ O
that -X- _ O
of -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
NAT -X- _ I-MethodName
with -X- _ O
KD -X- _ O
. -X- _ O

5.4 -X- _ O
Ablation -X- _ O
Study -X- _ O
We -X- _ O
conduct -X- _ O
several -X- _ O
extensive -X- _ O
experiments -X- _ O
to -X- _ O
analyze -X- _ O
our -X- _ O
method -X- _ O
further -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
8 -X- _ O
and -X- _ O
9 -X- _ O
. -X- _ O

Each -X- _ O
of -X- _ O
our -X- _ O
method -X- _ O
consistently -X- _ O
improves -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
AligNART -X- _ B-MethodName
. -X- _ O

Cross -X- _ O
Attention -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
8 -X- _ O
, -X- _ O
we -X- _ O
ablate -X- _ O
the -X- _ O
cross -X- _ O
attention -X- _ O
module -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
observe -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
and -X- _ O
alignment -X- _ O
learning -X- _ O
of -X- _ O
the -X- _ O
cross -X- _ O
attention -X- _ O
module -X- _ O
. -X- _ O

We -X- _ O
train -X- _ O
AligNART -X- _ B-MethodName
and -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
NAT -X- _ I-MethodName
without -X- _ O
a -X- _ O
cross -X- _ O
attention -X- _ O
module -X- _ O
for -X- _ O
comparison -X- _ O
. -X- _ O

AligNART -X- _ B-MethodName
without -X- _ O
the -X- _ O
cross -X- _ O
attention -X- _ O
module -X- _ O
has -X- _ O
a -X- _ O
smaller -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
than -X- _ O
the -X- _ O
deep -X- _ B-MethodName
- -X- _ I-MethodName
shallow -X- _ I-MethodName
NAT -X- _ I-MethodName
. -X- _ O

The -X- _ O
cross -X- _ O
attention -X- _ O
module -X- _ O
is -X- _ O
known -X- _ O
to -X- _ O
learn -X- _ O
alignments -X- _ O
between -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
tokens -X- _ O
( -X- _ O
Bahdanau -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
result -X- _ O
implies -X- _ O
that -X- _ O
aligned -X- _ O
decoder -X- _ O
inputs -X- _ O
signiﬁcantly -X- _ O
ofﬂoad -X- _ O
the -X- _ O
role -X- _ O
of -X- _ O
the -X- _ O
cross -X- _ O
attention -X- _ O
module -X- _ O
. -X- _ O

Deep -X- _ O
- -X- _ O
shallow -X- _ O
Architecture -X- _ O
Deep -X- _ O
- -X- _ O
shallow -X- _ O
architecture -X- _ O
heavily -X- _ O
affects -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
of -X- _ O
Alig- -X- _ O

Table -X- _ O
9 -X- _ O
: -X- _ O
Alignment -X- _ O
score -X- _ O
ﬁltering -X- _ O
ratio -X- _ O
and -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
on -X- _ O
WMT14 -X- _ O
En -X- _ O
- -X- _ O
De -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

NART -X- _ B-MethodName
as -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
8 -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
indicate -X- _ O
that -X- _ O
the -X- _ O
deep -X- _ O
encoder -X- _ O
assists -X- _ O
alignment -X- _ O
estimation -X- _ O
, -X- _ O
whereas -X- _ O
the -X- _ O
shallow -X- _ O
decoder -X- _ O
with -X- _ O
aligned -X- _ O
inputs -X- _ O
has -X- _ O
a -X- _ O
lower -X- _ O
impact -X- _ O
on -X- _ O
performance -X- _ O
degeneration -X- _ O
. -X- _ O

Alignment -X- _ O
Score -X- _ O
Filtering -X- _ O
We -X- _ O
investigate -X- _ O
the -X- _ O
trade -X- _ O
- -X- _ O
off -X- _ O
between -X- _ O
the -X- _ O
alignment -X- _ O
score -X- _ O
ﬁltering -X- _ O
ratio -X- _ O
and -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
using -X- _ O
AligNART -X- _ B-MethodName
( -X- _ O
GZ -X- _ O
) -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
9 -X- _ O
. -X- _ O

Samples -X- _ O
with -X- _ O
low -X- _ O
alignment -X- _ O
scores -X- _ O
are -X- _ O
more -X- _ O
likely -X- _ O
to -X- _ O
contain -X- _ O
noise -X- _ O
caused -X- _ O
by -X- _ O
distilled -X- _ O
targets -X- _ O
or -X- _ O
an -X- _ O
alignment -X- _ O
tool -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
ﬁltering -X- _ O
out -X- _ O
of -X- _ O
5 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
the -X- _ O
samples -X- _ O
improves -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
in -X- _ O
both -X- _ O
the -X- _ O
directions -X- _ O
. -X- _ O

Surprisingly -X- _ O
, -X- _ O
increasing -X- _ O
the -X- _ O
ﬁltering -X- _ B-HyperparameterName
ratio -X- _ I-HyperparameterName
up -X- _ O
to -X- _ O
20 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
preserves -X- _ O
the -X- _ O
performance -X- _ O
thanks -X- _ O
to -X- _ O
the -X- _ O
noise -X- _ O
ﬁltering -X- _ O
capability -X- _ O
. -X- _ O

6 -X- _ O
Related -X- _ O
Work -X- _ O
6.1 -X- _ O
Non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
After -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
proposed -X- _ O
NAT -X- _ B-MethodName
, -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
has -X- _ O
been -X- _ O
investigated -X- _ O
in -X- _ O
various -X- _ O
directions -X- _ O
to -X- _ O
maximize -X- _ O
translation -X- _ O
speed -X- _ O
while -X- _ O
maintaining -X- _ O
translation -X- _ O
quality -X- _ O
. -X- _ O

Shao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
Shao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Ghazvininejad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2020 -X- _ O
) -X- _ O
address -X- _ O
the -X- _ O
limitations -X- _ O
of -X- _ O
conventional -X- _ O
cross -X- _ O
entropy -X- _ O
based -X- _ O
objectives -X- _ O
that -X- _ O
overly -X- _ O
penalize -X- _ O
consistent -X- _ O
predictions -X- _ O
. -X- _ O

and -X- _ O
Lee -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
introduce -X- _ O
latent -X- _ O
variables -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
complex -X- _ O
dependencies -X- _ O
between -X- _ O
target -X- _ O
tokens -X- _ O
. -X- _ O

Saharia -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
Gu -X- _ O
and -X- _ O
Kong -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
apply -X- _ O
CTC -X- _ B-MetricName
loss -X- _ I-MetricName
to -X- _ O
the -X- _ O
NMT -X- _ B-TaskName
domain -X- _ O
. -X- _ O

Qian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
provide -X- _ O
target -X- _ O
tokens -X- _ O
to -X- _ O
the -X- _ O
decoder -X- _ O
during -X- _ O
training -X- _ O
using -X- _ O
the -X- _ O
glancing -X- _ O
sampling -X- _ O
technique -X- _ O
. -X- _ O

6.2 -X- _ O
Alignment -X- _ O
in -X- _ O
Parallel -X- _ O
Generative -X- _ O
Models -X- _ O
In -X- _ O
other -X- _ O
domains -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
speech -X- _ O
( -X- _ O
Ren -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
common -X- _ O
assumption -X- _ O
is -X- _ O
a -X- _ O
monotonicity -X- _ O
in -X- _ O
the -X- _ O
alignments -X- _ O
between -X- _ O
text -X- _ O
and -X- _ O
speech -X- _ O
. -X- _ O

Given -X- _ O
this -X- _ O
assumption -X- _ O
, -X- _ O
only -X- _ O
a -X- _ O
duration -X- _ O
predictor -X- _ O
is -X- _ O
required -X- _ O
to -X- _ O
alleviate -X- _ O
the -X- _ O
length -X- _ O
- -X- _ O
mismatch -X- _ O
problem -X- _ O
between -X- _ O
text -X- _ O
and -X- _ O
speech -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
modeling -X- _ O
the -X- _ O
alignment -X- _ O
in -X- _ O
the -X- _ O
NMT -X- _ B-TaskName
domain -X- _ O
is -X- _ O
challenging -X- _ O
since -X- _ O
the -X- _ O
alignment -X- _ O
contains -X- _ O
additional -X- _ O
ordering -X- _ O
and -X- _ O
grouping -X- _ O
information -X- _ O
. -X- _ O

Our -X- _ O
method -X- _ O
estimates -X- _ O
an -X- _ O
arbitrary -X- _ O
alignment -X- _ O
matrix -X- _ O
using -X- _ O
alignment -X- _ O
decomposition -X- _ O
.6.3 -X- _ O

Improving -X- _ O
NMT -X- _ B-TaskName
with -X- _ O
Enhanced -X- _ O
Information -X- _ O
To -X- _ O
alleviate -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
modality -X- _ O
problem -X- _ O
of -X- _ O
NART -X- _ B-MethodName
( -X- _ O
2021 -X- _ O
) -X- _ O
provide -X- _ O
additional -X- _ O
sentence -X- _ O
information -X- _ O
to -X- _ O
the -X- _ O
decoder -X- _ O
. -X- _ O

Alignment -X- _ O
is -X- _ O
considered -X- _ O
as -X- _ O
a -X- _ O
major -X- _ O
factor -X- _ O
in -X- _ O
machine -X- _ O
translation -X- _ O
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2007 -X- _ O
; -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Alkhouli -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
decompose -X- _ O
the -X- _ O
ART -X- _ O
model -X- _ O
into -X- _ O
alignment -X- _ O
and -X- _ O
lexical -X- _ O
models -X- _ O
. -X- _ O

Song -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
use -X- _ O
the -X- _ O
predicted -X- _ O
alignment -X- _ O
in -X- _ O
ART -X- _ O
models -X- _ O
to -X- _ O
constrain -X- _ O
vocabulary -X- _ O
candidates -X- _ O
during -X- _ O
decoding -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
alignment -X- _ O
estimation -X- _ O
in -X- _ O
NART -X- _ B-MethodName
is -X- _ O
much -X- _ O
challenging -X- _ O
since -X- _ O
the -X- _ O
information -X- _ O
of -X- _ O
decoding -X- _ O
outputs -X- _ O
is -X- _ O
limited -X- _ O
. -X- _ O

In -X- _ O
NART -X- _ B-MethodName
, -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
exploit -X- _ O
partial -X- _ O
information -X- _ O
from -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
alignments -X- _ O
. -X- _ O

In -X- _ O
contrast -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
the -X- _ O
alignment -X- _ O
decomposition -X- _ O
method -X- _ O
for -X- _ O
effective -X- _ O
alignment -X- _ O
estimation -X- _ O
in -X- _ O
NART -X- _ B-MethodName
where -X- _ O
we -X- _ O
leverage -X- _ O
the -X- _ O
complete -X- _ O
alignment -X- _ O
information -X- _ O
. -X- _ O

7 -X- _ O
Conclusion -X- _ O
and -X- _ O
Future -X- _ O
Work -X- _ O

In -X- _ O
this -X- _ O
study -X- _ O
, -X- _ O
we -X- _ O
leverage -X- _ O
full -X- _ O
alignment -X- _ O
information -X- _ O
to -X- _ O
directly -X- _ O
reduce -X- _ O
the -X- _ O
degree -X- _ O
of -X- _ O
the -X- _ O
multimodality -X- _ O
in -X- _ O
non -X- _ O
- -X- _ O
iterative -X- _ O
NART -X- _ B-MethodName
and -X- _ O
propose -X- _ O
an -X- _ O
alignment -X- _ O
decomposition -X- _ O
method -X- _ O
for -X- _ O
alignment -X- _ O
estimation -X- _ O
. -X- _ O

AligNART -X- _ B-MethodName
with -X- _ O
GIZA++ -X- _ O
shows -X- _ O
performance -X- _ O
comparable -X- _ O
to -X- _ O
that -X- _ O
of -X- _ O
the -X- _ O
recent -X- _ O
CTCbased -X- _ B-MethodName
implicit -X- _ I-MethodName
dependency -X- _ I-MethodName
modeling -X- _ I-MethodName
approach -X- _ O
on -X- _ O
WMT14 -X- _ O
En -X- _ O
- -X- _ O
De -X- _ O
and -X- _ O
modality -X- _ O
reduction -X- _ O
capability -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
AligNART -X- _ B-MethodName
depends -X- _ O
on -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
word -X- _ O
alignments -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
studied -X- _ O
in -X- _ O
the -X- _ O
future -X- _ O
work -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
study -X- _ O
on -X- _ O
the -X- _ O
combination -X- _ O
of -X- _ O
AligNART -X- _ B-MethodName
and -X- _ O
implicit -X- _ O
dependency -X- _ O
modeling -X- _ O
methods -X- _ O
. -X- _ O

Acknowledgement -X- _ O
This -X- _ O
work -X- _ O
was -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
National -X- _ O
Research -X- _ O
Foundation -X- _ O
of -X- _ O
Korea -X- _ O
( -X- _ O
NRF -X- _ O
) -X- _ O
grant -X- _ O
funded -X- _ O
by -X- _ O
the -X- _ O
Korea -X- _ O
government -X- _ O
( -X- _ O
Ministry -X- _ O
of -X- _ O
Science -X- _ O
and -X- _ O
ICT -X- _ O
) -X- _ O
of -X- _ O
the -X- _ O
Education -X- _ O
and -X- _ O
Research -X- _ O
Program -X- _ O
for -X- _ O
Future -X- _ O
ICT -X- _ O
Pioneers -X- _ O
, -X- _ O
Seoul -X- _ O
National -X- _ O
University -X- _ O
in -X- _ O
2021 -X- _ O
, -X- _ O
AIRS -X- _ O
Company -X- _ O
in -X- _ O
Hyundai -X- _ O
& -X- _ O
Kia -X- _ O
Motor -X- _ O
Company -X- _ O
through -X- _ O
HKMC -X- _ O
- -X- _ O
SNU -X- _ O
AI -X- _ O
Consortium -X- _ O
Fund -X- _ O
, -X- _ O
and -X- _ O
Kakao -X- _ O
Enterprise -X- _ O
. -X- _ O

10References -X- _ O
Nader -X- _ O
Akoury -X- _ O
, -X- _ O
Kalpesh -X- _ O
Krishna -X- _ O
, -X- _ O
and -X- _ O
Mohit -X- _ O
Iyyer -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Syntactically -X- _ O
supervised -X- _ O
transformers -X- _ O
for -X- _ O
faster -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
57th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
pages -X- _ O
1269–1281 -X- _ O
, -X- _ O
Florence -X- _ O
, -X- _ O
Italy -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Tamer -X- _ O
Alkhouli -X- _ O
, -X- _ O
Gabriel -X- _ O
Bretschner -X- _ O
, -X- _ O
and -X- _ O
Hermann -X- _ O
Ney -X- _ O
. -X- _ O
2018 -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
alignment -X- _ O
problem -X- _ O
in -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
attention -X- _ O
- -X- _ O
based -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ I-TaskName

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
Third -X- _ O
Conference -X- _ O
on -X- _ O
Machine -X- _ O
Translation -X- _ O
: -X- _ O
Research -X- _ O
Papers -X- _ O
, -X- _ O
pages -X- _ O
177–185 -X- _ O
, -X- _ O
Brussels -X- _ O
, -X- _ O
Belgium -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Dzmitry -X- _ O
Bahdanau -X- _ O
, -X- _ O
Kyunghyun -X- _ O
Cho -X- _ O
, -X- _ O
and -X- _ O
Yoshua -X- _ O
Bengio -X- _ O
. -X- _ O

2015 -X- _ O
. -X- _ O

Neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
by -X- _ O
jointly -X- _ O
learning -X- _ O
to -X- _ O
align -X- _ O
and -X- _ O
translate -X- _ O
. -X- _ O

In -X- _ O
3rd -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Learning -X- _ O
Representations -X- _ O
, -X- _ O
Conference -X- _ O
Track -X- _ O
Proceedings -X- _ O
. -X- _ O

Peter -X- _ O
F. -X- _ O
Brown -X- _ O
, -X- _ O
Stephen -X- _ O
A. -X- _ O
Della -X- _ O
Pietra -X- _ O
, -X- _ O
Vincent -X- _ O
J. -X- _ O
Della -X- _ O
Pietra -X- _ O
, -X- _ O
and -X- _ O
Robert -X- _ O
L. -X- _ O
Mercer -X- _ O
. -X- _ O
1993 -X- _ O
. -X- _ O

The -X- _ O
mathematics -X- _ O
of -X- _ O
statistical -X- _ O
machine -X- _ O
translation -X- _ O
: -X- _ O
Parameter -X- _ O
estimation -X- _ O
. -X- _ O

Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
19 -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
:263 -X- _ O
– -X- _ O
Jeff -X- _ O
Donahue -X- _ O
, -X- _ O
Sander -X- _ O
Dieleman -X- _ O
, -X- _ O
Mikolaj -X- _ O
Binkowski -X- _ O
, -X- _ O
Erich -X- _ O
Elsen -X- _ O
, -X- _ O
and -X- _ O
Karen -X- _ O
Simonyan -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

End -X- _ O
- -X- _ O
toend -X- _ O
adversarial -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
speech -X- _ O
. -X- _ O

In -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Learning -X- _ O
Representations -X- _ O
. -X- _ O

Chris -X- _ O
Dyer -X- _ O
, -X- _ O
Victor -X- _ O
Chahuneau -X- _ O
, -X- _ O
and -X- _ O
Noah -X- _ O
A. -X- _ O
Smith -X- _ O
. -X- _ O
2013 -X- _ O
. -X- _ O

A -X- _ O
simple -X- _ O
, -X- _ O
fast -X- _ O
, -X- _ O
and -X- _ O
effective -X- _ O
reparameterization -X- _ O
of -X- _ O
IBM -X- _ O
model -X- _ O
2 -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2013 -X- _ O
Conference -X- _ O
of -X- _ O
the -X- _ O
North -X- _ O
American -X- _ O
Chapter -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
: -X- _ O
Human -X- _ O
Language -X- _ O
Technologies -X- _ O
, -X- _ O
pages -X- _ O
644–648 -X- _ O
, -X- _ O
Atlanta -X- _ O
, -X- _ O
Georgia -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Marjan -X- _ O
Ghazvininejad -X- _ O
, -X- _ O
Vladimir -X- _ O
Karpukhin -X- _ O
, -X- _ O
Luke -X- _ O
Zettlemoyer -X- _ O
, -X- _ O
and -X- _ O
Omer -X- _ O
Levy -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

Aligned -X- _ O
cross -X- _ O
entropy -X- _ O
for -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
machine -X- _ O
translation -X- _ O
. -X- _ O

InProceedings -X- _ O
of -X- _ O
the -X- _ O
37th -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Machine -X- _ O
Learning -X- _ O
, -X- _ O
ICML -X- _ O
2020 -X- _ O
, -X- _ O
13 -X- _ O
- -X- _ O
18 -X- _ O
July -X- _ O
2020 -X- _ O
, -X- _ O
Virtual -X- _ O
Event -X- _ O
, -X- _ O
volume -X- _ O
119 -X- _ O
of -X- _ O
Proceedings -X- _ O
of -X- _ O
Machine -X- _ O
Learning -X- _ O
Research -X- _ O
, -X- _ O
pages -X- _ O
3515–3523 -X- _ O
. -X- _ O

PMLR -X- _ O
. -X- _ O

Marjan -X- _ O
Ghazvininejad -X- _ O
, -X- _ O
Omer -X- _ O
Levy -X- _ O
, -X- _ O
Yinhan -X- _ O
Liu -X- _ O
, -X- _ O
and -X- _ O
Luke -X- _ O
Zettlemoyer -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Mask -X- _ O
- -X- _ O
predict -X- _ O
: -X- _ O
Parallel -X- _ O
decoding -X- _ O
of -X- _ O
conditional -X- _ O
masked -X- _ O
language -X- _ O
models -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2019 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
and -X- _ O
the -X- _ O
9th -X- _ O
International -X- _ O
Joint -X- _ O
Conference -X- _ O
on -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
EMNLP -X- _ O
- -X- _ O
IJCNLP -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
6112 -X- _ O
– -X- _ O
6121 -X- _ O
, -X- _ O
Hong -X- _ O
Kong -X- _ O
, -X- _ O
China -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Alex -X- _ O
Graves -X- _ O
, -X- _ O
Santiago -X- _ O
Fernández -X- _ O
, -X- _ O
Faustino -X- _ O
J. -X- _ O
Gomez -X- _ O
, -X- _ O
and -X- _ O
Jürgen -X- _ O
Schmidhuber -X- _ O
. -X- _ O

2006 -X- _ O
. -X- _ O

Connectionist -X- _ O
temporal -X- _ O
classiﬁcation -X- _ O
: -X- _ O
labelling -X- _ O
unsegmented -X- _ O
sequence -X- _ O
data -X- _ O
with -X- _ O
recurrent -X- _ O
neural -X- _ O
networks -X- _ O
. -X- _ O

In -X- _ O
MachineLearning -X- _ O
, -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
Twenty -X- _ O
- -X- _ O
Third -X- _ O
International -X- _ O
Conference -X- _ O
( -X- _ O
ICML -X- _ O
2006 -X- _ O
) -X- _ O
, -X- _ O
Pittsburgh -X- _ O
, -X- _ O
PennACM -X- _ O
International -X- _ O
Conference -X- _ O
Proceeding -X- _ O
Series -X- _ O
, -X- _ O
Jiatao -X- _ O
Gu -X- _ O
, -X- _ O
James -X- _ O
Bradbury -X- _ O
, -X- _ O
Caiming -X- _ O
Xiong -X- _ O
, -X- _ O
Victor -X- _ O
O. -X- _ O
K. -X- _ O
Li -X- _ O
, -X- _ O
and -X- _ O
Richard -X- _ O
Socher -X- _ O
. -X- _ O
2018 -X- _ O
. -X- _ O

Nonautoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O

In -X- _ O
6th -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Learning -X- _ O
Representations -X- _ O
, -X- _ O
ICLR -X- _ O
2018 -X- _ O
, -X- _ O
Vancouver -X- _ O
, -X- _ O
BC -X- _ O
, -X- _ O
Canada -X- _ O
, -X- _ O
April -X- _ O
30 -X- _ O
- -X- _ O
May -X- _ O
3 -X- _ O
, -X- _ O
2018 -X- _ O
, -X- _ O
Conference -X- _ O
Track -X- _ O
Proceedings -X- _ O
. -X- _ O

OpenReview.net -X- _ O
. -X- _ O

Jiatao -X- _ O
Gu -X- _ O
and -X- _ O
Xiang -X- _ O
Kong -X- _ O
. -X- _ O

2021 -X- _ O
. -X- _ O

Fully -X- _ O
nonautoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
: -X- _ O
Tricks -X- _ O
of -X- _ O
the -X- _ O
trade -X- _ O
. -X- _ O

In -X- _ O
Findings -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
: -X- _ O
ACL -X- _ O
- -X- _ O
IJCNLP -X- _ O
2021 -X- _ O
, -X- _ O
pages -X- _ O
120 -X- _ O
– -X- _ O
133 -X- _ O
, -X- _ O
Online -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Junliang -X- _ O
Guo -X- _ O
, -X- _ O
Linli -X- _ O
Xu -X- _ O
, -X- _ O
and -X- _ O
Enhong -X- _ O
Chen -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

Jointly -X- _ O
masked -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
model -X- _ O
for -X- _ O
nonautoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
58th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
pages -X- _ O
376–385 -X- _ O
, -X- _ O
Online -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Eric -X- _ O
Jones -X- _ O
, -X- _ O
Travis -X- _ O
Oliphant -X- _ O
, -X- _ O
Pearu -X- _ O
Peterson -X- _ O
, -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
2001 -X- _ O
. -X- _ O

SciPy -X- _ O
: -X- _ O

Open -X- _ O
source -X- _ O
scientiﬁc -X- _ O
tools -X- _ O
for -X- _ O
Python -X- _ O
. -X- _ O

Jungo -X- _ O
Kasai -X- _ O
, -X- _ O
James -X- _ O
Cross -X- _ O
, -X- _ O
Marjan -X- _ O
Ghazvininejad -X- _ O
, -X- _ O
and -X- _ O
Jiatao -X- _ O
Gu -X- _ O
. -X- _ O
2020a -X- _ O
. -X- _ O

Non -X- _ O
- -X- _ O
autoregressive -X- _ O
machine -X- _ O
translation -X- _ O
with -X- _ O
disentangled -X- _ O
context -X- _ O
transformer -X- _ O
. -X- _ O

InProceedings -X- _ O
of -X- _ O
the -X- _ O
37th -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Machine -X- _ O
Learning -X- _ O
, -X- _ O
ICML -X- _ O
2020 -X- _ O
, -X- _ O
13 -X- _ O
- -X- _ O
18 -X- _ O
July -X- _ O
2020 -X- _ O
, -X- _ O
Virtual -X- _ O
Event -X- _ O
, -X- _ O
volume -X- _ O
119 -X- _ O
of -X- _ O
Proceedings -X- _ O
of -X- _ O
Machine -X- _ O
Learning -X- _ O
Research -X- _ O
, -X- _ O
pages -X- _ O
5144–5155 -X- _ O
. -X- _ O

PMLR -X- _ O
. -X- _ O

Jungo -X- _ O
Kasai -X- _ O
, -X- _ O
Nikolaos -X- _ O
Pappas -X- _ O
, -X- _ O
Hao -X- _ O
Peng -X- _ O
, -X- _ O
James -X- _ O
Cross -X- _ O
, -X- _ O
and -X- _ O
Noah -X- _ O
Smith -X- _ O
. -X- _ O
2020b -X- _ O
. -X- _ O

Deep -X- _ O
encoder -X- _ O
, -X- _ O
shallow -X- _ O
decoder -X- _ O
: -X- _ O
Reevaluating -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
machine -X- _ O
translation -X- _ O
. -X- _ O

In -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Learning -X- _ O
Representations -X- _ O
. -X- _ O

Jaehyeon -X- _ O
Kim -X- _ O
, -X- _ O
Sungwon -X- _ O
Kim -X- _ O
, -X- _ O
Jungil -X- _ O
Kong -X- _ O
, -X- _ O
and -X- _ O
Sungroh -X- _ O
Yoon -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

Glow -X- _ O
- -X- _ O
tts -X- _ O
: -X- _ O
A -X- _ O
generative -X- _ O
ﬂow -X- _ O
for -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
speech -X- _ O
via -X- _ O
monotonic -X- _ O
alignment -X- _ O
search -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
33 -X- _ O
: -X- _ O
Annual -X- _ O
Conference -X- _ O
on -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
2020 -X- _ O
, -X- _ O
NeurIPS -X- _ O
2020 -X- _ O
, -X- _ O
December -X- _ O
6Yoon -X- _ O
Kim -X- _ O
and -X- _ O
Alexander -X- _ O
M. -X- _ O
Rush -X- _ O
. -X- _ O
2016 -X- _ O
. -X- _ O

Sequencelevel -X- _ O
knowledge -X- _ O
distillation -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2016 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
pages -X- _ O
1317–1327 -X- _ O
, -X- _ O
Austin -X- _ O
, -X- _ O
Texas -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Diederik -X- _ O
P. -X- _ O
Kingma -X- _ O
and -X- _ O
Jimmy -X- _ O
Ba -X- _ O
. -X- _ O
2015 -X- _ O
. -X- _ O

Adam -X- _ O
: -X- _ O
A -X- _ O
method -X- _ O
for -X- _ O
stochastic -X- _ O
optimization -X- _ O
. -X- _ O

In -X- _ O
3rd -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Learning -X- _ O
Representations -X- _ O
, -X- _ O
Conference -X- _ O
Track -X- _ O
Proceedings -X- _ O
. -X- _ O

11Jason -X- _ O
Lee -X- _ O
, -X- _ O
Elman -X- _ O
Mansimov -X- _ O
, -X- _ O
and -X- _ O
Kyunghyun -X- _ O
Cho -X- _ O
. -X- _ O
2018 -X- _ O
. -X- _ O

Deterministic -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
neural -X- _ O
sequence -X- _ O
modeling -X- _ O
by -X- _ O
iterative -X- _ O
reﬁnement -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2018 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
pages -X- _ O
1173 -X- _ O
– -X- _ O
1182 -X- _ O
, -X- _ O
Brussels -X- _ O
, -X- _ O
Belgium -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Jason -X- _ O
Lee -X- _ O
, -X- _ O
Raphael -X- _ O
Shu -X- _ O
, -X- _ O
and -X- _ O
Kyunghyun -X- _ O
Cho -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

Iterative -X- _ O
reﬁnement -X- _ O
in -X- _ O
the -X- _ O
continuous -X- _ O
space -X- _ O
for -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2020 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
EMNLP -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
1006–1015 -X- _ O
, -X- _ O
Online -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Chi -X- _ O
- -X- _ O
Ho -X- _ O
Li -X- _ O
, -X- _ O
Minghui -X- _ O
Li -X- _ O
, -X- _ O
Dongdong -X- _ O
Zhang -X- _ O
, -X- _ O
Mu -X- _ O
Li -X- _ O
, -X- _ O
Ming -X- _ O
Zhou -X- _ O
, -X- _ O
and -X- _ O
Yi -X- _ O
Guan -X- _ O
. -X- _ O

2007 -X- _ O
. -X- _ O

A -X- _ O
probabilistic -X- _ O
approach -X- _ O
to -X- _ O
syntax -X- _ O
- -X- _ O
based -X- _ O
reordering -X- _ O
for -X- _ O
statistical -X- _ O
machine -X- _ O
translation -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
45th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
of -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
pages -X- _ O
720–727 -X- _ O
, -X- _ O
Prague -X- _ O
, -X- _ O
Czech -X- _ O
Republic -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Ye -X- _ O
Liu -X- _ O
, -X- _ O
Yao -X- _ O
Wan -X- _ O
, -X- _ O
Jianguo -X- _ O
Zhang -X- _ O
, -X- _ O
Wenting -X- _ O
Zhao -X- _ O
, -X- _ O
and -X- _ O
Philip -X- _ O
Yu -X- _ O
. -X- _ O
2021 -X- _ O
. -X- _ O

Enriching -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
transformer -X- _ O
with -X- _ O
syntactic -X- _ O
and -X- _ O
semantic -X- _ O
structures -X- _ O
for -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
16th -X- _ O
Conference -X- _ O
of -X- _ O
the -X- _ O
European -X- _ O
Chapter -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
: -X- _ O
Main -X- _ O
Volume -X- _ O
, -X- _ O
pages -X- _ O
1235–1244 -X- _ O
, -X- _ O
Online -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Xuezhe -X- _ O
Ma -X- _ O
, -X- _ O
Chunting -X- _ O
Zhou -X- _ O
, -X- _ O
Xian -X- _ O
Li -X- _ O
, -X- _ O
Graham -X- _ O
Neubig -X- _ O
, -X- _ O
and -X- _ O
Eduard -X- _ O
Hovy -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

FlowSeq -X- _ B-MethodName
: -X- _ O

Nonautoregressive -X- _ O
conditional -X- _ O
sequence -X- _ O
generation -X- _ O
with -X- _ O
generative -X- _ O
ﬂow -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2019 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
and -X- _ O
the -X- _ O
9th -X- _ O
International -X- _ O
Joint -X- _ O
Conference -X- _ O
on -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
EMNLPIJCNLP -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
4282–4292 -X- _ O
, -X- _ O
Hong -X- _ O
Kong -X- _ O
, -X- _ O
China -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Franz -X- _ O
Josef -X- _ O
Och -X- _ O
and -X- _ O
Hermann -X- _ O
Ney -X- _ O
. -X- _ O
2003 -X- _ O
. -X- _ O

A -X- _ O
systematic -X- _ O
comparison -X- _ O
of -X- _ O
various -X- _ O
statistical -X- _ O
alignment -X- _ O
models -X- _ O
. -X- _ O

Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
29 -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
:19–51 -X- _ O
. -X- _ O

Myle -X- _ O
Ott -X- _ O
, -X- _ O
Sergey -X- _ O
Edunov -X- _ O
, -X- _ O
Alexei -X- _ O
Baevski -X- _ O
, -X- _ O
Angela -X- _ O
Fan -X- _ O
, -X- _ O
Sam -X- _ O
Gross -X- _ O
, -X- _ O
Nathan -X- _ O
Ng -X- _ O
, -X- _ O
David -X- _ O
Grangier -X- _ O
, -X- _ O
and -X- _ O
Michael -X- _ O
Auli -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

fairseq -X- _ O
: -X- _ O

A -X- _ O
fast -X- _ O
, -X- _ O
extensible -X- _ O
toolkit -X- _ O
for -X- _ O
sequence -X- _ O
modeling -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2019 -X- _ O
Conference -X- _ O
of -X- _ O
the -X- _ O
North -X- _ O
American -X- _ O
Chapter -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
( -X- _ O
Demonstrations -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
48–53 -X- _ O
, -X- _ O
Minneapolis -X- _ O
, -X- _ O
Minnesota -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Lihua -X- _ O
Qian -X- _ O
, -X- _ O
Hao -X- _ O
Zhou -X- _ O
, -X- _ O
Yu -X- _ O
Bao -X- _ O
, -X- _ O
Mingxuan -X- _ O
Wang -X- _ O
, -X- _ O
Lin -X- _ O
Qiu -X- _ O
, -X- _ O
Weinan -X- _ O
Zhang -X- _ O
, -X- _ O
Yong -X- _ O
Yu -X- _ O
, -X- _ O
and -X- _ O
Lei -X- _ O
Li -X- _ O
. -X- _ O
2021 -X- _ O
. -X- _ O

Glancing -X- _ O
transformer -X- _ O
for -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
59th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
and -X- _ O
the -X- _ O
11th -X- _ O
International -X- _ O
Joint -X- _ O
Conference -X- _ O
on -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
Volume -X- _ O
1 -X- _ O
: -X- _ O
Long -X- _ O
Papers -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
1993–2003 -X- _ O
, -X- _ O
Online -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Qiu -X- _ O
Ran -X- _ O
, -X- _ O
Yankai -X- _ O
Lin -X- _ O
, -X- _ O
Peng -X- _ O
Li -X- _ O
, -X- _ O
and -X- _ O
Jie -X- _ O
Zhou -X- _ O
. -X- _ O
2021 -X- _ O
. -X- _ O

Guiding -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
decoding -X- _ O
with -X- _ O
reordering -X- _ O
information -X- _ O
. -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
AAAI -X- _ O
Conference -X- _ O
on -X- _ O
Artiﬁcial -X- _ O
IntelliYi -X- _ O
Ren -X- _ O
, -X- _ O
Yangjun -X- _ O
Ruan -X- _ O
, -X- _ O
Xu -X- _ O
Tan -X- _ O
, -X- _ O
Tao -X- _ O
Qin -X- _ O
, -X- _ O
Sheng -X- _ O
Zhao -X- _ O
, -X- _ O
Zhou -X- _ O
Zhao -X- _ O
, -X- _ O
and -X- _ O
Tie -X- _ O
- -X- _ O
Yan -X- _ O
Liu -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Fastspeech -X- _ O
: -X- _ O
Fast -X- _ O
, -X- _ O
robust -X- _ O
and -X- _ O
controllable -X- _ O
text -X- _ O
to -X- _ O
speech -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
32 -X- _ O
: -X- _ O
Annual -X- _ O
Conference -X- _ O
on -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
2019 -X- _ O
, -X- _ O
NeurIPS -X- _ O
2019 -X- _ O
, -X- _ O
December -X- _ O
Chitwan -X- _ O
Saharia -X- _ O
, -X- _ O
William -X- _ O
Chan -X- _ O
, -X- _ O
Saurabh -X- _ O
Saxena -X- _ O
, -X- _ O
and -X- _ O
Mohammad -X- _ O
Norouzi -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

Non -X- _ O
- -X- _ O
autoregressive -X- _ O
machine -X- _ O
translation -X- _ O
with -X- _ O
latent -X- _ O
alignments -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2020 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
EMNLP -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
1098–1108 -X- _ O
, -X- _ O
Online -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Chenze -X- _ O
Shao -X- _ O
, -X- _ O
Yang -X- _ O
Feng -X- _ O
, -X- _ O
Jinchao -X- _ O
Zhang -X- _ O
, -X- _ O
Fandong -X- _ O
Meng -X- _ O
, -X- _ O
Xilin -X- _ O
Chen -X- _ O
, -X- _ O
and -X- _ O
Jie -X- _ O
Zhou -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Retrieving -X- _ O
sequential -X- _ O
information -X- _ O
for -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
57th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
, -X- _ O
pages -X- _ O
3013–3024 -X- _ O
, -X- _ O
Florence -X- _ O
, -X- _ O
Italy -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Chenze -X- _ O
Shao -X- _ O
, -X- _ O
Jinchao -X- _ O
Zhang -X- _ O
, -X- _ O
Yang -X- _ O
Feng -X- _ O
, -X- _ O
Fandong -X- _ O
Meng -X- _ O
, -X- _ O
and -X- _ O
Jie -X- _ O
Zhou -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

Minimizing -X- _ O
the -X- _ O
bag -X- _ O
- -X- _ O
ofngrams -X- _ O
difference -X- _ O
for -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O

In -X- _ O
The -X- _ O
Thirty -X- _ O
- -X- _ O
Fourth -X- _ O
AAAI -X- _ O
Conference -X- _ O
on -X- _ O
Artiﬁcial -X- _ O
Intelligence -X- _ O
, -X- _ O
AAAI -X- _ O
2020 -X- _ O
, -X- _ O
The -X- _ O
Thirty -X- _ O
- -X- _ O
Second -X- _ O
Innovative -X- _ O
Applications -X- _ O
of -X- _ O
Artiﬁcial -X- _ O
Intelligence -X- _ O
Conference -X- _ O
, -X- _ O
IAAI -X- _ O
2020 -X- _ O
, -X- _ O
The -X- _ O
Tenth -X- _ O
AAAI -X- _ O
Symposium -X- _ O
on -X- _ O
Educational -X- _ O
Advances -X- _ O
in -X- _ O
Artiﬁcial -X- _ O
Intelligence -X- _ O
, -X- _ O
EAAI -X- _ O
2020 -X- _ O
, -X- _ O
New -X- _ O
York -X- _ O
, -X- _ O
NY -X- _ O
, -X- _ O
USA -X- _ O
, -X- _ O
FebruRaphael -X- _ O
Shu -X- _ O
, -X- _ O
Jason -X- _ O
Lee -X- _ O
, -X- _ O
Hideki -X- _ O
Nakayama -X- _ O
, -X- _ O
and -X- _ O
Kyunghyun -X- _ O
Cho -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

Latent -X- _ O
- -X- _ O
variable -X- _ O
nonautoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
with -X- _ O
deterministic -X- _ O
inference -X- _ O
using -X- _ O
a -X- _ O
delta -X- _ O
posterior -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
AAAI -X- _ O
Conference -X- _ O
on -X- _ O
Artiﬁcial -X- _ O
IntelKai -X- _ O
Song -X- _ O
, -X- _ O
Kun -X- _ O
Wang -X- _ O
, -X- _ O
Heng -X- _ O
Yu -X- _ O
, -X- _ O
Yue -X- _ O
Zhang -X- _ O
, -X- _ O
Zhongqiang -X- _ O
Huang -X- _ O
, -X- _ O
Weihua -X- _ O
Luo -X- _ O
, -X- _ O
Xiangyu -X- _ O
Duan -X- _ O
, -X- _ O
and -X- _ O
Min -X- _ O
Zhang -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

Alignment -X- _ O
- -X- _ O
enhanced -X- _ O
transformer -X- _ O
for -X- _ O
constraining -X- _ O
nmt -X- _ B-TaskName
with -X- _ O
pre -X- _ O
- -X- _ O
speciﬁed -X- _ O
translations -X- _ O
. -X- _ O

In -X- _ O
AAAI -X- _ O
, -X- _ O
pages -X- _ O
8886–8893 -X- _ O
. -X- _ O

Zhiqing -X- _ O
Sun -X- _ O
, -X- _ O
Zhuohan -X- _ O
Li -X- _ O
, -X- _ O
Haoqing -X- _ O
Wang -X- _ O
, -X- _ O
Di -X- _ O
He -X- _ O
, -X- _ O
Zi -X- _ O
Lin -X- _ O
, -X- _ O
and -X- _ O
Zhi -X- _ O
- -X- _ O
Hong -X- _ O
Deng -X- _ O
. -X- _ O
2019 -X- _ O
. -X- _ O

Fast -X- _ O
structured -X- _ O
decoding -X- _ O
for -X- _ O
sequence -X- _ O
models -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
32 -X- _ O
: -X- _ O
Annual -X- _ O
Conference -X- _ O
on -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
Zhiqing -X- _ O
Sun -X- _ O
and -X- _ O
Yiming -X- _ O
Yang -X- _ O
. -X- _ O
2020 -X- _ O
. -X- _ O

An -X- _ O
EM -X- _ O
approach -X- _ O
to -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
conditional -X- _ O
sequence -X- _ O
genera- -X- _ O

12tion -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
37th -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Machine -X- _ O
Learning -X- _ O
, -X- _ O
ICML -X- _ O
2020 -X- _ O
, -X- _ O
13 -X- _ O
- -X- _ O
18 -X- _ O
July -X- _ O
2020 -X- _ O
, -X- _ O
Virtual -X- _ O
Event -X- _ O
, -X- _ O
volume -X- _ O
119 -X- _ O
of -X- _ O
Proceedings -X- _ O
of -X- _ O
Machine -X- _ O
Learning -X- _ O
Research -X- _ O
, -X- _ O
pages -X- _ O
9249–9258 -X- _ O
. -X- _ O

PMLR -X- _ O
. -X- _ O

Ashish -X- _ O
Vaswani -X- _ O
, -X- _ O
Noam -X- _ O
Shazeer -X- _ O
, -X- _ O
Niki -X- _ O
Parmar -X- _ O
, -X- _ O
Jakob -X- _ O
Uszkoreit -X- _ O
, -X- _ O
Llion -X- _ O
Jones -X- _ O
, -X- _ O
Aidan -X- _ O
N. -X- _ O
Gomez -X- _ O
, -X- _ O
Lukasz -X- _ O
Kaiser -X- _ O
, -X- _ O
and -X- _ O
Illia -X- _ O
Polosukhin -X- _ O
. -X- _ O

2017 -X- _ O
. -X- _ O

Attention -X- _ O
is -X- _ O
all -X- _ O
you -X- _ O
need -X- _ O
. -X- _ O

In -X- _ O
Advances -X- _ O
in -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
30 -X- _ O
: -X- _ O
Annual -X- _ O
Conference -X- _ O
on -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
2017 -X- _ O
, -X- _ O
December -X- _ O
4Jinchao -X- _ O
Zhang -X- _ O
, -X- _ O
Mingxuan -X- _ O
Wang -X- _ O
, -X- _ O
Qun -X- _ O
Liu -X- _ O
, -X- _ O
and -X- _ O
Jie -X- _ O
Zhou -X- _ O
. -X- _ O
2017 -X- _ O
. -X- _ O

Incorporating -X- _ O
word -X- _ O
reordering -X- _ O
knowledge -X- _ O
into -X- _ O
attention -X- _ O
- -X- _ O
based -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
55th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
( -X- _ O
Volume -X- _ O
1 -X- _ O
: -X- _ O
Long -X- _ O
Papers -X- _ O
) -X- _ O
, -X- _ O
pages -X- _ O
1524–1534 -X- _ O
, -X- _ O
Vancouver -X- _ O
, -X- _ O
Canada -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Chunting -X- _ O
Zhou -X- _ O
, -X- _ O
Jiatao -X- _ O
Gu -X- _ O
, -X- _ O
and -X- _ O
Graham -X- _ O
Neubig -X- _ O
. -X- _ O

2020a -X- _ O
. -X- _ O

Understanding -X- _ O
knowledge -X- _ O
distillation -X- _ O
in -X- _ O
nonautoregressive -X- _ O
machine -X- _ O
translation -X- _ O
. -X- _ O

In -X- _ O
8th -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Learning -X- _ O
Representations -X- _ O
, -X- _ O
ICLR -X- _ O
2020 -X- _ O
, -X- _ O
Addis -X- _ O
Ababa -X- _ O
, -X- _ O
Ethiopia -X- _ O
, -X- _ O
April -X- _ O
26 -X- _ O
- -X- _ O
30 -X- _ O
, -X- _ O
2020 -X- _ O
. -X- _ O

OpenReview.net -X- _ O
. -X- _ O

Long -X- _ O
Zhou -X- _ O
, -X- _ O
Jiajun -X- _ O
Zhang -X- _ O
, -X- _ O
Yang -X- _ O
Zhao -X- _ O
, -X- _ O
and -X- _ O
Chengqing -X- _ O
Zong -X- _ O
. -X- _ O

2020b -X- _ O
. -X- _ O

Non -X- _ O
- -X- _ O
autoregressive -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
with -X- _ O
distortion -X- _ B-MethodName
model -X- _ O
. -X- _ O

In -X- _ O
CCF -X- _ O
International -X- _ O
Conference -X- _ O
on -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
and -X- _ O
Chinese -X- _ O
Computing -X- _ O
, -X- _ O
pages -X- _ O
403–415 -X- _ O
. -X- _ O

Springer -X- _ O
. -X- _ O

Appendix -X- _ O
A -X- _ O
Mappings -X- _ O
in -X- _ O
Alignment -X- _ O
In -X- _ O
general -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
one -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
, -X- _ O
one -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
many -X- _ O
, -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
, -X- _ O
and -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
many -X- _ O
mappings -X- _ O
excluding -X- _ O
zero -X- _ O
- -X- _ O
fertility -X- _ O
and -X- _ O
spurious -X- _ O
word -X- _ O
cases -X- _ O
( -X- _ O
see -X- _ O
Figure -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O

Distortion -X- _ B-MethodName
and -X- _ O
ReorderNAT -X- _ B-MethodName
can -X- _ O
not -X- _ O
represent -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
, -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
many -X- _ O
, -X- _ O
and -X- _ O
spurious -X- _ O
word -X- _ O
cases -X- _ O
. -X- _ O

The -X- _ O
grouping -X- _ O
predictor -X- _ O
in -X- _ O
AligNART -X- _ B-MethodName
models -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
one -X- _ O
and -X- _ O
many -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
many -X- _ O
mappings -X- _ O
. -X- _ O

The -X- _ O
addition -X- _ O
of -X- _ O
a -X- _ O
spurious -X- _ O
token -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
AligNART -X- _ B-MethodName
( -X- _ O
FA -X- _ O
) -X- _ O
, -X- _ O
enables -X- _ O
us -X- _ O
to -X- _ O
address -X- _ O
the -X- _ O
spurious -X- _ O
word -X- _ O
case -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
explained -X- _ O
in -X- _ O
Section -X- _ O
C.2 -X- _ O
. -X- _ O

During -X- _ O
the -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
the -X- _ O
introduction -X- _ O
of -X- _ O
a -X- _ O
spurious -X- _ O
token -X- _ O
degrades -X- _ O
the -X- _ O
performance -X- _ O
for -X- _ O
GIZA++ -X- _ O
. -X- _ O

We -X- _ O
guess -X- _ O
the -X- _ O
reason -X- _ O
of -X- _ O
the -X- _ O
degradation -X- _ O
is -X- _ O
that -X- _ O
alignment -X- _ O
matrix -X- _ O
from -X- _ O
GIZA++ -X- _ O
contains -X- _ O
more -X- _ O
than -X- _ O
two -X- _ O
times -X- _ O
as -X- _ O
many -X- _ O
empty -X- _ O
rows -X- _ O
as -X- _ O
that -X- _ O
of -X- _ O
fast -X- _ O
align -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
En -X- _ O
- -X- _ O
De -X- _ O
. -X- _ O
B -X- _ O
Architecture -X- _ O
of -X- _ O
Aligner -X- _ O
The -X- _ O
duplication -X- _ O
predictor -X- _ O
and -X- _ O
grouping -X- _ O
predictor -X- _ O
modules -X- _ O
consist -X- _ O
of -X- _ O
a -X- _ O
convolutional -X- _ O
layer -X- _ O
, -X- _ O
ReLU -X- _ O
acone -X- _ O
- -X- _ O
to -X- _ O
-one -X- _ O
one -X- _ O
- -X- _ O
to -X- _ O
-many -X- _ O
spurious -X- _ O
word -X- _ O
many -X- _ O
-to -X- _ O
- -X- _ O
one -X- _ O
many -X- _ O
-to -X- _ O
- -X- _ O
many -X- _ O
zero -X- _ O
- -X- _ O
fertilityFigure -X- _ O
2 -X- _ O
: -X- _ O
Types -X- _ O
of -X- _ O
mapping -X- _ O
in -X- _ O
word -X- _ O
alignments -X- _ O
. -X- _ O

Row -X- _ O
and -X- _ O
colum -X- _ O
correspond -X- _ O
to -X- _ O
the -X- _ O
target -X- _ O
and -X- _ O
source -X- _ O
tokens -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

tivation -X- _ O
, -X- _ O
layer -X- _ O
normalization -X- _ O
, -X- _ O
dropout -X- _ O
, -X- _ O
and -X- _ O
a -X- _ O
projection -X- _ O
layer -X- _ O
, -X- _ O
same -X- _ O
as -X- _ O
the -X- _ O
phoneme -X- _ O
duration -X- _ O
predictor -X- _ O
in -X- _ O
FastSpeech -X- _ O
( -X- _ O
Ren -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
a -X- _ O
parallel -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
speech -X- _ O
model -X- _ O
. -X- _ O

The -X- _ O
permutation -X- _ O
predictor -X- _ O
in -X- _ O
Aligner -X- _ O
consists -X- _ O
of -X- _ O
three -X- _ O
encoder -X- _ O
layers -X- _ O
: -X- _ O
pre -X- _ O
- -X- _ O
network -X- _ O
, -X- _ O
query -X- _ O
= -X- _ O
key -X- _ O
network -X- _ O
, -X- _ O
and -X- _ O
single -X- _ O
- -X- _ O
head -X- _ O
attention -X- _ O
module -X- _ O
for -X- _ O
the -X- _ O
outputs -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
the -X- _ O
outputs -X- _ O
of -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
network -X- _ O
are -X- _ O
passed -X- _ O
to -X- _ O
the -X- _ O
query -X- _ O
and -X- _ O
key -X- _ O
networks -X- _ O
. -X- _ O

To -X- _ O
prevent -X- _ O
the -X- _ O
predicted -X- _ O
permutation -X- _ O
matrix -X- _ O
from -X- _ O
being -X- _ O
an -X- _ O
identity -X- _ O
matrix -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
a -X- _ O
gate -X- _ O
function -X- _ O
to -X- _ O
the -X- _ O
last -X- _ O
attention -X- _ O
module -X- _ O
in -X- _ O
the -X- _ O
permutation -X- _ O
predictor -X- _ O
to -X- _ O
modulate -X- _ O
the -X- _ O
probabilities -X- _ O
of -X- _ O
un -X- _ O
- -X- _ O
permuted -X- _ O
and -X- _ O
permuted -X- _ O
cases -X- _ O
. -X- _ O

We -X- _ O
formulate -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
gated -X- _ O
attention -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
Ppred -X- _ O
= -X- _ O
softmax -X- _ O
( -X- _ O
M+QKT -X- _ O
) -X- _ O
( -X- _ O
15 -X- _ O
) -X- _ O
Ppred -X- _ O
= -X- _ O
Dg+ -X- _ O
( -X- _ O
I Dg -X- _ O
) -X- _ O
Ppred -X- _ O
; -X- _ O
( -X- _ O
16 -X- _ O
) -X- _ O

whereis -X- _ O
the -X- _ O
sigmoid -X- _ O
function -X- _ O
and -X- _ O
Q -X- _ O
= -X- _ O
K -X- _ O
is -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
query -X- _ O
= -X- _ O
key -X- _ O
network -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

g -X- _ O
is -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
an -X- _ O
un -X- _ O
- -X- _ O
permuted -X- _ O
case -X- _ O
. -X- _ O

Mis -X- _ O
a -X- _ O
diagonal -X- _ O
mask -X- _ O
matrix -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
values -X- _ O
of -X- _ O
the -X- _ O
diagonal -X- _ O
elements -X- _ O
are -X- _ O
 inf -X- _ O
. -X- _ O

Iis -X- _ O
an -X- _ O
identical -X- _ O
matrix -X- _ O
andDgis -X- _ O
a -X- _ O
diagonal -X- _ O
matrix -X- _ O
with -X- _ O
gas -X- _ O
the -X- _ O
main -X- _ O
diagonal -X- _ O
. -X- _ O

C -X- _ O
Alignment -X- _ O
Processing -X- _ O
C.1 -X- _ O
Word -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
subword -X- _ O
Alignment -X- _ O
To -X- _ O
reduce -X- _ O
the -X- _ O
complexity -X- _ O
of -X- _ O
alignment -X- _ O
, -X- _ O
we -X- _ O
further -X- _ O
assume -X- _ O
that -X- _ O
the -X- _ O
alignment -X- _ O
process -X- _ O
is -X- _ O
conducted -X- _ O
at -X- _ O
the -X- _ O
word -X- _ O
- -X- _ O
level -X- _ O
. -X- _ O

We -X- _ O
decompose -X- _ O
the -X- _ O
alignment -X- _ O
matrix -X- _ O
into -X- _ O
the -X- _ O
source -X- _ O
subword -X- _ O
to -X- _ O
source -X- _ O
word -X- _ O
matrix -X- _ O
Sand -X- _ O
the -X- _ O
source -X- _ O
word -X- _ O
to -X- _ O
target -X- _ O
subword -X- _ O
matrix -X- _ O
Awsas -X- _ O
depicted -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
. -X- _ O

Since -X- _ O
Sis -X- _ O
always -X- _ O
given -X- _ O
, -X- _ O
Awsis -X- _ O
the -X- _ O
only -X- _ O
target -X- _ O
to -X- _ O
be -X- _ O
learned -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O

𝐴𝐴𝑤𝑤𝑤𝑤 -X- _ O
� -X- _ O
𝑆𝑆Figure -X- _ O
3 -X- _ O
: -X- _ O
Example -X- _ O
of -X- _ O
word -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
subword -X- _ O
matrix -X- _ O
decomposition -X- _ O
technique -X- _ O
. -X- _ O

Row -X- _ O
and -X- _ O
column -X- _ O
correspond -X- _ O
to -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
tokens -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

yidenotes -X- _ O
thei -X- _ O
- -X- _ O
th -X- _ O
subword -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
sentence -X- _ O
. -X- _ O

xidenotes -X- _ O
thei -X- _ O
- -X- _ O
th -X- _ O
word -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
and -X- _ O
xi -X- _ O
jdenotes -X- _ O
thej -X- _ O
- -X- _ O
th -X- _ O
subword -X- _ O
of -X- _ O
the -X- _ O
i -X- _ O
- -X- _ O
th -X- _ O
word -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
. -X- _ O

we -X- _ O
derive -X- _ O
the -X- _ O
source -X- _ O
subword -X- _ O
to -X- _ O
target -X- _ O
subword -X- _ O
matrixAusing -X- _ O
the -X- _ O
alignment -X- _ O
tool -X- _ O
. -X- _ O

Awsis -X- _ O
achieved -X- _ O
by -X- _ O
clipping -X- _ O
the -X- _ O
maximum -X- _ O
value -X- _ O
of -X- _ O
AS -X- _ O
> -X- _ O
to -X- _ O
1.Aws -X- _ O
reduces -X- _ O
the -X- _ O
search -X- _ O
space -X- _ O
because -X- _ O
of -X- _ O
the -X- _ O
assumption -X- _ O
that -X- _ O
source -X- _ O
tokens -X- _ O
duplicate -X- _ O
, -X- _ O
permute -X- _ O
, -X- _ O
and -X- _ O
group -X- _ O
at -X- _ O
the -X- _ O
word -X- _ O
- -X- _ O
level -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
trade -X- _ O
- -X- _ O
off -X- _ O
between -X- _ O
the -X- _ O
simplicity -X- _ O
and -X- _ O
resolution -X- _ O
of -X- _ O
information -X- _ O
. -X- _ O

The -X- _ O
recovered -X- _ O
source -X- _ O
subword -X- _ O
to -X- _ O
target -X- _ O
subword -X- _ O
matrixAwsSloses -X- _ O
the -X- _ O
subword -X- _ O
- -X- _ O
level -X- _ O
information -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
rightmost -X- _ O
matrix -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
. -X- _ O

C.2 -X- _ O
Filling -X- _ O
Null -X- _ O
Rows -X- _ O
in -X- _ O
Alignment -X- _ O
Matrix -X- _ O

The -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
alignment -X- _ O
tool -X- _ O
usually -X- _ O
contains -X- _ O
empty -X- _ O
rows -X- _ O
which -X- _ O
means -X- _ O
that -X- _ O
no -X- _ O
aligned -X- _ O
source -X- _ O
token -X- _ O
exists -X- _ O
for -X- _ O
certain -X- _ O
target -X- _ O
tokens -X- _ O
. -X- _ O

We -X- _ O
select -X- _ O
two -X- _ O
strategies -X- _ O
to -X- _ O
ﬁll -X- _ O
the -X- _ O
null -X- _ O
rows -X- _ O
: -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
copy -X- _ O
the -X- _ O
alignment -X- _ O
from -X- _ O
the -X- _ O
previous -X- _ O
target -X- _ O
token -X- _ O
, -X- _ O
or -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
introduce -X- _ O
a -X- _ O
special -X- _ O
spurious -X- _ O
token -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
second -X- _ O
strategy -X- _ O
, -X- _ O
we -X- _ O
concatenate -X- _ O
a -X- _ O
special -X- _ O
spurious -X- _ O
token -X- _ O
at -X- _ O
the -X- _ O
end -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
. -X- _ O

If -X- _ O
the -X- _ O
current -X- _ O
and -X- _ O
previous -X- _ O
target -X- _ O
tokens -X- _ O
belong -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
word -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
remaining -X- _ O
target -X- _ O
tokens -X- _ O
of -X- _ O
the -X- _ O
null -X- _ O
alignment -X- _ O
are -X- _ O
aligned -X- _ O
to -X- _ O
the -X- _ O
spurious -X- _ O
token -X- _ O
. -X- _ O

C.3 -X- _ O
Details -X- _ O
of -X- _ O
Alignment -X- _ O
Tool -X- _ O
Conﬁguration -X- _ O
Forfast -X- _ O
align -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
the -X- _ O
default -X- _ O
setting -X- _ O
for -X- _ O
forward -X- _ O
/ -X- _ O
backward -X- _ O
directions -X- _ O
and -X- _ O
obtain -X- _ O
symmetrized -X- _ O
alignment -X- _ O
with -X- _ O
the -X- _ O
grow -X- _ O
- -X- _ O
diag-ﬁnal -X- _ O
- -X- _ O
and -X- _ O
option -X- _ O
. -X- _ O

We -X- _ O
apply -X- _ O
the -X- _ O
word -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
subword -X- _ O
alignment -X- _ O
technique -X- _ O
andspurious -X- _ O
token -X- _ O
strategy -X- _ O
for -X- _ O
null -X- _ O
alignments -X- _ O
. -X- _ O

For -X- _ O
GIZA++ -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
the -X- _ O
word -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
subword -X- _ O
alignment -X- _ O
technique -X- _ O
and -X- _ O
copy -X- _ O
the -X- _ O
alignment -X- _ O
from -X- _ O
the -X- _ O
previous -X- _ O
target -X- _ O
token -X- _ O
for -X- _ O
null -X- _ O
alignment -X- _ O
. -X- _ O

We -X- _ O
set -X- _ O
the -X- _ O
alignment -X- _ O
score -X- _ O
ﬁltering -X- _ O
ratio -X- _ O
to -X- _ O
5 -X- _ O
% -X- _ O
. -X- _ O

D -X- _ O
Case -X- _ O
Study -X- _ O
To -X- _ O
analyze -X- _ O
various -X- _ O
alignments -X- _ O
and -X- _ O
their -X- _ O
translations -X- _ O
during -X- _ O
re -X- _ O
- -X- _ O
scoring -X- _ O
decoding -X- _ O
, -X- _ O
we -X- _ O
conduct -X- _ O
acase -X- _ O
study -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
De -X- _ O
! -X- _ O

En -X- _ O
validation -X- _ O
set -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
4 -X- _ O
. -X- _ O

The -X- _ O
two -X- _ O
translations -X- _ O
have -X- _ O
different -X- _ O
orderings -X- _ O
: -X- _ O
the -X- _ O
telescope -X- _ O
’s -X- _ O
tasks -X- _ O
andthe -X- _ O
tasks -X- _ O
of -X- _ O
the -X- _ O
telescope -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
sample -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
AligNART -X- _ B-MethodName
( -X- _ O
i -X- _ O
) -X- _ O
can -X- _ O
capture -X- _ O
non -X- _ O
- -X- _ O
diagonal -X- _ O
alignments -X- _ O
, -X- _ O
( -X- _ O
ii -X- _ O
) -X- _ O
models -X- _ O
multiple -X- _ O
alignments -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
iii -X- _ O
) -X- _ O
translates -X- _ O
corresponding -X- _ O
to -X- _ O
the -X- _ O
given -X- _ O
alignments -X- _ O
. -X- _ O

Source -X- _ O
Eine -X- _ O
der -X- _ O
Aufgaben -X- _ O
des -X- _ O
Tel -X- _ O
_ -X- _ O
esk -X- _ O
_ -X- _ O

ops -X- _ O
: -X- _ O
Essollnach -X- _ O
Licht -X- _ O
von -X- _ O
den -X- _ O
ersten -X- _ O
Ster -X- _ O
_ -X- _ O
nenund -X- _ O
Galax -X- _ O
_ -X- _ O

iennach -X- _ O
dem -X- _ O
Ur -X- _ O
_ -X- _ O
kn -X- _ O
_ -X- _ O
all -X- _ O
suchen -X- _ O
. -X- _ O

Reference -X- _ O
One -X- _ O
of -X- _ O
the -X- _ O
tel -X- _ O
_ -X- _ O
esc -X- _ O
_ -X- _ O
ope -X- _ O
’s -X- _ O
tasks -X- _ O
is -X- _ O
to -X- _ O
search -X- _ O
for -X- _ O
light -X- _ O
from -X- _ O
the -X- _ O
first -X- _ O
stars -X- _ O
and -X- _ O
galax -X- _ O
_ -X- _ O

iesthat -X- _ O
emerged -X- _ O
after -X- _ O
the -X- _ O
Big -X- _ O
B -X- _ O
_ -X- _ O
ang -X- _ O
. -X- _ O

Alignments -X- _ O
# -X- _ O
1 -X- _ O
One -X- _ O
of -X- _ O
the -X- _ O
tel -X- _ O
_ -X- _ O
esc -X- _ O
_ -X- _ O
ope -X- _ O
’s -X- _ O
tasks -X- _ O
: -X- _ O
it -X- _ O
should -X- _ O
search -X- _ O
for -X- _ O
light -X- _ O
from -X- _ O
the -X- _ O
first -X- _ O
stars -X- _ O
and -X- _ O
galax -X- _ O
_ -X- _ O

iesafter -X- _ O
the -X- _ O
Big -X- _ O
B -X- _ O
_ -X- _ O
ang -X- _ O
. -X- _ O

Alignments -X- _ O
# -X- _ O
2 -X- _ O
One -X- _ O
of -X- _ O
the -X- _ O
tasks -X- _ O
of -X- _ O
the -X- _ O
tel_esc -X- _ O
_ -X- _ O
ope -X- _ O
: -X- _ O
it -X- _ O
should -X- _ O
search -X- _ O
for -X- _ O
light -X- _ O
from -X- _ O
the -X- _ O
first -X- _ O
stars -X- _ O
and -X- _ O
galax -X- _ O
_ -X- _ O

iesafter -X- _ O
the -X- _ O
Big -X- _ O
B -X- _ O
_ -X- _ O
ang -X- _ O
. -X- _ O

Alignments -X- _ O
# -X- _ O
1 -X- _ O
Alignments -X- _ O
# -X- _ O
2Figure -X- _ O
4 -X- _ O
: -X- _ O
Translation -X- _ O
and -X- _ O
alignment -X- _ O
estimation -X- _ O
example -X- _ O
on -X- _ O
WMT14 -X- _ B-DatasetName
De -X- _ O
! -X- _ O

En -X- _ O
validation -X- _ O
set -X- _ O
. -X- _ O

Tokens -X- _ O
matched -X- _ O
to -X- _ O
the -X- _ O
alignment -X- _ O
matrix -X- _ O
have -X- _ O
same -X- _ O
colors -X- _ O
( -X- _ O
blue -X- _ O
and -X- _ O
orange -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
special -X- _ O
token -X- _ O
" -X- _ O
_ -X- _ O
" -X- _ O
stands -X- _ O
for -X- _ O
the -X- _ O
subword -X- _ O
tokenization -X- _ O
. -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
2022 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
pages -X- _ O
1 -X- _ O
- -X- _ O
17 -X- _ O
December -X- _ O
7 -X- _ O
- -X- _ O
11 -X- _ O
, -X- _ O
2022 -X- _ O
© -X- _ O
2022 -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
Generative -X- _ O
Knowledge -X- _ O
Graph -X- _ O
Construction -X- _ O
: -X- _ O
A -X- _ O
Review -X- _ O
Hongbin -X- _ O
Ye1,2 -X- _ O
, -X- _ O
Ningyu -X- _ O
Zhang1,2∗ -X- _ O
, -X- _ O
Hui -X- _ O
Chen3 -X- _ O
, -X- _ O
Huajun -X- _ O
Chen1,2 -X- _ O
{ -X- _ O
yehongbin -X- _ O
, -X- _ O
zhangningyu -X- _ O
, -X- _ O
huajunsir -X- _ O
} -X- _ O
@ -X- _ O
zju.edu.cn -X- _ O
, -X- _ O
weidu.ch -X- _ O
@ -X- _ O
alibaba-inc.com -X- _ O
Abstract -X- _ O
Generative -X- _ O
Knowledge -X- _ B-TaskName
Graph -X- _ I-TaskName
Construction -X- _ I-TaskName
( -X- _ O
KGC -X- _ B-TaskName
) -X- _ O
refers -X- _ O
to -X- _ O
those -X- _ O
methods -X- _ O
that -X- _ O
leverage -X- _ O
the -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
framework -X- _ O
for -X- _ O
building -X- _ O
knowledge -X- _ O
graphs -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
flexible -X- _ O
and -X- _ O
can -X- _ O
be -X- _ O
adapted -X- _ O
to -X- _ O
widespread -X- _ O
tasks -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
study -X- _ O
, -X- _ O
we -X- _ O
summarize -X- _ O
the -X- _ O
recent -X- _ O
compelling -X- _ O
progress -X- _ O
in -X- _ O
generative -X- _ O
knowledge -X- _ O
graph -X- _ O
construction -X- _ O
. -X- _ O

We -X- _ O
present -X- _ O
the -X- _ O
advantages -X- _ O
and -X- _ O
weaknesses -X- _ O
of -X- _ O
each -X- _ O
paradigm -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
different -X- _ O
generation -X- _ O
targets -X- _ O
and -X- _ O
provide -X- _ O
theoretical -X- _ O
insight -X- _ O
and -X- _ O
empirical -X- _ O
analysis -X- _ O
. -X- _ O

Based -X- _ O
on -X- _ O
the -X- _ O
review -X- _ O
, -X- _ O
we -X- _ O
suggest -X- _ O
promising -X- _ O
research -X- _ O
directions -X- _ O
for -X- _ O
the -X- _ O
future -X- _ O
. -X- _ O

Our -X- _ O
contributions -X- _ O
are -X- _ O
threefold -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
We -X- _ O
present -X- _ O
a -X- _ O
detailed -X- _ O
, -X- _ O
complete -X- _ O
taxonomy -X- _ O
for -X- _ O
the -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
methods -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
We -X- _ O
provide -X- _ O
a -X- _ O
theoretical -X- _ O
and -X- _ O
empirical -X- _ O
analysis -X- _ O
of -X- _ O
the -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
methods -X- _ O
; -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
We -X- _ O
propose -X- _ O
several -X- _ O
research -X- _ O
directions -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
developed -X- _ O
in -X- _ O
the -X- _ O
future -X- _ O
. -X- _ O

1 -X- _ O
Introduction -X- _ O
Knowledge -X- _ O
Graphs -X- _ O
( -X- _ O
KGs -X- _ O
) -X- _ O
as -X- _ O
a -X- _ O
form -X- _ O
of -X- _ O
structured -X- _ O
knowledge -X- _ O
have -X- _ O
drawn -X- _ O
significant -X- _ O
attention -X- _ O
from -X- _ O
academia -X- _ O
and -X- _ O
the -X- _ O
industry -X- _ O
( -X- _ O
Ji -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
high -X- _ O
- -X- _ O
quality -X- _ O
KGs -X- _ O
rely -X- _ O
almost -X- _ O
exclusively -X- _ O
on -X- _ O
human -X- _ O
- -X- _ O
curated -X- _ O
structured -X- _ O
or -X- _ O
semi -X- _ O
- -X- _ O
structured -X- _ O
data -X- _ O
. -X- _ O

To -X- _ O
this -X- _ O
end -X- _ O
, -X- _ O
Knowledge -X- _ B-TaskName
Graph -X- _ I-TaskName
Construction -X- _ I-TaskName
( -X- _ O
KGC -X- _ B-TaskName
) -X- _ O
is -X- _ O
proposed -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
process -X- _ O
of -X- _ O
populating -X- _ O
( -X- _ O
or -X- _ O
building -X- _ O
from -X- _ O
scratch -X- _ O
) -X- _ O
a -X- _ O
KG -X- _ O
with -X- _ O
new -X- _ O
knowledge -X- _ O
elements -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
entities -X- _ O
, -X- _ O
relations -X- _ O
, -X- _ O
events -X- _ O
) -X- _ O
. -X- _ O

Conventionally -X- _ O
, -X- _ O
KGC -X- _ B-TaskName
is -X- _ O
solved -X- _ O
by -X- _ O
employing -X- _ O
task -X- _ O
- -X- _ O
specific -X- _ O
discriminators -X- _ O
for -X- _ O
the -X- _ O
various -X- _ O
types -X- _ O
of -X- _ O
information -X- _ O
in -X- _ O
a -X- _ O
pipeline -X- _ O
manner -X- _ O
( -X- _ O
Angeli -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Luan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
de -X- _ O
Sá -X- _ O
Mesquita -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
, -X- _ O
typically -X- _ O
including -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
entity -X- _ O
discovery -X- _ O
or -X- _ O
named -X- _ O
entity -X- _ O
recognition -X- _ O
( -X- _ O
Sang -X- _ O
and -X- _ O
Meulder -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
entity -X- _ O
linking -X- _ O
( -X- _ O
Milne -X- _ O
and -X- _ O
Witten -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
relation -X- _ O
extraction -X- _ O
( -X- _ O
Zelenko -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
event -X- _ O
extraction -X- _ O
( -X- _ O
Du -X- _ O
and -X- _ O
Cardie -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
this -X- _ O
presents -X- _ O
limitations -X- _ O
of -X- _ O
error -X- _ O
population -X- _ O
and -X- _ O
poor -X- _ O
adaptability -X- _ O
for -X- _ O
different -X- _ O
tasks -X- _ O
. -X- _ O

∗Corresponding -X- _ O
author -X- _ O
. -X- _ O

The -X- _ O
[ -X- _ O
United -X- _ O
States -X- _ O
] -X- _ O
E -X- _ O
- -X- _ O
loc -X- _ O
President -X- _ O
[ -X- _ O
Joe -X- _ O
Biden -X- _ O
] -X- _ O
E -X- _ O
- -X- _ O
per -X- _ O
visited -X- _ O
[ -X- _ O
Samsung -X- _ O
] -X- _ O
E -X- _ O
- -X- _ O
Org -X- _ O
. -X- _ O

( -X- _ O
b -X- _ O
) -X- _ O
T -X- _ O
agging -X- _ O
Model -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
Generation -X- _ O
ModelCountry -X- _ O
- -X- _ O
President -X- _ O
NoneNone -X- _ O
Extracted -X- _ O
Results -X- _ O
{ -X- _ O
United -X- _ O
states -X- _ O
, -X- _ O
Country -X- _ O
- -X- _ O
President -X- _ O
, -X- _ O
Joe -X- _ O
Biden -X- _ O
} -X- _ O
Input -X- _ O
T -X- _ O
ext -X- _ O
: -X- _ O
The -X- _ O
United -X- _ O
States -X- _ O
President -X- _ O
Joe -X- _ O
Biden -X- _ O
visited -X- _ O
Samsung -X- _ O
. -X- _ O

Final -X- _ O
Results -X- _ O
: -X- _ O
{ -X- _ O
United -X- _ O
states -X- _ O
, -X- _ O
Country -X- _ O
- -X- _ O
President -X- _ O
, -X- _ O
Joe -X- _ O
Biden -X- _ O
} -X- _ O
Input -X- _ O
T -X- _ O
ext -X- _ O
: -X- _ O
The -X- _ O
United -X- _ O
States -X- _ O
President -X- _ O
Joe -X- _ O
Biden -X- _ O
visited -X- _ O
Samsung -X- _ O
. -X- _ O

Seq2Seq -X- _ O
T -X- _ O
ext -X- _ O
: -X- _ O
< -X- _ O
triplet -X- _ O
> -X- _ O
United -X- _ O
States -X- _ O
< -X- _ O
subj -X- _ O
> -X- _ O

Joe -X- _ O
Biden -X- _ O
< -X- _ O
obj -X- _ O
> -X- _ O
Country -X- _ O
- -X- _ O
President -X- _ O
. -X- _ O

{ -X- _ O
United -X- _ O
states -X- _ O
, -X- _ O
Country -X- _ O
- -X- _ O
President -X- _ O
, -X- _ O
Joe -X- _ O
Biden -X- _ O
} -X- _ O
DelinearizationFigure -X- _ O
1 -X- _ O
: -X- _ O
Discrimination -X- _ O
and -X- _ O
generation -X- _ O
methodologies -X- _ O
for -X- _ O
relation -X- _ O
extraction -X- _ O
. -X- _ O

“ -X- _ O
Country -X- _ O
- -X- _ O
President -X- _ O
” -X- _ O
is -X- _ O
the -X- _ O
relation -X- _ O
, -X- _ O
and -X- _ O
“ -X- _ O
CP -X- _ O
” -X- _ O
is -X- _ O
short -X- _ O
for -X- _ O
“ -X- _ O
Country -X- _ O
- -X- _ O
President -X- _ O
. -X- _ O
” -X- _ O

Generative -X- _ O
Knowledge -X- _ O
Graph -X- _ O
Construction -X- _ O
. -X- _ O

Some -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
methods -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
( -X- _ O
Seq2Seq -X- _ O
) -X- _ O
framework -X- _ O
are -X- _ O
proposed -X- _ O
to -X- _ O
overcome -X- _ O
this -X- _ O
barrier -X- _ O
. -X- _ O

Early -X- _ O
work -X- _ O
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
has -X- _ O
explored -X- _ O
using -X- _ O
the -X- _ O
generative -X- _ O
paradigm -X- _ O
to -X- _ O
solve -X- _ O
different -X- _ O
entity -X- _ O
and -X- _ O
relation -X- _ O
extraction -X- _ O
tasks -X- _ O
. -X- _ O

Powered -X- _ O
by -X- _ O
fast -X- _ O
advances -X- _ O
of -X- _ O
generative -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
such -X- _ O
as -X- _ O
T5 -X- _ B-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
paradigm -X- _ O
has -X- _ O
shown -X- _ O
its -X- _ O
great -X- _ O
potential -X- _ O
in -X- _ O
unifying -X- _ O
widespread -X- _ O
NLP -X- _ O
tasks -X- _ O
. -X- _ O

Hence -X- _ O
, -X- _ O
more -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
works -X- _ O
( -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
; -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
have -X- _ O
been -X- _ O
proposed -X- _ O
, -X- _ O
showing -X- _ O
appealing -X- _ O
performance -X- _ O
in -X- _ O
benchmark -X- _ O
datasets -X- _ O
. -X- _ O

Figure -X- _ O
1 -X- _ O
illustrates -X- _ O
an -X- _ O
example -X- _ O
of -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
for -X- _ O
relation -X- _ O
extraction -X- _ O
. -X- _ O

The -X- _ O
target -X- _ O
triple -X- _ O
is -X- _ O
preceded -X- _ O
by -X- _ O
the -X- _ O
tag -X- _ O
< -X- _ O
triple -X- _ O
> -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
head -X- _ O
entity -X- _ O
, -X- _ O
tail -X- _ O
entity -X- _ O
, -X- _ O
and -X- _ O
relations -X- _ O
are -X- _ O
also -X- _ O
specially -X- _ O
tagged -X- _ O
, -X- _ O
allowing -X- _ O
the -X- _ O
structural -X- _ O
knowledge -X- _ O
( -X- _ O
corresponding -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
) -X- _ O
to -X- _ O
be -X- _ O
obtained -X- _ O
by -X- _ O
inverse -X- _ O
linearization -X- _ O
. -X- _ O

Despite -X- _ O
the -X- _ O
success -X- _ O
of -X- _ O
numerous -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
approaches -X- _ O
, -X- _ O
these -X- _ O
works -X- _ O
scattered -X- _ O
among -X- _ O
various -X- _ O
tasks -X- _ O
have -X- _ O
not -X- _ O
been -X- _ O
systematically -X- _ O
reviewed -X- _ O
and -X- _ O
analyzed.1 -X- _ O

Present -X- _ O
work -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
summarize -X- _ O
recent -X- _ O
progress -X- _ O
in -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
( -X- _ O
An -X- _ O
timeline -X- _ O
of -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
Appendix -X- _ O
A -X- _ O
) -X- _ O
and -X- _ O
maintain -X- _ O
a -X- _ O
public -X- _ O
repository -X- _ O
for -X- _ O
research -X- _ O
convenience1 -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
to -X- _ O
organize -X- _ O
relevant -X- _ O
work -X- _ O
by -X- _ O
the -X- _ O
generation -X- _ O
target -X- _ O
of -X- _ O
models -X- _ O
and -X- _ O
also -X- _ O
present -X- _ O
the -X- _ O
axis -X- _ O
of -X- _ O
the -X- _ O
task -X- _ O
level -X- _ O
( -X- _ O
Figure -X- _ O
3 -X- _ O
) -X- _ O
: -X- _ O
•Comprehensive -X- _ O
review -X- _ O
with -X- _ O
new -X- _ O
taxonomies -X- _ O
. -X- _ O

We -X- _ O
conduct -X- _ O
the -X- _ O
first -X- _ O
comprehensive -X- _ O
review -X- _ O
of -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
together -X- _ O
with -X- _ O
new -X- _ O
taxonomies -X- _ O
. -X- _ O

We -X- _ O
review -X- _ O
the -X- _ O
research -X- _ O
with -X- _ O
different -X- _ O
generation -X- _ O
targets -X- _ O
for -X- _ O
KGC -X- _ B-TaskName
with -X- _ O
a -X- _ O
comprehensive -X- _ O
comparison -X- _ O
and -X- _ O
summary -X- _ O
( -X- _ O
§ -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O

•Theoretical -X- _ O
insight -X- _ O
and -X- _ O
empirical -X- _ O
analysis -X- _ O
. -X- _ O

We -X- _ O
provide -X- _ O
in -X- _ O
- -X- _ O
depth -X- _ O
theoretical -X- _ O
and -X- _ O
empirical -X- _ O
analysis -X- _ O
for -X- _ O
typical -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
methods -X- _ O
, -X- _ O
illustrating -X- _ O
the -X- _ O
advantages -X- _ O
and -X- _ O
disadvantageous -X- _ O
of -X- _ O
different -X- _ O
methodologies -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
remaining -X- _ O
issues -X- _ O
( -X- _ O
§ -X- _ O
4 -X- _ O
) -X- _ O
. -X- _ O

•Wide -X- _ O
coverage -X- _ O
on -X- _ O
emerging -X- _ O
advances -X- _ O
and -X- _ O
outlook -X- _ O
on -X- _ O
future -X- _ O
directions -X- _ O
. -X- _ O

We -X- _ O
provide -X- _ O
comprehensive -X- _ O
coverage -X- _ O
of -X- _ O
emerging -X- _ O
areas -X- _ O
, -X- _ O
including -X- _ O
prompt -X- _ O
- -X- _ O
based -X- _ O
learning -X- _ O
. -X- _ O

This -X- _ O
review -X- _ O
provides -X- _ O
a -X- _ O
summary -X- _ O
of -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
and -X- _ O
highlights -X- _ O
future -X- _ O
research -X- _ O
directions -X- _ O
( -X- _ O
§ -X- _ O
5 -X- _ O
) -X- _ O
. -X- _ O

Related -X- _ O
work -X- _ O
As -X- _ O
this -X- _ O
topic -X- _ O
is -X- _ O
relatively -X- _ O
nascent -X- _ O
, -X- _ O
only -X- _ O
a -X- _ O
few -X- _ O
surveys -X- _ O
exist -X- _ O
. -X- _ O

Closest -X- _ O
to -X- _ O
our -X- _ O
work -X- _ O
, -X- _ O
Ji -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
covers -X- _ O
methods -X- _ O
for -X- _ O
knowledge -X- _ O
graph -X- _ O
construction -X- _ O
, -X- _ O
representation -X- _ O
learning -X- _ O
, -X- _ O
and -X- _ O
applications -X- _ O
, -X- _ O
which -X- _ O
mainly -X- _ O
focus -X- _ O
on -X- _ O
general -X- _ O
methods -X- _ O
for -X- _ O
KGC -X- _ B-TaskName
. -X- _ O

Zhu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
provides -X- _ O
a -X- _ O
systematic -X- _ O
survey -X- _ O
for -X- _ O
multi -X- _ O
- -X- _ O
modal -X- _ O
knowledge -X- _ O
graph -X- _ O
construction -X- _ O
and -X- _ O
review -X- _ O
the -X- _ O
challenges -X- _ O
, -X- _ O
progress -X- _ O
, -X- _ O
and -X- _ O
opportunities -X- _ O
. -X- _ O

For -X- _ O
general -X- _ O
NLP -X- _ O
, -X- _ O
Min -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
survey -X- _ O
recent -X- _ O
work -X- _ O
that -X- _ O
uses -X- _ O
these -X- _ O
large -X- _ O
language -X- _ O
models -X- _ O
to -X- _ O
solve -X- _ O
tasks -X- _ O
via -X- _ O
text -X- _ O
generation -X- _ O
approaches -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
overlaps -X- _ O
in -X- _ O
generation -X- _ O
methodologies -X- _ O
for -X- _ O
information -X- _ O
extraction -X- _ O
. -X- _ O

Different -X- _ O
from -X- _ O
those -X- _ O
surveys -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
conduct -X- _ O
a -X- _ O
literature -X- _ O
review -X- _ O
on -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
, -X- _ O
hoping -X- _ O
to -X- _ O
systematically -X- _ O
understand -X- _ O
the -X- _ O
methodologies -X- _ O
, -X- _ O
compare -X- _ O
different -X- _ O
methods -X- _ O
and -X- _ O
inspire -X- _ O
new -X- _ O
ideas -X- _ O
. -X- _ O

1https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
zjunlp -X- _ O
/ -X- _ O
Generative_KG -X- _ O
_ -X- _ O
Construction_Papers -X- _ O
tion -X- _ O
tasks -X- _ O
with -X- _ O
different -X- _ O
generative -X- _ O
paradigms -X- _ O
. -X- _ O

2 -X- _ O
Preliminary -X- _ O
on -X- _ O
Knowledge -X- _ B-TaskName
Graph -X- _ I-TaskName
Construction -X- _ I-TaskName
2.1 -X- _ O
Knowledge -X- _ B-TaskName
Graph -X- _ I-TaskName
Construction -X- _ I-TaskName
Knowledge -X- _ B-TaskName
Graph -X- _ I-TaskName
Construction -X- _ I-TaskName
mainly -X- _ O
aims -X- _ O
to -X- _ O
extract -X- _ O
structural -X- _ O
information -X- _ O
from -X- _ O
unstructured -X- _ O
texts -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
Named -X- _ O
Entity -X- _ O
Recognition -X- _ O
( -X- _ O
NER -X- _ O
) -X- _ O
( -X- _ O
Chiu -X- _ O
and -X- _ O
Nichols -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
Relation -X- _ O
Extraction -X- _ O
( -X- _ O
RE -X- _ O
) -X- _ O
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
Event -X- _ O
Extraction -X- _ O
( -X- _ O
EE -X- _ O
) -X- _ O
( -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
Entity -X- _ O
Linking -X- _ O
( -X- _ O
EL -X- _ O
) -X- _ O
( -X- _ O
Shen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Knowledge -X- _ B-TaskName
Graph -X- _ I-TaskName
Completion -X- _ I-TaskName
( -X- _ O
Lin -X- _ O
Generally -X- _ O
, -X- _ O
KGC -X- _ B-TaskName
can -X- _ O
be -X- _ O
regarded -X- _ O
as -X- _ O
structure -X- _ O
prediction -X- _ O
tasks -X- _ O
, -X- _ O
where -X- _ O
a -X- _ O
model -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
approximate -X- _ O
a -X- _ O
target -X- _ O
function -X- _ O
F -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
→y -X- _ O
, -X- _ O
where -X- _ O
x∈ -X- _ O
X -X- _ O
denotes -X- _ O
the -X- _ O
input -X- _ O
data -X- _ O
and -X- _ O

y∈ -X- _ O
Y -X- _ O
denotes -X- _ O
the -X- _ O
output -X- _ O
structure -X- _ O
sequence -X- _ O
. -X- _ O

For -X- _ O
instance -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
sentence -X- _ O
, -X- _ O
" -X- _ O
Steve -X- _ O
Jobs -X- _ O
and -X- _ O
Steve -X- _ O
Wozniak -X- _ O
co -X- _ O
- -X- _ O
founded -X- _ O
Apple -X- _ O
Named -X- _ O
Entity -X- _ O
Recognition -X- _ O
aims -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
types -X- _ O
of -X- _ O
entities -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
‘ -X- _ O
Steve -X- _ O
Job -X- _ O
’ -X- _ O
, -X- _ O
‘ -X- _ O
Steve -X- _ O
Wozniak -X- _ O
’ -X- _ O
⇒PERSON -X- _ O
, -X- _ O
‘ -X- _ O
Apple -X- _ O
’ -X- _ O
⇒ORG -X- _ O
; -X- _ O
Relation -X- _ O
Extraction -X- _ O
aims -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
relationship -X- _ O
of -X- _ O
the -X- _ O
given -X- _ O
entity -X- _ O
pair -X- _ O
⟨Steve -X- _ O

Job -X- _ O
, -X- _ O
Apple⟩as -X- _ O
founder -X- _ O
; -X- _ O
Event -X- _ O
Extraction -X- _ O
aims -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
event -X- _ O
type -X- _ O
asBusiness -X- _ O
Start -X- _ O
- -X- _ O
Org -X- _ O
where -X- _ O
‘ -X- _ O
co -X- _ O
- -X- _ O
founded -X- _ O
’ -X- _ O
triggers -X- _ O
the -X- _ O
event -X- _ O
and -X- _ O
( -X- _ O
Steve -X- _ O
Jobs -X- _ O
, -X- _ O
Steve -X- _ O
Wozniak -X- _ O
) -X- _ O
are -X- _ O
participants -X- _ O
in -X- _ O
the -X- _ O
event -X- _ O
as -X- _ O
AGENT -X- _ O
andApple -X- _ O
as -X- _ O
ORGrespectively -X- _ O
. -X- _ O

Entity -X- _ O
Linking -X- _ O
aims -X- _ O
to -X- _ O
link -X- _ O
the -X- _ O
mention -X- _ O
Steve -X- _ O
Job -X- _ O
toSteven -X- _ O
Jobs -X- _ O
( -X- _ O
Q19837 -X- _ O
) -X- _ O
on -X- _ O
Wikidata -X- _ O
, -X- _ O
and -X- _ O
Apple -X- _ O
toApple -X- _ O
( -X- _ O
Q312 -X- _ O
) -X- _ O
as -X- _ O
well -X- _ O
. -X- _ O

Knowledge -X- _ O
Graph -X- _ O
Completion -X- _ O
aims -X- _ O
to -X- _ O
complete -X- _ O
incomplete -X- _ O
triples -X- _ O
⟨Steve -X- _ O
Job -X- _ O
, -X- _ O
create -X- _ O
, -X- _ O
? -X- _ O
⟩for -X- _ O
blank -X- _ O
entities -X- _ O

Apple -X- _ O
, -X- _ O
NeXT -X- _ O
Inc -X- _ O
. -X- _ O

andPixar -X- _ O
.2 -X- _ O

Generative -X- _ O
KGC -X- _ B-TaskName
Taxonomy -X- _ O
Generation -X- _ O
TargetCopy -X- _ O
- -X- _ O
based -X- _ O
SequenceCopyRE -X- _ B-MethodName
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
CopyRRL -X- _ B-MethodName
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
CopyMTL -X- _ B-MethodName
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
TEMPGEN -X- _ B-MethodName
( -X- _ O
Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
Seq2rel -X- _ B-MethodName
( -X- _ O
Giorgi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
Structure -X- _ O
- -X- _ O
based -X- _ O
SequenceSeq2Seq4ATE -X- _ B-MethodName
( -X- _ O
Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
Nested -X- _ B-MethodName
- -X- _ I-MethodName
seq -X- _ I-MethodName
( -X- _ O
Straková -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
CGT -X- _ B-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
; -X- _ O
Ye -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
PolicyIE -X- _ B-MethodName
( -X- _ O
Ahmad -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
Text2Event -X- _ B-MethodName
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
HySPA -X- _ B-MethodName
( -X- _ O
Ren -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
REBEL -X- _ B-MethodName
( -X- _ O
Cabot -X- _ O
and -X- _ O
Navigli -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
SQUIRE -X- _ B-MethodName
DEEPSTRUCT -X- _ I-MethodName
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
De -X- _ B-MethodName
- -X- _ I-MethodName
Bias -X- _ I-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
) -X- _ O
, -X- _ O
KGT5 -X- _ B-MethodName
( -X- _ O
Saxena -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
KG -X- _ O
- -X- _ O
S2S -X- _ O
( -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
Label -X- _ B-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
SequenceANL -X- _ I-MethodName
( -X- _ O
Athiwaratkun -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020a -X- _ O
) -X- _ O
, -X- _ O
GENRE -X- _ B-MethodName
( -X- _ O
Cao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
TANL -X- _ B-MethodName
( -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O

Indice -X- _ O
- -X- _ O
based -X- _ O
SequencePNDec -X- _ B-MethodName
( -X- _ O
Nayak -X- _ O
and -X- _ O
Ng -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
SEQ2SEQ -X- _ B-MethodName
- -X- _ I-MethodName
PTR -X- _ I-MethodName
( -X- _ O
Rongali -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
GRIT -X- _ B-MethodName
( -X- _ O
Du -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
, -X- _ O
UGF -X- _ B-MethodName
for -X- _ I-MethodName
NER -X- _ I-MethodName
Blank -X- _ I-MethodName
- -X- _ I-MethodName
based -X- _ I-MethodName
SequenceCOMET -X- _ I-MethodName
( -X- _ O
Bosselut -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
BART -X- _ B-MethodName
- -X- _ I-MethodName
Gen -X- _ I-MethodName
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
, -X- _ O
GTT -X- _ B-MethodName
( -X- _ O
Du -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
, -X- _ O
DEGREE -X- _ B-MethodName
( -X- _ O
Hsu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
ClarET -X- _ B-MethodName
TasksNamed -X- _ O
Entity -X- _ O
RecognitionNested -X- _ O
- -X- _ O
seq -X- _ O
( -X- _ O
Straková -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
ANL -X- _ B-MethodName
( -X- _ O
Athiwaratkun -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020a -X- _ O
) -X- _ O
, -X- _ O
TANL -X- _ B-MethodName
( -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
HySPA -X- _ B-MethodName
( -X- _ O
Ren -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
UGF -X- _ B-MethodName
for -X- _ I-MethodName
NER -X- _ I-MethodName
( -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
, -X- _ O
DEEPSTRUCT -X- _ B-MethodName
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
De -X- _ B-MethodName
- -X- _ I-MethodName
Bias -X- _ I-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
) -X- _ O
, -X- _ O
UIE -X- _ B-MethodName
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
Relation -X- _ B-MethodName
ExtractionCopyRE -X- _ B-MethodName
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
CopyRRL -X- _ B-MethodName
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
PNDec -X- _ B-MethodName
( -X- _ O
Nayak -X- _ O
and -X- _ O
Ng -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
CopyMTL -X- _ B-MethodName
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
CGT -X- _ B-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
; -X- _ O
Ye -X- _ O
et -X- _ O

al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
TANL -X- _ O
( -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
HySPA -X- _ O
( -X- _ O
Ren -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
TEMPGEN -X- _ O
( -X- _ O
Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
REBEL -X- _ O
( -X- _ O
Cabot -X- _ O
and -X- _ O
Navigli -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
DEEPSTRUCT -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
UIE -X- _ O
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
Seq2rel -X- _ O

( -X- _ O
Giorgi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O

Event -X- _ O
ExtractionCGT -X- _ B-MethodName
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
; -X- _ O
Ye -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
TANL -X- _ B-MethodName
( -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
BART -X- _ B-MethodName
- -X- _ I-MethodName
Gen -X- _ I-MethodName
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
, -X- _ O
GTT -X- _ B-MethodName
( -X- _ O
Du -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
, -X- _ O
GRIT -X- _ B-MethodName
( -X- _ O
Du -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
, -X- _ O
Text2Event -X- _ B-MethodName
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
DEGREE -X- _ B-MethodName
( -X- _ O
Hsu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
ClarET -X- _ B-MethodName
( -X- _ O
Zhou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
GTEE -X- _ O
Entity -X- _ O
Linking -X- _ O
GENRE -X- _ B-MethodName
( -X- _ O
Cao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
EPGEL -X- _ B-MethodName
( -X- _ O
Lai -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O

Knowledge -X- _ O
Graph -X- _ O
CompletionCOMET -X- _ B-MethodName
( -X- _ O
Bosselut -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
SQUIRE -X- _ B-MethodName
( -X- _ O
Bai -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
GenKGC -X- _ B-MethodName
( -X- _ O
Xie -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
, -X- _ O
HuSe -X- _ B-MethodName
- -X- _ I-MethodName
Gen -X- _ I-MethodName
( -X- _ O
Saha -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
Figure -X- _ O
3 -X- _ O
: -X- _ O
Taxonomy -X- _ O
of -X- _ O
Generative -X- _ O
Knowledge -X- _ B-TaskName
Graph -X- _ I-TaskName
Construction -X- _ I-TaskName
. -X- _ O

2.2 -X- _ O
Discrimination -X- _ O
and -X- _ O
Generation -X- _ O
Methodologies -X- _ O
In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
the -X- _ O
background -X- _ O
of -X- _ O
discrimination -X- _ O
and -X- _ O
generation -X- _ O
methodologies -X- _ O
for -X- _ O
KGC -X- _ B-TaskName
. -X- _ O

The -X- _ O
goal -X- _ O
of -X- _ O
the -X- _ O
discrimination -X- _ O
model -X- _ O
is -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
possible -X- _ O
label -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
characteristics -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
given -X- _ O
annotated -X- _ O
sentence -X- _ O
xand -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
potentially -X- _ O
overlapping -X- _ O
triples -X- _ O
tj= -X- _ O
{ -X- _ O
( -X- _ O
s -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
o -X- _ O
) -X- _ O
} -X- _ O
inx -X- _ O
, -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
maximize -X- _ O
the -X- _ O
data -X- _ O
likelihood -X- _ O
during -X- _ O
the -X- _ O
training -X- _ O
process -X- _ O
: -X- _ O
pcls -X- _ O
( -X- _ O
t|x -X- _ O
) -X- _ O
= -X- _ O
/ -X- _ O
productdisplay -X- _ O
Another -X- _ O
method -X- _ O
of -X- _ O
discrimination -X- _ O
is -X- _ O
to -X- _ O
output -X- _ O
tags -X- _ O
using -X- _ O
sequential -X- _ O
tagging -X- _ O
for -X- _ O
each -X- _ O
position -X- _ O
i -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
for -X- _ O
an -X- _ O
n -X- _ O
- -X- _ O
word -X- _ O
sentence -X- _ O
x -X- _ O
, -X- _ O
ndifferent -X- _ O
tag -X- _ O
sequences -X- _ O
are -X- _ O
annotated -X- _ O
based -X- _ O
on -X- _ O
" -X- _ O
BIESO -X- _ O
" -X- _ O
( -X- _ O
Begin -X- _ O
, -X- _ O
Inside -X- _ O
, -X- _ O
End -X- _ O
, -X- _ O
Single -X- _ O
, -X- _ O
Outside -X- _ O
) -X- _ O
notation -X- _ O
schema -X- _ O
. -X- _ O

The -X- _ O
size -X- _ O
of -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
pre -X- _ O
- -X- _ O
defined -X- _ O
relations -X- _ O
is -X- _ O
|R| -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
related -X- _ O
role -X- _ O
orders -X- _ O
are -X- _ O
represented -X- _ O
by -X- _ O
" -X- _ O
1 -X- _ O
" -X- _ O
and -X- _ O
" -X- _ O
2 -X- _ O
" -X- _ O
. -X- _ O

During -X- _ O
the -X- _ O
training -X- _ O
model -X- _ O
, -X- _ O
we -X- _ O
maximize -X- _ O
the -X- _ O
log -X- _ O
- -X- _ O
likelihood -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
tag -X- _ O
sequence -X- _ O
using -X- _ O
the -X- _ O
hidden -X- _ O
vector -X- _ O
hiat -X- _ O
each -X- _ O
position -X- _ O
i -X- _ O
: -X- _ O
ptag -X- _ O
( -X- _ O
y|x -X- _ O
) -X- _ O
= -X- _ O
exp -X- _ O
( -X- _ O
hi -X- _ O
, -X- _ O
yi -X- _ O
) -X- _ O
/ -X- _ O
summationtext -X- _ O
y′∈Rexp -X- _ O
( -X- _ O
exp -X- _ O
( -X- _ O
hi -X- _ O
, -X- _ O
y′ -X- _ O
i -X- _ O
) -X- _ O
) -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
For -X- _ O
the -X- _ O
generation -X- _ O
model -X- _ O
, -X- _ O
if -X- _ O
xis -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
and -X- _ O
ythe -X- _ O
result -X- _ O
of -X- _ O
linearized -X- _ O
triplets -X- _ O
, -X- _ O
the -X- _ O
target -X- _ O
for -X- _ O
the -X- _ O
generation -X- _ O
model -X- _ O
is -X- _ O
to -X- _ O
autoregressively -X- _ O
generate -X- _ O
ygiven -X- _ O
x -X- _ O
: -X- _ O
pgen -X- _ O
( -X- _ O
y|x -X- _ O
) -X- _ O
= -X- _ O
len -X- _ O
( -X- _ O
y -X- _ O
) -X- _ O
/ -X- _ O
productdisplay -X- _ O
By -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
seq2seq -X- _ O
model -X- _ O
( -X- _ O
e.g. -X- _ O
MASS -X- _ B-MethodName
( -X- _ O
Song -X- _ O
( -X- _ O
Lewis -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
) -X- _ O
on -X- _ O
such -X- _ O
a -X- _ O
task -X- _ O
, -X- _ O
using -X- _ O
the -X- _ O
crossentropy -X- _ O
loss -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
maximize -X- _ O
the -X- _ O
log -X- _ O
- -X- _ O
likelihood -X- _ O
of -X- _ O
the -X- _ O
generated -X- _ O
linearized -X- _ O
triplets -X- _ O
. -X- _ O

2.3 -X- _ O
Advantages -X- _ O
of -X- _ O
the -X- _ O
Generation -X- _ O
Methods -X- _ O
While -X- _ O
the -X- _ O
previous -X- _ O
discriminative -X- _ O
methods -X- _ O
( -X- _ O
Wei -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Shang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
extracts -X- _ O
relational -X- _ O
triples -X- _ O
from -X- _ O
unstructured -X- _ O
text -X- _ O
according -X- _ O
to -X- _ O
a -X- _ O
predefined -X- _ O
schema -X- _ O
to -X- _ O
efficiently -X- _ O
construct -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
knowledge -X- _ O
graphs -X- _ O
, -X- _ O
these -X- _ O
elaborate -X- _ O
models -X- _ O
focus -X- _ O
on -X- _ O
solving -X- _ O
a -X- _ O
specific -X- _ O
task -X- _ O
of -X- _ O
KGC -X- _ B-TaskName
, -X- _ O
such -X- _ O
as -X- _ O
predicting -X- _ O
relation -X- _ O
and -X- _ O
event -X- _ O
information -X- _ O
from -X- _ O
a -X- _ O
segment -X- _ O
of -X- _ O
input -X- _ O
text -X- _ O
which -X- _ O
often -X- _ O
requires -X- _ O
multiple -X- _ O
models -X- _ O
to -X- _ O
process -X- _ O
. -X- _ O

The -X- _ O
idea -X- _ O
of -X- _ O
formulating -X- _ O
KGC -X- _ B-TaskName
tasks -X- _ O
as -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
problems -X- _ O
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
will -X- _ O
be -X- _ O
of -X- _ O
great -X- _ O
benefit -X- _ O
to -X- _ O
develop -X- _ O
a -X- _ O
universal -X- _ O
architecture -X- _ O
to -X- _ O
solve -X- _ O
different -X- _ O
tasks -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
free -X- _ O
from -X- _ O
the -X- _ O
constraints -X- _ O
of -X- _ O
dedicated -X- _ O
architectures -X- _ O
, -X- _ O
isolated -X- _ O
models -X- _ O
, -X- _ O
and -X- _ O
specialized -X- _ O
knowledge -X- _ O
sources.3 -X- _ O

News -X- _ O
of -X- _ O
the -X- _ O
list -X- _ O
’ -X- _ O
s -X- _ O
existence -X- _ O
unnerved -X- _ O
ofﬁcials -X- _ O
in -X- _ O
Khartoum -X- _ O
, -X- _ O
Sudan -X- _ O
’s -X- _ O
capital.capital -X- _ O
, -X- _ O
Sudan -X- _ O
, -X- _ O
Khartoum -X- _ O
, -X- _ O
contains -X- _ O
, -X- _ O
Sudan -X- _ O
, -X- _ O
Khartoum -X- _ O
Copied -X- _ O
entityPredicted -X- _ O
relation -X- _ O
AttentionGeneration -X- _ O
ModelFigure -X- _ O
4 -X- _ O
: -X- _ O
Copy -X- _ O
- -X- _ O
based -X- _ O
Sequence -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
generative -X- _ O
models -X- _ O
can -X- _ O
be -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
in -X- _ O
multiple -X- _ O
downstream -X- _ O
tasks -X- _ O
by -X- _ O
structurally -X- _ O
consistent -X- _ O
linearization -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
, -X- _ O
which -X- _ O
facilitates -X- _ O
the -X- _ O
transition -X- _ O
from -X- _ O
traditional -X- _ O
understanding -X- _ O
to -X- _ O
structured -X- _ O
understanding -X- _ O
and -X- _ O
increases -X- _ O
knowledge -X- _ O
sharing -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
contexts -X- _ O
with -X- _ O
nested -X- _ O
labels -X- _ O
in -X- _ O
NER -X- _ O
( -X- _ O
Straková -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
proposed -X- _ O
generative -X- _ O
method -X- _ O
implicitly -X- _ O
models -X- _ O
the -X- _ O
structure -X- _ O
between -X- _ O
named -X- _ O
entities -X- _ O
, -X- _ O
thus -X- _ O
avoiding -X- _ O
the -X- _ O
complex -X- _ O
multi -X- _ O
- -X- _ O
label -X- _ O
mapping -X- _ O
. -X- _ O

Extracting -X- _ O
overlapping -X- _ O
triples -X- _ O
in -X- _ O
RE -X- _ O
is -X- _ O
also -X- _ O
difficult -X- _ O
to -X- _ O
handle -X- _ O
for -X- _ O
traditional -X- _ O
discriminative -X- _ O
models -X- _ O
, -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2018 -X- _ O
) -X- _ O
introduce -X- _ O
a -X- _ O
fresh -X- _ O
perspective -X- _ O
to -X- _ O
revisit -X- _ O
the -X- _ O
RE -X- _ O
task -X- _ O
with -X- _ O
a -X- _ O
general -X- _ O
generative -X- _ O
framework -X- _ O
that -X- _ O
addresses -X- _ O
the -X- _ O
problem -X- _ O
by -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
model -X- _ O
. -X- _ O

In -X- _ O
short -X- _ O
, -X- _ O
new -X- _ O
directions -X- _ O
can -X- _ O
be -X- _ O
explored -X- _ O
for -X- _ O
some -X- _ O
hard -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
solve -X- _ O
problems -X- _ O
through -X- _ O
paradigm -X- _ O
shifts -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
the -X- _ O
discriminative -X- _ O
and -X- _ O
generative -X- _ O
methods -X- _ O
are -X- _ O
not -X- _ O
simply -X- _ O
superior -X- _ O
or -X- _ O
inferior -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
proliferation -X- _ O
of -X- _ O
related -X- _ O
studies -X- _ O
. -X- _ O

The -X- _ O
aim -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
is -X- _ O
to -X- _ O
summarize -X- _ O
the -X- _ O
characteristics -X- _ O
of -X- _ O
different -X- _ O
generative -X- _ O
paradigms -X- _ O
in -X- _ O
KGC -X- _ B-TaskName
tasks -X- _ O
and -X- _ O
provide -X- _ O
a -X- _ O
promising -X- _ O
perspective -X- _ O
for -X- _ O
future -X- _ O
research -X- _ O
. -X- _ O

3 -X- _ O
Taxonomy -X- _ O
of -X- _ O
Generative -X- _ B-TaskName
Knowledge -X- _ I-TaskName
Graph -X- _ I-TaskName
Construction -X- _ I-TaskName
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
mainly -X- _ O
consider -X- _ O
the -X- _ O
following -X- _ O
five -X- _ O
paradigms -X- _ O
that -X- _ O
are -X- _ O
widely -X- _ O
used -X- _ O
in -X- _ O
KGC -X- _ B-TaskName
tasks -X- _ O
based -X- _ O
on -X- _ O
generation -X- _ O
target -X- _ O
, -X- _ O
i.e. -X- _ O
copy -X- _ O
- -X- _ O
based -X- _ O
Sequence -X- _ O
, -X- _ O
structure -X- _ O
- -X- _ O
linearized -X- _ O
Sequence -X- _ O
, -X- _ O
label -X- _ O
- -X- _ O
augmented -X- _ O
Sequence -X- _ O
, -X- _ O
indice -X- _ O
- -X- _ O
based -X- _ O
Sequence -X- _ O
, -X- _ O
and -X- _ O
blank -X- _ O
- -X- _ O
based -X- _ O
Sequence -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
, -X- _ O
these -X- _ O
paradigms -X- _ O
have -X- _ O
demonstrated -X- _ O
strong -X- _ O
dominance -X- _ O
in -X- _ O
many -X- _ O
mainstream -X- _ O
KGC -X- _ B-TaskName
tasks -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
following -X- _ O
sections -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
each -X- _ O
paradigm -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
. -X- _ O
3.1 -X- _ O
Copy -X- _ O
- -X- _ O
based -X- _ O
Sequence -X- _ O
This -X- _ O
paradigm -X- _ O
refers -X- _ O
to -X- _ O
developing -X- _ O
more -X- _ O
robust -X- _ O
models -X- _ O
to -X- _ O
copy -X- _ O
the -X- _ O
corresponding -X- _ O
token -X- _ O
( -X- _ O
entity -X- _ O
) -X- _ O
directly -X- _ O
from -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
during -X- _ O
the -X- _ O
generation -X- _ O
process -X- _ O
. -X- _ O

Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
designs -X- _ O
an -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
model -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
copy -X- _ O
mechanism -X- _ O
to -X- _ O
solve -X- _ O
the -X- _ O
The -X- _ O
man -X- _ O
returned -X- _ O
to -X- _ O
Los -X- _ O
Angeles -X- _ O
from -X- _ O
Mexico -X- _ O
following -X- _ O
his -X- _ O
capture -X- _ O
T -X- _ O
uesday -X- _ O
by -X- _ O
bounty -X- _ O
hunters -X- _ O
. -X- _ O

Generation -X- _ O
Model -X- _ O
( -X- _ O
( -X- _ O
Transport -X- _ O
returned -X- _ O
( -X- _ O
Artifact -X- _ O
The -X- _ O
man -X- _ O
) -X- _ O
( -X- _ O
Destination -X- _ O
Los -X- _ O
Angeles -X- _ O
) -X- _ O
( -X- _ O
Origin -X- _ O
Mexico -X- _ O
) -X- _ O
) -X- _ O

( -X- _ O
Arrest -X- _ O
- -X- _ O
Jail -X- _ O
capture -X- _ O
( -X- _ O
Person -X- _ O
The -X- _ O
man -X- _ O
) -X- _ O
( -X- _ O
T -X- _ O
ime -X- _ O
T -X- _ O
uesday -X- _ O
) -X- _ O
( -X- _ O
Agent -X- _ O
bounty -X- _ O
hunters -X- _ O
) -X- _ O
) -X- _ O
Root -X- _ O
Transport -X- _ O
Arrest -X- _ O
- -X- _ O
Jail -X- _ O
Artifact -X- _ O

DestinationOriginreturned -X- _ O
The -X- _ O
man -X- _ O
Los -X- _ O
AngelesMexico -X- _ O
Event -X- _ O
SchemaFigure -X- _ O
5 -X- _ O
: -X- _ O
Structure -X- _ O
- -X- _ O
linearized -X- _ O
Sequence -X- _ O
. -X- _ O

triple -X- _ O
overlapping -X- _ O
problem -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
4 -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
copies -X- _ O
the -X- _ O
head -X- _ O
entity -X- _ O
from -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
and -X- _ O
then -X- _ O
the -X- _ O
tail -X- _ O
entity -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
relations -X- _ O
are -X- _ O
generated -X- _ O
from -X- _ O
target -X- _ O
vocabulary -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
restricted -X- _ O
to -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
special -X- _ O
relation -X- _ O
tokens -X- _ O
. -X- _ O

This -X- _ O
paradigm -X- _ O
avoids -X- _ O
models -X- _ O
generating -X- _ O
ambiguous -X- _ O
or -X- _ O
hallucinative -X- _ O
entities -X- _ O
. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
identify -X- _ O
a -X- _ O
reasonable -X- _ O
triple -X- _ O
extraction -X- _ O
order -X- _ O
, -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
converts -X- _ O
the -X- _ O
triplet -X- _ O
generation -X- _ O
process -X- _ O
into -X- _ O
a -X- _ O
reinforcement -X- _ O
learning -X- _ O
process -X- _ O
, -X- _ O
enabling -X- _ O
the -X- _ O
copy -X- _ O
mechanism -X- _ O
to -X- _ O
follow -X- _ O
an -X- _ O
efficient -X- _ O
generative -X- _ O
order -X- _ O
. -X- _ O

Since -X- _ O
the -X- _ O
entity -X- _ O
copy -X- _ O
mechanism -X- _ O
relies -X- _ O
on -X- _ O
unnatural -X- _ O
masks -X- _ O
to -X- _ O
distinguish -X- _ O
between -X- _ O
head -X- _ O
and -X- _ O
tail -X- _ O
entities -X- _ O
, -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2020 -X- _ O
) -X- _ O
maps -X- _ O
the -X- _ O
head -X- _ O
and -X- _ O
tail -X- _ O
entities -X- _ O
to -X- _ O
fused -X- _ O
feature -X- _ O
space -X- _ O
for -X- _ O
entity -X- _ O
replication -X- _ O
by -X- _ O
an -X- _ O
additional -X- _ O
nonlinear -X- _ O
layer -X- _ O
, -X- _ O
which -X- _ O
strengthens -X- _ O
the -X- _ O
stability -X- _ O
of -X- _ O
the -X- _ O
mechanism -X- _ O
. -X- _ O

For -X- _ O
document -X- _ O
- -X- _ O
level -X- _ O
extraction -X- _ O
, -X- _ O
Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
proposes -X- _ O
a -X- _ O
TOPk -X- _ O
copy -X- _ O
mechanism -X- _ O
to -X- _ O
alleviate -X- _ O
the -X- _ O
computational -X- _ O
complexity -X- _ O
of -X- _ O
entity -X- _ O
pairs -X- _ O
. -X- _ O

3.2 -X- _ O
Structure -X- _ O
- -X- _ O
linearized -X- _ O
Sequence -X- _ O
This -X- _ O
paradigm -X- _ O
refers -X- _ O
to -X- _ O
utilizing -X- _ O
structural -X- _ O
knowledge -X- _ O
and -X- _ O
label -X- _ O
semantics -X- _ O
, -X- _ O
making -X- _ O
it -X- _ O
prone -X- _ O
to -X- _ O
handling -X- _ O
a -X- _ O
unified -X- _ O
output -X- _ O
format -X- _ O
. -X- _ O

Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
proposes -X- _ O
an -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
event -X- _ O
extraction -X- _ O
model -X- _ O
based -X- _ O
on -X- _ O
T5 -X- _ B-MethodName
, -X- _ O
where -X- _ O
the -X- _ O
output -X- _ O
is -X- _ O
a -X- _ O
linearization -X- _ O
of -X- _ O
the -X- _ O
extracted -X- _ O
knowledge -X- _ O
structure -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
5 -X- _ O
. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
avoid -X- _ O
introducing -X- _ O
noise -X- _ O
, -X- _ O
it -X- _ O
utilizes -X- _ O
the -X- _ O
event -X- _ O
schema -X- _ O
to -X- _ O
constrain -X- _ O
decoding -X- _ O
space -X- _ O
, -X- _ O
ensuring -X- _ O
the -X- _ O
output -X- _ O
text -X- _ O
is -X- _ O
semantically -X- _ O
and -X- _ O
structurally -X- _ O
legitimate -X- _ O
. -X- _ O

Lou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
reformulates -X- _ O
event -X- _ O
detection -X- _ O
as -X- _ O
a -X- _ O
Seq2Seq -X- _ O
task -X- _ O
and -X- _ O
proposes -X- _ O
a -X- _ O
Multi -X- _ B-MethodName
- -X- _ I-MethodName
Layer -X- _ I-MethodName
Bidirectional -X- _ I-MethodName
Network -X- _ I-MethodName
( -X- _ O
MLBiNet -X- _ B-MethodName
) -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
document -X- _ O
- -X- _ O
level -X- _ O
association -X- _ O
of -X- _ O
events -X- _ O
and -X- _ O
semantic -X- _ O
information -X- _ O
simultaneously -X- _ O
. -X- _ O

Besides -X- _ O
, -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021b -X- _ O
) -X- _ O
; -X- _ O
Ye -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
introduce -X- _ O
a -X- _ O
contrastive -X- _ O
learning -X- _ O
framework -X- _ O
with -X- _ O
a -X- _ O
batch -X- _ O
dynamic -X- _ O
attention -X- _ O
masking -X- _ O
mechanism -X- _ O
to -X- _ O
overcome -X- _ O
the -X- _ O
contradiction -X- _ O
in -X- _ O
meaning -X- _ O
that -X- _ O
generative -X- _ O
architectures -X- _ O
may -X- _ O
produce -X- _ O
unreliable -X- _ O
sequences -X- _ O
( -X- _ O
Zhu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
Cabot -X- _ O
and -X- _ O
Navigli -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
employs -X- _ O
a -X- _ O
simple4 -X- _ O

Tolkien -X- _ O
’ -X- _ O
s -X- _ O
epic -X- _ O
novel -X- _ O
The -X- _ O
Lord -X- _ O
of -X- _ O
the -X- _ O
Rings -X- _ O
was -X- _ O
published -X- _ O
in -X- _ O
1954 -X- _ O
- -X- _ O
1955 -X- _ O
, -X- _ O
years -X- _ O
after -X- _ O
the -X- _ O
book -X- _ O
was -X- _ O
completed -X- _ O
. -X- _ O

Generation -X- _ O
Model -X- _ O
[ -X- _ O
Tolkien -X- _ O
| -X- _ O
person -X- _ O
] -X- _ O
’s -X- _ O
epic -X- _ O
novel -X- _ O
[ -X- _ O
The -X- _ O
Lord -X- _ O
of -X- _ O
the -X- _ O
Rings -X- _ O
| -X- _ O
book -X- _ O
| -X- _ O
author -X- _ O
= -X- _ O
T -X- _ O
olkien -X- _ O
] -X- _ O
was -X- _ O
published -X- _ O
in -X- _ O
1954 -X- _ O
- -X- _ O
1955 -X- _ O
, -X- _ O
years -X- _ O
after -X- _ O
the -X- _ O
book -X- _ O
was -X- _ O
completed -X- _ O
. -X- _ O

The -X- _ O
Lord -X- _ O
of -X- _ O
the -X- _ O
Rings -X- _ O
book -X- _ O
Tolkien -X- _ O
person -X- _ O
authorDecodingFigure -X- _ O
6 -X- _ O
: -X- _ O
Label -X- _ O
- -X- _ O
augmented -X- _ O
Sequence -X- _ O
. -X- _ O

triplet -X- _ O
decomposition -X- _ O
method -X- _ O
for -X- _ O
the -X- _ O
relation -X- _ O
extraction -X- _ O
task -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
flexible -X- _ O
and -X- _ O
can -X- _ O
be -X- _ O
adapted -X- _ O
to -X- _ O
unified -X- _ O
domains -X- _ O
or -X- _ O
longer -X- _ O
documents -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
nested -X- _ O
NER -X- _ O
task -X- _ O
, -X- _ O
Straková -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
proposes -X- _ O
a -X- _ O
flattened -X- _ O
encoding -X- _ O
algorithm -X- _ O
, -X- _ O
which -X- _ O
outputs -X- _ O
multiple -X- _ O
NE -X- _ O
tags -X- _ O
following -X- _ O
the -X- _ O
BILOU -X- _ O
scheme -X- _ O
. -X- _ O

The -X- _ O
multi -X- _ O
- -X- _ O
label -X- _ O
of -X- _ O
a -X- _ O
word -X- _ O
is -X- _ O
a -X- _ O
concatenation -X- _ O
of -X- _ O
all -X- _ O
intersecting -X- _ O
tags -X- _ O
from -X- _ O
highest -X- _ O
priority -X- _ O
to -X- _ O
lowest -X- _ O
priority -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022b -X- _ O
) -X- _ O
eliminates -X- _ O
the -X- _ O
incorrect -X- _ O
biases -X- _ O
in -X- _ O
the -X- _ O
generation -X- _ O
process -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
theory -X- _ O
of -X- _ O
backdoor -X- _ O
adjustment -X- _ O
. -X- _ O

In -X- _ O
EL -X- _ O
task -X- _ O
, -X- _ O
Cao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
proposes -X- _ O
Generative -X- _ O
ENtity -X- _ O
REtrieval -X- _ O
( -X- _ O
GENRE -X- _ O
) -X- _ O
in -X- _ O
an -X- _ O
autoregressive -X- _ O
fashion -X- _ O
conditioned -X- _ O
on -X- _ O
the -X- _ O
context -X- _ O
, -X- _ O
which -X- _ O
captures -X- _ O
fine -X- _ O
- -X- _ O
grained -X- _ O
interactions -X- _ O
between -X- _ O
context -X- _ O
and -X- _ O
entity -X- _ O
name -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
; -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
extends -X- _ O
the -X- _ O
domain -X- _ O
to -X- _ O
structural -X- _ O
heterogeneous -X- _ O
information -X- _ O
extraction -X- _ O
by -X- _ O
proposing -X- _ O
a -X- _ O
unified -X- _ O
task -X- _ O
- -X- _ O
agnostic -X- _ O
generation -X- _ O
framework -X- _ O
. -X- _ O

3.3 -X- _ O
Label -X- _ O
- -X- _ O
augmented -X- _ O
Sequence -X- _ O
This -X- _ O
paradigm -X- _ O
refers -X- _ O
to -X- _ O
utilizing -X- _ O
the -X- _ O
extra -X- _ O
markers -X- _ O
to -X- _ O
indicate -X- _ O
specific -X- _ O
entities -X- _ O
or -X- _ O
relationships -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
6 -X- _ O
, -X- _ O
Athiwaratkun -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020b -X- _ O
) -X- _ O
investigates -X- _ O
the -X- _ O
label -X- _ O
- -X- _ O
augmented -X- _ O
paradigm -X- _ O
for -X- _ O
various -X- _ O
structure -X- _ O
prediction -X- _ O
tasks -X- _ O
. -X- _ O

The -X- _ O
output -X- _ O
sequence -X- _ O
copies -X- _ O
all -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
, -X- _ O
as -X- _ O
it -X- _ O
helps -X- _ O
to -X- _ O
reduce -X- _ O
ambiguity -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
this -X- _ O
paradigm -X- _ O
uses -X- _ O
square -X- _ O
brackets -X- _ O
or -X- _ O
other -X- _ O
identifiers -X- _ O
to -X- _ O
specify -X- _ O
the -X- _ O
tagging -X- _ O
sequence -X- _ O
for -X- _ O
the -X- _ O
entity -X- _ O
of -X- _ O
interest -X- _ O
. -X- _ O

The -X- _ O
relevant -X- _ O
labels -X- _ O
are -X- _ O
separated -X- _ O
by -X- _ O
the -X- _ O
separator -X- _ O
" -X- _ O
| -X- _ O
" -X- _ O
within -X- _ O
the -X- _ O
enclosed -X- _ O
brackets -X- _ O
. -X- _ O

Meanwhile -X- _ O
, -X- _ O
the -X- _ O
labeled -X- _ O
words -X- _ O
are -X- _ O
described -X- _ O
with -X- _ O
natural -X- _ O
words -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
potential -X- _ O
knowledge -X- _ O
of -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
model -X- _ O
can -X- _ O
be -X- _ O
leveraged -X- _ O
( -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
Athiwaratkun -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020a -X- _ O
) -X- _ O
naturally -X- _ O
combines -X- _ O
tag -X- _ O
semantics -X- _ O
and -X- _ O
shares -X- _ O
knowledge -X- _ O
across -X- _ O
multiple -X- _ O
sequence -X- _ O
labeling -X- _ O
tasks -X- _ O
. -X- _ O

To -X- _ O
retrieve -X- _ O
entities -X- _ O
by -X- _ O
generating -X- _ O
their -X- _ O
unique -X- _ O
names -X- _ O
, -X- _ O
Cao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O

( -X- _ O
2021 -X- _ O
) -X- _ O
extends -X- _ O
the -X- _ O
autoregressive -X- _ O
framework -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
relations -X- _ O
between -X- _ O
context -X- _ O
and -X- _ O
entity -X- _ O
Anti -X- _ O
- -X- _ O
Ethiopia -X- _ O
riots -X- _ O
erupted -X- _ O
in -X- _ O
Mogadishu -X- _ O
, -X- _ O
the -X- _ O
capital -X- _ O
of -X- _ O
Somalia -X- _ O
, -X- _ O
on -X- _ O
Friday -X- _ O
, -X- _ O
while -X- _ O
masked -X- _ O
gunmen -X- _ O
emerged -X- _ O
... -X- _ O

Generation -X- _ O
Model9944 -X- _ O
9944Somalia -X- _ O
Mogadishu -X- _ O
Somalia -X- _ O
Mogadishu -X- _ O
/ -X- _ O
location -X- _ O
/ -X- _ O
location -X- _ O
/ -X- _ O
contains -X- _ O
/ -X- _ O
location -X- _ O
/ -X- _ O
country -X- _ O
/ -X- _ O
capitalDecoding -X- _ O
Somalia -X- _ O
Mogadishu -X- _ O
/ -X- _ O
location -X- _ O
/ -X- _ O
location -X- _ O
/ -X- _ O
contains -X- _ O
/ -X- _ O
location -X- _ O
/ -X- _ O
country -X- _ O
/ -X- _ O
capitalFigure -X- _ O
7 -X- _ O
: -X- _ O
Indice -X- _ O
- -X- _ O
based -X- _ O
Sequence -X- _ O
. -X- _ O

name -X- _ O
by -X- _ O
effectively -X- _ O
cross -X- _ O
- -X- _ O
encoding -X- _ O
both -X- _ O
. -X- _ O

Since -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
gold -X- _ O
decoder -X- _ O
targets -X- _ O
is -X- _ O
often -X- _ O
longer -X- _ O
than -X- _ O
the -X- _ O
corresponding -X- _ O
input -X- _ O
length -X- _ O
, -X- _ O
this -X- _ O
paradigm -X- _ O
is -X- _ O
unsuitable -X- _ O
for -X- _ O
document -X- _ O
- -X- _ O
level -X- _ O
tasks -X- _ O
because -X- _ O
a -X- _ O
great -X- _ O
portion -X- _ O
of -X- _ O
the -X- _ O
gold -X- _ O
labels -X- _ O
will -X- _ O
be -X- _ O
skipped -X- _ O
. -X- _ O

3.4 -X- _ O
Indice -X- _ O
- -X- _ O
based -X- _ O
Sequence -X- _ O
This -X- _ O
paradigm -X- _ O
generates -X- _ O
the -X- _ O
indices -X- _ O
of -X- _ O
the -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
of -X- _ O
interest -X- _ O
directly -X- _ O
and -X- _ O
encodes -X- _ O
class -X- _ O
labels -X- _ O
as -X- _ O
label -X- _ O
indices -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
output -X- _ O
is -X- _ O
strictly -X- _ O
restricted -X- _ O
, -X- _ O
it -X- _ O
will -X- _ O
not -X- _ O
generate -X- _ O
indices -X- _ O
that -X- _ O
corresponding -X- _ O
entities -X- _ O
do -X- _ O
not -X- _ O
exist -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
, -X- _ O
except -X- _ O
for -X- _ O
relation -X- _ O
labels -X- _ O
. -X- _ O

Nayak -X- _ O
and -X- _ O
Ng -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
apply -X- _ O
the -X- _ O
method -X- _ O
to -X- _ O
the -X- _ O
relation -X- _ O
extraction -X- _ O
task -X- _ O
, -X- _ O
enabling -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
find -X- _ O
all -X- _ O
overlapping -X- _ O
tuples -X- _ O
with -X- _ O
full -X- _ O
entity -X- _ O
names -X- _ O
of -X- _ O
different -X- _ O
lengths -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
7 -X- _ O
, -X- _ O
given -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
x -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
sequence -X- _ O
yis -X- _ O
generated -X- _ O
via -X- _ O
the -X- _ O
indices -X- _ O
: -X- _ O
biandeiindicates -X- _ O
the -X- _ O
begin -X- _ O
and -X- _ O
end -X- _ O
indices -X- _ O
of -X- _ O
a -X- _ O
entity -X- _ O
tuple -X- _ O
, -X- _ O
tiis -X- _ O
the -X- _ O
index -X- _ O
of -X- _ O
the -X- _ O
entity -X- _ O
type -X- _ O
, -X- _ O
and -X- _ O
k -X- _ O
is -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
entity -X- _ O
tuples -X- _ O
. -X- _ O

The -X- _ O
hidden -X- _ O
vector -X- _ O
is -X- _ O
computed -X- _ O
at -X- _ O
decoding -X- _ O
time -X- _ O
by -X- _ O
the -X- _ O
pointer -X- _ O
network -X- _ O
( -X- _ O
Vinyals -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
to -X- _ O
get -X- _ O
the -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
tuple -X- _ O
indices -X- _ O
. -X- _ O

Besides -X- _ O
, -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021b -X- _ O
) -X- _ O
explores -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
generating -X- _ O
indices -X- _ O
for -X- _ O
NER -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
different -X- _ O
settings -X- _ O
such -X- _ O
as -X- _ O
flat -X- _ O
, -X- _ O
nested -X- _ O
, -X- _ O
and -X- _ O
discontinuous -X- _ O
NER -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
Du -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021a -X- _ O
) -X- _ O
applies -X- _ O
the -X- _ O
method -X- _ O
to -X- _ O
a -X- _ O
role -X- _ O
- -X- _ O
filler -X- _ O
entity -X- _ O
extraction -X- _ O
task -X- _ O
by -X- _ O
implicitly -X- _ O
capturing -X- _ O
noun -X- _ O
phrase -X- _ O
coreference -X- _ O
structure -X- _ O
. -X- _ O

3.5 -X- _ O
Blank -X- _ O
- -X- _ O
based -X- _ O
Sequence -X- _ O
This -X- _ O
paradigm -X- _ O
refers -X- _ O
to -X- _ O
utilizing -X- _ O
templates -X- _ O
to -X- _ O
define -X- _ O
the -X- _ O
appropriate -X- _ O
order -X- _ O
and -X- _ O
relationship -X- _ O
for -X- _ O
the -X- _ O
generated -X- _ O
spans -X- _ O
. -X- _ O

Du -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021b -X- _ O
) -X- _ O
explores -X- _ O
a -X- _ O
blank -X- _ O
- -X- _ O
based -X- _ O
form -X- _ O
for -X- _ O
event -X- _ O
extraction -X- _ O
tasks -X- _ O
which -X- _ O
includes -X- _ O
special -X- _ O
tokens -X- _ O
representing -X- _ O
event -X- _ O
information -X- _ O
such -X- _ O
as -X- _ O
event -X- _ O
types -X- _ O
. -X- _ O

Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021b -X- _ O
) -X- _ O
frames -X- _ O
document -X- _ O
- -X- _ O
level -X- _ O
event -X- _ O
argument -X- _ O
extraction -X- _ O
as -X- _ O
con-5 -X- _ O

Generation -X- _ O
Model -X- _ O
Document -X- _ O
: -X- _ O
Elliott -X- _ O
testified -X- _ O
that -X- _ O
on -X- _ O
April -X- _ O
15 -X- _ O
, -X- _ O
McV -X- _ O
eigh -X- _ O
came -X- _ O
into -X- _ O
the -X- _ O
body -X- _ O
shop -X- _ O
and -X- _ O
< -X- _ O
tgr -X- _ O
> -X- _ O
reserved -X- _ O
< -X- _ O
tgr -X- _ O
> -X- _ O
the -X- _ O
truck -X- _ O
, -X- _ O
to -X- _ O
be -X- _ O
picked -X- _ O
up -X- _ O
at -X- _ O
4 -X- _ O
pm -X- _ O
two -X- _ O
days -X- _ O
later -X- _ O
... -X- _ O

Template -X- _ O
: -X- _ O
< -X- _ O
arg1 -X- _ O
> -X- _ O
bought -X- _ O
, -X- _ O
sold -X- _ O
, -X- _ O
or -X- _ O
traded -X- _ O
< -X- _ O
arg3 -X- _ O
> -X- _ O
to -X- _ O
< -X- _ O
arg2 -X- _ O
> -X- _ O
in -X- _ O
exchange -X- _ O
for -X- _ O
< -X- _ O
arg4 -X- _ O
> -X- _ O
for -X- _ O
the -X- _ O
benefit -X- _ O
of -X- _ O
< -X- _ O
arg5 -X- _ O
> -X- _ O
at -X- _ O
< -X- _ O
arg6 -X- _ O
> -X- _ O
place.+Elliott -X- _ O
bought -X- _ O
, -X- _ O
sold -X- _ O
or -X- _ O
traded -X- _ O
truck -X- _ O
to -X- _ O
McVeigh -X- _ O
in -X- _ O
exchange -X- _ O
for -X- _ O
$ -X- _ O
280.32 -X- _ O
for -X- _ O
the -X- _ O
benefit -X- _ O
of -X- _ O
at -X- _ O
body -X- _ O
shop -X- _ O
place -X- _ O
. -X- _ O

blank -X- _ O
role -X- _ O
mention -X- _ O
Arg -X- _ O
1 -X- _ O
Giver -X- _ O
Elliot -X- _ O
Arg -X- _ O
2 -X- _ O
Recipient -X- _ O
McVeigh -X- _ O
Arg -X- _ O
3Acquired -X- _ O
Entitytruck -X- _ O
Arg -X- _ O
4Payment -X- _ O

Arg -X- _ O
6 -X- _ O
Place -X- _ O
body -X- _ O
shopBlank -X- _ O
ExtractionFigure -X- _ O
8 -X- _ O
: -X- _ O
Blank -X- _ O
- -X- _ O
based -X- _ O
Sequence -X- _ O
. -X- _ O

ditional -X- _ O
generation -X- _ O
given -X- _ O
a -X- _ O
template -X- _ O
and -X- _ O
introduces -X- _ O
the -X- _ O
new -X- _ O
document -X- _ O
- -X- _ O
level -X- _ O
informative -X- _ O
to -X- _ O
aid -X- _ O
the -X- _ O
generation -X- _ O
process -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
8 -X- _ O
, -X- _ O
the -X- _ O
template -X- _ O
refers -X- _ O
to -X- _ O
a -X- _ O
text -X- _ O
describing -X- _ O
an -X- _ O
event -X- _ O
type -X- _ O
, -X- _ O
which -X- _ O
adds -X- _ O
blank -X- _ O
argument -X- _ O
role -X- _ O
placeholders -X- _ O
. -X- _ O

The -X- _ O
output -X- _ O
sequences -X- _ O
are -X- _ O
sentences -X- _ O
where -X- _ O
the -X- _ O
blank -X- _ O
placeholders -X- _ O
are -X- _ O
replaced -X- _ O
by -X- _ O
specific -X- _ O
event -X- _ O
arguments -X- _ O
. -X- _ O

Besides -X- _ O
, -X- _ O
Hsu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
focuses -X- _ O
on -X- _ O
low -X- _ O
- -X- _ O
resource -X- _ O
event -X- _ O
extraction -X- _ O
and -X- _ O
proposes -X- _ O
a -X- _ O
data -X- _ O
- -X- _ O
efficient -X- _ O
model -X- _ O
called -X- _ O
DEGREE -X- _ O
, -X- _ O
which -X- _ O
utilizes -X- _ O
label -X- _ O
semantic -X- _ O
information -X- _ O
. -X- _ O

Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
designs -X- _ O
a -X- _ O
languageagnostic -X- _ O
template -X- _ O
to -X- _ O
represent -X- _ O
the -X- _ O
event -X- _ O
argument -X- _ O
structures -X- _ O
, -X- _ O
which -X- _ O
facilitate -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
lingual -X- _ O
transfer -X- _ O
. -X- _ O

Instead -X- _ O
of -X- _ O
conventional -X- _ O
heuristic -X- _ O
threshold -X- _ O
tuning -X- _ O
, -X- _ O
Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
proposes -X- _ O
an -X- _ O
effective -X- _ O
yet -X- _ O
efficient -X- _ O
model -X- _ O
PAIE -X- _ O
for -X- _ O
extracting -X- _ O
multiple -X- _ O
arguments -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
role -X- _ O
. -X- _ O

3.6 -X- _ O
Comparison -X- _ O
and -X- _ O
Discussion -X- _ O
Recently -X- _ O
, -X- _ O
the -X- _ O
literature -X- _ O
on -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
has -X- _ O
been -X- _ O
growing -X- _ O
rapidly -X- _ O
. -X- _ O

A -X- _ O
unifying -X- _ O
theme -X- _ O
across -X- _ O
many -X- _ O
of -X- _ O
these -X- _ O
methods -X- _ O
is -X- _ O
that -X- _ O
of -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
architecture -X- _ O
or -X- _ O
the -X- _ O
idea -X- _ O
that -X- _ O
the -X- _ O
knowledge -X- _ O
extraction -X- _ O
can -X- _ O
be -X- _ O
redefined -X- _ O
as -X- _ O
text -X- _ O
sequence -X- _ O
to -X- _ O
structure -X- _ O
generation -X- _ O
task -X- _ O
. -X- _ O

Generative -X- _ O
models -X- _ O
can -X- _ O
decode -X- _ O
and -X- _ O
control -X- _ O
extraction -X- _ O
targets -X- _ O
on -X- _ O
demand -X- _ O
for -X- _ O
different -X- _ O
specific -X- _ O
tasks -X- _ O
, -X- _ O
scenarios -X- _ O
, -X- _ O
and -X- _ O
settings -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
different -X- _ O
schema -X- _ O
) -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
different -X- _ O
forms -X- _ O
of -X- _ O
specific -X- _ O
KGC -X- _ B-TaskName
tasks -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
still -X- _ O
some -X- _ O
disagreement -X- _ O
in -X- _ O
the -X- _ O
utilization -X- _ O
of -X- _ O
the -X- _ O
generation -X- _ O
paradigms -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
we -X- _ O
make -X- _ O
a -X- _ O
comprehensive -X- _ O
comparison -X- _ O
among -X- _ O
the -X- _ O
paradigms -X- _ O
mentioned -X- _ O
above -X- _ O
via -X- _ O
rating -X- _ O
based -X- _ O
on -X- _ O
different -X- _ O
evaluation -X- _ O
scopes -X- _ O
: -X- _ O
1 -X- _ O
) -X- _ O
Semantic -X- _ O
utilization -X- _ O
refers -X- _ O
to -X- _ O
the -X- _ O
degree -X- _ O
to -X- _ O
which -X- _ O
the -X- _ O
model -X- _ O
leverages -X- _ O
the -X- _ O
semantics -X- _ O
of -X- _ O
the -X- _ O
labels -X- _ O
. -X- _ O

In -X- _ O
principle -X- _ O
, -X- _ O
we -X- _ O
believe -X- _ O
that -X- _ O
the -X- _ O
closer -X- _ O
the -X- _ O
output -X- _ O
form -X- _ O
is -X- _ O
to -X- _ O
natural -X- _ O
language -X- _ O
, -X- _ O
the -X- _ O
smaller -X- _ O
the -X- _ O
gap -X- _ O
between -X- _ O
the -X- _ O
generative -X- _ O
model -X- _ O
and -X- _ O
the -X- _ O
training -X- _ O
task -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
the -X- _ O
blank -X- _ O
- -X- _ O
based -X- _ O
paradigm -X- _ O
has -X- _ O
a -X- _ O
clear -X- _ O
advantage -X- _ O
in -X- _ O
this -X- _ O
scope -X- _ O
, -X- _ O
which -X- _ O
uses -X- _ O
manually -X- _ O
constructed -X- _ O
templates -X- _ O
to -X- _ O
make -X- _ O
the -X- _ O
output -X- _ O
close -X- _ O
to -X- _ O
natural -X- _ O
language -X- _ O
fluency -X- _ O
. -X- _ O

2 -X- _ O
) -X- _ O
Search -X- _ O
spacerefers -X- _ O
to -X- _ O
the -X- _ O
vocabulary -X- _ O
space -X- _ O
searched -X- _ O
by -X- _ O
the -X- _ O
decoder -X- _ O
. -X- _ O

Due -X- _ O
to -X- _ O
the -X- _ O
application -X- _ O
of -X- _ O
the -X- _ O
constraint -X- _ O
decoding -X- _ O
mechanism -X- _ O
, -X- _ O
some -X- _ O
structure -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
can -X- _ O
be -X- _ O
reduced -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
decoding -X- _ O
space -X- _ O
as -X- _ O
the -X- _ O
copy -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
the -X- _ O
indice -X- _ O
- -X- _ O
based -X- _ O
paradigm -X- _ O
uses -X- _ O
a -X- _ O
pointer -X- _ O
mechanism -X- _ O
that -X- _ O
constrains -X- _ O
the -X- _ O
output -X- _ O
space -X- _ O
to -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
. -X- _ O

3 -X- _ O
) -X- _ O
Application -X- _ O
scope -X- _ O
refers -X- _ O
to -X- _ O
the -X- _ O
range -X- _ O
of -X- _ O
KGC -X- _ B-TaskName
tasks -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
applied -X- _ O
. -X- _ O

We -X- _ O
believe -X- _ O
that -X- _ O
architectures -X- _ O
with -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
organize -X- _ O
information -X- _ O
more -X- _ O
flexibly -X- _ O
have -X- _ O
excellent -X- _ O
cross -X- _ O
- -X- _ O
task -X- _ O
migration -X- _ O
capabilities -X- _ O
such -X- _ O
as -X- _ O
structure -X- _ O
- -X- _ O
based -X- _ O
, -X- _ O
label -X- _ O
- -X- _ O
based -X- _ O
and -X- _ O
blank -X- _ O
- -X- _ O
based -X- _ O
paradigms -X- _ O
. -X- _ O

4 -X- _ O
) -X- _ O
Template -X- _ O
cost -X- _ O
refers -X- _ O
to -X- _ O
the -X- _ O
cost -X- _ O
of -X- _ O
constructing -X- _ O
the -X- _ O
input -X- _ O
and -X- _ O
golden -X- _ O
output -X- _ O
text -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
most -X- _ O
paradigms -X- _ O
do -X- _ O
not -X- _ O
require -X- _ O
complex -X- _ O
template -X- _ O
design -X- _ O
and -X- _ O
rely -X- _ O
only -X- _ O
on -X- _ O
linear -X- _ O
concatenation -X- _ O
to -X- _ O
meet -X- _ O
the -X- _ O
task -X- _ O
requirement -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
blank -X- _ O
- -X- _ O
based -X- _ O
paradigm -X- _ O
requires -X- _ O
more -X- _ O
labor -X- _ O
consumption -X- _ O
to -X- _ O
make -X- _ O
the -X- _ O
template -X- _ O
conform -X- _ O
to -X- _ O
the -X- _ O
semantic -X- _ O
fluency -X- _ O
requirement -X- _ O
. -X- _ O

Totally -X- _ O
in -X- _ O
line -X- _ O
with -X- _ O
recent -X- _ O
trends -X- _ O
in -X- _ O
NLP -X- _ O
, -X- _ O
a -X- _ O
growing -X- _ O
number -X- _ O
of -X- _ O
unified -X- _ O
generation -X- _ O
strategies -X- _ O
require -X- _ O
more -X- _ O
universal -X- _ O
architectures -X- _ O
( -X- _ O
Deng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
, -X- _ O
as -X- _ O
they -X- _ O
allow -X- _ O
a -X- _ O
remarkable -X- _ O
degree -X- _ O
of -X- _ O
output -X- _ O
flexibility -X- _ O
. -X- _ O

We -X- _ O
think -X- _ O
that -X- _ O
future -X- _ O
research -X- _ O
should -X- _ O
focus -X- _ O
on -X- _ O
unifying -X- _ O
cross -X- _ O
- -X- _ O
task -X- _ O
models -X- _ O
and -X- _ O
further -X- _ O
improving -X- _ O
decoding -X- _ O
efficiency -X- _ O
. -X- _ O

4 -X- _ O
Analysis -X- _ O
4.1 -X- _ O
Theoretical -X- _ O
Insight -X- _ O

This -X- _ O
section -X- _ O
provides -X- _ O
theoretical -X- _ O
insight -X- _ O
into -X- _ O
optimization -X- _ O
and -X- _ O
inference -X- _ O
for -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
. -X- _ O

For -X- _ O
optimization -X- _ O
, -X- _ O
NLG -X- _ O
are -X- _ O
normally -X- _ O
modeled -X- _ O
by -X- _ O
parameterized -X- _ O
probabilistic -X- _ O
models -X- _ O
pgenover -X- _ O
text -X- _ O
strings -X- _ O
y=⟨y1 -X- _ O
, -X- _ O
y2 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O

.⟩decomposed -X- _ O

by -X- _ O
words -X- _ O
yt -X- _ O
: -X- _ O
pgen -X- _ O
( -X- _ O
y|x -X- _ O
) -X- _ O
= -X- _ O
len -X- _ O
( -X- _ O
y -X- _ O
) -X- _ O
/ -X- _ O
productdisplay -X- _ O
where -X- _ O
yconsists -X- _ O
of -X- _ O
all -X- _ O
possible -X- _ O
strings -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
constructed -X- _ O
from -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
model -X- _ O
’s -X- _ O
vocabularyV. -X- _ O
Note -X- _ O
that -X- _ O
the -X- _ O
output -X- _ O
ycan -X- _ O
take -X- _ O
on -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
forms -X- _ O
depending -X- _ O
on -X- _ O
the -X- _ O
task -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
entities -X- _ O
, -X- _ O
relational -X- _ O
triples -X- _ O
, -X- _ O
or -X- _ O
an -X- _ O
event -X- _ O
structure -X- _ O
. -X- _ O

Usually -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
will -X- _ O
limit -X- _ O
the -X- _ O
target -X- _ O
set -X- _ O
by -X- _ O
pre -X- _ O
- -X- _ O
defined -X- _ O
schema -X- _ O
as -X- _ O
YT⊂ -X- _ O
Y -X- _ O
. -X- _ O

The -X- _ O
optimization -X- _ O
procedure -X- _ O
will -X- _ O
be -X- _ O
taken -X- _ O
to -X- _ O
estimate -X- _ O
the -X- _ O
parameters -X- _ O
with -X- _ O
loglikelihood -X- _ O
maximization -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
L -X- _ O
( -X- _ O
θ -X- _ O
; -X- _ O
T -X- _ O
) -X- _ O
= -X- _ O
− -X- _ O
/ -X- _ O
summationdisplay -X- _ O

Taxonomy -X- _ O
Generative -X- _ O
Strategy -X- _ O
Representative -X- _ O
ModelEvaluation -X- _ O
Scope -X- _ O
SU↑SS↓AS↓TS↓ -X- _ O
Copy -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
§ -X- _ O
3.1 -X- _ O
) -X- _ O
Directly -X- _ O
copy -X- _ O
entity -X- _ O
CopyRE -X- _ O
( -X- _ O
Zeng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
L -X- _ O
L -X- _ O
M -X- _ O
L -X- _ O
Restricted -X- _ O
target -X- _ O
vocabulary -X- _ O
Seq2rel -X- _ O
( -X- _ O
Giorgi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
L -X- _ O
L -X- _ O
H -X- _ O
L -X- _ O
Structure -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
§ -X- _ O
3.2 -X- _ O
) -X- _ O
Per -X- _ O
- -X- _ O
token -X- _ O
tag -X- _ O
encoding -X- _ O
Nested -X- _ O
- -X- _ O
seq -X- _ O
( -X- _ O
Straková -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
L -X- _ O
L -X- _ O
H -X- _ O
L -X- _ O
Faithful -X- _ O
contrastive -X- _ O
learning -X- _ O
CGT -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O

M -X- _ O
M -X- _ O
H -X- _ O
L -X- _ O
Prefix -X- _ O
tree -X- _ O
constraint -X- _ O
decoding -X- _ O
TEXT2EVENT -X- _ O
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O

M -X- _ O
M -X- _ O
H -X- _ O
L -X- _ O
Triplet -X- _ O
linearization -X- _ O
REBEL -X- _ O
( -X- _ O
Cabot -X- _ O
and -X- _ O
Navigli -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O

M -X- _ O
H -X- _ O
M -X- _ O
L -X- _ O
Entity -X- _ O
- -X- _ O
aware -X- _ O
hierarchical -X- _ O
decoding -X- _ O
GenKGC -X- _ B-TaskName
( -X- _ O
Xie -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O

M -X- _ O
L -X- _ O
M -X- _ O
L -X- _ O
Unified -X- _ O
structure -X- _ O
generation -X- _ O
UIE -X- _ O
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O

M -X- _ O
H -X- _ O
H -X- _ O
L -X- _ O
Reformulating -X- _ O
triple -X- _ O
prediction -X- _ O
DEEPSTRUCT -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O

M -X- _ O
H -X- _ O
H -X- _ O
L -X- _ O
Query -X- _ O
Verbalization -X- _ O
KGT5 -X- _ B-MethodName
( -X- _ O
Saxena -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O

M -X- _ O
H -X- _ O
M -X- _ O
L -X- _ O
Label -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
§ -X- _ O
3.3 -X- _ O
) -X- _ O
Augmented -X- _ O
natural -X- _ O
language -X- _ O
TANL -X- _ O
( -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O

M -X- _ O
H -X- _ O
H -X- _ O
L -X- _ O
Indice -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
§ -X- _ O
3.4 -X- _ O
) -X- _ O
Pointer -X- _ O
mechanism -X- _ O
PNDec -X- _ O
( -X- _ O
Nayak -X- _ O
and -X- _ O
Ng -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
L -X- _ O
L -X- _ O
M -X- _ O
L -X- _ O
Pointer -X- _ O
selection -X- _ O
GRIT -X- _ O
( -X- _ O
Du -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O

M -X- _ O
L -X- _ O
M -X- _ O
L -X- _ O
Blank -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
§ -X- _ O
3.5 -X- _ O
) -X- _ O
Template -X- _ O
filling -X- _ O
as -X- _ O
generation -X- _ O
GTT -X- _ O
( -X- _ O
Du -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O

H -X- _ O
H -X- _ O
H -X- _ O
H -X- _ O
Prompt -X- _ O
semantic -X- _ O
guidance -X- _ O
DEGREE -X- _ O
( -X- _ O
Hsu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O

H -X- _ O
H -X- _ O
H -X- _ O
H -X- _ O
Language -X- _ O
- -X- _ O
agnostic -X- _ O
template -X- _ O
X -X- _ O
- -X- _ O
GEAR -X- _ O
( -X- _ O
Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O

H -X- _ O
M -X- _ O
H -X- _ O
H -X- _ O
Table -X- _ O
1 -X- _ O
: -X- _ O
Comparison -X- _ O
of -X- _ O
generation -X- _ O
methods -X- _ O
from -X- _ O
different -X- _ O
evaluation -X- _ O
scopes -X- _ O
. -X- _ O

" -X- _ O
SU -X- _ O
" -X- _ O
indicates -X- _ O
semantic -X- _ O
utilization -X- _ O
, -X- _ O
" -X- _ O
SS -X- _ O
" -X- _ O
indicates -X- _ O
search -X- _ O
space -X- _ O
, -X- _ O
" -X- _ O
AS -X- _ O
" -X- _ O
indicates -X- _ O
application -X- _ O
scope -X- _ O
, -X- _ O
and -X- _ O
" -X- _ O
TS -X- _ O
" -X- _ O
indicates -X- _ O
template -X- _ O
cost -X- _ O
. -X- _ O

We -X- _ O
divide -X- _ O
the -X- _ O
degree -X- _ O
into -X- _ O
three -X- _ O
grades -X- _ O
: -X- _ O
L -X- _ O
( -X- _ O
low -X- _ O
) -X- _ O
, -X- _ O
M -X- _ O
( -X- _ O
middle -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
H -X- _ O
( -X- _ O
high -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
↑indicates -X- _ O
that -X- _ O
the -X- _ O
higher -X- _ O
grade -X- _ O
performance -X- _ O
is -X- _ O
better -X- _ O
while -X- _ O
the -X- _ O
↓is -X- _ O
the -X- _ O
opposite -X- _ O
. -X- _ O

where -X- _ O
θare -X- _ O
the -X- _ O
model -X- _ O
parameters -X- _ O
. -X- _ O

Notably -X- _ O
, -X- _ O
with -X- _ O
small -X- _ O
output -X- _ O
space -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
methods -X- _ O
with -X- _ O
the -X- _ O
indicebased -X- _ O
sequence -X- _ O
in -X- _ O
§ -X- _ O
3.4 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
can -X- _ O
converge -X- _ O
faster -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
a -X- _ O
small -X- _ O
output -X- _ O
space -X- _ O
may -X- _ O
fail -X- _ O
to -X- _ O
utilize -X- _ O
rich -X- _ O
semantic -X- _ O
information -X- _ O
from -X- _ O
labels -X- _ O
or -X- _ O
text -X- _ O
( -X- _ O
like -X- _ O
models -X- _ O
in -X- _ O
§ -X- _ O
3.5 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
short -X- _ O
, -X- _ O
the -X- _ O
design -X- _ O
of -X- _ O
output -X- _ O
space -X- _ O
is -X- _ O
vital -X- _ O
for -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
, -X- _ O
and -X- _ O
it -X- _ O
is -X- _ O
necessary -X- _ O
to -X- _ O
balance -X- _ O
parametric -X- _ O
optimization -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
semantic -X- _ O
utilization -X- _ O
. -X- _ O

For -X- _ O
inference -X- _ O
, -X- _ O
we -X- _ O
argue -X- _ O
that -X- _ O
sequence -X- _ O
decoding -X- _ O
in -X- _ O
the -X- _ O
generation -X- _ O
is -X- _ O
an -X- _ O
essential -X- _ O
procedure -X- _ O
for -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
. -X- _ O

Given -X- _ O
the -X- _ O
probabilistic -X- _ O
nature -X- _ O
of -X- _ O
q -X- _ O
, -X- _ O
the -X- _ O
decoding -X- _ O
process -X- _ O
will -X- _ O
select -X- _ O
words -X- _ O
that -X- _ O
maximize -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
resulting -X- _ O
string -X- _ O
. -X- _ O

Vanilla -X- _ O
decoding -X- _ O
solutions -X- _ O
such -X- _ O
as -X- _ O
beam -X- _ O
search -X- _ O
or -X- _ O
greedy -X- _ O
have -X- _ O
been -X- _ O
investigated -X- _ O
in -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
. -X- _ O

On -X- _ O
the -X- _ O
one -X- _ O
hand -X- _ O
, -X- _ O
knowledge -X- _ O
- -X- _ O
guided -X- _ O
( -X- _ O
or -X- _ O
schema -X- _ O
- -X- _ O
guided -X- _ O
) -X- _ O
decoding -X- _ O
has -X- _ O
become -X- _ O
the -X- _ O
mainstay -X- _ O
for -X- _ O
many -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
tasks -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
proposes -X- _ O
Text2Event -X- _ O
in -X- _ O
which -X- _ O
words -X- _ O
are -X- _ O
decoded -X- _ O
through -X- _ O
a -X- _ O
prefix -X- _ O
tree -X- _ O
based -X- _ O
on -X- _ O
pre -X- _ O
- -X- _ O
defined -X- _ O
schema -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
parallel -X- _ O
decoding -X- _ O
has -X- _ O
also -X- _ O
been -X- _ O
leveraged -X- _ O
for -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
. -X- _ O

Sui -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
formulates -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
knowledge -X- _ O
base -X- _ O
population -X- _ O
as -X- _ O
a -X- _ O
direct -X- _ O
set -X- _ O
generation -X- _ O
problem -X- _ O
, -X- _ O
avoiding -X- _ O
considering -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
multiple -X- _ O
facts -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
the -X- _ O
decoding -X- _ O
mechanism -X- _ O
plays -X- _ O
a -X- _ O
vital -X- _ O
role -X- _ O
in -X- _ O
inference -X- _ O
speed -X- _ O
and -X- _ O
quality -X- _ O
. -X- _ O

We -X- _ O
argue -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
necessary -X- _ O
to -X- _ O
develop -X- _ O
sophisticated -X- _ O
, -X- _ O
efficientdecoding -X- _ O
strategies -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
with -X- _ O
guidance -X- _ O
from -X- _ O
KG -X- _ O
) -X- _ O
for -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
. -X- _ O

4.2 -X- _ O
Empirical -X- _ O
Analysis -X- _ O
To -X- _ O
investigate -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
different -X- _ O
generation -X- _ O
methods -X- _ O
, -X- _ O
we -X- _ O
conduct -X- _ O
an -X- _ O
analysis -X- _ O
of -X- _ O
the -X- _ O
experimental -X- _ O
results -X- _ O
of -X- _ O
existing -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
work -X- _ O
. -X- _ O

Due -X- _ O
to -X- _ O
space -X- _ O
limitations -X- _ O
of -X- _ O
the -X- _ O
article -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
select -X- _ O
two -X- _ O
representative -X- _ O
tasks -X- _ O
of -X- _ O
entity -X- _ O
/ -X- _ O
relation -X- _ O
extraction -X- _ O
and -X- _ O
event -X- _ O
extraction -X- _ O
with -X- _ O
NYT -X- _ B-DatasetName
and -X- _ O
ACE -X- _ B-DatasetName
datasets2 -X- _ O
. -X- _ O

Table -X- _ O
2 -X- _ O
shows -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
discrimination -X- _ O
models -X- _ O
and -X- _ O
generative -X- _ O
models -X- _ O
on -X- _ O
the -X- _ O
NYT -X- _ B-DatasetName
datasets -X- _ O
. -X- _ O

We -X- _ O
can -X- _ O
observe -X- _ O
that -X- _ O
: -X- _ O
1 -X- _ O
) -X- _ O
Structurebased -X- _ O
and -X- _ O
label -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
both -X- _ O
achieve -X- _ O
similar -X- _ O
extraction -X- _ O
performance -X- _ O
compared -X- _ O
with -X- _ O
all -X- _ O
discrimination -X- _ O
models -X- _ O
on -X- _ O
NYT -X- _ B-DatasetName
datasets -X- _ O
. -X- _ O

We -X- _ O
believe -X- _ O
this -X- _ O
is -X- _ O
because -X- _ O
they -X- _ O
can -X- _ O
better -X- _ O
utilize -X- _ O
label -X- _ O
semantics -X- _ O
and -X- _ O
structural -X- _ O
knowledge -X- _ O
than -X- _ O
other -X- _ O
generation -X- _ O
methods -X- _ O
. -X- _ O

2 -X- _ O
) -X- _ O
Although -X- _ O
the -X- _ O
discrimination -X- _ O
methods -X- _ O
obtain -X- _ O
good -X- _ O
performance -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
generation -X- _ O
methods -X- _ O
has -X- _ O
been -X- _ O
improved -X- _ O
more -X- _ O
vastly -X- _ O
in -X- _ O
recent -X- _ O
years -X- _ O
, -X- _ O
so -X- _ O
we -X- _ O
have -X- _ O
reason -X- _ O
to -X- _ O
believe -X- _ O
that -X- _ O
they -X- _ O
will -X- _ O
have -X- _ O
greater -X- _ O
application -X- _ O
scope -X- _ O
in -X- _ O
the -X- _ O
near -X- _ O
future -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
show -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
method -X- _ O
on -X- _ O
two -X- _ O
datasets -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
discuss -X- _ O
the -X- _ O
promising -X- _ O
value -X- _ O
of -X- _ O
this -X- _ O
method -X- _ O
in -X- _ O
§ -X- _ O
5 -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
parallel -X- _ O
generation -X- _ O
of -X- _ O
the -X- _ O
unordered -X- _ O
triple -X- _ O
set -X- _ O
can -X- _ O
obtain -X- _ O
comparable -X- _ O

Type -X- _ O
ModelsNYT -X- _ O
P -X- _ O
R -X- _ O
F -X- _ O
DiscriminationCasRel -X- _ O
( -X- _ O
Wei -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
89.7 -X- _ O
89.5 -X- _ O
89.6 -X- _ O
Label -X- _ O
- -X- _ O
based -X- _ O
TANL -X- _ O
( -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
- -X- _ O
- -X- _ O
90.8 -X- _ O
Table -X- _ O
2 -X- _ O
: -X- _ O
Main -X- _ O
results -X- _ O
of -X- _ O
NYT -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

The -X- _ O
top -X- _ O
section -X- _ O
refers -X- _ O
to -X- _ O
the -X- _ O
discrimination -X- _ O
models -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
bottom -X- _ O
section -X- _ O
indicates -X- _ O
generation -X- _ O
models -X- _ O
. -X- _ O

" -X- _ O
* -X- _ O
" -X- _ O
refers -X- _ O
to -X- _ O
the -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
models -X- _ O
. -X- _ O

Type -X- _ O
ModelsTrigger -X- _ O
Argument -X- _ O
I -X- _ O
d -X- _ O

Cl -X- _ O

I -X- _ O
d -X- _ O
Cl -X- _ O
Structure -X- _ O
- -X- _ O
basedTEXT2EVENT -X- _ O
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
- -X- _ O
71.9 -X- _ O
- -X- _ O
53.8 -X- _ O
Table -X- _ O
3 -X- _ O
: -X- _ O
F1 -X- _ O
results -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
of -X- _ O
ACE-2005 -X- _ O
. -X- _ O

The -X- _ O
top -X- _ O
section -X- _ O
refers -X- _ O
to -X- _ O
the -X- _ O
discrimination -X- _ O
models -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
bottom -X- _ O
section -X- _ O
indicates -X- _ O
the -X- _ O
generation -X- _ O
models -X- _ O
. -X- _ O

I -X- _ O
d -X- _ O
is -X- _ O
Identification -X- _ O
, -X- _ O
and -X- _ O
Cl -X- _ O
is -X- _ O
Classification -X- _ O
. -X- _ O

" -X- _ O
* -X- _ O
" -X- _ O
refers -X- _ O
to -X- _ O
experiments -X- _ O
only -X- _ O
in -X- _ O
argument -X- _ O
extraction -X- _ O
tasks -X- _ O
with -X- _ O
the -X- _ O
golden -X- _ O
trigger -X- _ O
. -X- _ O

performance -X- _ O
with -X- _ O
advanced -X- _ O
discriminative -X- _ O
models -X- _ O
, -X- _ O
noting -X- _ O
that -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
methods -X- _ O
have -X- _ O
better -X- _ O
decoding -X- _ O
efficiency -X- _ O
and -X- _ O
training -X- _ O
efficiency -X- _ O
. -X- _ O

From -X- _ O
Table -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
generation -X- _ O
methods -X- _ O
can -X- _ O
obtain -X- _ O
comparable -X- _ O
performance -X- _ O
compared -X- _ O
with -X- _ O
discrimination -X- _ O
models -X- _ O
on -X- _ O
event -X- _ O
extraction -X- _ O
tasks -X- _ O
. -X- _ O

Since -X- _ O
the -X- _ O
framework -X- _ O
of -X- _ O
event -X- _ O
extraction -X- _ O
has -X- _ O
a -X- _ O
hierarchical -X- _ O
structure -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
usually -X- _ O
decomposed -X- _ O
into -X- _ O
two -X- _ O
subtasks -X- _ O
: -X- _ O
trigger -X- _ O
extraction -X- _ O
and -X- _ O
argument -X- _ O
extraction -X- _ O
) -X- _ O
, -X- _ O
structure -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
have -X- _ O
a -X- _ O
supervised -X- _ O
learning -X- _ O
framework -X- _ O
for -X- _ O
the -X- _ O
sequenceto -X- _ O
- -X- _ O
structure -X- _ O
generation -X- _ O
, -X- _ O
while -X- _ O
schema -X- _ O
constraints -X- _ O
guarantee -X- _ O
structural -X- _ O
and -X- _ O
semantic -X- _ O
legitimacy -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
owing -X- _ O
to -X- _ O
the -X- _ O
complete -X- _ O
template -X- _ O
design -X- _ O
of -X- _ O
the -X- _ O
Blank -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
, -X- _ O
PLMs -X- _ O
can -X- _ O
understandcomplex -X- _ O
task -X- _ O
knowledge -X- _ O
, -X- _ O
structural -X- _ O
knowledge -X- _ O
of -X- _ O
the -X- _ O
extraction -X- _ O
framework -X- _ O
, -X- _ O
and -X- _ O
label -X- _ O
semantics -X- _ O
in -X- _ O
a -X- _ O
natural -X- _ O
language -X- _ O
manner -X- _ O
. -X- _ O

5 -X- _ O
Future -X- _ O
Directions -X- _ O
Though -X- _ O
lots -X- _ O
of -X- _ O
technical -X- _ O
solutions -X- _ O
have -X- _ O
been -X- _ O
proposed -X- _ O
for -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
as -X- _ O
surveyed -X- _ O
, -X- _ O
there -X- _ O
remain -X- _ O
some -X- _ O
potential -X- _ O
directions -X- _ O
: -X- _ O
Generation -X- _ O
Architecture -X- _ O
. -X- _ O

Most -X- _ O
of -X- _ O
the -X- _ O
recent -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
frameworks -X- _ O
face -X- _ O
serious -X- _ O
homogenization -X- _ O
with -X- _ O
Transformer -X- _ O
. -X- _ O

For -X- _ O
enhancing -X- _ O
interpretability -X- _ O
, -X- _ O
we -X- _ O
argue -X- _ O
that -X- _ O
neuro -X- _ O
- -X- _ O
symbolic -X- _ O
models -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
a -X- _ O
reasoning -X- _ O
system -X- _ O
that -X- _ O
integrates -X- _ O
neural -X- _ O
and -X- _ O
symbolic -X- _ O
) -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
; -X- _ O
Galassi -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Negro -X- _ O
and -X- _ O
Pons -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
can -X- _ O
be -X- _ O
designed -X- _ O
for -X- _ O
generative8 -X- _ O

KGC -X- _ B-TaskName
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
some -X- _ O
cutting -X- _ O
- -X- _ O
edge -X- _ O
technologies -X- _ O
such -X- _ O
as -X- _ O
spiking -X- _ O
neural -X- _ O
network -X- _ O
( -X- _ O
Tavanaei -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
dynamic -X- _ O
neural -X- _ O
networks -X- _ O
( -X- _ O
Xu -X- _ O
and -X- _ O
McAuley -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
ordinary -X- _ O
differential -X- _ O
equations -X- _ O
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
and -X- _ O
diffusion -X- _ O
models -X- _ O
( -X- _ O
Dhariwal -X- _ O
and -X- _ O
Nichol -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
can -X- _ O
also -X- _ O
provide -X- _ O
promising -X- _ O
architectures -X- _ O
. -X- _ O

Generation -X- _ O
Quality -X- _ O
. -X- _ O

Considering -X- _ O
the -X- _ O
target -X- _ O
reliability -X- _ O
of -X- _ O
generation -X- _ O
methods -X- _ O
, -X- _ O
more -X- _ O
sophisticated -X- _ O
strategies -X- _ O
can -X- _ O
be -X- _ O
leveraged -X- _ O
to -X- _ O
control -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
, -X- _ O
including -X- _ O
: -X- _ O
1 -X- _ O
) -X- _ O
Control -X- _ O
code -X- _ O
construction -X- _ O
( -X- _ O
Keskar -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Dou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
; -X- _ O
2 -X- _ O
) -X- _ O
Decoding -X- _ O
strategy -X- _ O
such -X- _ O
as -X- _ O
introducing -X- _ O
external -X- _ O
feedback -X- _ O
( -X- _ O
Holtzman -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
generative -X- _ O
discriminator -X- _ O
( -X- _ O
Krause -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
; -X- _ O
3 -X- _ O
) -X- _ O
Loss -X- _ O
function -X- _ O
design -X- _ O
( -X- _ O
Chan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
; -X- _ O
4 -X- _ O
) -X- _ O
Prompt -X- _ O
design -X- _ O
augmentation -X- _ O
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
) -X- _ O
; -X- _ O
6 -X- _ O
) -X- _ O
Write -X- _ O
- -X- _ O
then -X- _ O
- -X- _ O
Edit -X- _ O
strategy -X- _ O
( -X- _ O
Dathathri -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
; -X- _ O

7 -X- _ O
) -X- _ O
Diffusion -X- _ O
proTraining -X- _ O
Efficiency -X- _ O
. -X- _ O

In -X- _ O
practical -X- _ O
applications -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
essential -X- _ O
to -X- _ O
reduce -X- _ O
data -X- _ O
annotation -X- _ O
and -X- _ O
training -X- _ O
costs -X- _ O
. -X- _ O

One -X- _ O
idea -X- _ O
is -X- _ O
to -X- _ O
freeze -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
generation -X- _ O
model -X- _ O
parameters -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
; -X- _ O
Li -X- _ O
and -X- _ O
Liang -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
) -X- _ O
or -X- _ O
leverage -X- _ O
prompt -X- _ O
learning -X- _ O
( -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022e -X- _ O
) -X- _ O
. -X- _ O

Another -X- _ O
idea -X- _ O
is -X- _ O
that -X- _ O
knowledge -X- _ O
decoupling -X- _ O
intervention -X- _ O
training -X- _ O
models -X- _ O
can -X- _ O
reduce -X- _ O
parameter -X- _ O
redundancy -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Borgeaud -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Khandelwal -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
Universal -X- _ O
Deployment -X- _ O
. -X- _ O

Inspired -X- _ O
by -X- _ O
the -X- _ O
T5 -X- _ B-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
transforms -X- _ O
all -X- _ O
NLP -X- _ O
tasks -X- _ O
into -X- _ O
Text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
Text -X- _ O
tasks -X- _ O
, -X- _ O
generation -X- _ O
models -X- _ O
can -X- _ O
be -X- _ O
generalized -X- _ O
to -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
modal -X- _ O
domain -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
instead -X- _ O
of -X- _ O
improvements -X- _ O
being -X- _ O
prone -X- _ O
to -X- _ O
be -X- _ O
exclusive -X- _ O
to -X- _ O
a -X- _ O
single -X- _ O
task -X- _ O
, -X- _ O
domain -X- _ O
, -X- _ O
or -X- _ O
dataset -X- _ O
, -X- _ O
we -X- _ O
argue -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
beneficial -X- _ O
to -X- _ O
study -X- _ O
the -X- _ O
framework -X- _ O
to -X- _ O
advocate -X- _ O
for -X- _ O
a -X- _ O
unified -X- _ O
view -X- _ O
of -X- _ O
KGC -X- _ B-TaskName
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
wonderful -X- _ O
work -X- _ O
UIE -X- _ O
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
efficient -X- _ O
for -X- _ O
real -X- _ O
- -X- _ O
world -X- _ O
deployment -X- _ O
when -X- _ O
we -X- _ O
can -X- _ O
provide -X- _ O
a -X- _ O
single -X- _ O
model -X- _ O
to -X- _ O
support -X- _ O
widespread -X- _ O
KGC -X- _ B-TaskName
tasks -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020a -X- _ O
) -X- _ O
. -X- _ O

Inference -X- _ O
Speed -X- _ O
. -X- _ O

To -X- _ O
be -X- _ O
noted -X- _ O
, -X- _ O
although -X- _ O
previous -X- _ O
work -X- _ O
has -X- _ O
treated -X- _ O
KGC -X- _ B-TaskName
as -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
generative -X- _ O
tasks -X- _ O
, -X- _ O
they -X- _ O
are -X- _ O
still -X- _ O
limited -X- _ O
by -X- _ O
auto -X- _ O
- -X- _ O
regressive -X- _ O
decoders -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
autoregressive -X- _ O
decoder -X- _ O
generates -X- _ O
each -X- _ O
token -X- _ O
based -X- _ O
on -X- _ O
previously -X- _ O
generated -X- _ O
tokens -X- _ O
during -X- _ O
inference -X- _ O
, -X- _ O
and -X- _ O
this -X- _ O
process -X- _ O
is -X- _ O
not -X- _ O
parallelizable -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
beneficial -X- _ O
to -X- _ O
develop -X- _ O
a -X- _ O
fast -X- _ O
inference -X- _ O
model -X- _ O
for -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
. -X- _ O

Previously -X- _ O
, -X- _ O
Sui -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
utilizes -X- _ O
the -X- _ O
transformerbased -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
decoder -X- _ O
( -X- _ O
Gu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
as -X- _ O
a -X- _ O
triple -X- _ O
set -X- _ O
generator -X- _ O
that -X- _ O
can -X- _ O
predict -X- _ O
all -X- _ O
triples -X- _ O
at -X- _ O
once -X- _ O
. -X- _ O

Sui -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
also -X- _ O
formulates -X- _ O
end -X- _ O
- -X- _ O
toend -X- _ O
knowledge -X- _ O
base -X- _ O
population -X- _ O
as -X- _ O
a -X- _ O
direct -X- _ O
set -X- _ O
generation -X- _ O
problem -X- _ O
. -X- _ O

Zhang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020b -X- _ O
) -X- _ O
proposes -X- _ O
a -X- _ O
two -X- _ O
- -X- _ O
dimensional -X- _ O
unordered -X- _ O
multitree -X- _ O
allowing -X- _ O
prediction -X- _ O
deviations -X- _ O
not -X- _ O
to -X- _ O
aggregate -X- _ O
and -X- _ O
affect -X- _ O
other -X- _ O
triples -X- _ O
. -X- _ O

To -X- _ O
sum -X- _ O
up -X- _ O
, -X- _ O
the -X- _ O
non -X- _ O
- -X- _ O
autoregressive -X- _ O
approach -X- _ O
applied -X- _ O
to -X- _ O
KGC -X- _ B-TaskName
proves -X- _ O
to -X- _ O
be -X- _ O
effective -X- _ O
in -X- _ O
solving -X- _ O
the -X- _ O
exposure -X- _ O
bias -X- _ O
and -X- _ O
overfitting -X- _ O
problems -X- _ O
. -X- _ O

Likewise -X- _ O
, -X- _ O
the -X- _ O
semi -X- _ O
- -X- _ O
autoregressive -X- _ O
decoding -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
preserves -X- _ O
the -X- _ O
autoregressive -X- _ O
approach -X- _ O
within -X- _ O
the -X- _ O
block -X- _ O
to -X- _ O
ensure -X- _ O
consistency -X- _ O
while -X- _ O
improving -X- _ O
the -X- _ O
tuple -X- _ O
output -X- _ O
efficiency -X- _ O
. -X- _ O

Additionally -X- _ O
, -X- _ O
pathways -X- _ O
( -X- _ O
Barham -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
can -X- _ O
dynamically -X- _ O
assign -X- _ O
competencies -X- _ O
to -X- _ O
different -X- _ O
parts -X- _ O
of -X- _ O
the -X- _ O
neural -X- _ O
network -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
faster -X- _ O
and -X- _ O
more -X- _ O
efficient -X- _ O
as -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
activate -X- _ O
the -X- _ O
entire -X- _ O
network -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
. -X- _ O

6 -X- _ O
Conclusion -X- _ O
and -X- _ O
Vision -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
provide -X- _ O
an -X- _ O
overview -X- _ O
of -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
with -X- _ O
new -X- _ O
taxonomy -X- _ O
, -X- _ O
theoretical -X- _ O
insight -X- _ O
and -X- _ O
empirical -X- _ O
analysis -X- _ O
, -X- _ O
and -X- _ O
several -X- _ O
research -X- _ O
directions -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
the -X- _ O
generative -X- _ O
paradigm -X- _ O
for -X- _ O
KGC -X- _ B-TaskName
has -X- _ O
the -X- _ O
potential -X- _ O
advantages -X- _ O
of -X- _ O
unifying -X- _ O
different -X- _ O
tasks -X- _ O
and -X- _ O
better -X- _ O
utilizing -X- _ O
semantic -X- _ O
information -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
future -X- _ O
, -X- _ O
we -X- _ O
envision -X- _ O
a -X- _ O
more -X- _ O
potent -X- _ O
synergy -X- _ O
between -X- _ O
the -X- _ O
methodologies -X- _ O
from -X- _ O
the -X- _ O
NLG -X- _ O
and -X- _ O
knowledge -X- _ O
graph -X- _ O
communities -X- _ O
. -X- _ O

We -X- _ O
hope -X- _ O
sophisticated -X- _ O
and -X- _ O
efficient -X- _ O
text -X- _ O
generation -X- _ O
models -X- _ O
to -X- _ O
be -X- _ O
increasingly -X- _ O
contributed -X- _ O
to -X- _ O
improving -X- _ O
the -X- _ O
KGC -X- _ B-TaskName
performance -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
converse -X- _ O
, -X- _ O
we -X- _ O
expect -X- _ O
symbolic -X- _ O
structure -X- _ O
in -X- _ O
KG -X- _ O
can -X- _ O
have -X- _ O
potential -X- _ O
guidance -X- _ O
for -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O

7 -X- _ O
Limitations -X- _ O
In -X- _ O
this -X- _ O
study -X- _ O
, -X- _ O
we -X- _ O
provide -X- _ O
a -X- _ O
review -X- _ O
of -X- _ O
generative -X- _ O
KGC -X- _ B-TaskName
. -X- _ O

Due -X- _ O
to -X- _ O
the -X- _ O
page -X- _ O
limit -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
not -X- _ O
afford -X- _ O
the -X- _ O
technical -X- _ O
details -X- _ O
for -X- _ O
models -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
review -X- _ O
the -X- _ O
works -X- _ O
within -X- _ O
five -X- _ O
years -X- _ O
, -X- _ O
mainly -X- _ O
from -X- _ O
the -X- _ O
ACL -X- _ O
, -X- _ O
EMNLP -X- _ O
, -X- _ O
NAACL -X- _ O
, -X- _ O
COLING -X- _ O
, -X- _ O
AAAI -X- _ O
, -X- _ O
IJCAI -X- _ O
, -X- _ O
etc -X- _ O
. -X- _ O

We -X- _ O
will -X- _ O
continue -X- _ O
adding -X- _ O
more -X- _ O
related -X- _ O
works -X- _ O
with -X- _ O
more -X- _ O
detailed -X- _ O
analysis -X- _ O
. -X- _ O

Acknowledgment -X- _ O
We -X- _ O
want -X- _ O
to -X- _ O
express -X- _ O
gratitude -X- _ O
to -X- _ O
the -X- _ O
anonymous -X- _ O
reviewers -X- _ O
. -X- _ O

This -X- _ O
work -X- _ O
was -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
National -X- _ O
Natural -X- _ O
Science -X- _ O
Foundation -X- _ O
of -X- _ O
China -X- _ O
jiang -X- _ O
Provincial -X- _ O
Natural -X- _ O
Science -X- _ O
Foundation -X- _ O
of -X- _ O
China -X- _ O
( -X- _ O
No -X- _ O
. -X- _ O
LGG22F030011 -X- _ O
) -X- _ O
, -X- _ O
Ningbo -X- _ O
Natural -X- _ O
Science -X- _ O
Foundation -X- _ O
( -X- _ O
2021J190 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Yongjiang -X- _ O
Talent -X- _ O
Introduction -X- _ O
Programme -X- _ O
( -X- _ O
2021A-156 -X- _ O
- -X- _ O
G -X- _ O
) -X- _ O
.9 -X- _ O

References -X- _ O
Wasi -X- _ O
Uddin -X- _ O
Ahmad -X- _ O
, -X- _ O
Jianfeng -X- _ O
Chi -X- _ O
, -X- _ O
Tu -X- _ O
Le -X- _ O
, -X- _ O
Thomas -X- _ O
Norton -X- _ O
, -X- _ O
Yuan -X- _ O
Tian -X- _ O
, -X- _ O
and -X- _ O
Kai -X- _ O
- -X- _ O
Wei -X- _ O
Chang -X- _ O
. -X- _ O

2021 -X- _ O
. -X- _ O

Intent -X- _ O
classification -X- _ O
and -X- _ O
slot -X- _ O
filling -X- _ O
for -X- _ O
privacy -X- _ O
policies -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
59th -X- _ O
Annual -X- _ O
Meeting -X- _ O
of -X- _ O
the -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
and -X- _ O
the -X- _ O
11th -X- _ O
International -X- _ O
Joint -X- _ O
Conference -X- _ O
on -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
ACL -X- _ O
/ -X- _ O
IJCNLP -X- _ O
2021 -X- _ O
, -X- _ O
( -X- _ O
Volume -X- _ O
1 -X- _ O
: -X- _ O
Long -X- _ O
Papers -X- _ O
) -X- _ O
, -X- _ O
Virtual -X- _ O
Event -X- _ O
, -X- _ O
August -X- _ O
1 -X- _ O
- -X- _ O
6 -X- _ O
, -X- _ O
2021 -X- _ O
, -X- _ O
pages -X- _ O
4402 -X- _ O
– -X- _ O
4417 -X- _ O
. -X- _ O

Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Gabor -X- _ O
Angeli -X- _ O
, -X- _ O
Victor -X- _ O
Zhong -X- _ O
, -X- _ O
Danqi -X- _ O
Chen -X- _ O
, -X- _ O
Arun -X- _ O
Tejasvi -X- _ O
Chaganty -X- _ O
, -X- _ O
Jason -X- _ O
Bolton -X- _ O
, -X- _ O
Melvin -X- _ O
Jose -X- _ O
Johnson -X- _ O
Premkumar -X- _ O
, -X- _ O
Panupong -X- _ O
Pasupat -X- _ O
, -X- _ O
Sonal -X- _ O
Gupta -X- _ O
, -X- _ O
and -X- _ O
Christopher -X- _ O
D. -X- _ O
Manning -X- _ O
. -X- _ O
2015 -X- _ O
. -X- _ O

Bootstrapped -X- _ O
self -X- _ O
training -X- _ O
for -X- _ O
knowledge -X- _ O
base -X- _ O
population -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2015 -X- _ O
Text -X- _ O
Analysis -X- _ O
Conference -X- _ O
, -X- _ O
TAC -X- _ O
2015 -X- _ O
, -X- _ O
Gaithersburg -X- _ O
, -X- _ O
Maryland -X- _ O
, -X- _ O
USA -X- _ O
, -X- _ O
November -X- _ O
16Ben -X- _ O
Athiwaratkun -X- _ O
, -X- _ O
Cícero -X- _ O
Nogueira -X- _ O
dos -X- _ O
Santos -X- _ O
, -X- _ O
Jason -X- _ O
Krone -X- _ O
, -X- _ O
and -X- _ O
Bing -X- _ O
Xiang -X- _ O
. -X- _ O
2020a -X- _ O
. -X- _ O

Augmented -X- _ O
natural -X- _ O
language -X- _ O
for -X- _ O
generative -X- _ O
sequence -X- _ O
labeling -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2020 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
, -X- _ O
EMNLP -X- _ O
Association -X- _ O
for -X- _ O
Computational -X- _ O
Linguistics -X- _ O
. -X- _ O

Ben -X- _ O
Athiwaratkun -X- _ O
, -X- _ O
Cicero -X- _ O
Nogueira -X- _ O
dos -X- _ O
Santos -X- _ O
, -X- _ O
Jason -X- _ O
Krone -X- _ O
, -X- _ O
and -X- _ O
Bing -X- _ O
Xiang -X- _ O
. -X- _ O

2020b -X- _ O
. -X- _ O

Augmented -X- _ O
natural -X- _ O
language -X- _ O
for -X- _ O
generative -X- _ O
sequence -X- _ O
labeling -X- _ O
. -X- _ O

In -X- _ O
Proceedings -X- _ O
of -X- _ O
the -X- _ O
2020 -X- _ O
Conference -X- _ O
on -X- _ O
Empirical -X- _ O
Methods -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
( -X- _ O
EMNLP -X- _ O
) -X- _ O
. -X- _ O

Yushi -X- _ O
Bai -X- _ O
, -X- _ O
Xin -X- _ O
Lv -X- _ O
, -X- _ O
Juanzi -X- _ O
Li -X- _ O
, -X- _ O
Lei -X- _ O
Hou -X- _ O
, -X- _ O
Yincen -X- _ O
Qu -X- _ O
, -X- _ O
Zelin -X- _ O
Dai -X- _ O
, -X- _ O
and -X- _ O
Feiyu -X- _ O
Xiong -X- _ O
. -X- _ O

2022 -X- _ O
. -X- _ O

SQUIRE -X- _ O
: -X- _ O

A -X- _ O
sequenceto -X- _ O
- -X- _ O
sequence -X- _ O
framework -X- _ O
for -X- _ O
multi -X- _ O
- -X- _ O
hop -X- _ O
knowledge -X- _ O
graph -X- _ O
reasoning -X- _ O
. -X- _ O

