-DOCSTART- We use the 2018 Duolingo Shared Task on Second Language Acquisition Modeling ( Settles et al . , 2018 ) dataset , which contains questions and responses for Duolingo users over the first 30 days of learning a second language . While the original task 's goal was to identify token - level mistakes , we collapse these errors into binary ( correct / incorrect ) per - question labels . We use the provided train / dev / test splits for users learning Spanish and French . We create separate held - out sets from the test set to evaluate the LM - KT and question generation models . For both models , we finetune separate GPT-2 ( Radford et al . , 2019 )
In Table 2 , we observe that lexical alignment is more beneficial for En - Mk . This can be explained by the limited vocabulary overlap of the two languages , which does not provide sufficient crosslingual signal for the training of MLM . By contrast , initializing an MLM with pretrained embeddings largely improves performance , even for a higherperforming model , such as RE - LM . In En - Sq , the effect of our approach is smaller yet consistent . This can be attributed to the fact that the two languages use the same script .
ISA on fine - grained VL phenomena In ISA tasks , models are typically confronted with highly discrepant negative samples ( non - matching imagecaptions ) . To evaluate VL models in a more finegrained manner , we examine their MM score on the VALSE benchmark ( Parcalabescu et al . , 2022 ) , where foiled captions were created by altering phrases pertaining to 6 specific linguistic phenomena : existence , counting , plurality , spatial relations , actions , and coreference , such that image and foiled caption do not match . For completeness , we also test on noun phrase foils as introduced in the FOILit ! dataset ( Shekhar et al . , 2017 ) .
For the relation recognition procedure in S 1 ( see Section 3.2 and Figure 4 ) , we empirically set the Both numbers are the 50 percentiles of the corresponding similarity scores calculated from the reduced segment pair set ( T ) . Note that , in this work , we adopt a rule - based heuristic method for recognizing relations using similarity functions with hard thresholds . We leave the exploration of other similarity functions , thresholds , and approaches to future work .
LAMBADA starts by calling the Fact Check module on the goal which fails to prove or disprove it . So Rule Selection is called which identifies two rules that can be applied : Rule3 and Rule6 . Since Rule6 is shorter , the reranker ranks it higher ; LAM - BADA starts with this rule and calls the Goal Decomposition module which breaks the goal into two sub - goals : " Dave is nice . " and " Dave is kind . " . Starting with the first sub - goal , Face Check fails on it so Rule Selection is called which selects Rule2 and Goal Decomposition decomposes the sub - goal into " Dave is green . " .
• A novel dataset of US computing curricula and relevant programs from Latin America . • An examination of attention , metric learning , and BERT modules to generate more intuitive embedding representations . • An application that compares a computing curriculum to international standards .
Due to space limitations we omit high - level discussions on benchmarking ( Wang et al . , 2018 ) and sentence - level probing ( Conneau et al . , 2018a ) , and focus on the recent findings related to the representation of linguistic structure in BERT . Surface - level information generally tends to be represented in the lower layers of deep encoders , while higher layers store hierarchical and semantic information ( Belinkov et al . , 2017;Lin et al . , 2019 ) . Tenney et al . ( 2019a ) show that the abstraction strategy applied by the English pre - trained BERT encoder follows the order of the classical NLP pipeline . Strengthening the claim about linguistic capabilities of BERT , Hewitt and Manning ( 2019 ) demonstrate that BERT implicitly learns syntax , and Reif et al . ( 2019 ) show that it encodes fine - grained lexicalsemantic distinctions . Rogers et al . ( 2020 ) provide a comprehensive overview of BERT 's properties discovered to date .
1 . Initialize the loss as
This paper presented WinoQueer , a new bias benchmark for measuring anti - queer and anti - trans bias in large language models . WinoQueer was developed via a large survey of LGBTQ+ individuals , meaning it is grounded in real - world harms and based on the experiences of actual queer people . We detail our method for participatory benchmark development , and we hope that this method will be extensible to developing community - in - the - loop benchmarks for LLM bias against other marginalized communities .
This section describes the data collection and preprocessing for the argument generation pipeline .
deepQuest : deepQuest ( I ve et al . , 2018 ) is a neural - based framework that provides state - of - theart models for multi - level QE . We use the BiRNN model , a light - weight architecture which can be trained at either sentence or document level .
There was no significant trend in the learners ' frequency of consulting the additional resources concerning language and types of additional resources . Still , learners of all languages replied that the dictionary setting was more helpful for data annotation than the translation setting . and SA was the easiest . This result looks awkward considering that language learners achieved the highest accuracy in NER .
Parallel Data for Post - Pretraining
As described in the previous section , the teacher model is trained using a direct transfer approach : it learns to generate language - independent representations from the labeled source - language data so that it can be directly applied to unlabeled targetlanguage data . However , in our proposed hybrid knowledge transfer approach , we expect the student model to reap the benefits of the data transfer paradigm . Hence , we train the student model using target - language data so that it may learn from syntactical features and word / label relations .
• Base : The original NMT models without finetuning on the schema dataset . • H2H : The NMT models that are fine - tuned on our schema translation dataset in a headerto - header manner . • H2H+CXT : The NMT models are fine - tuned by concatenating a target header and its context as input and translating the target header . • H2H+CXT+ExtL : The NMT models with two extra Transformers layers at the end of the encoder , and are fine - tuned with the same setting as H2H+CXT .
In this section , we conduct class - specific lexical analysis to identify the clinical and domainagnostic characteristics of annotation artifacts associated with each set of hypotheses in MedNLI .
We treat these as hyper - parameters . We represent the position as k , where k corresponds to a fraction of the conversation , as defined earlier . For example , the word " house " spoken by the seller in the first 10 % of turns in a negotiation would be tokenized as " s1 - house " . In the LED experiments , we omit the inner cross validation and use a batch size of 4 , the largest possible batch size given our memory constraints . 5 We select the best performing learning rate out of { 5e − 5 , 3e − 4 , 3e − 3 } and early stop based on training loss convergence .
However , linking to unseen KBs requires handling entities with an arbitrary number and type of attributes . The same entity ( Douglas Adams ) can be represented in a different KB using attributes such as " name " , " place of birth " , etc . ( top of Figure 1 ) . This raises the question of whether such models , that harness the power of pre - trained language models , generalize to linking mentions to unseen KBs , including those without such textual descriptions . This section presents multiple ideas to this end .
In particular , τ = 0 refers to the original Transformer ( Vaswani et al . , 2017 ) and τ = H means that XL PE will propagate over all attention heads .
Results are shown in Table 1 . Electric scores better than BERT , showing the energy - based formulation improves cloze model pre - training . However , Electric scores slightly lower than ELECTRA . One possible explanation is that Electric 's noise distribution is worse because a two - tower cloze model is less expressive than a masked LM . We tested this hypothesis by training ELECTRA with the same two - tower noise model as Electric . Performance did indeed go down , but it only explained about half the gap . The surprising drop in performance suggests that learning the difference between the data and generations from a low - capacity model leads to better representations than only learning the data distribution , but we believe further research is needed to fully understand the discrepancy .
In this section , we describe the heuristic used for selecting the languages . As a first step , we take a quantitative stance and choose 30 languages ( L30 ) roughly based on their percent of web content 2 . As a second step , we consider an additional five languages ( L5 ) 3 to cover low - resource languages with ish ( da ) , Dutch ( nl ) , Filipino ( fil ) , Finnish ( fi ) , French ( fr ) , German ( de ) , Greek ( el ) , Hebrew ( he ) , Hindi ( hi ) , Hungarian ( hu ) , Indonesian ( i d ) , Italian ( it ) , Japanese ( ja ) , Korean ( ko ) , Norwegian ( no ) , Persian ( fa ) , Polish ( pl ) , Portuguese ( pt ) , Romanian ( ro ) , Russian ( ru ) , Spanish ( es ) , Swedish ( sv ) , Thai ( th ) , Turkish ( tr ) , Ukrainian ( uk ) , Vietnamese ( vi ) .
Glancing Training . According to previous studies , glancing training ( Qian et al . , 2021 ) can considerably improve the translation quality of non - iterative NAR models . We apply glancing training technique to our method . More specifically , we first randomly sample reference tokens as NAR decoder inputs like Qian et al . ( 2021 ) , and then let the weak AR decoder make predictions based on the NAR decoder hidden states .
B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? Not applicable . Left blank .
SEMBLEU BLEU ( Papineni et al . , 2002 ) , which assesses text quality by comparing n - grams , is frequently adopted in machine translation evaluation . To extend BLEU for matching AMR graphs , SEM - BLEU ( Song and Gildea , 2019 ) linearizes AMR graphs through breadth - first traversal and extracts n - gram for comparison . The metric is alignmentfree and thus computationally efficient . Experimental results show that SEMBLEU achieved slightly higher consistency with human judgment than SMATCH .
Baseline Models As described in Section 5 , the baseline models provided for bgGLUE include fairly small encoder - only Transformer architectures . We leave for future work other modeling architectures and modeling techniques that are known for improving the efficiency and the computational requirements of the used models , e.g. , few - shot and zero - shot in - context learning and instruction - based evaluation , multi - task learning , etc .
The lower these metrics are , the less biases the model has . 4 We also conducted the same experiments here with bidirectional LSTM networks ( BiLSTMs ) which required a different way to generate the word clouds ( see Appendix C ) . The results on BiLSTMs , however , are not as promising as on CNNs . This might be because the way we created word clouds for each BiLSTM feature was not an accurate way to reveal its behavior . Unlike for CNNs , understanding recurrent neural network features for text classification is still an open problem .
Since the two findings are more or less surprising , some study is necessary to reveal the underlying reasons behind the findings . We hope the study helps to discover better metrics for paraphrase generation . In - depth analysis to the two findings are shown in Sec 3 and Sec 4 respectively .
Additionally , we propose and experiment with BERT - BiRNN , a variant of the BiRNN model . Rather than training the token embeddings with the task at hand , we use large - scale pre - trained token - level representations from the multilingual cased base BERT model ( Devlin et al . , 2019 ) . During training , the BERT model is fine - tuned by unfreezing the weights of the last four hidden layers along with the token embedding layer . This performs comparably to the state - of - the - art predictorestimator neural model in Kepler et al . ( 2019 ) .
We examine whether our model learns effective embeddings for rare entities using the CoNLL dataset . Following Ganea and Hofmann ( 2017 ) , we use the mentions of which entity candidates contain their gold entities and measure the performance by dividing the mentions based on the frequency of their entities in the Wikipedia annotations used to train the embeddings .
The observations from this informal survey might be surprising to researchers , who typically consider language technology to be broadly useful and beneficial to all people . An awareness that is lacking in the research field , however , is that the purpose of language technologies and their development process might vary significantly when applied to indigenous and endangered languages . For many of these languages , the priorities of the speech communities are how to more effectively document , teach , and reclaim their language ; how to save the cultural heritage passed down from the elders ; and how to let their language have a voice among other widely - spoken or dominant languages .
We treat the annotation task as identifying instances of a semantic frame ( Fillmore , 1976 ) that represents SOFC - related experiments . We include ( 1 ) cases that introduce novel content ; ( 2 ) descriptions of specific previous work ; ( 3 ) general knowledge that one could find in a textbook or survey ; and also ( 4 ) suggestions for future work .
MLQA Table 4 shows results on MLQA measured by F1 score . We notice the mBERT baseline from the original MLQA paper is significantly lower than that from , so we use the latter as our baseline . Our 2 M model outperforms the baseline by 2.3 % for zero - shot and is also 0.2 % better than XLM - R Base , which uses 57 % more model parameters than mBERT as Table 3 shows . For translate - train , our 250k model is 1.3 % better than the baseline .
The design of new experiments in scientific domains heavily depends on domain knowledge as well as on previous studies and their findings . However , the amount of publications available is typically very large , making it hard or even impossible to keep track of all experiments conducted for a particular research question . Since scientific experiments are often time - consuming and expensive , effective knowledge base population methods for finding promising settings based on the published research would be of great value ( e.g. , Auer et al . , 2018;Manica et al . , 2019;Mrdjenovich et al . , 2020 ) . While such real - life information extraction tasks have received consid- erable attention in the biomedical domain ( e.g. , Cohen et al . , 2017;Demner - Fushman et al . , 2018 , there has been little work in other domains ( Nastase et al . , 2019 ) , including materials science ( with the notable exception of the work by Mysore et al . , 2017Mysore et al . , , 2019 .
We take the edge probing setup by Tenney et al . ( 2019b ) as our starting point . Edge probing aims to predict a label given a pair of contextualized span or word encodings . More formally , we encode a WP - tokenized sentence [ wp 1 , wp 2 , ... wp k ] with a frozen pre - trained model , producing contextual embeddings [ e 1 , e 2 , ... e k ] , each of which is a layered representation over L = { l 0 , l 1 , ... l m } layers , with encoding at layer l n for the wordpiece wp i further denoted as e n i . A trainable scalar mix is applied to the layered representation to produce the final encoding given the per - layer mixing weights { a 0 , a 1 .. a m } and a scaling parameter γ :
W s ∈ R 3×d and b s are the trainable weight and bias , respectively . The cross entropy loss L S for the sentiment classification is then given as follows :
It can benefit the scenarios when communications become a bottleneck , such as federal learning , distributed training , and edge computing . However , since we do not apply explicit differentiable privacy methods to these updated parameters , the method can be vulnerable to specific attacks ( e.g. , man - inthe - middle attack ) .
Future work includes comparing our method to other question generation methods ( including supervised methods : , Puri et al . ( 2020 ) ) in order to assess the effect of both the generation method and the questions quality on the final performances of our models . Also , we will further compare different variations of our question generation and distractor refining methods in order to more thoroughly understand the effect of hyper - parameters such as the number of candidate distractors .
We refer the reader to Appendix A for an overview of the dataset licenses . We also refer the reader to Appendix B for an overview of the computatinal resources .
Because our aim was to obtain overall accuracy estimates for each condition , we did not require judgements for every individual annotation and context in the test set . However , we were able to ensure good coverage of the dataset , including annotations from all 125 tangrams and over 600 unique descriptions in each condition .
For the implementation of the transformer and convolutional sequence - to - sequence models , we built on the PyTorch ( Paszke et al . , 2019 ) library fairseq ( Ott et al . , 2019 ) by Facebook AI Research . Since , as depicted in the evaluation , the perplexity does not properly reflect the accuracy , we extended fairseq by an option to measure the quality of models with an exact match accuracy and the Levenshtein distance .
As shown in Section 4.3 , the simple architecture and high efficiency enable DecT to scale on more training data , while baseline methods struggle to finish training within acceptable time limits . To explore the scalability of DecT beyond the few - shot setting , we conduct experiments with increased training data ( n = 64 and 256 ) . For reference , we compare DecT with fine - tuning , the strongest baseline which update full model parameters .
ED as sequential decision task . Past studies Fang et al . , 2019 ) have solved ED by casting it as a sequential decision task to capture global contextual information . We adopt a similar method with an enhanced Transformer architecture , a training task , and an inference method to implement the global ED model based on BERT .
• annot -the visual feature vector is used after the encoding of the two input sentences by the two bi - directional RNNs ; • last -the visual feature vector is used just before the last layer .
• QQP : Quora Question Pairs ( Iyer et al . , 2017 ) . The task is to determine whether a pair of questions are semantically equivalent . The dataset contains 364k train examples from the community question - answering website Quora .
Chosen Metrics We select the following well known metrics : BLEU ( Papineni et al . , 2002 ) , ROUGE ( Lin , 2004 ) , METEOR ( Banerjee and Lavie , 2005 ) , BERTScore ( Zhang et al . , 2019 ) , andBARTScore ( Yuan et al . , 2021 ) . Specifically , we consider two variants of BERTScore : BERTScore ( B ) and BERTScore ( R ) , based on BERT ( Devlin et al . , 2019 ) and RoBERTa ( Liu et al . , 2019 ) respectively . For each metric M , we consider its two variants , i.e. , a reference - based version and a reference - free version ' M .Free ' . In the reference - free version , the quality of a candidate C is estimated by M ( C , X ) , where X is the input . Similarly , in the reference - based version , the formula is M ( C , R ) , where R is the reference .
Our results reveal a gap between AI and humanlevel humor " understanding . " In the from pixels setting , our best multimodal model ( fine - tuned CLIP ViT - L / 14 ( Radford et al . , 2021 ) ) achieves 62 % accuracy on a 5 - way multiple choice task , but humans achieve 94 % in the same setting . Even with significant manual annotation of the cartoons in the from description setting ( and despite significant improvements in language modeling performance since this work 's submission 2 ) large language models still fall short : human explanations are still preferred in more than two - thirds of cases compared to our best explanation model , 5 - shot GPT-4 .
Finally , we integrate the prediction layer distillation and the intermediate layer distillation . Let T denote the set of the student 's layer indexes , the whole training loss of the student model is the summation of losses w.r.t . all sentences in D in :
To facilitate multilingual experiments , we use the multilingual BERT - base ( mBERT ) published by Devlin et al . ( 2019 ) . Although several recent encoders have outperformed BERT on benchmarks Lan et al . , 2019;Raffel et al . , 2019 ) , we use the original BERT architecture , since it allows us to inherit the probing methodology and to build upon the related findings .
This work was supported by the National Natural Science Foundation of China ( No . 61976051 ) , the Major Key Project of PCL ( No . PCL2021A09 , PCL2021A02 , PCL2022A03 ) , and Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies ( 2022B1212010005 ) .
The prompt template used to evaluate our models MMLU is the prompt template from the AI2 Reasoning Challenge ( AI2 - ARC ) concatenated with 5 passages in MS MARCO ( Nguyen et al . , 2016 ) . These 5 passages are selected via dense retrival using T5 - ANCE ( Ge et al . , 2023 ; Ni et al . , 2021 ) , which maps a query to a single vector to retrieve similar passage from the corpus . Adding densely - retrieved passages to prompts is a standard approach to enhance LM 's performance on zero - shot prompting . This approach is named retrieval augmentation . All T0 and METRO - T0 results reported in Table 3 are evaluated using this prompt template with retrieval augmentation .
Our evaluation shows that the Arg - CTRL is able to produce aspect - specific , high - quality arguments , applicable to automatic counter - argument generation . The contributions are as follows : ( i ) We adapt and fine - tune the CTRL for aspect - controlled neural argument generation . ( ii ) We show that detecting argument aspects and conditioning the generation model on them are necessary steps to control the model 's training process and its perspective while generating . ( iii ) We propose several methods to analyze and evaluate the quality of ( controllable ) argument generation models . ( iv ) We develop a new scheme to annotate argument aspects and release a dataset with 5,032 samples .
Hypernym Heuristic This heuristic applies when the premise contains clinical condition(s ) , medication(s ) , finding(s ) , procedure(s ) or event(s ) , the target class is entailment , and the generated hypothesis contains term(s ) that can be interpreted as super - types for a subset of elements in the premise ( e.g. , clindamycin < : antibiotic ) .
2 Related Work 2.1 QA over Text and KB Over time , the QA task has evolved into two main streams : 1 ) QA over unstructured data ( e.g. , freetext corpora like Wikipedia ) ; 2 ) QA over structured data ( e.g. , large structured KBs like DBpedia ( Lehmann et al . , 2015 ) , Wikidata ( Vrandecic and Krötzsch , 2014 ) ) . As structured and unstructured data are intuitively complementary information sources ( Oguz et al . , 2022 ) , several attempts have been made to combines the best of both worlds .
Zero - shot We fine - tuned the BERT - base model on the e - SNLI c training set with the binary token classification cross - entropy objective ( See Section 3.3 for details ) and used this as a zero - shot approach for financial signal highlighting .
• Define a binary classifier D that estimates the probability of a data point being positive as
To investigate how global contextual information helps our model to improve performance , we manually analyze the difference between the predictions of the local , natural - order , and confidence - order models . We use the fine - tuned model using the CoNLL dataset with the YAGO+KB candidates . Although all models perform well on most mentions , the local model often fails to resolve mentions of common names referring to specific entities ( e.g. , " New York " referring to New York Knicks ) . Global models are generally better to resolve such difficult cases because of the presence of strong global contextual information ( e.g. , mentions refer - ring to basketball teams ) .
Mostly agree : Although the machine explanation refers to documents that are not explicitly evident , the human explanation assumes a less likely interpretation of " delicious " .
Similarly to most of the recent * ACL conferences , we implemented the author response period : a week during which the authors have the opportunity to read the reviews and send their response . The goal of this process is improving the quality of the reviews , and we supplemented that goal with the above new option for the authors to flag specific types of review issues ( § 5.3 ) . The authors could ( but did n't have to ) provide a response and flag review issues ; this was done for 88.3 % of reviewed submissions . In 57.3 % review forms the reviewers indicated that they read the response ( it is possible that more did read the response but did not fill in the form ) .
To address the need for a dataset for the new schema translation task , we construct the first parallel schema translation dataset . It consists of 3,158 tables with 11,979 headers written in six different languages , including English , Chinese , French , German , Spanish , and Japanese . In this section , we will first introduce our construction methodology and then analyze the characteristics of our dataset .
The hyper - parameters used in the fine - tuning on the CoNLL dataset are detailed in Table 5 . We select these hyper - parameters from the search space described in Devlin et al . ( 2019 ) based on the accuracy on the development set of the CoNLL dataset .
In this section , we conduct a qualitative analysis on the effectiveness of CAST based on M2M-100 for three types of headers : headers with special tokenization , abbreviation headers , and polysemy headers . We list some of the example translations in Table 7 .
ViLT uses a single encoder that takes as input both the text and image inputs together . ViLT pre - training also uses aligned image - text data , but from existing benchmarks ( Lin et al . , 2014 ; Krishna et al . , 2016 ; Ordonez et al . , 2011 ; Sharma a person wearing a robe a person wearing a robe with a head , a collar , and a body Figure 6 : Illustration of the language and vision modalities under the different experimental conditions . et al . , 2018 ) . It is pre - trained using multiple selfsupervised objectives , including image - text matching via a binary classification head , which is suitable for our task out of the box . We implement f using this classification head . Given a textx and an image I ∈ I , we compute their similarity using the matching classification head .
The evidence of this argument is based upon the two underlined aspects . While these aspects encode a negative stance towards the topic of " Nuclear Energy " , the topic itself is not mentioned explicitly in the argument .
Controllable Text Generation aims to steer
We apply the concept of controlled neural text generation to the domain of argument generation . Our Arg - CTRL is conditioned on topics , stances , and aspects and can reliably create arguments using these control codes . We show that arguments generated with our approach are genuine and of high argumentative and grammatical quality in general . Moreover , we show that our approach can be used to generate counter - arguments in a transparent and interpretable way . We fine - tune the Arg - CTRL on two different data sources and find that using mixed data from Common - Crawl results in a higher quality of generated arguments than using user discussions from Reddit - Comments . Further , we define argument aspect detection for controlled argument generation and introduce a novel annotation scheme to crowdsource argument aspect annotations , resulting in a high - quality dataset . We publish the model weights , data , and all code necessary to train the Arg - CTRL .
Accuracy of rationale generation . To evaluate the performance of the generation , we adopt ROUGE and BLEU as the metrics . ROUGE 5 is a commonly used metric in the NLP task . We keep the results of ROUGE-1 , ROUGE-2 , and ROUGE - L. BLEU 6 ( Papineni et al . , 2002 ) is an automatic evaluation for text generation tasks that highly correlates with human evaluation . We keep the result of BLEU-1 and BLEU - N ( an average of BLEU-1 , BLEU2 , BLEU-3 , and BLEU-4 ) .
Entity Extraction Evaluation on the Synthesis Procedures Dataset
-DOCSTART- Pre - Training Transformers as Energy - Based Cloze Models
to significant degeneration of the generalization performance for text generation tasks ( Cheng et al . , 2019 ) . Table 1 provides a typical example to show the vulnerability of the model to perturbations . We can see that the words " aeroplane " and " injuries " in the original text are revised as " aeroplanes " and " accident " respectively , which do not alter the meaning of the input . However , the output generated by BART obviously hallucinates the input .
All prompting methods are trying to extract knowledge from the Large Language Models ( LLMs ) .
Special tokens When computing token - wise contributions , we do not take [ SEP ] and [ CLS ] tokens into account ( i.e. , they are always assigned zero contribution ) , since their functionality is to aggregate cross - modal information , e.g. for classification , and hence they can not be attributed to one modality exclusively .
The cloze task ( Taylor , 1953 ) of predicting the identity of a token given its surrounding context has proven highly effective for representation learning over text . BERT ( Devlin et al . , 2019 ) implements the cloze task by replacing input tokens with [ MASK ] , but this approach incurs drawbacks in efficiency ( only 15 % of tokens are masked out at a time ) and introduces a pre - train / fine - tune mismatch where BERT sees [ MASK ] tokens in training but not in fine - tuning . ELECTRA ( Clark et al . , 2020 ) uses a different pre - training task that alleviates these disadvantages . Instead of masking tokens , ELECTRA replaces some input tokens with fakes sampled from a small generator network . The pre - training task is then to distinguish the original vs. replaced tokens . While on the surface it appears quite different from BERT , in this paper we elucidate a close connection between ELECTRA and cloze modeling . In particular , we develop a new way of implementing the cloze task using an energy - based model ( EBM ) . Then we show the resulting model , which we call Electric , is closely related to ELECTRA , as well as being useful in its own right for some applications . 1 EBMs learn an energy function that assigns low energy values to inputs in the data distribution and high energy values to other inputs . They are flexible because they do not have to compute normalized probabilities . For example , Electric does not use masking or an output softmax , instead producing a scalar energy score for each token where a low energy indicates the token is likely given its context . Unlike with BERT , these likelihood scores can be computed simultaneously for all input tokens rather than only for a small masked - out subset . We propose a training algorithm for Electric that efficiently approximates a loss based on noise - contrastive estimation ( Gutmann and Hyvärinen , 2010 ) . Then we show that this training algorithm is closely related to ELECTRA ; in fact , ELECTRA can be viewed as a variant of Electric using negative sampling instead of noise - contrastive estimation .
Reference Summary : The Royal Mail has painted a postbox gold in the Oxford - shire town of Henley - on - Thames -in recognition of its medal winning rowing club .
Local - global - combined approaches ( or hybrid approaches ) can be seen as an improvement on local ones . The method HMCN ( Hierarchical Multilabel Classification Networks ) ( Wehrmann et al . , 2018 ) is probably the first hybrid model . In HMCN , local classifiers are arranged in series and global classification is conducted to coordinate these local classifiers . The method HARNN ( Hierarchical Attention - based Recurrent Neural Network ) ( Huang et al . , 2019 ) is another typical hybrid model . It shares a similar architecture with HMCN , but uses the multi - label attention mechanism to extract label - wise text features . However , since errors in the prediction of higher - level categories may provide misleading information for lower levels , these hybrid approaches might suffer from error propagation ( Rojas et al . , 2020 ) .
With the aim of fostering research on challenging information extraction tasks in the scientific domain , we target the domain of SOFC - related experiments as a starting point . Our findings based on this sample use case are transferable to similar experimental domains , which we illustrate by applying our best model configurations to a previously existing related corpus ( Mysore et al . , 2019 ) , achieving state - of - the - art results .
For IMDb dataset and AGNews dataset , we leave 10 % of the training set as validation data , and others as training data . For the AGNews dataset , we use the description for text generation and wrote a script to resolve HTML tags . For Jigsaw dataset , we apply a binary setting where we keep the " nontoxic " class unchanged and group all other classes into " toxic " class .
In Section 3.2 , we propose the following embedding learning objective : J = d∈D w i ∈d w j ∈C ( w i , h ) exp ( u T w i vw j )
• Experiments on our proposed dataset demonstrate that our approach significantly outperforms the state - of - the - art neural machine translation models in schema translation .
|x| ) is the entire sequence tokens corresponding to student s j , consisting of all their past questions and answers . Using the softmax output of the LM - KT model ( p θ KT ) , we estimate a student 's ( inverse ) difficulty in answering a specific question as d qs = p θ KT ( < Y>|s , q ) . We find that p θ KT is well - calibrated ( Section 4.2 ) , yielding a good proxy for the true question difficulty .
As mentioned in the results section of the main paper , we vary the latency weight hyperparameter ( λ ) to train different models to obtain different latency regimes . We also vary the step - size / speech segment size during inference . In total , we obtain 18 different data points corresponding to each model . In Table 3 , we compare the results obtained using MMA , MMA - XLM and MMA - SLM under similar hyperparameter settings . It will help the reader to quantify the benefits obtained from our proposed approach .
Results Table 4 shows accuracy for 15 languages . We observe that the differences between variants are relatively small compared with retrieval and mining tasks . We think this is because judging the relationship between two sentences does not rely on cosine similarity , so the pre - training can not be directly transferred to the downstream task . mBERT variants all show positive results and DAP has the largest improvement . But for XLM - R variants , only DAP maintains the performance as the base model . The TR and TLM variants suffer from performance degradation . We think this is because XLM - R has already been a well - trained multilingual model and our continued pre - training is insufficient to improve the classification capacity . However , we demonstrate DAP will not harm classification performance for a well - trained base model .
To identify directions for future research in hyperrelational extraction , we analyze the model performance separately for each general qualifier category . As shown in Table 4 , there is a variance in model performance across qualifier categories that can not be fully explained by their proportion in the dataset . For instance , although the " Time " category comprises a majority of the qualifiers , it does not have the highest performance . This suggests that future research may focus on areas such as temporal reasoning , which is an open challenge for language models ( Vashishtha et al . , 2020 ; Dhingra et al . , 2022 ) . In addition , CubeRE demonstrates strong performance across all categories which suggests that it can serve as a general extraction model for different qualifiers .
To achieve this , we propose a simple but effective method , named dynamic k strategy , to roughly estimate how many proposals need to be assigned for each golden tuple . Specifically , given a sentence containing M golden tuples , for the m - th golden tuple , instead of using fixed k , we calculate the top q m proposals according to IoU values . The top q m number could be obtained by summing up the corresponding proposals ' IoU values from the IoU matrix :
To perform the analysis presented in Section 4 , we cast each hypothesis string in the MedNLI training dataset to lowercase . We then use a scispaCy model pre - trained on the en_core_sci_lg corpus for tokenization and clinical named entity recognition ( CNER ) ( Neumann et al . , 2019a ) . Next , we merge multi - token entities , using underscores as delimiters - e.g. , " brain injury " → " brain_injury " .
Augmenting SANs with position representation SANs ignore the position of each token due to its position - unaware " bag - of - words " assumption . The most straightforward strategy is adding the position representations as part of the token representations ( Vaswani et al . , 2017;Shaw et al . , 2018 lingual position information between languages .
Typically the context is represented as the input sequence with x t replaced by a special [ MASK]placeholder token . This masked sequence is encoded into vector representations by a transformer network ( Vaswani et al . , 2017 ) . Then the representation at position t is passed into a softmax layer to produce a distribution over tokens p θ ( x t |x \t ) for the position .
Information extraction for scientific publications . Recently , several studies addressed information extraction and knowledge base construction in the scientific domain ( Augenstein et al . , 2017;Luan et al . , 2018;Jiang et al . , 2019;Buscaldi et al . , 2019 ) . We also aim at knowledge base construction but target publications about materials science experiments , a domain understudied in NLP to date . Information extraction for materials science . The work closest to ours is the one of Mysore et al . ( 2019 ) also retrieve synthesis procedures and extract recipes , though with a coarser - grained label set , focusing on different synthesis operation types . create a dataset for named entity recognition on abstracts of materials science publications . In contrast to our work , their label set ( e.g. , Material , Application , Property ) is targeted to document indexing rather than information extraction . A notable difference to our work is that we perform full - text annotation while the aforementioned approaches annotate a pre - selected set of paragraphs ( see also . Mysore et al . ( 2017 ) apply the generative model of Kiddon et al . ( 2015 ) to induce action graphs for synthesis procedures of materials from text . In Section 7.1 , we implement a similar entity extraction system and also apply our algorithms to the dataset of Mysore et al . ( 2019 ) . train word2vec ( Mikolov et al . , 2013 ) embeddings on materials science publications and show that they can be used for recommending materials for functional applications . Other works adapt the BERT model to clinical and biomedical domains ( Alsentzer et al . , 2019;Sun and Yang , 2019 ) , or generally to scientific text ( Beltagy et al . , 2019 ) .
Our architecture adds a new token to the XLM - R tokeniser called < GAP > which is inserted between the words in the target . We then concatenate the source and the target with a [ SEP ] token and we feed them into XLM - R. A simple linear layer is added on top of word and < GAP > embeddings to predict whether it is " Good " or " Bad " as shown in Figure 1 . The training configurations and the system specifications are presented in the supplementary material . We used several language pairs for which word - level QE annotations were available : English - Chinese ( En - Zh ) , English - Czech ( En - Cs ) , English - German ( En - De ) , English - Russian ( En - Ru ) , English - Latvian ( En - Lv ) and German - English ( De - En ) . The texts are from a variety of domains and the translations were produced using both neural and statistical machine translation systems . More details about these datasets can be found in Table 1 and in Fonseca et al . , 2019 ; .
Inputting - level XL SANs As illustrated in Fig . 2a , we employ a non - linear function TANH(• ) to fuse PE abs and PE XL :
This requires handling non - stationary data distribution which humans are quite adept at , partly because we can decompose a complex task in a modular fashion ( Berwick et al . , 2013 ) . For example , when learning to classify objects , we acquire modular knowledge exclusive to each class . This allows us to robustly classify irrespective of any label space manipulations such as label omission or learning over new label spaces . This notion of modularity at the level of each class label is what we call label modularity and is a desirable quality for NLP models to generalize to practical non - stationary classification settings .
In general , the principle of FIND is understanding the features and then disabling the irrelevant ones . The process makes visualizations and interpretability more actionable . Over the past few years , we have seen rapid growth of scientific research in both topics ( visualizations and interpretability ) aiming to understand many emerging advanced models including the popular transformer - based models ( Jo and Myaeng , 2020;Voita et al . , 2019;Hoover et al . , 2020 ) . We believe that our work will inspire other researchers to foster advances in both topics towards the more tangible goal of model debugging .
B2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?
In schema translation , both the meaning of the headers and the structural information like order and numbers must be completely transferred to the target language . Obviously , this requirement can not be met by translating schema as a whole with the traditional sequence - to - sequence NMT models because it can not achieve precisely token level alignment . For example , when concatenating all headers with a separator " | " , the separator can be easily lost during translation . To meet this requirement , we employ a header - to - header translation manner in this work , which translates one header at a time .
In our example , the missing article " the " is considered as an implicit error because it is a smaller imperfection in grammar .
The rationale generation module aims to generate the charge rationale and penalty rationale according to the fact description .
Table 1 shows the results of our approach compared to two pretraining approaches that rely on In the case of XLM , the effect of cross - lingual lexical alignment is more evident for En - Mk , as Mk is less similar to En , compared to Sq . This is mainly the case because the two languages use a different alphabet ( Latin for En and Cyrillic for Mk ) . This is also true for RE - LM when translating out of En , showing that enhancing the fine - tuning step of MLM with pretrained embeddings is helpful and improves the final UNMT performance .
An advantage of Electric over BERT is that it can efficiently produce pseudo - log - likelihood ( PLL ) scores for text ( Wang and Cho , 2019 ) . PLLs for Electric are
We have developed an energy - based cloze model we call Electric and designed an efficient training algorithm for Electric based on noise - contrastive estimation . Although Electric can be derived solely from the cloze task , the resulting pre - training method is closely related to ELECTRA 's GANlike pre - training algorithm . While slightly underperforming ELECTRA on downstream tasks , Electric is useful for its ability to quickly produce pseudo - log - likelihood scores for text . Furthermore , it offers a clearer and more principled view of the ELECTRA objective as a " negative sampling " version of cloze pre - training .
To achieve this goal , an unsupervised Prefix - Tuning based OOD detection framework ( PTO ) is proposed in this paper . The key idea of PTO is intuitive : an in - distribution specific prefix , optimized with the training data via maximum likelihood , could steer PLMs to assign higher likelihoods to ID samples than PLMs without the prefix , while OOD samples should be assigned lower likelihood . Thus we propose to use the likelihood change triggered by the prefix to detect OODsamples whose improvement is not obvious ( e.g. , less than a predefined threshold ) . Note that the training process of PTO does not involve the sample labels , expanding its application to situations where obtaining labeled data is cost - prohibitive .
An additional benefit of being able to link to multiple KBs is the ability to train on more than one dataset , each of which links to a different KB with different schemas . While prior work has been unable to do so due to its reliance on knowledge of KB test , this ability is more crucial in the settings we investigate , as it allows us to stack independent datasets for training . This allows us to answer our third research question . Specifically , we compare the [ SEP]-separation baseline with our full model that uses attribute - separators , attributeshuffle , and attribute - OOV . We ask whether the % of TAC 4 ) . In contrast , the baseline model observes a bigger increase in accuracy from 49.1 % to 62.6 % . While the difference between the two models reduces , the full model remains more accurate . These results also show that the seamless stacking of multiple datasets allowed by our models is effective empirically .
For GLUE , Amazon , and IMDB text classification tasks , only the [ CLS ] token is used for prediction . When we finetune or predict with ContextFirst on a GLUE / Amazon / IMDB task , the feedforward layers only need to operate on the [ CLS ] token . When we finetune or predict with SparseQueries , only the [ CLS ] token is used in the queries of the
We normalize the length value to obtain the length coefficient for each category :
2 . TFWSVD versus FWSVD : Compared to FWSVD , TFWSVD needs extra time for fac- torizing the weighted matrices through optimizations . The time cost of factorization is decided by the number of parameters in a model , and is fixed for all its downstream tasks . For GLUE tasks trained with the BERT model , TFWSVD will cost 1.5 more V100 GPU hours than FWSVD .
As shown in Figure 9 , we used a slightly different user interface in Experiment 1 for the Amazon Products dataset which is a multiclass classification task . In this setting , we did not provide the options for mostly and partly relevant ; otherwise , there would have been nine options per question which are too many for the participants to answer accurately . With the user interface in Figure 9 , we gave a score to the feature f i based on the participant answer . To explain , we re - scaled values in the i th column of W to be in the range [ 0,1 ] using min - max normalization and gave the normalized value of the chosen class as a score to the feature f i . If the participant selects None , this feature gets a zero score . The distribution of the average feature scores for this task ( one CNN ) is displayed in Figure 10 .
We report Spearman correlation for STS , Matthews correlation coefficient ( MCC ) for CoLA , exact match for SQuAD , and accuracy for the other tasks . We use the provided evaluation script for SQuAD 6 , scipy to compute Spearman scores 7 , and sklearn to compute MCC 8 . We use the standard train / dev / test splits .
POET - Math and POET - Logic each focus on one specific reasoning skill , making the pre - training task heavily dependent on the downstream task . Different from them , POET - SQL is proposed to allow LMs to master different reasoning skills simultaneously . In our implementation , POET - SQL is pre - trained with an integrated SQL executor , since we believe that SQL queries are complex enough to encompass a wide variety of computational procedures ( Table 2 ) .
Following BERT , we submit the best of 10 models fine - tuned with different random seeds to the GLUE leaderboard for test set results .
Overall , our results demonstrate that Hindi word order preferences are influenced by discourse predictability maximization considerations . The actual mechanisms of discourse effects are plausibly lexical and syntactic priming .
A For every submission : A1 . Did you describe the limitations of your work ? Section 6 Limitations A2 . Did you discuss any potential risks of your work ? Not applicable . We currently do not identify any potential risks inherently associated with our work .
Overall Performance . The overall performances of two NMT models across five target languages show similar trends . Firstly , compared with Base , which is trained only on plain text , H2H gains significant improvement . For example , H2H based on M2M-100 outperforms Base by 17.7 , 24.7 , 26.7 , 15.5 , and 16.6 BLEU in translating schema from En to Zh , Es , Fr , De , and Ja , respectively . It demonstrates a big difference between plain text and tabular data , and fine - tuning on schema translation data could alleviate the difference to some extent .
An important drawback of character - level models is that they typically require more computations than sub - word and word - level models . This is because character - level tokenization produces longer token sequences compared to sub - word or word based approaches , and the computational and memory demands of the self - attention mechanism grow quadratically with the sequence length . In order to address this challenge , CANINE ( Clark et al . , 2022b ) leverages strided convolution to downsample the character sequence , while Charformer ( Tay et al . , 2021 ) uses average pooling . Although these methods improve the computational efficiency of character - level models , they require a predefined static downsampling rate . Such downsampling operation often breaks the boundary of basic linguistic units , including morphemes and words .
Our model to classify cosponsorship decisions based on the legislator and bill data described in the previous section consists of two main elements , an Encoder and a Relational Graph Convolutional Network ( RGCN ) . The Encoder computes high dimensional representations of legislators ' bills and speeches based on their texts and transcripts , respectively . These representations are used by an RGCN and a downstream Feed - Forward Neural Network ( FFNN ) allowing us to predict how ( i.e. , active or passive ) a cosponsor supports a bill .
The document to use our code and pre - trained model is in the supplementary materials . C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? Section 3
Sentence - level probes . Utilizing the BERTspecific sentence representation [ CLS ] allows us to incorporate the sentence - level natural language inference ( NLI ) probe into our kit .
Figure 3 : Results of secondarily finetuning T5 - seq with dialogues , to help understand whether prompting or finetuning is more effective . The examples used for finetuning are derived from the set of dialogues used as prompts across the 5 trials of SDT - seq . From this , we observe that prompting with a single dialogue demonstration outperforms few - shot finetuning .
Given a document with N mentions , each of which has K entity candidates , our model solves ED by selecting a correct referent entity from the entity candidates for each mention .
The concept of framing dimensions ( Boydstun et al . , 2014 ) is close to argument aspects . In the field of argument mining , Ajjour et al . ( 2019 ) recently applied frames to label argument clusters . Yet , their method does not allow to detect frames . Other works present methods to automatically label sentences of news articles and online discussions with frames ( Hartmann et al . , 2019;Naderi and Hirst , 2017 ) . These methods are , however , limited to a small set of predefined frames that represent high - level concepts . Contrarily , we operate on a fine - grained span - level to detect aspects that are explicitly mentioned in arguments .
Using Kronecker decomposition for large compression factors leads to a reduction in the model expressiveness . This is due to the nature of the Kronecker product and the fact that elements in this representation are tied together . To address this issue , we propose to distill knowledge from the intermediate layers of the original uncompressed network to the Kronecker network during training .
This lets us talk about degrees of inequity , and therefore , measure progress towards our ideals .
We use ELECTRA 's top - level classifiers and hyperparameter values for fine - tuning as well . For GLUE tasks , a simple linear classifier is added on top of the pre - trained transformer . For SQuAD , a question answering module similar XLNet 's ( Yang et al . , 2019 ) is added on top of the transformer , which is slightly more sophisticated than BERT 's in that it jointly rather than independently predicts the start and end positions and has an " answerability " classifier added for SQuAD 2.0 . ELECTRA 's hyperparameters are similar to BERT 's , with the main difference being the addition of a layer - wise learning rate decay where layers of the network closer to the output have a higher learning rate .
Ablations . We conduct an ablation study ( in Table 5 ) of the HierGNN encoder , graph - selection attention , sparse MTC and graph fusion layer . The ablation is done on our HierGNN - PGN LIR model trained on XSum . The ablation in HierGNN reasoning module significantly degrades the model , which suggests the positive contribution of the functionality in across - sentence reasoning . The scores without GSA also confirm the guidance of graph - level information is beneficial . By removing the graph fusion layer , we again observe the performance decreases , which proves the benefits of fusing the neighbor feature from multiple hopping distances .
To properly evaluate SiMT performance , the test sets should be representative of the characteristics of real - time simultaneous translation , in both content and translation style . In addition to the official test sets described earlier , we choose to adapt the WMT newstest2015 De→En data set for realtime speech translation . We select 500 sentence pairs from this data set and ask professional translators to produce new reference translations , with as much monotonicity as linguistically possible without compromising the translation quality . The detail of this annotation task can be found in the Appendix D .
A Appendix PCA analysis Figure 4 shows an almost linear relationship between the number of principal components and the explained variance of the PCA ( see Section 3.1 ) , i.e. the higher the number of principal components , the larger the explained variance . Therefore , we experimented with various numbers of components up to 15 ( 1 , 2 , 3 , 5 , 10 , and 15 ) on the development set to find the best settings for quality prediction . Complete results Tables 5 and 6 present the full set of results of our experiments on document and sentence - level multimodal QE on our main test set , the WMT'18 test set . These are a super - set of the results presented in the main paper but include all combinations of multimodality integration and fusion strategies for sentence - level prediction , as well as different numbers of principal components kept for document - level QuEst prediction models .
On token dropping vs. token averaging . Comparing " token drop + stage-2 " with " token avg + stage-2 , " we see that average pooling instead of dropping unimportant tokens yields slightly worse results . This means that our importance - driven token selection is more efficient than directly averaging embedding across every nearby token pair .
There is still a significant gap between models used in this work and schema - aware models that are trained on the same KB as the inference KB . One way to close this gap is by using automatic table - to - text generation techniques to convert arbitrary entities into fluent and adequate text ( Kukich , 1983;McKeown , 1985;Reiter and Dale , 1997;Wiseman et al . , 2017;Chisholm et al . , 2017 ) . Another promising direction is to move beyond BERT to other pre - trained representations that are better known to encode entity information ( Zhang et al . , 2019;Guu et al . , 2020;Poerner et al . , 2020 ) .
Table 5 shows the entity linking performance and KBQA performance on GRAILQA of various methods . Compared to the popularity - based baseline ( Bert Ranking ) , Our entity disambiguation model is effective and successfully improves the entity linking F1 by 7.4 , which boosts the final KBQA F1 score by 7.0 . Our entity linking model is also better than the Bootleg approach ( Orr et al . , 2021 ) used in ReTrack ( Chen et al . , 2021 ) . Furthermore , our method without the entity disambiguation modules outperforms Bert Ranking with a substantially large margin ( 11.4 F1 score ) . Our method even beats ReTrack when it is built upon a much better entity linking model . The results suggest the strong effectiveness of our rankand - generate framework .
Analysis We conduct a χ 2 test for each heuristic to determine whether we are able to reject the null hypothesis that pattern - satisfying premisehypothesis pairs are uniformly distributed over classes . The results support our hypotheses regarding each of the three heuristics . Notably , the percentage of heuristic - satisfying pairs accounted for by the top class is lowest for the HYPERNYM hypothesis , which we attribute to the high degree of semantic overlap between entailed and neutral hypotheses .
Firstly , it is clear that erasing entity types decreases the performance of the schema translation Table 7 : Qualitative analysis for models ' performance in schema translation from En to Zh on three kinds of headers . For each predicting result , we add extra explanations for their meanings in the brackets . Results with underline denote the correct translation for the header . models . Comparing CAST ( w/o entity type ) with CAST , for instance , We can see a 0.5 and 0.5 decrease of BLEU for En - De and En - Fr respectively . Secondly , the comparison between CAST ( w/o structural relation ) and CAST shows that the structure relations also play an important role in bettering the performance of context modeling . As seen in the En - Fr translation setting , CAST(w / o structural relation ) obtains a 1.0 lower BLEU score over CAST . Finally , when erasing both kinds of edges and the models give the lowest performance .
The results for CommonsenseQA and QASC using the same selection of sentences from Wikipedia are reported in table 3 . Overall , we obtain similar results to SciQ with a large improvement of performances when generating questions and a further boost with refined distractors . However compared to SciQ , the improvement brought by the distractor refining process is less significant . This could be partly explained by the fact that the distractors in the original QASC and CommonsenseQA datasets are overall easier and therefore it is less advantageous for a model to be trained on harder questions .
In our method , the construction of the optimal policy relies on the performance of the translation model . Therefore , the training of the translation model needs to be further explored . As shown in Table 5 , our method obtains the best performance . Training from scratch yields the worst performance , as the model lacks the ability to distinguish between good and poor translations . Fine - tuning from the Full - sentence model achieves better performance , but it does not have the ability to generate high - quality translation with partial source information . Our method , fine - tuned from Multipath , is capable of generating high - quality translation under all latency .
The primary contribution of this work is a novel framework for entity linking against unseen target KBs with unknown schemas . To this end , we introduce methods to generalize existing models for zero - shot entity linking to link to unseen KBs . These methods rely on converting arbitrary entities represented using a set of attribute - value pairs into a string representation that can be then consumed by models from prior work .
A Layer - wise Relevance Propagation
For generic compact methods ( MiniLM , Distil - BERT , and TinyBERT ) , we use the models provided by the original authors as the initialization , then directly fine - tune them on the training data of the target task . The fine - tuning is optimized by Adam with a learning rate of 2 × 10 −5 and batch size of 32 on one GPU .
We conduct experiments on four datasets of three text generation tasks : text summarization , tableto - text generation , and dialogue generation . For text summarization , we use XSum ( Nallapati et al . , 2016 ) and CNN / DM ( Hermann et al . , 2015 ) for evaluation . For table - to - text generation , we use WIKIPERSON ( Wang et al . , 2018 ) . For dialogue generation , we apply dialogue NLI ( Welleck et al . , 2019 ) . Details of all datasets are listed in Table 10 .
To the best of our knowledge , our work does not involve any potential risk .
Computing Infrastructure and Runtime Specifications . Models were trained on Google Colab 's GPU . On average , each experiment took ∼1:30 hours to train .
In the near future , it is worth exploring to alleviate the possible knowledge incompleteness in practical KG by developing a hybrid questionanswering method on both knowledge graphs and web texts . In addition , this paper focuses only on temporal intent , while problems in real configurations may contain both complex non - temporal and temporal intents . Therefore , it would be helpful to combine SF - TCons with general KGQA systems for complex questions .
The models in Section 3 were designed to operate in settings where the entities in the target KB were only represented using a textual description . For example , the entity Douglas Adams would be represented in such a database using a description as follows : " Douglas Adams was an English author , screenwriter , essayist , humorist , satirist and dramatist . He was the author of The Hitchhiker 's Guide to the Galaxy . "
In entity linking , mentions of named entities in raw text are disambiguated against a knowledge base ( KB ) . This work focuses on linking to unseen KBs that do not have training data and whose schema is unknown during training . Our approach relies on methods to flexibly convert entities with several attribute - value pairs from arbitrary KBs into flat strings , which we use in conjunction with state - of - the - art models for zero - shot linking . We further improve the generalization of our model using two regularization schemes based on shuffling of entity attributes and handling of unseen attributes . Experiments on English datasets where models are trained on the CoNLL dataset , and tested on the TAC - KBP 2010 dataset show that our models are 12 % ( absolute ) more accurate than baseline models that simply flatten entities from the target KB . Unlike prior work , our approach also allows for seamlessly combining multiple training datasets . We test this ability by adding both a completely different dataset ( Wikia ) , as well as increasing amount of training data from the TAC - KBP 2010 training set . Our models are more accurate across the board compared to baselines .
We study the contribution of each negative sample construction strategy for improving the coherence of the outputs . As in Table 3 , removing each strategy leads to a performance degradation , indicating the effectiveness of all types of negative samples to enhance the contrastive learning . Among all negatives , removing REPLACE shows the most effects on both datasets . We hypothesize that replacing target sentences breaks the original logical flow and thus is more likely to encourage the model to focus on the global coherence . In contrast , DIFFERENT shows the least effects . One possible explanation is that this strategy focuses more on topical relatedness between the input and output , instead of the logical flow within the output as the negative sample itself is inherently coherent .
One way of using these models for linking against arbitrary KBs is by defining an attribute - to - text function f , that maps arbitrary entities with any set of attributes { k i , v i } n i=1 to a string representation e that can be consumed by BERT , i.e.
We extract POS n - gram patterns of hyperbole from the training set of HYPO dataset 8 and obtain 262 distinct POS n - grams . As a motivating example , the following three hyperbolic spans , " faster than light " , " sweeter than honey " , " whiter than snow " , share the same POS n - gram of " JJR+IN+NN " .
• We propose a novel distractor refining method able to select harder distractors for a given generated question and show its superiority compared to a random selection .
-DOCSTART- A Matter of Framing : The Impact of Linguistic Formalism on Probing Results
This section introduces our proposed Post - Pretraining Alignment ( PPA ) method . We first describe the MoCo contrastive learning framework and how we use it for sentence - level alignment . Next , we describe the finer - grained word - level alignment with TLM . Finally , when training data in the target language is available , we incorporate sentence - level code - switching as a form of both alignment and data augmentation to complement PPA . Figure 1 shows our overall model structure .
The key difference between our tiny - attention and an ordinary attention is that our per - head dimension is tiny : i.e. , z ( ℓ , m ) t ∈ R D and D is very small . Throughout our experiments , we set D = 1 .
At the bottom of Table 1 , we present the results of ablations on the external knowledge graph and the two contrastive losses . We observe that the performance drops consistently if we directly use class names to initialize W ( 0 ) without introducing external KG . This suggests that our knowledgeable label word initialization does provide rich and essential information . The ablation study on loss functions shows that both L att s2w and L att w2s contribute to fasttuning , though the former has a more significant impact .
• RTE : Recognizing Textual Entailment ( Giampiccolo et al . , 2007 ) . Given a premise sentence and a hypothesis sentence , the task is to predict whether the premise entails the hypothesis or not . The dataset contains 2.5k train examples from a series of annual textual entailment challenges .
In Figure 9 , we provide the search trace of LAM - BADA for an example in ProofWriter ( Depth-5 ) for
Table 7 lists the standard deviations of each methods . We report the results across five runs with random seeds .
where T is a set of all sub - populations we consider ( i.e. , T = { male , female } ) . FPR and FNR stand for false positive rate and false negative rate , respectively . The subscript t means that we calculate the metrics using data examples mentioning the sub - population t only . We used the following keywords to identify examples which are related to or mentioning the sub - populations . Male gender terms :
In this paper , we propose a framework which allows humans to debug and improve deep text classifiers by disabling hidden features which are irrelevant to the classification task . We name this framework FIND ( Feature Investigation aNd Disabling ) . FIND exploits an explanation method , namely layer - wise relevance propagation ( LRP ) ( Arras et al . , 2016 ) , to understand the behavior of a classifier when it predicts each training instance .
For example , the H2H+CXT+ExtL model based on M2M100 obtains 47 . 1 , 48.6 , 53.0 , 46.6 , and 40.4 BLEU points on En - Zh , En - Es , En - Fr , En - De , and En - Ja , respectively .
To perform this top - k search , we use efficient similarity search libraries such as FAISS ( Johnson et al . , 2017 ) .
Background : Contrastive Learning Instance discrimination - based contrastive learning aims to bring two views of the same source image closer to each other in the representation space while encouraging views of different source images to be dissimilar through a contrastive loss . Recent advances in this area , such as SimCLR ( Chen et al . , 2020 ) and MoCo ( He et al . , 2020 ) have bridged the gap in performance between self - supervised representation learning and fully - supervised methods on the ImageNet ( Deng et al . , 2009 ) dataset . As a key feature for both methods , a large number of negative examples per instance are necessary for the models to learn such good representations . SimCLR uses in - batch negative example sampling , thus requiring a large batch size , whereas MoCo stores negative examples in a queue and casts the contrastive learning task as dictionary ( query - key ) lookup . In what follows , we first describe MoCo and then how we use it for sentence - level alignment .
There are many tasks in natural language processing ( NLP ) that require extracting relational structures from texts . For example , the event argument extraction task aims to identify event arguments and their corresponding roles for a given event trigger ( Huang et al . , 2022 ; Wang et al . , 2019 ) . In entity relation extraction , the model identifies the tail - entities and head - entities that forms specific relations Yu et al . , 2020 ) . In taskoriented semantic parsing , the model predicts the slots and their semantic roles for a given intent in an * The authors contribute equally .
• aspect_pos_string : The aspects as a list of strings .
A single data sample is represented by an argument and an 1 - to 4 - gram of this argument , separated by the BERT architecture 's [ SEP ] token . This technique expands the 800 original samples of the dataset to around 80,336 . The model is trained for 5 epochs , with a learning rate of 5 × 10 −5 , and a batch size of 8 . We use the mean squared error as loss and take the recall@k to compare the models . The in - and cross - topic results of the bestperforming model ( MT - DNN BASE ) are reported in Table 2 . All results are the average over runs with five different seeds ( and over all four splits for the cross - topic experiments ) .
To solve this problem , we create a lattice to model all possible segmenations of the cipher using the existing key . Then we use a pretrained language model to choose the best possible segmentation ( i.e. the segmentation that gives the most probable plaintext according to the language model ) .
In this section we outline the experimental setups ( § 3.1 ) , followed by downstream G2 T generation results in full ( § 3.2 ) and low - resource scenarios ( § 3.3 ) . We also present a set of generated outputs from our models ( § 3.4 ) , and finish by providing an analysis ( § 3.5 ) on the effect of pre - training data size , and an ablation on the role of input augmentation with level markers .
All the experiments with NMT models in this paper can be run on a single Tesla V100 GPU . On average , the training process of models in different languages can be finished in four hours . We implement our model with the Transformer 6 tools in Pytorch 7 , and the data will be released with the paper .
• COMET : a neural quality estimation metric by Rei et al . ( 2020a ) which was shown to be the state - of - the - art reference - based method ( Kocmi et al . , 2021 ) .
Online education platforms can increase the accessibility of high quality educational resources for students around the world . Adaptive techniques that allow for more individualized learning strategies can help such technologies be more inclusive for students who make less - common mistakes or have different prior backgrounds ( Lee and Brunskill , 2012 ) . However , our method is subject to biases found in the training data , and careful consideration of using safe and appropriate data is crucial in an education context . Moreover , our specific use of pre - trained LMs relies on the significant progress of NLP tools for English language -further research and development of these tools for other languages can help ensure our method benefits a larger population of students .
We also discuss the effects on the number of categories . Table 7 compares different models under 10 - way 1 - shot and 10 - way 5 - shot settings . We draw similar conclusions from this table as in Table 1 that our method still outperforms previous state - of - the - art baselines .
• to use transfer learning via the warm - start strategy , and
The emergence of deep pre - trained contextualized encoders has had a major impact on the field of natural language processing . Boosted by the availability of general - purpose frameworks like AllenNLP and Transformers ( Wolf et al . , 2019 ) , pre - trained models like ELMO and BERT ( Devlin et al . , 2019 ) have caused a shift towards simple architectures where a strong pre - trained encoder is paired with a shallow downstream model , often outperforming the intricate task - specific architectures of the past .
Both World Values Survey and PEW survey are publicly available to use for research purposes . We accept and follow the terms and conditions for using these datasets , which can be found in https : / / www.worldvaluessurvey . org / WVSContents.jsp ? CMSID = Documentation , and https : / / www.pewresearch.org / about / terms - and - conditions / .
A dog ravenously eats a pie in a pet restaurant In a pet restaurant a dog eats a pie ravenously
Post - pretraining embedding alignment is an efficient means of improving cross - lingual transferability of pretrained multilingual LMs , especially when pretraining from scratch is not feasible . We showed that our self - supervised sentence - level and word - level alignment tasks can greatly improve mBERT 's performance on downstream tasks of NLI and QA , and the method can potentially be applied to improve other pretrained multilingual LMs .
Effects of Initialization . Before downstream adaptation , additional parameters ( e.g. , extra modules defined by delta tuning , the classification head , etc . ) may be introduced ; in addition , Wu et al . ( 2022 ) recently show that adding noise to the pretrained weights improves the fine - tuning performance on downstream tasks . Thus , both finetuning and delta tuning require proper initialization for the tunable parameters . Since different initialization could lead to distinct optimization trajectories , we explore the effects of initialization on PLM 's mode connectivity .
( a ) Parameter - sharing of AR decoders . As all AR decoders are homogeneous , we can tie their parameters to reduce the total number of parameters .
Deep pre - trained contextualized encoders like BERT ( Devlin et al . , 2019 ) demonstrate remarkable performance on a range of downstream tasks . A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pretraining . While most work in probing operates on the task level , linguistic tasks are rarely uniform and can be represented in a variety of formalisms . Any linguistics - based probing study thereby inevitably commits to the formalism used to annotate the underlying data . Can the choice of formalism affect probing results ? To investigate , we conduct an in - depth cross - formalism layer probing study in role semantics . We find linguistically meaningful differences in the encoding of semantic role - and proto - role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism . Our results suggest that linguistic formalism is an important dimension in probing studies and should be investigated along with the commonly used cross - task and cross - lingual experimental settings .
As the input corpus for training our model , we use the December 2018 version of Wikipedia , comprising approximately 3.5 billion words and 11 million entity annotations . We generate input sequences by splitting the content of each page into sequences comprising ≤ 512 words and their entity annotations ( i.e. , hyperlinks ) . The input text is tokenized using BERT 's tokenizer with its vocabulary consisting of V w = 30 , 000 words . Similar to Ganea and Hofmann ( 2017 ) , we create an entity vocabulary consisting of V e = 128 , 040 entities , which are contained in the entity candidates in the datasets used in our experiments .
The high precision indicates that our secondary annotator marks essentially the same mentions as our primary annotator , but recall suggests a few missing cases . The difference in marking EXPERI - MENT can be explained by the fact that the primary annotator sometimes marks several verbs per sentence as experiment - evoking elements , connecting them with same exp or exp variation , while the secondary annotator links the mentions of relevant slots to the first experiment - evoking element ( see also Supplementary Material Section B ) . Overall , the high agreement between domain expert annotators indicates high data quality . Identifying experiment slot fillers . We compute agreement on the task of identifying the slots of an experiment frame filled by the mentions in a sentence on the subset of sentences that both annotators marked as experiment - describing . Slot fillers are the dependents of the respective edges starting at the experiment - evoking element . Table 4 shows F1 scores for the most frequent ones among those categories . See Supplementary Material Section C for all slot types . Overall , our agreement study provides support for the high quality of our annotation scheme and validates the annotated dataset .
We identify three natural levels of approximation for score ( X → Y Z ) , which we explicate below .
As mentioned above , relevant entity mentions and their types are only annotated for sentences containing experiment information and neighboring sentences . Therefore , we here compute agreement on the detection of entity mention and type assignment on the subset of 90 sentences that both annotators considered as containing experimental information . We again look at precision and recall of the annotators versus each other , see Table 3 .
hension , algebra , and deductive logic . Meanwhile , pre - trained LMs can effectively handle sequences from a wide range of modalities ( Madani et al . , 2020;Polu and Sutskever , 2020 ) . In this work , we focus on natural language sequences , where recent progress in language modeling has shown great success at capturing abstract properties of language ( Hewitt and Manning , 2019;Liu et al . , 2019 ) . Specifically , we show how pre - trained LMs can be easily leveraged to adaptively generate questions for a given student and target difficulty in a reverse translation task , using difficulty at answering questions as a proxy for more complex future learning objectives .
The key insight in these aggregated results is that while FOP - based methods are not always the best - performing system for each metric , they are consistently the most reliable . Specifically , CGMH has high success rate , but lowest F1 and human scores . Prompt , on the other hand has the highest human evaluation scores but the worst success rate . This is not too surprising . It is , after all , an unmodified language model , so it should be fluent and on topic when viewed by a human . However , given its extremely low success rate , it is not viable for long - form controlled generation . In contrast , FOPbased methods are either the top 1 or 2 performing system across all summary statistics .
WinoBias ( Zhao et al . , 2018 ) is an intra - sentence coreference resolution task that evaluates a system 's ability to correctly link a gendered pronoun to an occupation across both pro - stereotypical and anti - stereotypical contexts . Coreference can be inferred based on syntactic cues in Type 1 sentences or on more challenging semantic cues in Type 2 sentences . We first fine - tune the model on the OntoNotes 5.0 dataset ( Hovy et al . , 2006 ) before evaluating on the WinoBias benchmark . We report the average F1 - scores for pro - stereotypical and anti - stereotypical instances , and the true positive rate difference in average F1 - scores , across Type 1 and Type 2 examples .
1 . Hypernym Heuristic : a premise - hypothesis pair satisfies this heuristic if specific clinical concept(s ) appearing in the premise appear in a more general form in the hypothesis . Formally : { ( p , h)|ϕ(p ) ϕ(h ) } . MeSH tree numbers are organized hierarchically , and increase in length with specificity . Thus , when a premise entity and hypothesis entity are leftaligned , the hypothesis entity is a hypernym for the premise entity if the hypothesis entity is a substring of the premise entity . To provide a concrete example : diabetes mellitus is an endocrine system disease ; the associated MeSH tree numbers are C19.246 and C19 , respectively .
In this study , you will negotiate the price to buy or sell a house with other participants . The study consists of two rounds of negotiation . The person you negotiate with in round 1 will differ from the person you negotiate with in round 2 . In each round , one of you will play the role of the house buyer , whereas the other will play the role of the house seller . In both rounds , you will play the same role as either buyer or seller .
Our final controlled argument generation pipeline ( see Figure 1 ) works as follows : ( 1 ) We gather several million documents for eight different topics from two large data sources . All sentences are classified into pro- , con- , and non - arguments . We detect aspects of all arguments with a model trained on a novel dataset and concatenate arguments with the same topic , stance , and aspect into training documents . ( 2 ) We use the collected classified data to condition the Arg - CTRL on the topics , stances , and aspects of all gathered arguments .
As shown in the figure , the points of the base models ( left magenta plots ) scatters over a wider range , whereas our SIEF training ( right cyan plots ) makes them more concentrated on the diagonal , indicating that the prediction P ij on the entire document is mostly the same asP
The manner of transfer conveniently also introduces a possibility of backdoor injection on PET . Most existing works focus on the fine - tuning of pretrained models through different training methods to enable backdoor injection into the model ( Kurita et al . , 2020 ; Li et al . , 2021 ) . Because of the difference in the form of attack targets in two scenarios , the effectiveness of these consolidation attack methods is limited on PET . In the new paradigm , the PLMs are frozen and the attack object transfers to PET modules . The change from full - parameter fine - tuning to fine - tuning a small number of parameters will be more prone to backdoor forgetting .
Prior work ( Raiman and Raiman , 2018;Cao et al . , 2018;Wu et al . , 2020;Févry et al . , 2020 ) reports higher accuracies on the TAC data but they are fundamentally incomparable with our numbers due to the simple fact that we are solving a different task with three key differences : ( 1 ) Models in prior work are trained and evaluated using mentions that link to the same KB . On the contrary , we show how far we can go without such in - KB training mentions .
MLQA is an evaluation dataset for QA that covers seven languages . The dataset is derived from a three step process . We focus on XLT in this work . For zero - shot crosslingual transfer , we train on the English SQuAD v1.1 ( Rajpurkar et al . , 2016 ) training set . For translate - train , we train on translation data provided in Hu et al . ( 2020 ) 5
Table 12 reports full statistics for the task of identifying experiment - describing sentences , including precision and recall in the dev setting .
Furthermore , we find that the confidence - order model works especially well for mentions that require a highly detailed context to resolve . For example , a mention of " Matthew Burke " can refer to two different former Australian rugby players . Although the local and natural - order models incorrectly resolve this mention to the player who has the larger number of occurrences in our Wikipediabased corpus , the confidence - order model successfully resolves this by disambiguating its contextual mentions , including his teammates , in advance . We provide detailed inference sequence of the corresponding document in Appendix D.
To annotate a single error in S i , annotators select the error span t j ∈ S i and the coherence error type e j ( error taxonomy outlined in Section 3.2 ) to construct the error triple a j = ( S i , t j , e j ) . This process is repeated until all errors in segment S i have been added , after which they proceed to the next segment S i+1 for annotation . At the end of the annotation , workers produce the full set of annotations A = { a j ∀j } across all the text segments . The outcome of this is shown in Figure 2 .
-DOCSTART- Unsupervised multiple - choice question generation for out - of - domain Q&A fine - tuning
We conduct our experiments on the SciQ dataset ( Welbl et al . , 2017 choice questions ( 4 choices ) featuring subjects centered around physics , biology and chemistry . An example of question can be found in Figure 1 . We focus on the SciQ dataset because it has not yet been used for training UnifiedQA and it requires precise scientific knowledge . Furthermore , our experiments reveal that the direct application of UnifiedQA on the SciQ benchmark leads to a much lower performance than when fine - tuning it on the SciQ training set ( see Section 4 ) . Our objective in this work is to solve this gap between UnifiedQA and UnifiedQA fine - tuned on supervised data with the unsupervised question generation approach described in Section 2 . We additionally test our method on two commonly used multiple choice question answering datasets : Common - senseQA ( Talmor et al . , 2019 ) and QASC . These datasets contain questions with similar domains to SciQ even though the questions are slightly less specific . Furthermore , neither of them has been used during the initial training of UnifiedQA .
In Algorithm 1 , we detail the procedure for obtaining label assignments for our synthetic tasks . Given that our rules are in an " IF ... THEN .. " format , we split each rule into an antecedent and a consequent based on the position of THEN . Note that our voting - based approach to choose the final label for an example helps to tackle ( 1 ) negation on a label for multiclass tasks and ( 2 ) choose the most suited label in case antecedents from multiple rules are satisfied by an example .
( 1 ) identifying sentences describing relevant experiments , ( 2 ) identifying mentions of materials , values , and devices , and ( 3 ) recognizing mentions of slots and their values related to these experiments . We propose and compare several machine learning methods for the different sub - tasks , including bidirectional long - short term memory ( BiLSTM ) networks and BERT - based models . In our results , BERT - based models show superior performance . However , with increasing complexity of the task , it is beneficial to combine the two approaches .
To address this problem , we propose two novel and effective types of soft noise to enable safer exploration , namely High - temperature Generation and Soft Pseudo Text , in what follows .
Morphological Difference . The morphology of table headers differs from that of plain text in the following four aspects . First , headers are always phrases and they usually contain a lot of domainspecific abbreviations ( e.g. , as shown in Figure 1 , " No . " is the abbreviation of " Number " and the " Loc . " is short for " Location " ) and special symbols ( e.g. , " $ " means " dollar " in Figure 1 ) . Second , verb - object phrases are frequently used as headers which indicate a subject - object relationship between two columns . For example , " Hosted by " in Figure 1 indicates a host relationship between the second and the third columns . Third , special tokenizations like CamelCase and underscore are idiomatic usages in headers . At last , capitalized words are particularly preferred in order to capture more readers ' attention for headers . These special word - forms are commonly used in headers but rarely seen in plain text . Therefore , the NMT models trained with a massive amount of plain text can not be directly applied to schema translation .
In order to alleviate the noisy label problem in the DS data , we construct a pre - denoising DocRE model to generate pseudo labels . As shown in Figure 2 ( a ) , we adopt BERT ( Devlin et al . , 2019 ) to capture the contextual representation { z i } n i=1 , where n is the number of tokens in a document . We also adopt a dropout layer to enhance the generalization ability of our DocRE model .
We first experiment with text - only models , including RoBERTa ( Liu et al . , 2019 ) and POLITICS ( Liu et al . , 2022a ) , a RoBERTa model further pretrained with a political ideology objective and thus specialized for ideology prediction and stance detection .
Then we feed them to the relational aware layers and get the final contextualized sequence of embedding X
Fast Pseudo - Log - Likelihood Scoring
a. Despite surface - level differences , the sentences express the same meaning , suggesting an underlying semantic representation in which these sentences are equivalent . One such representation is offered by role semantics -a shallow predicatesemantic formalism closely related to syntax . In terms of role semantics , Mary , book and John are semantic arguments of the predicate give , and are assigned roles from a pre - defined inventory , for example , Agent , Recipient and Theme .
• Olive Garden is the place that makes the best pastas . I try to visit the place as much I can with my friends • I love Olive Garden especially the original one in Orlando they opened in 1982 .
The attributes k i collectively form the schema of KB . The disambiguation of each m ∈ M is aided by the context c in which m appears .
We evaluate fine - tuning the Electric model on the GLUE natural language understanding benchmark and the SQuAD 2.0 question answering dataset ( Rajpurkar et al . , 2018 ) . We report exact - match for SQuAD , average score 3 over the GLUE tasks 4 , and accuracy on the multi - genre natural language inference GLUE task . Reported scores are medians over 10 fine - tuning runs with different random seeds . We use the same finetuning setup and hyperparameters as ELECTRA .
The argument aspect detection dataset contains a total of 5,032 samples in JSONL - format , i.e. each dataset sample has a separate line and can be parsed as JSON . A sample contains the keys :
The experimental results show that MRD greatly improves downstream task performance and pretraining efficiency ( and more discussion on MRD in Appendix A ) .
Finally , equipped with the relation - aware module , CAST can make the best use of the context and obtain significant improvement over H2H across all settings . For models based on M2M-100 , CAST outperforms H2H by 2.6 , 1.4 , 0.3 , 1.8 , and 1.9 BLEU in En - Zh , En - Es , En - Fr , En - De , and En - Ja , respectively . When it comes to models based on MBart-50M2 M , CAST obtains 1.6 , 2.7 , 1.9 , 0.9 , 0.2 improvements of BLEU points over H2H in translating schema from En to 5 target languages . It is also noticeable that CAST can help denoise the concatenated context for H2H+CXT . For instance , CAST based on M2M-100 achieves 1.5 and 1.2 improvements of BLEU points over H2H+CXT for schema translation from En to Es and Fr respectively . This improvement shows CAST can better model the target header and its context . We also run a Wilcoxon signed - rank tests between CAST and H2H+CXT and the results show the improvement are significant with p < 0.05 in 3 out of 5 languages . For the rest of the languages CAST achieves comparable results .
Compared to Cao et al . ( 2020 ) , which use 250k parallel sentences per language from the same sources as we do for post - pretraining alignment , our 250k model does better for all languages considered and we do not rely on the word - to - word pre - alignment step using FastAlign , which is prone to error propagation to the rest of the pipeline .
In traditional entity linking , the training mentions M train and test mentions M test both link to the same KB . Even in the zero - shot settings of Logeswaran et al . ( 2019 ) , while the training and target domains and KBs are mutually exclusive , the schema of the KB is constant and known . On the contrary , our goal is to link test mentions M test to a knowledge base KB test which is not known during training . The objective is to train models on mentions M train that link to KB train and directly use these models to link M test to KB test .
Overall Summ . Trans . vite more errors ( 13 ∼ 47 % ) . This verifies our assumption that the pipeline annotation protocol , which ignores valuable input context , can lead to poor data quality .
We plot the loss on FewRel training set and the 5 - way 1 - shot accuracy on FewRel validation set as functions of the number of iterations . As we can see from Figure 8
Under this taxonomy , a common error ( 33 % ) is predicting a more specific entity than that indicated by the mention ( the city of Hartford , Connecticut , rather than the state ) . The reverse is also observed ( i.e. the model predicts a more general entity ) , but far less frequently ( 6 % ) . Another major error category ( 33 % ) is when the model fails to pick up the correct signals from the context and assigns a similarly named entity of a similar type ( e.g. the river Mobile , instead of the city Mobile , both of which are locations ) . 21 % of the errors are cases where the model predicts an entity that is related to the gold entity , but is neither more specific , nor more generic , but rather of a different type ( Santos Football Club instead of the city of Santos ) .
For each position t = 1 to n : add to the loss − log
Data Limitations Regarding the HyperRED dataset , the distant supervision method of data collection may not align all valid facts present in the text articles . This is due to the possible incompleteness of the knowledge graph which is an open research challenge ( Nickel et al . , 2016 ) . On the other hand , it is not feasible to manually annotate all possible facts due to constraints in annotation time and cost . Furthermore , there are a large number of relation and qualifier labels to consider , resulting in a challenging task for human annotators . A promising and practical method to address the challenges in distant supervision is to adopt a human - in - theloop annotation scheme for RE ( Tan et al . , 2022b ) .
The reference data was crawled from two debate portals 8 and consists of pro - and con - paragraphs discussing the eight topics of the UKP - Corpus . As the paragraphs may include non - arguments , we filter these out by classifying all sentences with the ArgumenText API into arguments and nonarguments . This leaves us with 349 pro - and 355 con - arguments over all topics ( see App . D for the topic - wise distribution ) . Next , we detect all aspects in these arguments . Arguments with the same topic , stance , and aspect are then grouped and used as reference for arguments from the ( a ) generated arguments and ( b ) retrieval approach arguments if these hold the same topic , stance , and aspect . The results reveal that both the average METEOR and ROUGE - L scores are only marginally lower than the retrieval scores ( METEOR is 0.5/1.1 points lower for the Arg - CTRL REDDIT /Arg - CTRL CC , see Table 5 ) . It not only shows the strength of the architecture , but also the success in generating sound aspect - specific arguments with our approach . Overlap with Training Data We find arguments generated by the models to be genuine , i.e. demonstrating substantial differences to the training data . For each of the 7,991 generated arguments , we find the most similar argument in the training data based on the cosine similarity of their BERT embeddings We compare all models by verifying whether or not the aspect used for generation ( including synonyms and their stems and lemmas ) can be found in the generated arguments . For the original models conditioned on aspects , this is true in 79 % of Generated sentence : We do n't need more gun control laws when we already have enough restrictions on who can buy guns in this country .
Step 15k Effects of Data Domain . PLMs are shown to generalize well on out - of - distribution data ( Hendrycks et al . , 2020 ) , implying the connection of minima trained with different data distributions . To gain a deeper understanding , we choose two natural language inference datasets ( MNLI and ANLI ( Nie et al . , 2020 ) ) , and two sentiment analysis datasets ( Rotten Tomatoes ( Pang and Lee , 2005 ) and Yelp Polarity ( Zhang et al . , 2015 ) ) sourced from different domains . Then we fine - tune two copies of T5 BASE on two datasets of the same task , and evaluate the linear mode connectivity between two minima . Note previous works typically study mode connectivity on the same dataset ; while in our setting , we extend the analysis by evaluating the interpolations on two datasets .
FrameNet takes a meaning - driven stance on the role encoding by modeling it in terms of frame semantics : predicates are grouped into frames ( e.g. Commerce buy ) , which specify role - like slots to be filled . FrameNet offers fine - grained frame distinctions , and roles in FrameNet are frame - specific , e.g. Buyer , Seller and Money . The resource accompanies each frame with a description of the situation and its core and peripheral participants .
C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? No response .
The authors would like to thank colleagues from Amazon AI for many helpful discussions that shaped this work , and for reading and providing feedback on earlier drafts of the paper . They also thank all the anonymous reviewers for their helpful feedback .
We advocate the adoption of a mechanism design perspective , so as to develop modified annotation tasks that reduce the cognitive load placed on expert annotators while incentivizing the production of domain - specific NLI datasets with high downstream utility ( Ho et al . , 2015;Liu and Chen , 2017 ) . An additional option is to narrow the generative scope by defining a set of inferences deemed to be useful for a specific task . Annotators can then map ( premise , relation ) tuples to relation - satisfying , potentially fuzzy subsets of this pool of useful inferences , or return partial functions when more information is needed .
Training Settings . We measure performance of various models in both finetuned and zero - shot settings . First , we directly finetune the base pretrained model on the model on MEETINGQA . Next , to supplement training data we explore intermediatetraining ( Phang et al . , 2018 ; Pruksachatkun et al . , 2020 ) with SQuAD v2.0 ( Rajpurkar et al . , 2018 ) 6 or a combination including silver data from Section 3.3 prior to finetuning on MEETINGQA , increasing the training data by 5x and 10x respectively . Additional details on checkpoints , hyperparameters , and training are present in Appendix B .
For the kNN approach , we compare the performance by using only the query - document similarity for obtaining the relevance score ( i.e. dropping the second term in Equation 1 ) . On average this results in a drop of 5.6 percentage points , proving the effectiveness of injecting feedback documents in the kNN re - ranking approach .
• The binary classifier is trained to distinguish positive vs negative data points , with k negatives sampled for every n positive data points .
First , we compare the effects of recency of the event described ( TIMESINCEEVENT : a continuous variable representing the log time since the event ) . 9 Then , we contrast recalled stories to their retold counterparts in pairwise comparisons . Finally , we measure the effect of how frequently the experienced event is thought or talked about ( FREQUENCYOFRECALL : a continuous variable ranging from very rarely to very frequently ) . 10 As in § 4 , we Holm - correct for multiple comparisons .
C.1 Linearity with Varying Context Size Shown in Figure 5 , we compare the negative loglikelihood of sentences when conditioned on varying history sizes ( using the story summary as context E ) . As expected , conditioning on longer histories increases the predictability of a sentence . However , this effect is significantly larger for imagined stories , which suggests that imagined stories flow more linearly than recalled stories .
Can these differences affect the probing results ? This question is intriguing for several reasons . Lin - guistic formalisms are well - documented , and if the choice of formalism indeed has an effect on probing , cross - formalism comparison will yield new insights into the linguistic knowledge obtained by contextualized encoders during pre - training . If , alternatively , the probing results remain stable despite substantial differences between formalisms , this prompts a further scrutiny of what the pretrained encoders in fact encode . Finally , on the reverse side , cross - formalism probing might be used as a tool to empirically compare the formalisms and their language - specific implementations . To the best of our knowledge we are the first to explicitly address the influence of formalism on probing .
We follow the method used by the original paper on MetaQA to build the relation graph of KQA Pro . As mentioned in Section 5.2 , we use half of its KB triples as the label form . We constructe the text form by extracting sentences from Wikipedia . Following the original paper , we use exact match of surface forms for entity recognition Algorithm 2 Generation of HQDT Input : a complex question q 0 , a question decomposer M θ , a question generator M ϕ . Output : a list T representing the HQDT , where element ( q i , p i g , f ai ) in T denote a sub - question q i , certainty score of q i and the father of q i , respectively .
As there are no existing models for hyper - relational extraction , we introduce two strong baselines that leverage pretrained language models . The pipeline baseline is based on a competitive table - filling model for joint entity and relation extraction , while the generative baseline is extended from a state - ofthe - art approach for end - to - end relation extraction .
Our statistical model ( see Table 4 ) reveals similar conclusions : we see the main factor of P ( β = 0.0148 , p < 1e-15 ) is significant and its interaction with G ( β = -0.0032 , p < 0.053 ) are marginally significant . This indicates that P is generally positive correlated with performance gain and there is a weak tendency that the coefficients of P reduces as G increases . In other words , paraphrasing improves the downstream performance but becomes less effective when adding more gold data ( a similar trend is also seen in Figure 1 ) .
• Entity / Pronoun / Date / Number Swap ( ES / PS / DS / NS ) : An NER system is first applied to both source and target text . The entities from the target text are randomly swapped with entities from the source text if they share the same entity type .
The authors would like to thank the anonymous reviewers for their insightful comments . This work was supported by the Natural Science Foundation of China ( 62076133 and 62006117 ) , and the Natural Science Foundation of Jiangsu Province for Young Scholars ( BK20200463 ) and Distinguished Young Scholars ( BK20200018 ) .
iii We inspect few - shot and zero - shot word - level quality estimation with the bilingual and multilingual models . We report how the sourcetarget direction , domain and MT type affect the predictions for a new language pair .
Given the dialogue context C and the retrieved knowledge K , we first encode them into distributed representations with contextualized encoders . Specifically , we add special tokens to differentiate the roles of user and system as well as different types of knowledge as : Pretrained language models ( PLMs ) , e.g. , GPT2 ( Radford et al . , 2019 ) , have shown superior capability of generating high - quality responses in many dialogue systems , especially those PLMs pretrained on dialogue corpus , e.g. , BlenderBot ( Roller et al . , 2021 ) . To leverage the advantages of these generative PLMs , we reformulate the mixedinitiative emotional support conversation problem as a Seq2Seq problem , which linearizes the input and output as a sequence of tokens as follows :
Baseline . We choose two state - of - the - art NMT models , including M2M-100 and MBart-50M2 M ( Tang et al . , 2020 ) , as our baselines . Specifically , both of the baseline models employ the Transformer sequence - to - sequence architecture ( Vaswani et al . , 2017 ) to capture features from source language input and generate the translation . The M2M-100 is directly trained on large - scaled translation data while MBart-50M2 M is firstly pre - trained with a " Multilingual Denoising Pretraining " objective and then fine - tuned in machine - translation task . We evaluate the baseline models with the following settings :
• Waseem : The authors of ( Waseem and Hovy , 2016 ) kindly provided the dataset to us by email . We considered " racism " and " sexism " examples as " Abusive " and " neither " examples as " Non - abusive " .
We present a novel message - passing mechanism over the learned hierarchical graph . This mechanism realizes the inter - sentence reasoning where connectors can aggregate information from their related information nodes while propagating the information to others . For the i - th sentence node , the edge marginal controls the aggregation from its K information nodes ; and the root probability controls the neighbouring information is combined as i - th node 's update u ( l ) in the l - th reasoning layer ,
To identify the shortcomings of our best model , we categorize 100 random mentions that are incorrectly linked by this model into six categories ( demonstrated with examples in Table 6 ) , inspired by the taxonomy of .
To understand how the model M works , we analyze the patterns or characteristics of the input that activate each feature f i . Specifically , using LRP 1 , for each f i of an example x j in the training dataset , we calculate a relevance vector r ij ∈ R L showing the relevance scores ( the contributions ) of each word in x j for the value of f i . After doing this for all d features of all training examples , we can produce word clouds to help the users better understand the model M . Word clouds -For each feature f i , we create ( one or more ) word clouds to visualize the patterns in the input texts which highly activate f i . This can be done by analyzing r ij for all x j in the training data and displaying , in the word clouds , words or n - grams which get high relevance scores . Note that different model architectures may have different ways to generate the word clouds so as to effectively reveal the behavior of the features .
Step 2 : Annotation scheme Instead of free spanlevel annotations , we present annotators with a ranked list of aspect recommendations . To generate meaningful recommendations , we train a ranking model using the preliminary annotations ( Step 1 ) .
Token embedding is the embedding of the corresponding token . The matrices of the word and entity token embeddings are represented as A ∈ R Vw×H and B ∈ R Ve×H , respectively , where H is the size of the hidden states of BERT , and V w and V e are the number of items in the word vocabulary and that of the entity vocabulary , respectively .
In natural language processing ( NLP ) , text generation is an important research topic that aims to automatically produce understandable text in human language from input data ( Li et al . , 2022 ) . In recent decades , various approaches have been widely applied to a variety of text generation tasks Gehring et al . , 2017 ; Li et al . , 2021a ) , especially the emergence of pretrained language models ( PLMs ) ( Li et al . , 2021c ) . By involving large - scale parameters pretrained on massive general corpora , PLMs such as GPT-3 ( Brown et al . , 2020 ) have achieved substantial progress in text generation . Through the fine - tuning paradigm , PLMs can adapt to various text generation tasks by directly adjusting the model parameters with labelled datasets .
But also some customers believe that website refunds money the service is not 100 % perfect .
The grammar is basically accurate , but there are some problems , such as inaccurate words , improper collocation , and insufficient fluency , which can be fixed at a small cost .
LMs towards desired attributes . Examples include using reinforcement learning to control quality metrics ( Ranzato et al . , 2016 ) , adjusting sampling weights to control for poetry style ( Ghazvininejad et al . , 2017 ) , and learning to condition on valence or domain - specific codes ( Keskar et al . , 2019;Peng et al . , 2018 ) . To the best of our knowledge , we are
Fully detached are the easiest to detect . As expected , fully detached hallucinations are the easiest to detect : all methods detect them entirely when taking 20 % of the hallucination dataset ( Figure 4 ) , and they are the most frequent among the examples flagged by the best performing methods ( Figure 3 ) . This agrees with Guerreiro et al . ( 2022 ) that oscillatory and strongly detached hallucinations are more difficult to detect , and shows that improvements with our methods mostly come from these types .
A play on the term " paperless " - " going paperless " is a goal for many companies because it would save money and be more efficient . But here , the company is so far from that goal that they have a tiger on top of their filing cabinet . So instead of " going paperless , " this company is going " tiger - full . "
Topic : Nuclear Energy Argument : Running nuclear reactors is costly as it involves long - time disposal of radioactive waste .
To understand BiLSTM features , we created two word clouds for each feature . The first word cloud contains top three words which gain the highest positive relevance scores from each training example , while the second word cloud does the same but for the top three words which gain the lowest negative relevance scores ( see Figure 11 ) .
For English⇒German , the training set consists of 4.5 million sentence pairs and newstest2013 & 2014 are used as the dev . and test sets , respectively . BPE with 32 K merge operations is used to handle low - frequency words . For Japanese⇒English , we follow Morishita et al . ( 2017 ) to use the first two sections as training data , which consists of 2.0 million sentence pairs . The dev . and test sets contain 1790 and 1812 sentences . For Chinese⇔English , we follow Hassan et al . ( 2018 ) sentence pairs . We develop on devtest2017 and test on newstest2017 . We use SacreBLEU ( Post , 2018 ) as the evaluation metric with statistical significance test ( Collins et al . , 2005 ) . We evaluate the proposed XL PE strategies on Transformer . The baseline systems include Relative PE ( Shaw et al . , 2018 ) and directional SAN ( DiSAN , Shen et al . 2018 ) . We implement them on top of OpenNMT ( Klein et al . , 2017 ) . In addition , we report the results of previous studies ( Hao et al . , 2019;Chen et al . , 2019b , a;Du and Way , 2017;Hassan et al . , 2018 ) .
( 4 ) Transfer learning incurs further improvement , with scores equal or better to the ones achieved with semi - supervised NMT . Here , each metric favors a different setting . ChrF indicates a significant improvement with fine tuning , TER prefers warm start , whereas BLEU indicates only a very small difference between the two .
With the developments of Neural Machine Translation ( NMT ) systems ( Sutskever et al . , 2014;Bahdanau et al . , 2015 ) , tremendous success has been achieved by existing studies on machine translation tasks . For instance , Vaswani et al . ( 2017 ) greatly improved bilingual machine translation systems with the Transformer architectures , ( Edunov et al . , 2018 ) achieved state - of - the - art on the WMT ' 14 English - German tasks with back - translations augmentation , Weng et al . ( 2020 ) and Yang et al . ( 2020 ) explored ways to boost the performance of NMT systems with pre - trained language models . Recent works saw the potential to improve NMT models in many - to - many settings and proposed models that can perform machine translation on various language pairs . While the above - mentioned studies focus on sentence - level translation in plain text , they are not suitable for schema translation .
Layer - wise Relevance Propagation ( LRP ) is a technique for explaining predictions of neural networks in terms of importance scores of input features ( Bach et al . , 2015 ) . Originally , it was devised to explain predictions of image classifiers by creating a heatmap on the input image highlighting pixels that are important for the classification . Then Arras et al . ( 2016 ) and Arras et al . ( 2017 ) extended LRP to work on CNNs and RNNs for text classification , respectively .
The combined effect size of each of the models is examined on WEAT stimulus 6 , which contains target words of career / family and attribute words of male / female names . This was the only one that detected bias on a pre - trained RoBERTa ( CES close to 0.5 and p < 0.05 ) . The points that we kept in our analysis are those where p < 0.05 , which make up 90 % of the points in occupation prediction and 95 % of the points in coreference resolution .
LGTM consistently outperforms other baselines in most of the tasks except competitive results on SST-2 . This indicates the robustness of our method which suggests its wide usage in various knowledge distillation settings .
As mentioned previously , existing meta - learning methods often require a sufficiently large dataset to build diverse episodes for meta - training . Otherwise , the performance will drop seriously .
where I m denotes the imaginary part of a quaternion representation .
Concretely , MoCo employs a dual - encoder architecture . Given two views v 1 and v 2 of the same image , v 1 is encoded by the query encoder f q and v 2 by the momentum encoder f k . v 1 and v 2 form a positive pair . Negative examples are views of different source images , and are stored in a queue ∈ K , which is randomly initialized . K is usually a large number ( e.g. , K = 65 , 536 for ImageNet ) . Negative pairs are formed by comparing v 1 with each item in the queue . Similarity between pairs is measured by dot product . MoCo uses the InfoNCE loss ( van den Oord et al . , 2019 ) to bring positive pairs closer to each other and push negative pairs apart . After a batch of view pairs are processed , those encoded by the momentum encoder are added to the queue as negative examples for future queries . During training , the query encoder is updated by the optimizer while the momentum encoder is updated by the exponential moving average of the query encoder 's parameters to maintain queue consistency :
Positive F1 Accuracy Macro F1 Original 0.767 ± 0.02 0.800 ± 0.00 0.785 ± 0.01 0.789 ± 0.01 Disabling ( MTurk ) 0.786 ± 0.00 0.804 ± 0.00 0.795 ± 0.00 0.796 ± 0.00
Compared to XLM , our 250k , 600k and 2 M settings represent 3.1 % , 7 % and 17.8 % of the parallel data used by XLM , respectively ( see Table 1 ) . The XLM model also has 45 % more parameters than ours as Table 3 shows . Furthermore , XLM trained with MLM only is already significantly better than mBERT even though the source of its training data is the same as mBERT from Wikipedia . One reason could be that XLM contains 45 % more model parameters than mBERT as model depth and capacity are shown to be key to cross - lingual success ( K et al . , 2020 ) . Additionally , Wu and Dredze ( 2019 ) hypothesize that limiting pretraining to the languages used by downstream tasks may be beneficial since XLM models are pretrained on the 15 XNLI languages only . Our 2 M model bridges the gap between mBERT and XLM from 7.5 % to 2.8 % for zero - shot transfer . Note that , for bg , our total processed pool of en - bg data consists of 456k parallel sentences , so there is no difference in en - bg data between our 600k and 2 M settings . For translatetrain , our model achieves comparable performance to XLM with the further help of code - switching during finetuning .
We train two Electric models the same size as BERT - Base ( 110 M parameters ): one on Wikipedia and BooksCorpus ( Zhu et al . , 2015 ) for comparison with BERT and one on OpenWebTextCorpus ( Gokaslan and Cohen , 2019 ) for comparison 2 with GPT-2 . The noise distribution transformers T LTR and T RTL are 1/4 the hidden size of Electric . We do no hyperparameter tuning , using the same hyperparameter values as ELECTRA . Further details on training are in the appendix .
To analyze which speaker features influence codeswitch predictions , we ablate a phrase , corresponding to one of six speaker features ( age , gender , country of origin , language and code - switching preferences , and speaker order ) . Table 7 indicates that linguistic preferences are most influential .
Most importantly , Shapley values are not based on model accuracy or performance , but solely on the model 's input and its prediction , e.g. , the probability for an image and a caption to match . This is an important property for our MM score , since its objective is to quantify how much inputs of either modality matter for prediction -even if the cooperation between ( multimodal ) inputs is not sufficient to reach success , i.e. , yielding the correct outcome .
Takeaways . We conclude this section with three concrete recommendations for future work .
Finally , our representations achieve state - of - theart performance for voting prediction . This is remarkable , as our result comes from a zero - shot prediction , i.e. , our representation has not been trained on any voting data . This further emphasizes the value of our legislator representation as a general proxy for legislators ' ideology .
BERT LARGE predicts classes B and I with an F 1 of .65 and .53 , hence aspects with more than one token are less well identified . A difference is to be expected , as the class balance of B 's to I 's is 2,768 to 2,103 . While the ranker performs worse based on the shown metrics , it has a slightly higher recall for class I. We assume this is due to the fact that it generally ranks aspects with more than one token on top , i.e. there will often be at least one or more I 's in the prediction . In contrast to that , BERT LARGE focuses more on shorter aspects , which is also in accordance with the average aspect length of 1.8 tokens per aspect in the dataset . In total , BERT LARGE outperforms the ranker by almost 6 percentage points in F 1 macro .
In broad terms , a sequence labeling model consists of an encoder E and a classifier C. The encoder consumes a sequence of input tokens t i and outputs a sequence of contextualized representations h i ( Eq . 1 ) . These representations are then fed to the classifier which produces a probability distribution over all of the possible types . A candidate label is selected by choosing the type with the largest probability . The model loss L C is then computed via negative log - likelihood with the classifier - selected labels and the expected gold labels ( Eq . 2 ) .
Our question generation model demonstrates the ability to generate novel questions that do not exist in the entire Duolingo question dataset , especially when a sampling penalty is applied to encourage more diverse outputs . However , this comes at a cost to fluency . Below we include a set of outputs generated by our model for 1 Spanish student and 1 French student from the Duolingo dataset , with a target difficulty of d = 0.1 , and both with and without a repetition penalty . We observe that while applying a penalty results in a far more novel questions generated , several of these are also non - fluent , using a combination of manual judgement and the Python language - check package ( https://pypi.org/project/language-check/ ) .
Although the context information of tables is important , how to effectively use it for schema translation is challenging . On the one hand , the NMT model needs to make use of the context information to make word - sense disambiguation for polysemy headers and abbreviation headers . For another , the context information should not bring additional noise when translating the target header .
Given any autoregressive language model ( e.g. GPT-2 ( Radford et al . , 2019 ) , we can fine - tune a LM - KT model ( p θ KT ) to predict whether an individual student will correctly answer the next question . If this model has well - calibrated uncertainty , we can use its predicted probability of a correct answer as a proxy for the difficulty of a question to a student . We then train a question generation model ( p θ QG ) to generate a new question conditioned on a student and desired target difficulty .
Human Evaluation . Since the machine evaluation metrics can not absolutely make sure whether the predicted result is correct or not , we conduct a human evaluation on the test set for a more precise evaluation . Specifically , we invite two experts to evaluate each language pair . For each case , they compare the machine translation and the human annotation . The label is set as 1 if they think the translation is equivalent to the annotation , otherwise 0 . We report the human evaluation results for the Base , H2H , H2H+CXT , and CAST based on M2M-100 on the En - Zh setting in Table 5 . According to human evaluation , H2H achieves 14.84 % improvement over Base , and the performance is further boosted by 3.11 % when the context is added . Finally , enhanced by the relationaware structure , CAST obtains 2.3 % improvement over H2H+CXT , which demonstrates the effectiveness of our approach .
We divide errors into two categories : a ) Coherence errors : these measure whether the summary is well - structured and events in the summary make narrative sense , and b ) Language errors : these measure other aspects of the quality of generated text , such as grammar . While these do not come under the ambit of coherence errors , we found it useful to provide these additional error types for crowd workers to anchor other " badness " in text to .
We introduce MetaICL : Meta - training for In - Context Learning . Table 1 provides an overview of the approach . The key idea is to use a multi - task learning scheme over a large collection of metatraining tasks , in order for the model to learn how to condition on a small set of training examples , recover the semantics of a task , and predict the output based on it . Following previous literature ( Brown et al . , 2020 )
-DOCSTART- Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas
Effectiveness of the proxy KL - divergence loss . We use the proposed proxy KL - divergence loss to compute the head importance to identify the general language knowledge in the LM without using the LM 's original pre - training data ( Sec . 3.1 ) .
This work is inspired by the seminal work of on CoT prompting . They demonstrate that prefixing an input with 2 - 8 exemplars of CoT reasoning encourages LMs to do the same , reaching state - of - the - art performance on datasets such as GSM8 K ( Cobbe et al . , 2021 ) . show that task accuracy can be further improved by using self - consistency in CoT prompting . Selfconsistency samples CoT reasoning paths from a model 's decoder and returns the most consistent path by taking the majority vote . Subsequently , Chung et al . ( 2022 ) explore finetuning a FLANbased ( Wei et al . , 2021 ) version of PaLM on manually generated CoT data .
We construct HIPPOCORPUS , containing 6,854 stories ( Table 1 ) , to enable the study of imagined and recalled stories , as most prior corpora are either limited in size or topic ( e.g. , Greenberg et al . , 1996;Ott et al . , 2011 ) . See Appendix A for additional details ( e.g. , worker demographics ; § A.2 ) .
To find out why a low PPL can not lead to a better GEC performance , we carry out a detailed analysis on the ensemble results and get some insights on GEC : 1 ) In the test data , human references are insufficient , while PLM - based ensemble strategies produce valuable candidates , after being human checked , which may be considered as necessary complement to human references .
( with 1 being very easy and 5 being very difficult )
We evaluated our proposed tiny - attention adapter on a range of natural language understanding tasks including GLUE and FewGLUE . Our method is implemented in PyTorch ( Paszke et al . , 2019 ) and heavily relies on HuggingFace ( Wolf et al . , 2020 ) . Our code is submitted for review and it will be publicly released after the paper is published .
Experiment detection . The task of experiment detection can be modeled as a binary sentence classification problem . It can also be conceived as a retrieval task , selecting sentences as candidates for experiment frame extraction . We implement a bidirectional long short - term memory ( BiLSTM ) model with attention for the task of experiment sentence detection . Each input token is represented by a concatenation of several pretrained word embeddings , each of which is fine - tuned during training . We use the Google News word2vec embeddings ( Mikolov et al . , 2013 ) , domain - specific word2vec embeddings ( mat2vec , , see also Section 2 ) , subword embeddings based on byte - pair encoding ( bpe , Heinzerling and Strube , 2018 ) , BERT ( Devlin et al . , 2019 ) , and SciBERT ( Beltagy et al . , 2019 ) embeddings . For BERT and SciBERT , we take the embeddings of the first word piece as token representation . The embeddings are fed into a BiLSTM model followed by an attention layer that computes a vector for the whole sentence . Finally , a softmax layer decides whether the sentence contains an experiment .
Open - sourcing such language models also encourages the work on counter - measures to detect malicious use : While many works have been published on the topic of automatic fake news detection in texts ( Kaliyar et al . , 2020;Reis et al . , 2019;Hanselowski et al . , 2018;Pérez - Rosas et al . , 2018 ) , the recent emergence of large - scale language models has also encouraged research to focus on detecting the creator of these texts ( Varshney et al . , 2020;Zellers et al . , 2019 ) . The former approaches are aimed at detecting fake news in general , i.e. independent of who ( or what ) composed a text , whereas the latter approaches are designed to recognize if a text was written by a human or generated by a language model . We encourage the work on both types of methods . Ideally , social networks and news platforms would indicate if a statement was automatically generated in addition to its factual correctness .
In contrast , if we understand which patterns or qualities of the input are captured in each feature , we can comprehend the overall reasoning mechanism of the model as the dense layer in the classification part then becomes interpretable . In this paper , we make this possible using LRP . By understanding the model , humans can check whether the input patterns detected by each feature are relevant for classification . Also , the features should be used by the subsequent dense layer to support the right classes . If these are not the case , debugging can be done by disabling the features which may be harmful if they exist in the model . Figure 1 shows the overview of our debugging framework , FIND .
for cognitive skills such as knowing what is not true or what not to think ( MacDonald , 1965 ; Minsky , 1997 ; Barker and Jago , 2012 ) . Therefore , we ask this question : Do LLMs ( such as GPT-3 models ) acquire such implicit negative knowledge through extensive language modeling pre - training ?
D2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )
We also present the results on the SGD dataset in Table 2 , where Prompter shows improvements on average . We share results over 6 representative domains along with results for official unseen domain performance . Once more , Prompter demonstrates superior performance on average in unfamiliar domains . Compared to the results reported in the original paper by Wang et al . ( 2022 ) for four domains ( Columns 1 through 4 of Table Table 2
where Y t ′ is a vector in which each element Y j t ′
Unexpectedness Hyperbolic spans are less coherent with the literal contexts and thus their vector representations are distant from the context vectors . Troiano et al . ( 2018 ) have verified this intuition with the unexpectedness metric . They define the unexpectedness score U s of a sentence s with the token sequence { x 0 , x 1 , ... , x N } as the average cosine distance among all of its word pairs .
For homogeneous moral norm inference , we compute Pearson correlation between 1 ) the empirical homogeneous moral ratings , obtained by aggregating the human moral ratings toward a topic from all countries , and 2 ) language model inferred moral scores , estimated from our homogeneous probing method ( i.e. , without specifying country in prompts ) .
Then it aggregates all the information using word clouds to create a global visual picture of the model . This enables humans to comprehend the features automatically learned by the deep classifier and then decide to disable some features that could undermine the prediction accuracy during testing . The main differences between our work and existing work are : ( i ) first , FIND leverages human feedback on the model components , not the individual predictions , to perform debugging ; ( ii ) second , FIND targets deep text classifiers which are more convoluted than traditional classifiers used in existing work ( such as Naive Bayes classifiers and Support Vector Machines ) .
The second regularization scheme discourages the model from memorizing the order in which particular attributes occur . Under attribute - shuffle , every time an entity is encountered during training , its attribute / values are randomly shuffled before it is converted to a string representation using the techniques from Section 4.1 .
Our method generalizes to any education activity that can be represented with text sequences . Due to the availability of real student learning data , we focus on a reverse language translation task , where a student translates phrases from their native language ( e.g. English , " she eats " ) to the second language they are learning ( e.g. Spanish , " ella come " ) .
The contents of thousands of historical documents are still unknown to the contemporary age , even though they are encrypted using classical methods . Example documents include books from secret societies , diplomatic correspondences , and pharmacological books . Previous work has been done on collecting historical ciphers from libraries and archives and making them available for researchers ( Megyesi et al . , 2019 ( Megyesi et al . , , 2020 . However , decipherment of classical ciphers is an essential step to reveal the contents of those historical documents .
To further restrict malicious use , we release the training data for the Arg - CTRLs with an additional clause that forbids use for any other than research purposes . Also , all the training datasets for the Arg - CTRLs will be accessible only via access control ( e - mail , name , and purpose of use ) . Lastly , this work has been reviewed by the ethics committee of the Technical University of Darmstadt that issued a positive vote . page 833 - 838 , USA . American Association for Artificial Intelligence .
In this paper , we used two metrics to quantify biases in the models -False positive equality difference ( FPED ) and False negative equality difference ( FNED ) -with the following definitions ( Dixon et al . , 2018 ) .
Our results show that MP obtains comparable linear guarding in one projection where INLP requires multiple iterations . They also confirm that the rest of the space remains more stable through fewer changes in nearest neighbors of randomly chosen words and similarity scores that are closer to the original scores compared to INLP . Two results led to further questions : similarity scores improved after applying INLP and INLP resulted in better WEAT scores than MP . We conducted additional experiments to test whether the extra debiasing iterations performed in INLP end up removing more subtle representations of gender bias that are missed by the single MP iteration .
• We propose a Neural Vocabulary Selection ( NVS ) model based on the contextualized deep encoder representation ( § 3 ) .
As described in section 5 , we train two sentence fusion baselines using a pre - trained auto - encoder BART base model ( Lewis et al . , 2020 ) , on PYRFUS and PYRFUS++ respectively . We used the training script 9 made available by the transformers library ( Wolf et al . , 2020 ) with the following parameters : 4 training epochs and a learning rate of 3e-5 . A " steps " evaluation parameter was used with 5000 evaluation steps and an evaluation beam of 6 . Max source input was limited to 265 while max target length was set to 30 . Minimum target length were
We proposed FIND , a framework which enables humans to debug deep text classifiers by disabling irrelevant or harmful features . Using the proposed framework on CNN text classifiers , we found that ( i ) word clouds generated by running LRP on the training data accurately revealed the behaviors of CNN features , ( ii ) some of the learned features might be more useful to the task than the others and ( iii ) disabling the irrelevant or harmful features could improve the model predictive performance and reduce unintended biases in the model .
A.1 Hypothesis - only Baseline Analysis
In Table 2 , we show the related words for SST-2 .
C2 . Did you discuss the experimental setup , including hyperparameter search and best - found hyperparameter values ? We introduce the experiment settings in section 5.1 .
The rest of this paper is organized as follows . Section 2 explains related work about analyzing , explaining , and human - debugging text classifiers . Section 3 proposes FIND , our debugging framework . Section 4 explains the experimental setup followed by the three human experiments in Section 5 to 7 . Finally , Section 8 discusses generalization of the framework and concludes the paper . Code and datasets of this paper are available at https://github.com/plkumjorn/FIND .
• A negative data point is the same except x t , the token at position t , is replaced with a noise tokenx t sampled from q.
To mitigate the hallucination problem that caused by the polarized optimization objectives in knowledge grounded dialogue generation , we take inspiration from human communicating , and propose the Augmentative and Contrastive Knowledge Dialogue Expansion Framework ( ACK - DEF ) . Our ACK - DEF aims to soften the polarized training optimization objectives of current knowledgegrounded dialogue generation methods , and guide the dialogue system reply patterns for the knowledge with different level of errors . To achieve this end , we design two effective expansion method , which will be detailed in below .
In order to obtain better representations of textual curricula , we propose to use pre - trained BERT ( Devlin et al . , 2019 ) embeddings that have been finetuned in a computing discipline classification task , using an approach that combines a novel coursebased attention mechanism and metric learning .
We use three different LMs as our baseline models , each with different architectures : RoBERTa ( Liu et al . , 2019 ) , GPT-2 ( Radford et al . , 2019 ) , and T5 ( Raffel et al . , 2020 ) . These architectures compose transformer layers in various typical fashions :
While we could not find previously published work on this problem , we can see that our best method ( Unigram LM 2 ) achieves an average SegER of 27 % on the three real homophonic ciphers , with the best score of 15 % on the longest , 2,239 - digit F283 cipher .
The following are the 25 news sources in AllSides : BBC News , Breitbart News , CBN , Christian Science Monitor , CNN , Fox News , HuffPost , National Review , New York Times , Newsmax , NPR , Politico , Reason , Reuters , Salon , The Guardian , The Hill , TheBlaze.com , Townhall , USA TODAY , Vox , Wall Street Journal , Washington Examiner , Washington Post , Washington Times .
A document is split if it is longer than 512 words , which is the maximum word length of the BERT model .
The second unmodified BERT model ( i.e. not containing the indicator embeddings as in the mention encoder ) independently encodes each e ∈ KB into vectors . The candidates E for a mention are the K entities whose representations are most similar to v m . Both BERT models are fine - tuned jointly using a cross - entropy loss to maximize the similarity between a mention and its corresponding correct entity , when compared to other random entities .
Prompt - Finetuning . We finetune each of our pretrained METRO - T5 models on three multi - task mixtures : T0 / T0+ / T0++ Train , using the same prompt templates and shuffling strategy as Sanh et al . ( 2022 ) does . Each model is finetuned for 125k steps , using the same hyperparameters as pretraining , except the peak learning rate is reduced to 0.1x . We do not perform any checkpoint selection and simply use the last checkpoint at 125k steps for evaluation .
" female " , " females " , " girl " , " girls " , " woman " , " women " , " lady " , " ladies " , " she " , " her " , " herself " , " sister " , " daughter " , " wife " , " girlfriend " , " mother " , " aunt " , " mom " . All the bios are from Common Crawl August 2018 Index .
c that are aligned with words in the corresponding English sentence , x ( i ) en . 3 Denote by 2 We investigate the similarity of translation pairs via their multilingual representations in Appendix C , finding that translation pairs do form similar inputs for a multilingual model . 3 We use English as the reference language since our cross-
To obtain the final score s i for each O i , we concatenate the dual matching features f QO i and f QC i and feed them into a two - layer multi - layer perceptron ( MLP ) :
Formally , the ACE using the do - notation can be calculated by conditioning on Z. Specifically , we integrate over the distribution of P ( Z ) , and calculate the difference in the conditional probability distribution P ( S = s|M = 1 , Z = z ) − P ( S = s|M = 0 , Z = z ) of S given the data - model di- rection match value M conditioned on the other key variables Z for each of its possible value z , as shown in Eq . ( 2 ) :
D3 . Did you discuss whether and how consent was obtained from people whose data you 're using / curating ? For example , if you collected data via crowdsourcing , did your instructions to crowdworkers explain how the data would be used ? Not applicable . Left blank .
• For the embedding - based metrics , we use ( i ) Vector Extrema ( VE ) ( Forgues and Pineau , 2014 ) , ( ii ) Greedy Matching ( GM ) ( Rus and Lintean , 2012 ) , ( iii ) Embedding Averaging ( EA ) ( Landauer and Dumais , 1997 ) , ( iv ) LabSE ( Feng et al . , 2020 ) & ( v ) LASER ( Artetxe and Schwenk , 2019 ) embeddings and ( vi ) BERTScore ( Zhang et al . , 2020 ) .
The experimental results of all models are summarized in Table 3 . First of all , it should be noted that there are only a few studies on readability assessment , and there is no unified standard for data division and experimental parameter configuration . This has led to large differences in the results of different research works .
In this section , we describe our schema translation approach in detail . We first introduce the requirement and our definition for the schema translation task and then introduce the model architecture .
Combining preprocessed data We collect different types of source text applied with different preprocessing methods of Section 4.2 . For PHOENIX , we combine the original , the normalized , the lemmatized , and the lemmatized+normalized text with the copied target glosses . For DGS , we mix the original and lemmatized text with the corresponding target glosses into a new training dataset .
A much bigger problem for applying knowledge of prototypical functions is that physical objects are often mentioned when they are not used at all ! For example , the sentences below mention a knife , but the knife is not being used :
HuffPost Reuters Amazon Average 1 shot 5 shot 1 shot 5 shot 1 shot 5 shot 1 shot 5 shot 1 shot 5 shot Table 7 : Results of 10 - way 1 - shot and 10 - way 5 - shot text classification accuracies ( % ) on four benchmark test sets . The test set of Amazon product data only contains nine categories . Thus we test the 9 - way 1 - shot and 9 - way 5 - shot performance instead .
The deep learning based models that have been developed to solve text - based ( logical ) reasoning tasks can be categorized as follows ( see Huang and Chang 2022 for a recent survey of the literature ) .
For reproducibility , we define a fixed cross - topic split with the data of two topics as test set ( gun control , school uniforms ) , the data of one topic as development set ( death penalty ) , and the data of the remaining five topics as train set . We also create a fixed in - topic split by randomly taking 3,532 samples of all topics for training , 500 for development , and 1,000 for testing . nuclear AND ( energy OR fission OR power OR plant ) In addition , we must continue developing safer technologies like small modular reactors which will help us meet our nation 's need for reliable , emission - free sources of low - emission energy while also creating jobs and providing solutions to some of the world s most pressing problems : climate change , food security and sustainable development . ( 0.96 ) nuclear energy CON leak . " We are concerned about the possibility of further releases of radioactivity due to possible melting or cracking of fuel rods at the No . ( 0.47 ) death penalty CON inhuman . Amnesty International opposes the death penalty in all cases as the ultimate form of cruel , inhuman or degrading punishment and a violation of fundamental rights -the right to life and the prohibition of torture .
We report our model and training hyperparameters in table 8 . We did not perform explicit hyperparameter tuning , besides some manual testing early in development on a subset of the MRP shared task data . Those data are annotated with SLR frameworks other than the ones we compare here , and we ended up excluding them from our experiments for lack of overlap with most of the other frameworks ' annotations .
In this process , we found that Google translator is not good enough in schema translation since industry jargon and abbreviations are commonly used in column headers . Table 1 shows some example headers and their paraphrases under different domains in our dataset . However , domain information is implicit , and the meaning of the header needs to be inferred carefully from the entire table context . To get more precise translations , we provide three kinds of additional information as a schema context : ( 1 ) a whole table with structural information , including its table name , column headers and cell values ; ( 2 ) an original web - page URL for the table from the Wikipedia website ; ( 3 ) some natural language question / answer pairs about the table 2 . Our translators are asked to first understand the context of the given schema before validating the translations . We find that the modification rate is 40 % , which indicates that the provided context is very useful . Finally , we further verify the annotated data by asking a different translator to check if the headers are correctly translated .
To solve the program , we follow the convex relaxation approach developed in ( Srikant et al . , 2021 ) . Specifically , the boolean variables ( for tweet and word selection ) are relaxed into the continuous space so that they can be optimized by gradientbased methods over a convex hull . Two main implementations of the optimization - based attack generation method are proposed : joint optimization ( JO ) solver and alternating greedy optimization ( AGO ) solver . JO calls projected gradient descent method to optimize the tweet and word selection variables and word replacement variables simultaneously . AGO uses an alternative optimization procedure to sequentially update the discrete selection variables and the replacement selection variables . More details on the optimization program and the solvers can be found in Appendix A .
We evaluate the re - ranking model ( Section 3 ) in several settings to answer the following questions : For all experiments , we report the mean and standard deviation of the accuracy across five runs with different random seeds .
The generation mode is independent of the generation model , so the generation models and the generation modes can be combined arbitrarily . We employ two generation modes here .
The task of finding experiment - specific information can be modeled as a retrieval task ( i.e. , finding relevant information in documents ) and at the same time as a semantic - role - labeling task ( i.e. , identifying the slot fillers ) . We identify three sub - tasks :
• TASK TYPE defines the nature of the mapping from instance inputs to outputs ( e.g. , question answering , classification , etc . ) .
After planning the target T and the keyword sequence W , we train CONPER to generate the whole story conditioned on the input and plans with the standard language model loss L ST . Since we extract one sentence from a story as the target , we do not train CONPER to regenerate the sentence in the story generation stage . And we insert a special token Target in the story to specify the position of the target during training . In the inference time , CONPER first plans the target and plot , then generates the whole story , and finally places the target into the position of Target .
We use the standard training , development and test datasets from the WMT'18 Task 4 track . For feature - based systems , we follow the built - in crossvalidation in QuEst++ , and train a single model with the hyperparameters found by cross - validation . For neural - based models , we use early - stopping with a patience of 10 to avoid over - fitting , and all reported figures are averaged over 5 runs corresponding to different seeds .
Neural Retriever Augmented Language Modeling ( NRALM ) : Augmenting language models with neural retrieval has been shown to be very effective , such as by retrieving nearest neighbor words for LM tasks ( Khandelwal et al . , 2020 ; or Machine Translation ( Khandelwal et al . , 2021 ) . Dinan et al . ( 2019 ) proposed a decomposed transformer for conversation tasks , which enabled pre - computation of the external knowledge embeddings . ORQA proposed the ICT task to pre - train a decomposed retriever , and DPR enhanced this approach with in - batch negatives and hard negatives to eliminate the pre - training . Synthetic Data Augmentation is also commonly used , such as in DPR - PAQ ( Oguz et al . , 2021 ) , PAIR , Hu et al . ( 2021 ) . Per - token embeddings or multiple embeddings were used in ColBERT , ME - BERT ( Luan et al . , 2021 , Lee et al . ( 2021 ) .
, and v I i is obtained from Eq . ( 4 ) . Next , the complete class - level embeddings are obtained as :
Template : Wrong Answer with input : < input > . Expected output is < output_1 > , but generated output is < output_2 > . Rewrite the code . Example : Wrong Answer with input : 2 5 3 . Expected output is 1 , but generated output is 0 . Rewrite the code .
In this paper , we propose an unsupervised process to generate questions , answers and associated distractors in order to fine - tune and improve the performance of the state - of - the - art model UnifiedQA on unseen domains . This method , being unsupervised , needs no additional annotated domain specific data requiring only a set of unannotated sentences of the domain of interest from which the questions are created . Contrarily to most of the aforementioned works , our aim is not to train a new completely unsupervised model but rather to incorporate new information into an existing stateof - the - art model and thus to take advantage of the question - answering knowledge already learned .
We begin by outlining our experimental setup , and describe the benchmarks , model , baselines , and other relevant experimental settings .
We evaluate the quality ( intrinsic evaluation ) of the Arg - CTRL and its performance on an exemplary task ( extrinsic evaluation ) . As a basis , we use the 7,991 arguments generated in Section 5 .
After an alignment model is trained with PPA , we extract the query encoder from MoCo and finetune it on downstream tasks for evaluation . We follow the standard way of finetuning BERT - like models for sequence classification and QA tasks :
Position Encoding To tackle the position unaware problem , absolute position information is injected into the SANs :
Table 10 shows the sources and number of arguments for all topics of the reference dataset . The dataset is used to compare the argument generation models to a retrieval approach .
Audio preprocessing . We use Kaldi ( Povey et al . , 2011 ) to create Mel - filter bank features ( FBANK ) from the raw audio signals . Specifically , we use the Hanning window , 128 triangular Melfrequency bins , and 10 millisecond frameshift . We always use the first audio channel when an audio clip has more than one channel . We apply two normalizations : ( 1 ) before applying Kaldi , we subtract the mean from the raw audio signals ; and ( 2 ) we compute the mean and standard deviation of FBANK on the unbalanced AS training set , and then normalize the FBANK of each audio clip . For data augmentation , inspired by , we use frequency masking and time masking : we randomly mask out one - fifth FBANK along the time dimension and one - forth FBANK along the frequency dimension during training .
In real - life applications factual knowledge is apt to evolve over time ( Nonaka et al . , 2000 ; Roddick and Spiliopoulou , 2002 ; Hoffart et al . , 2011 ; Gottschalk and Demidova , 2018 ) ; for instance , The host city of the Winter Olympic Games in 2018 was South Korea , while in 2022 it was Beijing . In this connection , there is a current trend to investigate knowledge graphs ( KGs ) involving time , and these KGs are coined as temporal knowledge graphs ( TKGs ) . In a TKG , fact triplets are equipped with temporal information ( e.g. , timestamps ) , and a temporal fact can be stated in the form like " ( Beijing , held , Winter Olympic Games , 2022 ) " .
A binary classifier identical to Fin - GRU , but utilizes LSTM ( Hochreiter and Schmidhuber , 1997 ) to encode temporal dependence . The model is trained in the same manner as FinGRU .
Limitations have been described as separate section after the Conclusions , as required by the ACL instructions .
Relation - Aware Self - Attention . First , we introduce self - attention and then its extension , relationaware self - attention . Consider a sequence of inputs
We provide an example answer generated by finetuning and our inducer - tuning with LoRA on CoQA in Table 6 .
We have tried two kinds of MRD to dynamically adjust the masking ratio , namely linear decay and cosine decay as follows :
To perform the statistical analysis presented in Section 5 , we take the premise - hypothesis pairs from the MedNLI training , dev , and test splits , and combine them to produce a single corpus . We use a scispaCy model pre - trained on the en_core_sci_lg corpus for tokenization and entity linking ( Neumann et al . , 2019b ) , and link against the Medical Subject Headings ( MeSH ) knowledge base . We take the top - ranked knowledge base entry for each linked entity . Linking against MeSH provides a unique concept i d , canonical name , alias(es ) , a definition , and one or more MeSH tree numbers for each recovered entity . Tree numbers convey semantic type information by embedding each concept into the broader MeSH hierarchy 3 . We operationalize each of our heuristics with a set of MeSH - informed semantic properties , which are defined as follows :
We conducted three human experiments ( one feasibility study and two debugging experiments ) to demonstrate the usefulness of FIND . For all the experiments , we used as classifiers convolutional neural networks ( CNNs ) ( Kim , 2014 ) , which are a popular , well - performing architecture for many text classification tasks including the tasks we experimented with ( Gambäck and Sikdar , 2017;Johnson and Zhang , 2015;Zhang et al . , 2019 ) . The overall results show that FIND with human - in - the - loop can improve the text classifiers and mitigate the said problems in the datasets . After the experiments , we discuss the generalization of the proposed framework to other tasks and models . Overall , the main contributions of this paper are :
The scores presented are significantly different ( p < 0.05 ) from the respective baseline . CHRF1 refers to character n - gram F1 score ( Popović , 2015 ) . The models in italics are ours .
1 . If the target word is the surname of a person , the example should be tagged surname . 15 2 . If the entity ( as a whole ) refers to one of the word - meanings , it should be labeled as such . For example , Quitobaquito Springs label should refer to a natural source of water .
Stability w.r.t . the number of runs K. Figure 1 shows the results on stability . In light of limited computation resources , we only experiment with some representative strategies . Both CV and MDL represent strategies whose number of runs are coupled with the size of data split , while Multi - Splits represents strategies that have a fixed ratio and independent K. We observe : ( 1 ) Multi - Splits ( blue lines ) is the most stable in correlation and performance , while other strategies CV and MDL are more sensitive to the choice of K .
where L impt is a task - specific / domain - specific loss function . The gradient can be used as the importance score because changing g lh is liable to have a large effect on the model if I lh has a high value . Although Eq . 6 offers a way to compute the importance of attention heads w.r.t . a given loss L impt , we are unable to directly apply it : If we use the domain data at hand and the MLM loss as L impt , ∇ g lh only indicates the importance score for domain - specific knowledge . However , our goal is to estimate the attention heads importance for the general knowledge in LM which requires the data used in training the LM to compute the L impt .
We also fine - tune the original BERT and SciB - ERT sequence tagging models on this task . Since we use BIO labels , we extend it with a CRF output layer to enable it to correctly label multi - token mentions and to enable it to learn transition scores between labels . As a non - neural baseline , we train 5 https://github.com/huggingface/ transformers 6 We use sklearn , https://scikit-learn.org .
For tasks like table - to - text that word overlapping is enough to infer the precise causality , we randomly sample several spans from y as Y and find the corresponding X by word overlapping . While , for highly abstractive generation like text summarization , we use the global information y as Y and infer X by gradient ranking , which measures the salience of a token by gradient norm ( Simonyan et al . , 2014 ; Li et al . , 2016 ) . After the X is inferred , we search around the neighbors of word embedding space for meaning preserving swapping , utilizing the semantic textual similarity of word embeddings ( Li et al . , 2020a ) . To make the adversarial samples more challenging , we search at the direction of gradient ascent to replace x t ∈ X withx t :
• SQuAD 2.0 : Stanford Question Answering Dataset version 2.0 ( Rajpurkar et al . , 2018 ) . This task adds addition questions to SQuAD whose answer does not exist in the context ; models have to recognize when these questions occur and not return an answer for them .
Dataset Analysis . To have a more quantitative analysis of our dataset , we count the ratio of headers containing four lexical features , including abbreviation , symbol characters , verb - object phrase and capitalized character . As we can see in table 2 , these lexical features commonly occur in headers , making them quite different from plain text .
We evaluate LM - KT two ways : first , its ability to predict if an individual student will answer a novel question correctly on a held - out test set of real Duolingo student responses . Second , how wellcalibrated these predictions are , which is crucial to our later use of LM - KT for question generation . Table 1 compares AUC - ROC on a held - out test set for our LM - KT model with standard DKT , which uses question IDs instead of text , and a baseline that ignores the student state , only using the question text representation . This question only baseline would perform well if the Duolingo dataset largely consisted of universally " easy " and " difficult " questions , independent of individual student . Our results show that incorporating the student state is crucial for accurately predicting Duolingo user responses , and including question text also leads to a significant improvement . LM - KT outperforms Standard DKT especially on novel questions - a necessary generalization ability for generation .
Fine - tuning UnifiedQA on synthetic questions with random distractors improves the results as compared to the baseline and , as expected , the closer the unlabeled sentences are to the topics of the questions , the better is the accuracy . Hence , generating questions from only the train set of SciQ gives performances that are comparable but slightly lower to the ones obtained from the combined train , dev and test set of SciQ. Finally , questions selected from Wikipedia also improve the results , despite being loosely related to the target test corpus . Our distractor selection method further boosts the accuracy results in all setups . This suggests that a careful selection of distractors is important , and that the hard selection criterion used here seems adequate in our context .
PLLs can be used to re - rank the outputs of an NMT or ASR system . While historically log - likelihoods from language models have been used for such reranking , recent work has demonstrated that PLLs from masked language models perform better ( Shin et al . , 2019 ) . However , computing PLLs from a masked language model requires n passes of the transformer : once with each token masked out . Salazar et al . ( 2020 ) suggest distilling BERT into a model that uses no masking to avoid this cost , but this model considerably under - performed regular LMs in their experiments .
Multimodality is achieved with two changes in our monomodal models : multimodality integration ( where to integrate the visual features in the architecture ) , and fusion strategy ( how to fuse the visual and textual features ) . We propose the following places to integrate the visual feature vector into the BiRNN architecture :
Energy - based models have been widely explored in machine learning ( Dayan et al . , 1995 et al . , 2007 ) . While many training methods involve sampling from the EBM using gradientbased MCMC ( Du and Mordatch , 2019 ) or Gibbs sampling ( Hinton , 2002 ) , we considered these approaches too slow for pre - training because they require multiple passes through the model per sample . We instead use noise - contrastive estimation ( Gutmann and Hyvärinen , 2010 ) , which has widely been used in NLP for learning word vectors ( Mnih and Kavukcuoglu , 2013 ) and text generation models ( Jean et al . , 2014;Józefowicz et al . , 2016 ) . While EBMs have previously been applied to leftto - right ( Wang et al . , 2015 ) or globally normalized ( Rosenfeld et al . , 2001;Deng et al . , 2020 ) text generation , they have not previously been applied to cloze models or for pre - training NLP models . Several papers have pointed out the connection between EBMs and GANs ( Zhao et al . , 2016;Finn et al . , 2016 ) , which is similar to the Electric / ELECTRA connection .
Although self - attention networks ( SANs ) ( Lin et al . , 2017 ) have achieved the state - of - the - art performance on several natural language processing ( NLP ) tasks ( Vaswani et al . , 2017;Devlin et al . , 2019;Radford et al . , 2018 ) , they possess the innate disadvantage of sequential modeling due to the lack of positional information . Therefore , absolute position encoding ( APE ) ( Vaswani et al . , 2017 ) and relative position encoding ( RPE ) ( Shaw et al . , 2018 ) were introduced to better capture the sequential dependencies . However , either absolute or relative PE is language - independent and its embedding remains fixed . This inhibits the capacity of SANs when modelling multiple languages , which have diverse word orders and structures ( Gell - Mann and Ruhlen , 2011 ) . Recent work have shown that modeling cross - lingual information ( e.g. , alignment or reordering ) at encoder or attention level improves translation performance for different language pairs ( Cohn et al . , 2016;Du and Way , 2017;Zhao et al . , 2018;Kawara et al . , 2018 ) . Inspired by their work , we propose to augment SANs with cross - lingual representations , by encoding reordering indices at embedding level . Taking English⇒Chinese translation task for example , we first reorder the English sentence by deriving a latent bracketing transduction grammar ( BTG ) tree ( Wu , 1997 ) ( Fig . 1a ) . Similar to absolute position , the reordering information can be represented as cross - lingual position ( Fig . 1b ) . In addition , we propose two strategies to incorporate cross - lingual position encoding into SANs . We conducted experiments on three commonlycited datasets of machine translation . Results show that exploiting cross - lingual PE consistently improves translation quality . Further analysis reveals that our method improves the alignment quality ( § Sec . 4.3 ) and context - free Transformer ( Tang et al . , 2019 ) ( § Sec . 4.4 ) . Furthermore , contrastive evaluation demonstrates that NMT models benefits from the cross - lingual information rather than denoising ability ( § Sec . 4.5 ) .
Consider a neuron k whose value is computed using n neurons in the previous layer ,
We confirm the presence of annotation artifacts in MedNLI and proceed to identify their lexical and semantic characteristics . We then conduct adversarial filtering to partition MedNLI into easy and difficult subsets ( Sakaguchi et al . , 2020 ) . We find that performance of off - the - shelf fastText - based hypothesis - only and hypothesis - plus - premise classifiers is lower on the difficult subset than on the full and easy subsets ( Joulin et al . , 2016 ) . We provide partition information for downstream use , and conclude by advocating alternative dataset construction strategies for knowledge - intensive domains . 1
We follow the definition of an unrestricted compression ratio ( Grusky et al . , 2018 ) , dividing the ( token ) length of an article by the associated summary ( token ) length . This carries the same semantic value as inverse definitions of compression ratio , such as used by Bommasani and Cardie ( 2020 ) . When looking at the token - level compression ratio displayed in Figure 3 , a comparatively high mean is observed , despite extremely long summary documents . Comparing compression ratios reported by ( Zhong et al . , 2019 ) for news - based datasets indicates that EUR - Lex - Sum has a mean compression ratios similar to the CNN / DailyMail ( Hermann et al . , 2015 ) and NYT ( Sandhaus , 2008 ) corpora .
Judgment includes the referred law articles , the charge and the term of penalty , which is determined by the judge according to the fact and rationales . Here , we denote the referred law article as a , the charge as c , and the term of penalty as p. The law article and the charge are in the form of labels , and the term of penalty is represented in months ( numerical values ) .
The number of scored candidate distractors is an hyper - parameter . A small number of candidates may result in a situation where none of the candidates are credible enough , while a large number requires more computation time , since the score of each candidate for every question needs to be computed , and has a higher risk of proposing multiple valid answers . In our experiments , we use a number of 64 candidates in order to limit computation time .
Results are shown in Table 2 . Electric scores better than GPT-2 when trained on comparable data . While scoring worse than BERT , Electric is much faster to run . It also slightly outperforms ELECTRA - TT , which is consistent with the finding from Labeau and Allauzen ( 2018 ) that NCE outperforms negative sampling for training language models . Furthermore , Electric is simpler and faster than ELETRA - TT in that it does not require running the generator to produce PLL scores . TwoTower scores lower than Electric , presumably because it is not a " deeply " bidirectional model and instead just concatenates forward and backward hidden states .
For each output token , the LM prediction is obtained by feeding the prefix upto that token to the LM model . These predictions are pre - computed for training and validation sets . This ensures parallelization and avoids the overhead to run the LM simultaneously during the training process . During inference , the LM model is called every time a new output token is written .
There exists a rich body of work on precisely modeling student " ability " and learning . For example , Item Response Theory ( IRT ) seeks to model individual student ability based on their responses to different questions , creating a strong factorization between students and test items ( Lord , 1980;Hambelton and Jodoin , 2003 ) . Meanwhile , Computer Adaptive Testing ( CAT ) techniques are used to determine a fixed student ability as quickly as possible by selecting test items based on information utility ( Weiss and Kingsbury , 1984;Thissen and Mislevy , 2000;Settles et al . , 2020 ) . However , these methods , which have been used to develop efficient standardized tests , do not necessarily optimize a student 's learning experience ( Mu et al . , 2018 ) . We instead focus on tracking each student 's evolving knowledge , choosing questions to target difficulty .
( 2 ) Hypothesis Faithfulness . Does H faithfully follow T p ? To check this , we extract the highest conclusion int h in T p ( e.g. , int 2 in Figure 2 ) and verify if int h can support H. Following Dalvi et al . ( 2021 ) , we use BLEURT ( Sellam et al . , 2020 ) as the scorer V h to estimate the sentence similarity between int h and H. In addition , we check whether H is entailed by int h with the step verifier V s .
Parallel Data All parallel data we use involve English as the source language . Specifically , we collect en - fr , en - es , en - de parallel pairs from Europarl , en - ar , en - zh from MultiUN ( Ziemski et al . , 2016 ) , en - hi from IITB ( Kunchukuttan et al . , 2018 ) , and en - bg from both Europarl and EUbookshop . All datasets were downloaded from the OPUS 3 website ( Tiedemann , 2012 ) . In our experiments , we vary the number of parallel sentence pairs for PPA . For each language , we take the first 250k , 600k , and 2 M English - translation parallel sentence pairs except for those too short ( where either sentence has less than 10 WordPiece tokens ) , or too long ( where both sentences concatenated together have more than 128 WordPiece tokens ) . Table 1 shows the actual number of parallel pairs in each of our 250k , 600k , and 2 M settings .
In addition to zero - shot cross - lingual transfer , we also showed that code - switching with English during finetuning provides additional alignment signals , when training data is available for the target language .
For both French and Spanish question generation models , we select 15 students unseen during training and generate 30 questions across 9 difficulties from 0.1 to 0.9 , using nucleus sampling ( Holtzman et al . , 2020 ) ( p = 0.99 ) with a maximum output length of 20 tokens . We also vary a repetition penalty ( Keskar et al . , 2019 ) that penalizes for previous tokens ( including those in the student state ) . Lastly , we resize all prompts ( student state and target difficulty ) to fit into the GPT-2 Model by taking the most recent 1024 tokens , as in training . This is a limitation of our work , as the full student history is not able to be considered for students who have answered a large set of questions .
We found that answer generation more easily over - fits compared to the retrieval during finetuning . To prevent this over - fitting , the model is once reinitialized from the pre - trained YONO model at the 6 th iteration after the model achieves acceptable recall on the downstream task .
Head - level XL SANs Instead of projecting XL PE to all attention heads , we feed partial of them , such that some heads contain XL PE and others contain APE , namely HeadXL . As shown in Fig . 2b , we fist add APE and XL PE for X , respectively :
Nonetheless , there are side - effects of suboptimal datasets that can not be predicted and are only found after training thanks to post - hoc error analysis . To rectify such problems , there have been attempts to enable humans to fix the trained models ( i.e. , to perform model debugging ) ( Stumpf et al . , 2009;Teso and Kersting , 2019 ) . Since the models are usually too complex to understand , manually modifying the model parameters is not possible . Existing techniques , therefore , allow humans to provide feedback on individual predictions instead . Then , additional training examples are created based on the feedback to retrain the models . However , such local improvements for individual predictions could add up to inferior overall performance ( Wu et al . , 2019 ) . Furthermore , these existing techniques allow us to rectify only errors related to examples at hand but provide no way to fix problems kept hidden in the model parameters .
where p tnc is the element from the output feature tensor P , and l tmc is the element from ground truth matrix L. Then , DetIE utilizes the Hungarian algorithm ( Kuhn , 1955 ) that matches one proposal for each ground truth with the global minimum IoU .
Here the r ij terms encode the known relationships between the two tokens x i and x j in the input sequence . In this way , this self - attention is biased toward some pre - defined relationships using the relation vector r ij in each layer when learning the contextualized embedding . Specifically , they use it to represent the relative position information between sequence elements . More details could be found in their work ( Shaw et al . , 2018 ) . Figure 2 : An overview of CAST with an illustrative example of English - to - Chinese schema translation . Firstly , the target header " Chinese " and its context are modeled as a directed graph . Then a stack of relation - aware transformers encodes the input sequence X to X 0 with a relational matrix R induced from the graph .
where [ C ] is the placeholder separator for the following relation . For example in Figure 2 , the target RE task contains three novel relations : em - ployee_of , ceo_of , and others , of which the relation descriptions are then concatenated altogether to form the multi - choice prompt " [ C ] employee of [ C ] ceo of [ C ] others " . After obtaining the multichoice prompt , we then feed it accompanied with the input sentence into the instance encoder , and the representations at separator [ C ] is regarded as the representation of its following relation .
6 Exp 2 : Training Data with Biases
Furthermore , we also conducted Experiment 1 for BiLSTMs . Each direction of the recurrent layer had 15 hidden units and the feature vector was obtained by taking element - wise max of all the hidden states ( i.e. , d = 15 × 2 = 30 ) . We adapted the code of ( Arras et al . , 2017 ) to run LRP on BiLSTMs . Regarding human feedback collection , we collected feedback from Amazon Mechanical Turk workers by splitting the pair of word clouds into two and asking the question about the relevant class independently of each other . The answer of the positive relevance word cloud should be consistent with the weight matrix W , while the answer of the negative relevance word cloud should be the opposite of the weight matrix W. The score of a BiLSTM feature is the sum of its scores from the positive word cloud and the negative word cloud .
Data Usage and License ConvSumX is based on two public English conversation summarization datasets , namely DIALOGSUM and QMSum . Both datasets are freely available online under the MIT license , which has no constraint to academic use , modification , and further distribution . We will follow the MIT license to make our data ( annotated target summaries / queries and corrected English summaries / queries ) freely available online .
Setting JGA Unfreeze all up to 2nd layer 37.9 Unfreeze all up to 3rd layer 37.7 Unfreeze all up to 4th layer 38.8 Unfreeze all up to 5th layer 34.5 Unfreeze all up to 6th layer 39.1 Unfreeze the first and last layers ( ours ) 39.7 Unfreeze the first two and last two layers 30.2 all layers except the first layer of the encoder and decoder after the initial 1,000 steps . Our findings revealed that the optimal approach is to freeze all layers except the first and last layers of both the encoder and decoder after 1,000 steps .
The absolute value of s ( ij ) measures the strength of the influence of z ( i ) on z ( j ) . The sign of s ( ij ) show the direction of influence . A negative s ( ij ) means that removing z ( i ) decreases the loss at z ( j ) , i.e. z ( i ) is harmful to z ( j ) . s ( ij ) has high variance because it depends on a single ( arbitrary ) data point z ( j ) . To better estimate the influence of z ( i ) on the entire data distribution , researchers average the influence scores of z ( i ) over a reference set Z ′
In particular , we modify the original sentences by replacing terms describing one of the protected groups ( dominant or minoritized ) with identity words for the other group , e.g. , he → she , Michael → Elizabeth , etc . Denote the original sentence as c 1 , and the modified sentence as c 2 . Also , we replace the identity words with some neutral terms that do not imply identity of any protected groups ( e.g. , he → person ) to create an unbiased reference r. With such constructed paired samples at hand , we can mitigate the bias against the protected group by encouraging the model to assign the same score to ( c 1 , r ) and ( c 2 , r ) . Formally , the instance - wise loss can be described as follows ,
We collect annotations from two types of annotators : experts and crowdworkers .
Electric also models p data ( x t |x \t ) , but does not use masking or a softmax layer . Electric first maps the unmasked input x = [ x 1 , ... , x n ] into contextualized vector representations h(x ) = [ h 1 , ... , h n ] using a transformer network . The model assigns a given position t an energy score E(x ) t = w T h(x ) t using a learned weight vector w. The energy function defines a distribution over the possible tokens at position t as
We have demonstrated that the choice of linguistic formalism can have substantial , linguistically meaningful effects on role - semantic probing results . We have shown how probing classifiers can be used to detect discrepancies between formalism implementations , and presented evidence of semantic proto - role encoding in the pre - trained mBERT model . Our refined implementation of the edge probing framework coupled with the anchor task methodology enabled new insights into the processing of predicate - semantic information within mBERT . Our findings suggest that linguistic formalism is an important factor to be accounted for in probing studies . This prompts several recommendations for the follow - up probing studies . First , the formalism and implementation used to prepare the linguistic material underlying a probing study should be always explicitly specified . Second , if possible , results on multiple formalisations of the same task should be reported and validated for several languages . Finally , assembling corpora with parallel cross - formalism annotations would facilitate further research on the effect of formalism in probing .
In some cases , we observe that the edit - pass@1 outperforms the pass@5 . It demonstrates that editing the candidate code is very sample efficient . With the editor model , the number of required programs sampled from the LLM can be reduced .
• Hyper - parameters : Besides what explained in the paper , we did not specifically run hyperparameter exploration . However , on our early experiments both models we tried several learning rate variations around the reported 10 −4 and manually picked the best for our main experimental setup .
Figure 1 presents the high - level architecture of the document - level BiRNN model , with the various multimodality integration and fusion approaches .
There are no significant differences between the performance of H2H+CXT and H2H+CXT+ExtL which has two extra Transformers layers since the pre - trained NMT models have already had 12 Transformers layers .
Figure 1 : Intra - sentence similarity by layer L of the multilingual BERT - base . Functional tokens are similar in L = 0 , syntactic groups emerge at higher levels .
In Stage 2 , we use the teacher model to rescore each of the b 1 partial hypotheses conditioned on the full source sentence and only keep the top b 2 ( b 2 < b 1 ) partial hypotheses for the next step in the two - stage beam search process . With this strategy , future information in the source sentence is utilized to improve the quality of top partial hypotheses , while also preserving the local word order dictated by the prefix source .
Our experimental setup follows Le and Titov ( 2018 ) . In particular , we test the proposed ED models using six standard datasets : AIDA - CoNLL ( CoNLL ) ( Hoffart et al . , 2011 ) , MSNBC , AQUAINT , ACE2004 , WNED - CWEB ( CWEB ) , and WNED - WIKI ( WIKI ) ( Guo and Barbosa , 2018 ) . We consider only the mentions that refer to valid entities in Wikipedia . For all datasets , we use the KB+YAGO entity candidates and their associatedp(e|m ) ( Ganea and Hofmann , 2017 ) , and use the top 30 candidates based onp(e|m ) .
This section evaluates the performance of our proposed PBML . We conduct extensive experiments on four widely - used text classification datasets under few - shot settings and make a full - scale comparison with existing state - of - the - art baselines . We report our implementation details in Appendix E .
For the data preprocessing , at source side , we perform lemmatization on both corpora and alphabet normalization specifically on the PHOENIX ( the letters ü , ö , ä , and ß in the glosses are prenormalized by dataset creators ) . We then apply Byte Pair Encoding ( BPE ; Sennrich et al . , 2016b ) to decompose the words and build vocabulary . In the end , we set the lemmatized+normalized sentences with lowercased glosses of PHOENIX and lemmatized sentences with generalized glosses of the DGS corpus to train the models . We present the relevant statistics in Appendixes A and B .
The versatility of pre - trained representations implies that they encode some aspects of general
In doing so we iv ) re - evaluate prior findings on visual vs. textual unimodal collapse and v ) showcase MM - SHAP 's abilities for interpreting predictions for individual samples , for error analysis .
MedNLI is Not Immune from Artifacts
• We propose using word clouds as visual explanations of the features learned .
Update original θs : θs ← θs − ηs∇ θs Ls ( θs , θt , z r ) 10 :
Perceived Learning Effect . Table 17 shows similar trends to the previous results that basic - level learners perceived more learning effects on both vocabulary and grammar . They tend to show more willingness to re - participate in data annotation .
In this paper , we focus on the task of OoD detection with only in - distribution texts available during learning for its capability of dealing with diverse scenarios such as non - classification applications while requiring the least data collection effort .
Context - aware schema encoding has received considerable attention in both recent semantic parsing literature ( Hwang et al . , 2019;Gong et al . , 2019 ) and Table - to - Text literature ( Gong et al . , 2019 ) . In general , there are two sorts of techniques : 1 ) . add additional entity type embedding and special separator token from the input sequence to distinguish the table structure ( i.e. , Type - SQL and IRNET ) ; 2 ) . encode the schema as a directed graph . For example , Bogin et al . ( 2019 ) use a Graph Neural Network ( Scarselli et al . , 2008 ) , and ; Shaw et al . ( 2019 ) use a transformer self - attention mechanism to encode the schema over predefined schema relationships . Unlike these works , we explore the suitability of schema encoding techniques for the newly proposed schema translation task .
Blagojevich is currently serving a 14 - year sentence at the Federal Correctional Institution Englewood near Denver .
Why does PER benefit more than other entity types ? To answer this , we count the fraction of mentions of each entity type that have at least one column represented using attribute separators . This counting reveals that approximately 56 - 58 % of mentions of type ORG , GPE , and UKN have at least one such column . On the other hand , this number is 71 % for PER mentions . This suggests that the difference is directly attributable to more PER entities having a column that has been modeled using attribute separators , further highlighting the benefits of this modeling decision .
Our framing of the quality ranking task could be interpreted as seemingly prescriptive ( i.e. , that joke A is " objectively " better than joke B ) , but New Yorker editorial selections should not be taken as ground truth for funniness ; disagreement about what is funny is expected and valid . Our tasks operationalize the prediction of only average preferences ( rather than individual ones ) , and these preferences may include a partiality or bias towards items that conform to the characteristics of prior contest winners or published New Yorker cartoons .
FLONET is the state - of - the - art approach for learning flowchart grounded task oriented dialogs in an end - to - end manner . It follows the RAG sequence model ( Lewis et al . , 2020 ) . The RAG sequence model has two main components : ( 1 ) a retriever p con η ( z|h i ) which computes a distribution over retrievable documents z ( i.e. , flowchart nodes and FAQs ) based on the dialog history h i and ( 2 ) a generator p θ ( y t |h i , z , y 1 : t−1 ) which generates the agent response token - by - token . The overall RAG model is given by ,
• We provide a new corpus of 45 materialsscience publications in the research area of SOFCs , manually annotated by domain experts for information on experimental settings and results ( Section 4 ) . Our corpus is publicly available . 1 Our inter - annotator agreement study provides evidence for high annotation quality ( Section 5 ) .
• We propose the task of schema translation , and discuss its differences with a plain text translation . To facilitate the research study , we construct the first parallel schema translation dataset .
Table 2 shows the evaluation results . The distilled datasets with the hard labels , i.e. , only optimizing the input embeddings and not applying the attention labels , still achieved 87.4 , 81.6 , and 68.6 for AGNews , SST-2 , and QNLI , respectively , which is 92.4 , 88.0 , and 74.7 % performance of the full dataset . Furthermore , using the soft labels further improved these performances , especially by almost 8 points for QNLI . However , for MRPC , the distilled dataset achieved only the same performance as the majority class baseline regardless of the use of the soft labels .
Debugging text classifiers using human feedback -Early work in this area comes from the human - computer interaction community . Stumpf et al . ( 2009 ) studied the types of feedback humans usually give in response to machine - generated predictions and explanations . Also , some of the feedback collected ( i.e. , important words of each category ) was used to improve the classifier via a user co - training approach . Kulesza et al . ( 2015 ) presented an explanatory debugging approach in which the system explains to users how it made each prediction , and the users then rectify the model by adding / removing words from the explanation and adjusting important weights . Even without explanations shown , an active learning framework proposed by Settles ( 2011 ) asks humans to iteratively label some chosen features ( i.e. , words ) and adjusts the model parameters that correspond to the features . However , these early works target simpler machine learning classifiers ( e.g. , Naive Bayes classifiers with bag - of - words ) and it is not clear how to apply the proposed approaches to deep text classifiers .
MQM Score = 1 − n min + 5n maj + 10n cri n
Queries are present in 23.05 % and 25.31 % of valid seen and unseen splits , respectively . This is a key challenge as it demonstrates a clear use case for dialogue and limitation of current models .
• We propose a header - to - header context - aware schema translation model , called CAST , for the new schema translation task . Specifically , we use the transformer self - attention mechanism to encode the schema over predefined entity types and structural relationships , making it aware of the schema context .
MedNLI is domain - specific evaluation dataset inspired by general - purpose NLI datasets , including SNLI and MultiNLI ( Romanov and Shivade , 2018;Bowman et al . , 2015;Williams et al . , 2017 ) . Much like its predecessors , MedNLI consists of premisehypothesis pairs , in which the premises are drawn 1021 from the Past Medical History sections of a randomly selected subset of de - identified clinical notes contained in MIMIC - III ( Johnson et al . , 2016;Goldberger et al . , 2000 ( June 13 ) . MIMIC - III was created from the records of adult and neonatal intensive care unit ( ICU ) patients . As such , complex and clinically severe cases are disproportionately represented , relative to their frequency of occurrence in the general population .
Instead , the goal of these models is to produce a small but high - recall candidate list E. Ergo , the success of this stage is measured using a metric such as recall@K i.e. whether the candidate list contains the correct entity .
We have presented a new dataset for information extraction in the materials science domain consisting of 45 open - access scientific articles related to solid oxide fuel cells . Our detailed corpus and interannotator agreement studies highlight the complexity of the task and verify the high annotation quality . Based on the annotated structures , we suggest three information extraction tasks : the detection of experiment - describing sentences , entity mention recognition and typing , and experiment slot filling . We have presented various strong baselines for them , generally finding that BERT - based models outperform other model variants . While some categories remain challenging , overall , our models show solid performance and thus prove that this type of data modeling is feasible and can lead to systems that are applicable in production settings . Along with this paper , we make the annotation guidelines and the annotated data freely available .
CL Algorithm CL is a cooperative learning algorithm proposed by Shekhar et al . ( 2019 ) to model the question - player . The algorithm is based primarily on a self - play learning phase ( Das et al . , 2017 ) which learns from machine - machine dialogue . This is used in addition to ( after ) a more traditional supervised learning phase ( i.e. , on human - human dialogue ) . See Appendix A.6 for details .
The reordered source sentences are generated by BTG - based preordering model ( Neubig et al . , 2012 ) trained with above sub - word level 1 parallel corpus . At training phase , we first obtain word alignments from parallel data using GIZA++ or FastAlign , and then the training process is to find the optimal BTG tree for source sentence consistent with the order of the target sentence based on the word alignments and parallel data . At decoding phase , we only provide source sentences as input and the model can output reordering indices , which will be fed into NMT model . Thus , bilingual alignment information is only used to preprocess training data , but not necessary at decoding time .
We design a commonsense extraction tool that aligns sentences in stories with commonsense tuples , using a heuristic matching algorithm . Given a story , we match possible ATOMIC events to sentences by selecting events that share noun chunks and verb phrases with sentences . For every sentence s i that matches an event E in ATOMIC , we check surrounding sentences for mentions of commonsense inferences ( using the same noun and verb phrase matching strategy ) ; specifically , we check the n c preceding sentences for matches of causes of E , and the n e following sentences for event E 's effects .
In this paper , we introduce a new information extraction use case from the materials science domain and propose a series of new challenging information extraction tasks . We target publications about solid oxide fuel cells ( SOFCs ) in which the interdependence between chosen materials , measurement conditions and performance is complex ( see Figure 1 ) . For making progress within natural language processing ( NLP ) , the genre - domain combination presents interesting challenges and characteristics , e.g. , domain - specific tokens such as material names and chemical formulas .
Following the common finetuning practice , we do not use any additional training strategies . We train all tasks 5 times respectively and report the average scores . For GLUE , We use 8 widely used tasks in GLUE . 5
Outlook . In Section 7.1 , we have shown that our findings generalize well by applying model architectures developed on our corpus to another dataset . A natural next step is to combine the datasets in a multi - task setting to investigate to what extent models can profit from combining the information annotated in the respective datasets . Further research will investigate the joint modeling of entity extraction , typing and experiment frame recognition . In addition , there are also further natural language processing tasks that can be researched using our dataset . They include the detection of events and sub - events when regarding the experiment - descriptions as events , and a more linguistically motivated evaluation of the framesemantic approach to experiment descriptions in text , e.g. , moving away from the one - experimentper - sentence and one - sentence - per - experiment assumptions and modeling the graph - based structures as annotated .
We list the performance of two other large pretrained BERT - based models on the PAN XL dataset splits in Tab . 6 . The large gap between other models and siamBERT ( sB ) could be due to how the model functions , without learning over both documents simultaneously . BERT processes a pair of sequences , so the word - piece representations interact at every level before making a prediction based on the sequence pair embedding h [ CLS ] . In contrast , siamBERT processes each sequence separately , making the word - pieces ' interact ' at the end through the sequence embeddings h
Difficulty Control To explore whether our question generation model indeed depends on target difficulty and the individual student , we first measure the model 's perplexity on a held - out test set of Duolingo questions , compared to permutation baselines . Table 2 ( top ) shows that perplexity is lower for true student / target difficulty inputs than when either or both of these are permuted . The target difficulty values in this analysis were defined by the LM - DKT model . We can remove this dependence by using the actual student responses from Duolingo : we set the target difficulty to 1 if the student was correct and 0 otherwise . Table 2 ( bottom ) shows our model prefers questions paired with these " true correctness " targets than paired with random ones .
To evaluate how well INLP , MP , and TMP do in terms of linear guarding , we consider the number of misclassifications with respect to a * by the best possible linear classifier after a single projection ; the higher the number of misclassifications , the better the method performs . If a single projection is sufficient to achieve linear guarding , then arguably the semantic encoding of other attributes in our word embeddings are preserved as well as possible .
We use the publicly provided train / dev / test splits from the Shared Task , which are temporally ordered in sequence . We therefore construct student states by tracking user IDs throughout the datasets and appending each new question and response to the current student state . When evaluating our LM - KT model , we use the true responses of preceding questions in the test set to form the student state for a given question . Overall , we find that the dataset is severely imbalanced ( as in the original task ) -about 30 % of questions are answered incorrectly across students studying both French and Spanish .
Unlike previous work , our models also allow seamless mixing of multiple training datasets which link to different KBs with different schemas . We investigate the impact of training on multiple datasets in two sets of experiments involving additional training data that links to ( a ) a third KB that is different from our original training and testing KBs , and ( b ) the same KB as the test data . These experiments reveal that our models perform favorably under all conditions compared to baselines .
Additional test set Tables 7 and 8 present the full set of results of our experiments on the WMT'19 Task 2 test set on document and sentencelevel multimodal QE , respectively . This was the follow - up edition of the WMT'18 Task 4 , where the same training set is used , but a new test set is released .
This is the detailed datasheet , including ethical considerations , of the unlabelled pretraining dataset proposed in Section 4.1 .
We tie the embedding and output ( projection ) layers of both LM and NMT models ( Press and Wolf , 2017 ) . We use a dropout rate of 0.1 and GELU activations ( Hendrycks and Gimpel , 2017 ) . We use the default parameters of Lample and Conneau ( 2019 ) in order to train our models .
Novelty and Fluency By leveraging a pretrained language model 's ability to manipulate structure , we can generate novel questions not present in the entire Duolingo question set ( See Table 3 ) . Across 4,050 questions generated for Spanish learners , we found that with a repetition penalty ( Keskar et al . , 2019 ) , around 43 % of all questions , and 66 % of high difficulty ( d = 0.1 ) required to rank all questions in the pool , varying its size ( Figure 4 ) . On one NVIDIA Titan XP GPU , we find that , averaged across all target difficulties , our question generation model takes half the time to achieve the same quality as pool selection . The gap increases when trying to sample harder questions ( d < 0.5 ) -even a pool size of 1000 does not have sufficient difficult questions , likely due to a skew in the Duolingo question set . Additional controls , such as for style or topic , can easily be combined with our generation method , but would make pool selection exponentially more complex . Pool Sampling ( all targets ) Pool Sampling ( difficult targets only ) Generation ( all targets ) Generation ( difficult targets only )
Since our science is based around the belief that femoids can not be forever alone , we have the logical conclusion . The results of this argument could be considered as a valid conclusion .
In this study , we propose a global ED model based on BERT ( Devlin et al . , 2019 ) . Our model treats words and entities in the document as input tokens , and is trained by predicting randomly masked entities in a large entity - annotated corpus obtained from Wikipedia . This training enables the model to learn how to disambiguate masked entities based on words and non - masked entities . At the inference time , our model disambiguates * Work done at RIKEN .
To conduct the analysis presented in Section 3 , we take the MedNLI training dataset as input , and exclude the premise text for each training example . We cast the text of each training hypothesis to lowercase , but do not perform any additional preprocessing . We use an off - the - shelf fastText classifier , with all model hyperparameters set to their default values with the exception of wordNgrams , which we set equal to 2 to allow the model to use bigrams in addition to unigrams ( Joulin et al . , 2016 ) . We evaluate the trained classifier on the hypotheses contained in the MedNLI dev and test datasets , and report results for each split .
Fenwick 's wife becomes frightened when a tramp threatens to kill her and her child . " These refer to text spans that contradict previous content ( either in the context or the next segment box itself . )
We apply AFLite to two different versions of MedNLI : ( 1 ) X h , m : hypothesis - only , multi - token entities merged , and ( 2 ) X ph , m : premise and hypothesis concatenated , multi - token entities merged . AFLIte maps each version to an easy and difficult partition , which can in turn be split into training , dev , and test subsets . We report results for the fastText classifier trained on the original , hypothesis - only ( hypothesis + premise ) MedNLI training set , and evaluated on the full , easy and difficult dev and test subsets of X h , m ( X ph , m ) , and observe that performance decreases on the difficult partition :
While there is a great effort to enlarge Transformer - based LMs such as PALM ( Chowdhery et al . , 2022 ) and Minerva ( Lewkowycz et al . , 2022 ) , to improve the performance in symbolic and logical reasoning , our result reveals that it might be necessary to demonstrate the action sequence with reasonable abstraction to the Transformer to leverage its full strength .
x∈n - best(f , s ) f ( x|s ) + λPLL(x )
Then , we tag each example with its rank indexq i referring toq i :
In this section , we first introduce the theoretical background before we lay out the four tasks and discuss dataset generation . The dataset is available at https : / / github.com / sysulic / trac .
Our alignment - oriented method is , to a large degree , upper - bounded by the English performance , since all our parallel data involve English and all the other languages are implicitly aligning with English through our PPA objectives . Our 2 M model is able to improve the English performance to 82.4 from the mBERT baseline , but it is still lower than XLM ( MLM ) , and much lower than XLM ( MLM+TLM ) . We hypothesize that more highquality monolingual data and model capacity are needed to further improve our English performance , thereby helping other languages better align with it .
where x i 2 R dx . Self - attention introduced by Vaswani et al . ( 2017 ) transforms each x i into z i 2 R dx as follows :
We also design multiple - choice question prompts to leverage the question - answering capabilities of GPT3 ( denoted as GPT3 - QA ) . Similar to the wording used in our ground - truth survey datasets , questions are followed by three options each describing a degree of moral acceptability . We repeat this question - answering process 5 times for each topic - country pair and take the average of the model responses . Table 2 in the Appendix shows our prompts for all models .
Results on English datasets : Table 6 compares the performance of Perplection to random baselines on three English datasets . Perplection consistently tops the comparison in almost all cases except for SST-2 with RoBERTa . This observation supports the supposition that Perplection is agnostic to the pre - trained model used , and shows that it is promising to extrapolate results to other languages .
The results of the extra BiLSTM experiments are shown in Table 4 and 5 . Table 4 shows unexpected results after disabling features . For instance , disabling rank B features caused a larger performance drop than removing rank A features . This suggests that how we created word clouds for each BiLSTM feature ( i.e. , displaying top three words with the highest positive and lowest negative rel- evance ) might not be an accurate way to explain the feature . Nevertheless , another observation from Table 4 is that even when we disabled two - third of the BiLSTM features , the maximum macro F1 drop was less than 5 % . This suggests that there is a lot of redundant information in the features of the BiLSTMs .
If all entities in the KB are represented using such string representations , then the models described in Section 3 can directly be used for arbitrary schemas . This leads to the question : how can we generate string representations for entities from arbitrary KBs such that they can be used for BERT - based models ? Alternatively , what form can f take ?
Model The goal of a statistical language model is to learn the conditional probability of the next word given all ( or a subset of ) the previous ones ( Bengio et al . , 2003 ) . That is , for a sequence of tokens x = ( x 1 , ... , x n ) , the model learns p(x i |x < i ) where x i is the i - th word of sequence x. For this work , we use the 1.63 billion - parameter Conditional Transformer Language Model ( CTRL ) by Keskar et al . ( 2019 ) , which is built on a transformerbased sequence to sequence architecture ( Vaswani et al . , 2017 ) . The CTRL has shown to produce high quality text , is general enough to be adapted for conditioning on the control codes we aim to use , and we do not need to pre - train the weights from scratch . Formally , the CTRL adds an extra condition to each sequence by prepending a control code c , hence learning p(x i |x < i , c ) . The control code is represented by a single token and can then be used to direct the model output at inference . We extend the model from its previous limit of a singletoken control code to accept multiple tokens . For The respective control code is prepended to each sequence of 256 subwords of a document .
To alleviate the above issue , we propose adaptive modeling of the virtual key vectors . For a query Q i , we suggest taking a vector close to Q i itself as the corresponding virtual key vector ( the length of the new prefix is thus 1 ) , in the hope of leading to better inference .
In Study 1 , before scoring answers to a question prompt , the participants were required to answer the following three questions to indicate their interest , familiarity , and perceived difficulty of the question prompt on a rating scale of [ 1,5 ] :
All models are implemented in PyTorch and experiments are run on 1 NVIDIA Tesla T4 GPU . Model hyperparameters are reported in appendix A.5 .
searching , understanding , and analysis ( Zhang and Balog , 2018;Deng et al . , 2019;Sherborne et al . , 2020 ) . Note that in this work , we focus on translating the headers instead of the entire table content , since for each entity in table content , it is hard to decide if it needs to be translated or not . Over translation could even have negative effects in reality . Despite its importance , most research efforts are dedicated to plain text machine translation ( Sutskever et al . , 2014;Bahdanau et al . , 2015;Vaswani et al . , 2017;Yang et al . , 2020 ) , and schema translation is not well studied in the community , to the best of our knowledge . According to our preliminary study , state - of - the - art neural machine translation ( NMT ) systems can not work well on schema translation because of two intrinsic differences between plain text and tabular data : morphological difference and context difference .
Additionally , our models perform worse than Yang et al . ( 2018 ) on the CWEB dataset . This is because this dataset is significantly longer on average than other datasets , i.e. , approximately 1,700 words per document on average , which is more than three times longer than the 512 - word limit that can be handled by BERT - based models including ours . Yang et al . ( 2018 ) achieved excellent performance on this dataset because their model uses various hand - engineered features capturing document - level contextual information .
Previous prompting methods which take singlerelation inputs clearly fail to apply in this iterative setting due to the complexity of the input context q , c 1 , ... , c j−1 . Task - level prompting methods such as Prompt - Tuning ( Lester et al . , 2021 ) and Prefix - Tuning ( Li and Liang , 2021 ) are applicable here , where T is treated as a static parameter . However , as described earlier , this modeling is not ideal for T to fully capture variabilities across different inference steps . In this work , we model T as the output of our Prompter , a learnable function mapping f W which dynamically synthesizes T w.r.t . the current step input context :
The goal of GEC . This is a significant issue . Is it enough to just get a sentence rid of errors ? Taking coding into example , can we say a piece of code " good " when all the " errors " are clear but pages of " warnings " are flashing ? In " Good " samples , we compare the human references and automatically generated sentences , and find many of references are only correct but not so idiomatic . On the other hand , many output sentences of PLM - based ensemble strategies are more natural and like native speakers . If a GEC system is aimed at helping overseas students with their language learning , for example , then idiomaticity should be taken into consideration .
• Wikitoxic : The dataset can be downloaded here 10 . We used only examples which were given the same label by all the annotators .
its posterior p ( Ψ|D ) is in the same parametric family as the prior p ( Ψ|Ω ) . Therefore , given a test utterance x * , the predictive posterior p ( y * |D ) has
Ref1 : He looked like a rapper in drugs .
Experiments on our dataset demonstrate that CAST significantly outperforms state - of - the - art neural machine translation models . Our contributions are summarized as follows .
. GR is the task to recognize the goal from the partial observation of actions . We use a simplified version , where systems observe a partial action sequence and need to figure out if the given goal is the true objective : Given an initial state s , a potential goal g , and a sequence ⃗ a of N actions as the observation , decide if g is the true objective . That is , decide if ⃗ a is a prefix of any optimal plans to achieve g. The context is s and ⃗ a , and the query is g.
High School Examinations ( EXAMS ) Figure 2 shows the average number of options per question for each subset in the datasets . Both the Dev and Test subset have four options , but Train contains questions with three answers coming from online history exams collected from Hardalov et al . ( 2019 ) . These examples also affect the subject distribution for the training set .
In general , the bidirectional encoder shows poor performance for simultaneous MT . This can be explained by the fact that there exists a mismatch between the training condition ( whole source available ) and the inference condition ( only a prefix of the source is available for k < 32 ) . These results are consistent with ( Elbayad et al . , 2020a ) . Keep in mind that this bidirectional model is different from the offline one because it has been subject to the constraints of Eq . 7 during training . As a result of the BLEU scores reported in Figure 2 , the streaming MT system with h = 60 and PBE was used in the rest of the German - English experiments .
We observe that when training on original DS data , the performance of baselines on the RE - DocRED dataset is obviously lower than the Do - cRED dataset . This is because there are more positive instances in the RE - DocRED dataset than in the DocRED dataset , which makes the noise problem more obvious . Thus , the performance improvement of models trained on our denoised data will also be more obvious . The performance improvements of baselines that are fine - tuned on humanannotated data can be seen in Appendix A.2 .
We conducted an error analysis analyzing the model performance w.r.t the different topics of the bills . Our models provides significantly robust performances across most topics in fig . 10 . Furthermore , we analyze the model performance on each legislator of the U.S. Congress . We obtain an average F1 - Score per legislator of 0.889 with a stand deviation of 0.05 . Unsurprisingly , our model performance drops for legislators with less than 8 speeches achieving an average F1 - score of 0.758 with a standard deviation of 0.09
Furthermore , to address the challenges in schema translation , we propose a Context Aware Schema Translation ( CAST ) model , which is a header - to - header neural machine translation model augmented with table context . Specifically , we model a target header and its context as a directed graph to represent their entity types and structural relations . Then CAST encodes the graph with a relational - aware transformer and uses another transformer to decode the header in the target language . The advantages of our approach come from two folds : ( 1 ) The structure relationships make the transformer encoder capture the structural information and learn a contextualized representation for the target header ; ( 2 ) The entity types differentiate the target header from its context and thus help denoise the target header translation .
In the clinical domain , the ability to conduct natural language inference ( NLI ) on unstructured , domainspecific texts such as patient notes , pathology reports , and scientific papers , plays a critical role in the development of predictive models and clinical decision support ( CDS ) systems .
Semantic roles and their properties have received extensive attention in linguistics ( Fillmore , 1968;Levin and Rappaport Hovav , 2005;Dowty , 1991 ) and are considered a universal feature of human language . The size and organization of the role and predicate inventory are subject to debate , giving rise to a variety of role - semantic formalisms .
He becomes infatuated with Madame de Pastourelles , a beautiful and intelligent artist .
To determine whether MedNLI contains annotation artifacts that may artificially inflate the performance of models trained on this dataset , we train a simple , premise - unaware , fastText classifier to predict the label of each premise - hypothesis pair , and compare the performance of this classifier to a majority - class baseline , in which all training examples are mapped to the most commonly occurring class label ( Joulin et al . , 2016;Poliak et al . , 2018;Gururangan et al . , 2018 ) . Note that since annotators were asked to create an entailed , contradictory , and neutral hypothesis for each premise , MedNLI is class - balanced . Thus , in this setting , a majority class baseline is equivalent to choosing a label uniformly at random for each training example .
Given a question Q and options O , following previous works Weir and Durme , 2022 ) , we first convert them into declarative hypotheses { H 1 , . . . , H |O| } . 2 We then try to generate an entailment tree for each hypothesis in a forward chaining manner and select the most plausible option based on the validity and faithfulness of trees .
Both during training and inference , we only retain the 100 most frequent attributes in the respective KBs . The attribute - separators ( Section 4.1 ) are created corresponding to the 100 most frequent attributes in the training KB . Candidates and mentions ( with context ) are represented using strings of 128 sub - word tokens each , across all models .
Similarly , the label - to - text direction is trained on { y , x } pairs from D S by minimizing the standard maximum likelihood loss :
We show detailed results on GLUE and SQuAD in Table 4 and detailed results on LibriSpeech reranking in Table 5 . Following BERT , we do not show results on the WNLI GLUE task , as it is difficult to beat even the majority classifier using a standard fine - tuning - as - classifier approach . We show dev rather than test results on GLUE in the main paper because they are more reliable ; the performance of fine - tuned models varies substantially based on the random seed ( Phang et al . , 2018;Clark et al . , 2019;Dodge et al . , 2020 ) , but GLUE only supports submitting a single model rather than getting a median score of multiple models . While 6 https://worksheets . codalab.org/rest/bundles/ 0x6b567e1cf2e041ec80d7098f031c5c9e/ contents / blob/ 7 https://docs.scipy.org/doc/ scipy / reference / generated / scipy.stats . spearmanr.html
To supplement our analyses , we compute several coarse - grained lexical counts for each story in HIPPOCORPUS . Such approaches have been used in prior efforts to investigate author mental states , temporal orientation , or counterfactual thinking in language ( Tausczik and Pennebaker , 2010;Schwartz et al . , 2015;Son et al . , 2017 ) .
For the final crowdsourcing study , we use Amazon Mechanical Turk . Workers had to take a qualification test , have an acceptance rate of at least 95 % , and location within the US . We paid $ 7.6 per hour ( minimum wage is $ 7.25 per hour ) . Each data sample is annotated by eight crowdworkers . In case the ranker cut off the real aspect(s ) from the list of candidates , crowdworkers could select any sequence up to four tokens from a second list .
Our sentence - level alignment falls under the general problem of bringing two views of inputs from the same source closer in the representation space while keeping those from different sources dissimilar through a contrastive loss . From a crosslingual alignment perspective , we treat an English sequence S en i and its translation S tr i in another language tr ∈ L as two manifestations of the same semantics . At the same time , sentences that are not translations of each other should be further apart in the representation space . Given parallel corpora consisting of { ( S en 1 , S tr 1 ) , . . . , ( S en N , S tr N ) } , we align sentence representations in all the different languages together using MoCo .
Factoid QA by Retrieval This model is a baseline ( Lukovnikov et al . , 2017 ) that individually retrieves the entities and relations based on their embedding - level similarities to input queries . Then , it merges the retrieved entities and relations with the KG - specific schema to construct the triplets .
Model Noise Dist . Binary Classifier Electric Two - Tower Cloze Model σ E(x)t + log k•q(x|x \t ) n ELECTRA Masked LM σ(E(x)t )
Step 3 : Annotation study We use Amazon Mechanical Turk to annotate each sample by eight different workers located in the US , paying $ 7.6 per hour ( minimum wage is $ 7.25 per hour ) . Based on a subset of 232 samples , we compute an α u of .67 between crowdworkers and experts ( three doctoral researchers ) . Compared to the initial study , the new approach increases the inter - annotator agreement between experts by approx . 11 points ( see App . A for further details on the annotation study ) . Based on this promising result , we create a dataset of 5,032 high - quality samples that are labelled with aspects , as well as with their original stance labels from the UKP - Corpus . We show the most frequent ( lemmatized ) aspects that appear in some topics in Table 1 .
To analyse how the MM contributions are affected by fine - tuning , we compare 4 ALBEF 5 models fine - tuned on ( 1 ) image retrieval on MSCOCO , ( 2 ) image retrieval on Flickr30k ( Plummer et al . , 2015 ) , ( 3 ) visual grounding on RefCOCO+ ( Yu et al . , 2016 ) and ( 4 ) VQA ( Goyal et al . , 2017 ) .
The 2018 Duolingo Shared Task on Second Language Acquisition Modeling ( Settles et al . , 2018 ) dataset contains questions and responses for Duolingo users over the first 30 days of learning a second language . The dataset contains three different question types : reverse translate ( free response translation of a given prompt in the language they are learning ) , reverse tap ( a selection - based equivalent of reverse translate ) , and listen , where students listen to a vocal utterance . We focus on the reverse translate question type for English - speaking students learning French and Spanish . The dataset size for French learners ( 1.2k users ) is roughly half the size of that for Spanish learners ( 2.6k users ) .
Table 9 shows the performance of our models when trained and evaluated on the synthesis procedures dataset . Detailed scores by entity type can be found in the Supplementary Material . We chose to use the data split suggested by the authors for the NER task , using 200 documents for training , and 15 documents for each dev and test set . Among the non - BERT - based systems , the BiLSTM variant using both mat2vec and word2vec performs best , indicating that the two pre - trained embeddings contain complementary information with regard to this task . The best performance is reached by the BiL - STM model including word2vec , mat2vec , bpe and SciBERT embeddings , with 92.2 micro - average F1 providing a strong baseline for future work .
where e i is the logit of paragraph i in the evidence selection step , n is the number of paragraphs and
The above two steps of recognizing relevant sentences and marking coarse - grained entity types are in general applicable to a wide range of experiment types within the materials science domain . We now define a set of slot types particular to experiments on SOFCs . During annotation , we mark these slot types as links between the experimentevoking phrase and the respective slot filler ( entity mention ) , see Figure 1 . As a result , experiment frames are represented by graphs rooted in the node corresponding to the frame - evoking element .
While recent results indicate that BERT successfully represents lexical - semantic and grammatical information , the evidence of its high - level semantic capabilities is inconclusive . Tenney et al . ( 2019a ) show that the English PropBank semantics can be extracted from the encoder and follows syntax in the layer structure . However , out of all formalisms PropBank is most closely tied to syntax , and the results on proto - role and relation probing do not follow the same pattern . Kovaleva et al . ( 2019 ) identify two attention heads in BERT responsible for FrameNet relations . However , they find that disabling them in a fine - tuning evaluation on the GLUE ( Wang et al . , 2018 ) benchmark does not result in decreased performance .
The dataset contains 67k train examples from movie reviews .
Dataset shift is a problem where the joint distribution of inputs and outputs differs between training and test stage ( Quionero - Candela et al . , 2009 ) . Many classifiers perform poorly under dataset shift because some of the learned features are inapplicable ( or sometimes even harmful ) to classify test documents . We hypothesize that FIND is useful for investigating the learned features and disabling the overfitting ones to increase the generalizability of the model .
B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? No response .
• aspect_pos : List of string tuples " ( begin , length ) " , marking the character position and length of each aspect within the argument .
Instead of a fixed role label , each argument is assessed via a 11 - dimensional cardinal feature set including Proto - Agent and Proto - Patient properties like volitional , sentient , destroyed , etc . The feature - based approach eliminates some of the theoretical issues associated with categorical role inventories and allows for more flexible modeling of role semantics .
We used human responses on MTurk to assign ranks to features . As each classifier had 30 original features ( d = 30 ) , we divided them into three ranks ( A , B , and C ) each of which with 10 features . We expected that features in rank A are most relevant and useful for the prediction task , and features in rank C least relevant , potentially undermining the performance of the model . To make the annotation more accessible to lay users , we designed the questions to ask whether a given word cloud is ( mostly or partially ) relevant to one of the classes or not , as shown in Figure 3 . If the answer matches how the model really uses this feature ( as indicated by W ) , the feature gets a positive score from this human response . For example , if the CNN feature of the word cloud in Figure 3 is used by the model for the negative sentiment class , the scores of the five options in the figure are -2 , -1 , 0 , 1 , 2 , respectively . We collected ten responses for each question and used the average score to sort the features descendingly . After sorting , the 1 st -10 th features , 11 th -20 th features , and 21 st -30 th features are considered as rank A , B , and C , respectively . 3 To show the effects of feature disabling , we compared the original model M with the modified model M with features in rank X disabled where X ∈ { A , B , C , A and B , A and C , B and C } .
TriviaQA FEVER WoW R - Prec R @ 5 R - Prec R @ 5 R - Prec R @ 5 R - Prec R @ 5 R - Prec R @ However , there is a sharp decline in the performance of TriviaQA , in retrieval metrics . This is true despite the fact that retrieving these passages greatly improves answer accuracy and F1 . This suggests some incompleteness in the provenance ground truth for TriviaQA .
We explore feature - based and neural - based models from two open - source frameworks : QuEst++ : QuEst++ ( Specia et al . , 2015 ) is a feature - based QE framework composed of two modules : a feature extractor module , to extract the relevant QE features from both the source sentences and their translations , and a machine learning module . We only use this framework for our experiments on document - level QE , since it does not perform well enough for sentence - level prediction . We use the same model ( Support Vector Regression ) , hyperparameters and feature settings as the baseline model for the document - level QE task at WMT'18 .
It is known that neural networks suffer from the miscalibration problem between accuracy and confidence ( Guo et al . , 2017;Wang et al . , 2020 ) . Overconfidence can lead to predicting high confidence for poor translations , while under - confidence leads to low confidence for good translations . Here , we evaluate whether CANMT alleviates the over- and under - confidence problem on Zh→En multidomain tests . For each method , we split sentences into five bins according to the predicted scores ( ranging from low quality to high ) and compute the average human scores . We plot these values of CANMT and confidence - based QE methods ( TP and D - TP ) in Figure 3 .
Table 1 : Example of incorrectly machine - translated text : the word shorts is used to indicate short trousers , but gets translated in French as court , the adjective short . Here multimodality could help to detect the error ( extracted from the Amazon Reviews Dataset of McAuley et al . , 2015 ) . creasingly accompanied with visual elements such as images or videos , especially in social media but also in domains such as e - commerce . Multimodality has not yet been applied to QE . Table 1 shows an example from our e - commerce dataset in which multimodality could help to improve QE . Here , the English noun shorts is translated by the adjective court ( for the adjective short ) in French , which is a possible translation out of context . However , as the corresponding product image shows , this product is an item of clothing , and thus the machine translation is incorrect . External information can hence help identify mismatches between translations which are difficult to find within the text . Progress in QE is mostly benchmarked as part of the Conference on Machine Translation ( WMT ) Shared Task on QE . This paper is based on data from the WMT'18 edition 's Task 4 -documentlevel QE . This Task 4 aims to predict a translation quality score for short documents based on the number and the severity of translation errors at the word level ( Specia et al . , 2018a ) . This data was chosen as it is the only one for which meta information ( images in this case ) is available . We extend this dataset by computing scores for each sentence for a sentence - level prediction task . We consider both feature - based and neural state - of - theart models for QE . Having these as our starting points , we propose different ways to integrate the visual modality .
The correlation between MQM - based scores and metric scores , measured using Pearson and Kendalltau correlations on 1400 segments per language as shown in Table 1 . We observe that out of the overlap - based metrics , chrF++ has the highest correlation across all languages , but overall overlapbased metrics are the worst performing which is in line with the findings of Kocmi et al . ( 2022 ) . Among the embedding - based metrics , LabSE embeddings yields better correlations than any of the COMET - metric variants have the highest overall correlations for all the languages .
In this section , we analyze the memory usage of the end - to - end method LED and the proposed CGSN . Assuming the length of a document is L ( L ≥ 4 K ) , the local window size 9 is W ( W ≥ 512 ) , the number of global tokens is G t , the memory usage of LED method is O ( L ( W + G t ) ) . When G t ≪ W , the memory usage is O ( LW ) . For CGSN , set the paragraph number in a segment as B. For a fair comparison , the maximum length of a paragraph is 9 Attention to the W 2 tokens ahead and W 2 tokens behind .
We also evaluate the DecT on CPM - Bee 3 , which is a bilingual generative pre - trained language model with 10B parameters . Table 6 presents the results of CPM - Bee in different settings . The results show that DecT strongly enhances the adaptation of large PLM on downstream tasks . Moreover , CPM - Bee achieves great performance on NLI tasks , which flags that DecT could deal with more difficult tasks with powerful backbone models .
Our second limitation relates to the estimation of legislator 's ideology . Ideology is a latent concept . This means that it can not be directly measured and no ground - truth data exists . Therefore , to validate that our legislator representations encode ideology , we need to prove their performance in a variety of tasks in which the political science literature suggests ideology is important . In our work , we studied three tasks : ( i ) active / passive cosponsorship prediction , ( ii ) party affiliation recovery , and ( iii ) voting prediction . We argue that this is a representative set of tasks . However , legislators are involved in additional ideology - driven tasks , e.g. , the release of public statements . Showing that our representations are also predictive of these additional tasks might be considered an even more robust and convincing validation of our results .
Unlike 20Newsgroups , Amazon Clothes does not seem to have obvious artifacts . Still , the re - sponses from crowd workers suggested that we disable 6 features . The disabled features were correlated to , but not the reason for , the associated class . For instance , one of the disabled features was highly activated by the pattern " my .... year old " which often appeared in positive reviews such as " my 3 year old son loves this . " . However , these correlated features are not very useful for the three outof - distribution datasets ( Music , Mixed , and Yelp ) . Disabling them made the model focus more on the right evidence and increased the average macro F1 for the three datasets , as shown in Figure 8 ( right ) . Nonetheless , the performance improvement here was not as apparent as in the previous task because , even without feature disabling , the majority of the features are relevant to the task and can lead the model to the correct predictions in most cases . 6
iv We release the code and the pre - trained models as part of an open - source framework 1 . ( Kepler et al . , 2019 ) . However , the current state of the art in word - level QE is based on transformers like BERT ( Devlin et al . , 2019 ) and XLM - R ( Conneau et al . , 2020 ) where a simple linear layer is added on top of the transformer model to obtain the predictions ( Lee , 2020 ) . All of these approaches consider quality estimation as a language - specific task and build a different model for each language pair . This approach has many drawbacks in real - world applications , some of which are discussed in Section 1 .
As discussed in Section 3.3 , we use authorship and citation prediction as two additional self - supervised tasks to train our model . Here we discuss some of the details about the implementation of these two tasks . In particular , we first discuss how the data are generated and two how the model performances on these tasks are .
Discussion As shown in Table 2 ( right ) , crossplatform importance of tokens for the hateful class demotes scores ( and thus , ranks ) of lexical artifacts which are likely to be more indicative on some platforms only ( e.g. , " RT " ) , while consolidating the informativeness of cross - platform items ( e.g. , " jews " , " hate " , " # # s " , " # # es " , 7 " people " ) . This confirms our hypothesis that encompassing multiple platforms is beneficial for capturing lexical items that are likely to be predictive across distributions .
The average validation performances of the 8 classification tasks when performing GAP on the OPT LMs are shown in Figure 2 . While GAP fails to provide consistent improvements for 350 M LMs and 2.7B LMs , mostly resulting in a degradation of performance as shown by the median performance underperforming the baselines , the LMs show considerable performance gains in some cases for the larger LMs . This result suggests that although GAP does not show steady improvement of generalization for the classification tasks unlike the dialogue 5 Further study details are in Appendix F .
In this paper , we presented a novel cross - lingual position encoding to augment SANs by considering cross - lingual information ( i.e. , reordering indices ) for the input sentence . We designed two strategies to integrate it into SANs . Experiments indicated that the proposed strategies consistently improve the translation performance . In the future , we plan to extend the cross - lingual position encoding to non - autoregressive MT ( Gu et al . , 2018 ) and unsupervised NMT ( Lample et al . , 2018 ) .
We propose a novel noisy training method called PATS to optimize fine - tuning of PLMs . Since aggressive fine - tuning PLMs will leave a large number of insensitive parameters which contribute little to the overall model , PATS activates them and balance the contributions of all parameters in downstream tasks by adding noise to each parameter according to its sensitivity in the process of training . PATS is a simple mechanism without much computational and memory overhead compared to adversarial training which requires additional backwards passes . Extensive experiments on eight tasks of the GLUE benchmark show that PATS can consistently improve the performance of PLMs on downstream tasks with the sensitivity of the pa - rameters more concentrated , which is especially pronounced on small datasets .
( 1 ) on XNLI , we concatenate the premise with the hypothesis , and add a [ SEP ] token in between .
We present the statistical analysis of all sign language corpora in Table 5 and Table 6 . Out - of - Vocabulary ( OOV ) are the words that only appear in development or test set and singletons are the least frequent words appearing only once .
MolXPT 80.0 ± 0.5 77.1 ± 0.2 95.3 ± 0.2 78.1 ± 0.4 88.4 ± 1.0 71.7 ± 0.2 81.9 Prompt - based finetuning : MolXPT can be finetuned for downstream tasks about molecules and text . Adding classification or regression heads to pre - trained backbone models introduces the gap between pre - training and finetuning ( Brown et al . , 2020 ; Gu et al . , 2022 ) . Therefore , we adopt prompt - based finetuning ( Gao et al . , 2021 ) to unify different tasks into a sequence generation task , which is consistent with the pre - training objective . Briefly , given a task , we convert the input and output into text and / or SMILES sequences , equip the sequences with task - specific prompts and finetune using language modeling loss .
-DOCSTART- Global Entity Disambiguation with BERT
• the novel method used for developing Wino - Queer from a community survey , which can be extended to develop bias benchmarks for other marginalized communities .
Entity linking consists of linking mentions of entities found in text against canonical entities found in a target knowledge base ( KB ) . Early work in this area was motivated by the availability of large KBs with millions of entities ( Bunescu and Paşca , 2006 ) . Most subsequent work has followed this tradition of linking to a handful of large , publicly available KBs such as Wikipedia , DBPedia ( Auer et al . , 2007 ) or the KBs used in the now decade - old TAC - KBP challenges ( McNamee and Dang , 2009;Ji et al . , 2010 ) . As a result , previous work always assumes complete knowledge of the schema of the target KB that entity linking models are trained for , i.e. how many and which attributes are used to represent entities in the KB . This allows training supervised machine learning models that exploit the schema along with labeled data that link mentions to this a priori known KB . However , this strong assumption breaks down in scenarios which require linking to KBs that are not known at training time . For example , a company might want to automatically link mentions of its products to an internal KB of products that has a rich schema with several attributes such as product category , description , dimensions , etc . It is very unlikely that the company will have training data of this nature , i.e. mentions of products linked to its database .
Data Filter . All these raw posts are then preprocessed by employing the data filtering rule . For text data , we remove text with fewer than 3 words , correct the spelling mistakes , and check if each text is composed of illegible characters via the NLTK package ( Bird et al . , 2009 ) . For their visual counterparts , we remove the images with low resolution and resize all images to the same size .
The results of this experiment are displayed in Figure 7 . For Biosbias , on average , the participants ' responses suggested us to disable 11.33 out of 30 CNN features . By doing so , the FPED of the models decreased from 0.250 to 0.163 , and the FNED decreased from 0.338 to 0.149 . After investigating the word clouds of the CNN features , we found that some of them detected patterns containing both gender - related terms and occupation - related terms such as " his surgical expertise " and " she supervises nursing students " . Most of the MTurk participants answered that these word clouds were relevant to the occupations , and thus the corresponding features were not disabled . However , we believe that these features might contain gender biases . So , we asked one annotator to consider all the word clouds again and disable every feature for which the prominent n - gram patterns contained any genderrelated terms , no matter whether the patterns detect occupation - related terms . With this new disabling policy , 12 out of 30 features were disabled on average , and the model biases further decreased , as shown in Figure 7 ( Debugged ( One ) ) . The sideeffect of disabling 33 % of all the features here was only a slight drop in the macro F1 from 0.950 to 0.933 . Hence , our framework was successful in reducing gender biases without severe negative effects in classification performance .
Schema translation is the task of automatically translating headers of tabular data from one language to another . High - quality schema translation plays an important role in crosslingual table searching , understanding and analysis . Despite its importance , schema translation is not well studied in the community , and state - of - the - art neural machine translation models can not work well on this task because of two intrinsic differences between plain text and tabular data : morphological difference and context difference . To facilitate the research study , we construct the first parallel dataset for schema translation , which consists of 3,158 tables with 11,979 headers written in 6 different languages , including English , Chinese , French , German , Spanish , and Japanese . Also , we propose the first schema translation model called CAST , which is a header - to - header neural machine translation model augmented with schema context . Specifically , we model a target header and its context as a directed graph to represent their entity types and relations . Then CAST encodes the graph with a relational - aware transformer and uses another transformer to decode the header in the target language . Experiments on our dataset demonstrate that CAST significantly outperforms state - of - the - art neural machine translation models . Our dataset will be released at https://github.com/microsoft/ContextualSP .
assuming that the bias term b k is distributed equally to the n neurons . LRP works by propagating the activation of a neuron of interest back through the previous layers in the network proportionally . We call the value each neuron receives a relevance score ( R ) of the neuron . To back propagate , if the relevance score of the neuron k is R k , the relevance score that the neuron j receives from the neuron
All arguments of the training documents are tokenized with a BPE model ( Sennrich et al . , 2016 ) trained by the authors of the CTRL ( Keskar et al . , 2019 ) . Both the Arg - CTRL CC and the Arg - CTRL REDDIT are fine - tuned on a Tesla V100 with 32 GB of Memory . We mainly keep the default hyperparameters but reduce the batch size to 4 and train both models for 1 epoch . Each model takes around five days to train on the 1.6 M training sentences .
Evaluation Metrics . We evaluate the performances of different models with the 4 - gram BLEU ( Papineni et al . , 2002 ) score of the translations . Following the evaluation step in M2M-100 , before computing BLEU , we de - tokenize the data and apply standard tokenizers for each language . We use SacreBLEU tokenizer for Chinese , Kytea 4 for Japanese , and Moses tokenizer 5 for the rest of the languages . Besides BLEU , we also conduct a human evaluation for a more precise analysis .
-DOCSTART- Translating Headers of Tabular Data : A Pilot Study of Schema Translation
This section includes additional results from our analysis of knowledge acquisition during multilingual pretraining :
We demonstrate that our measures can uncover differences in imagined and recalled stories in HIPPOCORPUS . Imagined stories contain more commonsense events and elaborations , whereas recalled stories are more dense in concrete events . Additionally , imagined stories flow substantially more linearly than recalled stories . Our findings provide evidence that surface language reflects the differences in cognitive processes used in imagining and remembering .
We pretrain LMRec TL + agnostic with OBS , OTA , and ECOMM and then transfer to the product collection recommendation ( target task ) . The mean pooled task - specific and task - agnostic user features are used as the final user features . During the 14 days of online experimentation , we measured two important metrics for the online recommender system , CTR and GMV , to track user satisfaction with the platform . CTR represents the click / view rate of recommendation , and GMV is the total value of sold products through recommendation . All models take the same amount of user traffic . Section 3 descrbies it . We used Amazon review dataset proposed in https://nijianmo.github.io/amazon/.
As shown in Fig . 2 , we propose two strategies to integrate the cross - lingual position encoding ( XL PE ) into SANs : inputting - level XL ( InXL ) SANs and head - level ( HeadXL ) SANs .
where B * ∈ R K×H and b * o ∈ R K consist of the entity token embeddings and the bias corresponding to the entity candidates , respectively . Note that B * and b * o are the subsets of B and b o , respectively . Global ED Model . Our global ED model resolves mentions sequentially for N steps ( see Algorithm 1 ) . First , the model initializes the entity of each mention using the [ MASK ] token . Then , for each step , it predicts an entity for each [ MASK ] token , selects the prediction with the highest probability produced by the softmax function in Eq.(3 ) , and resolves the corresponding mention by assigning the predicted entity to it . This model is denoted as confidence - order . We also test a model that selects mentions according to their order of appearance in the document and denote it by natural - order .
Besides , previous studies have probed the influence of word order perturbation on natural lan - guage understanding ( NLU ) tasks ( Abdou et al . , 2022 ; Pham et al . , 2021 ; Clouâtre et al . , 2021 ) to determine whether these tasks are sensitive to word order . The findings indicate that some NLU tasks do require word order information although others do not . It should also be noted that word order is particularly important for natural language generating ( NLG ) tasks , such as machine translation . This is because metrics used to evaluate generated results , such as BLEU ( Papineni et al . , 2002 ) , are sensitive to word order . Therefore , word order is a critical aspect of natural language processing .
Previous approaches involving actions in NLP are more application - centric and usually contain specific tasks ( e.g. instruction following , prediction ) ( Li et al . , 2021 ; Zellers et al . , 2021 ) without considering both preconditions and effects . In general these tasks are not verified logically . On the other hand , there exist some template - based datasets , such as the one in ( Clark et al . , 2020 ) , which are logically sound and generated from several simple rules . Unfortunately , they pay more attention to general deduction in a static perspective , while RAC problems require repeated changes .
Twitter We additionally collect a dataset of 2.1 M political tweets from Twitter from the past 10 years using the Twitter Decahose stream , selecting tweets by political figures included in a list of 9,981 US politicians and their Twitter handles ( Panda et al . , 2020 ) . In contrast to AllSides , Twitter does not explicitly annotate discrete ideologies . Thus , we label tweets with their author 's ideology , identified based on their DW - NOMINATE 9 dimension ( Boche et al . , 2018 ) , a measure of a politician 's voting history : a positive number indicates conservative leaning ( e.g. Donald Trump , 0.403 ) , while a negative number indicates liberal leaning ( e.g. Barack Obama , -0.343 ) .
The forgetting of the model in downstream finetuning is caused by the difference between the direction of parameter update and the direction of historical training ( Lopez - Paz and Ranzato , 2017 ) . Inspired by Kurita et al . ( 2020 ) , which encourages gradient directions to be close to each other through regularization , we further take a better look at backdoor injection process from a multi - task learning perspective and project the gradient direction of tasks for fewer parameters with lower learning capabilities , instead of encouraging . We propose Intra - Layer gradient direction Projection ( ILProj ) as shown in Figure 6 .
We generate unnatural paraphrase x ′ i+1 by replacing randomly sampled non - stop English words from x i+1 with their synonyms from Wordnet . For synonym sampling , we rank the Wordnet synonyms according to word2vec embedding ( Mikolov et al . , 2013 ) similarity and sample from the top k synonyms . To make paraphrasing unnatural , 1 / 4 th of words are replaced with synonyms which are least likely to be used by humans while keeping it grammatically correct . ( See the table 2 for example . )
For evaluation , we used the approach proposed in the WMT shared tasks in which the classification performance is calculated using the multiplication of F1 - scores for the ' OK ' and ' BAD ' classes against the true labels independently : words in the target ( ' OK ' for correct words , ' BAD ' for incorrect words ) , gaps in the target ( ' OK ' for genuine gaps , ' BAD ' for gaps indicating missing words ) and source words ( ' BAD ' for words that lead to errors in the target , ' OK ' for other words ) . In recent WMT shared tasks , the most popular category was predicting quality for words in the target . Therefore , in Section 5 we only report the F1 - score for words in the target . Other results are presented in the supplementary material . Prior to WMT 2019 , organisers provided separate scores for gaps and words in the target , while after WMT 2019 they produce a single result for target gaps and words . We follow this latter approach .
First , we built a BTG - based reordering model ( Neubig et al . , 2012 ) to generate a reordered source sentence according to the word order of its corresponding target sentence . Second , we obtained the reordered word indices pos XL that correspond with the input sentence X. To output the cross - lingual position matrix PE XL , we inherit the sinusoidal function in Eq . ( 1 ) . Formally , the process is :
For the final answer , we use the solve rate metric to evaluate whether the model generates the final correct answer to each MWP . Since generating meaningful steps is also key , we use the BLEU metric ( Papineni et al . , 2002 ) to evaluate language generation quality . For intermediate steps , we use the equation match accuracy ( ACC - eq ) metric to evaluate whether a generated step contains a math expression ( including numbers ) that matches the ground truth . Since LMs generate math equations as strings , we decompose the equation string into tokens and calculate the token level match rate instead of the overall string match . We also use the operation match accuracy ( ACC - op ) metric to evaluate whether a generated step 's operation label matches the ground truth .
while uniformity measures how well the embeddings are uniformly distributed in the representation space :
As computing the exact likelihood is intractable , training energy - based models such as Electric with standard maximum - likelihood estimation is not possible . Instead , we use ( conditional ) Noise - Contrastive Estimation ( NCE ) ( Gutmann and Hyvärinen , 2010;Ma and Collins , 2018 ) , which provides a way of efficiently training an unnormalized model that does not compute Z θ ( x \t ) . NCE learns the parameters of a model by defining a binary classification task where samples from the data distribution have to be distinguished from samples generated by a noise distribution q(x t |x \t ) . First , we define the un - normalized output p θ ( x t |x \t ) = exp ( −E(x ) t )
There are two common sampling algorithms for contrastive learning :
Hello everyone , thank you for your patience as we waited for everyone to arrive . I am the study leader . You are about to participate in a study on negotiation , and you will be paid for your participation via an Amazon eGiftcard , privately emailed to you by the Yale SOM Behavioral Lab within two business days after the conclusion of the study .
Language models ( Bengio et al . , 2003 ) allow to generate text through learned distributions of a language and have been applied to a variety of areas like machine translation ( Bahdanau et al . , 2015 ) , summarization ( Paulus et al . , 2018 ) , or dialogue systems ( Wen et al . , 2017 ) . A rather new field for these models is the task of producing text with argumentative content ( Wang and Ling , 2016 ) . We believe this technology can support humans in the challenging task of finding and formulating arguments . A politician might use this to prepare for a debate with a political opponent or for a press conference . It may be used to support students in writing argumentative essays or to enrich one - sided discussions with counter - arguments . In contrast to retrieval methods , generation allows to combine and stylistically adapt text ( e.g. arguments ) based on a given input ( usually the beginning of a sentence ) . Current argument generation models , however , produce lengthy texts and allow the user little control over the aspect the argument should address Hua and Wang , 2018 ) . We show that argument generation can be enhanced by allowing for a fine - grained control and limiting the argument to a single but concise sentence .
Quality Estimation ( QE ) is the task of assessing the quality of a translation without having access to a reference translation ( Specia et al . , 2009 ) . Translation quality can be estimated at different levels of granularity : word , sentence and document level ( I ve et al . , 2018 ) . So far the most popular task has been sentence - level QE , in which QE models provide a score for each pair of source and target sentences . A more challenging task , which is currently receiving a lot of attention from the research community , is word - level quality estimation . This task provides more fine - grained information about the quality of a translation , indicating which words from the source have been incorrectly translated in the target , and whether the words inserted between these words are correct ( good vs bad gaps ) . This information can be useful for post - editors by indicating the parts of a sentence on which they have to focus more .
Text features : For the feature - based approach , we extract the same 15 features as those for the baseline of WMT'18 at document level . For the neural - based approaches , text features are either the learned word embeddings ( BiRNN ) or pre - trained word embeddings ( BERT - BiRNN ) .
In this section , we provide additional details to facilitate reproducing our results and findings . We submit data and code as supplementary material and commit to make them publicly available upon acceptance to facilitate reproduction .
Most studies on word - level Quality Estimation ( QE ) of machine translation focus on languagespecific models . The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language - specific models . To overcome these problems , we explore different approaches to multilingual , word - level QE . We show that multilingual QE models perform on par with the current language - specific models . In the cases of zeroshot and few - shot QE , we demonstrate that it is possible to accurately predict word - level quality for any given new language pair from models trained on other language pairs . Our findings suggest that the word - level QE models based on powerful pre - trained transformers that we propose in this paper generalise well across languages , making them more useful in real - world scenarios .
The neural architectures of our models are identical to BERT - Base ( Devlin et al . , 2019 ) , although we believe incorporating additions such as relative position encodings ( Shaw et al . , 2018 ) • QNLI : Question Natural Language Inference ; constructed from SQuAD ( Rajpurkar et al . , 2016 ) . The task is to predict whether a context sentence contains the answer to a question sentence . The dataset contains 108k train examples from Wikipedia .
In the following , we describe the architecture and the training process of the Arg - CTRL and analyze its performance .
where REPLACE(x , t , x ) denotes replacing the token at position t with x and V is the vocabulary , in practice usually word pieces ( Sennrich et al . , 2016 ) . Unlike with BERT , which produces the probabilities for all possible tokens x using a softmax layer , a candidate x is passed in as input to the transformer . As a result , computing p θ is prohibitively expensive because the partition function Z θ ( x \t ) requires running the transformer |V| times ; unlike most EBMs , the intractability of Z θ ( x \t ) is due to the expensive scoring function rather than having a large sample space .
( 1 / 3 ) A reference to Evil Knievel , a stuntman who jumps motorcycles over cars . This pile of cars is going to be an obstacle in an Evil Knievel stunt , and the man in the top car is hoping that the stuntman coordinating this both knows how to jump over and also how to get the drivers out of this precarious situation .
Results With no other previous work on explicit control of argument generation ( to the best of our knowledge ) , we decide to proof our concept of aspect - controlled neural argument generation by 7 Not counting non - arguments from the splits .
Token type ( ttype ) predicts the type of a word . This requires contextual processing since a word might consist of several wordpieces ; Token position ( token.ix ) predicts the linear position of a word , cast as a regression task over the first 20 words in the sentence . Again , the task is non - trivial since it requires the words to be assembled from the wordpieces . Part - of - speech ( pos ) predicts the languagespecific part - of - speech tag for the given token . Lexical unit ( lex.unit ) predicts the lemma and POS of the given word -a common input representation for the entries in lexical resources . We extract coarse POS tags by using the first character of the language - specific POS tag .
( ii ) premise and hypothesis contradict ; or ( iii ) they are neutral . This setting makes the task suitable to be modeled as a text classification task .
The framework proposed in this work is an encoderdecoder based generative model . It is thus more time - consuming than standard discriminative models for training and evaluation , which in turn results in higher carbon footprint . Specifically , we run our experiments on 1 single NVIDIA RTX A6000 with significant CPU resources . The training time for our model is usually around 5 hours .
The gap between XLM - R and mBERT is reduced by 1 point absolute by the SlavicBERT 's additional fine - tuning on downstream tasks . Although the largest improvement is observed for NER tasks , 3 we see an increase by 1 - 2 points also on Cred.-N , Fake - N , and XNLI , compared to mBERT . However , we also see that the downstream pre - fine - tuning is not beneficial for all tasks ( Poth et al . , 2021 ) , and we see a drop in performance for ranking ( CT21.T1 ) and question answering 4 ( EXAMS ) tasks .
In this paper , we propose a new challenging translation task called schema translation , and construct the first parallel dataset for this task . To address the challenges for this new task , we propose CAST , which uses a relational - aware transformer to encode a header and its context over predefined relationships , making it aware of the table context .
Extensive experiments and ablations demonstrate the effectiveness of our proposed approach . Our future work includes exploration of various text segmentation techniques to improve our understanding of the latent document structure . Another direction would be to extend our study to the realm of neural abstractive summarization with the help of learned document structure .
We follow the evaluation method of the WMT QE tasks : Pearson 's r correlation as the main metric ( Graham , 2015 ) , Mean - Absolute Error ( MAE ) and Root - Mean - Squared Error ( RMSE ) as secondary metrics . For statistical significance on Pearson 's r , we compute Williams test ( Williams , 1959 ) as suggested by Graham and Baldwin ( 2014 ) .
Ablation Table 5 shows the contribution of each component of our method on XNLI . Removing TLM ( -TLM ) consistently leads to about 1 % accuracy drop across the board , showing positive effects of the word - alignment objective . To better understand TLM 's consistent improvement , we replace TLM with MLM ( repl TLM w/ MLM ) , where we treat S en i and S tr i from the parallel corpora as separate monolingual sequences and perform MLM on each of them . The masking scheme is the same as TLM described in Section 2 . We observe that MLM does not bring significant improvement . This confirms that the improvement of TLM is not from the encoders being trained with more data and iterations . Instead , the word - alignment nature of TLM does help the multilingual training .
In the past years , deep learning models have greatly improved their performances on a large range of question answering tasks , especially using pretrained models such as BERT ( Devlin et al . , 2019 ) , RoBERTa ( Liu et al . , 2019 ) and T5 ( Raffel et al . , 2020 ) . More recently , these models have shown even better performances when fine - tuned on multiple question answering datasets at once . Such a model is UnifiedQA ( Khashabi et al . , 2020 ) , which , starting from a T5 model , is trained on a large number of question answering datasets including multiple choices , yes / no , extractive and abstractive question answering . UnifiedQA is , at the time of writing , state - of - the - art on a large number of question answering datasets including multiple - choice datasets like OpenBookQA ( Mihaylov et al . , 2018 ) or ARC . However , even if Uni - fiedQA achieves good results on previously unseen datasets , it often fails to achieve optimal performances on these datasets until it is further finetuned on dedicated human annotated data . This tendency is increased when the target dataset deals with questions about a very specific domain .
' not every medicine is for everyone , but as one who has prescribed most of the major pharmaceuticals for major depression , panic attacks , severe anxiety and anxiety related bouts of obsessive compulsive disorder , i can tell you lexapro is the only medicine that i 've been able to stay on and be effective for my mental well - being ... it is the only one i 've had no side effects . pill antidepressants have either : made me more anxious and / or depressed , dry mouth , bad weight gain , or extreme fatigue making me into a walking zombie during the day . i 've been on lexapro 6 years F ( s adv , l = " 10.0 " ) = 1.0 Cos . = 0.04
Translation memory ( TM ) is basically a database of segmented and paired source and target texts that translators can access in order to re - use previous translations while translating new texts ( Christensen and Schjoldager , 2010 ) . For human translators , such similar translation pieces can lead to higher productivity and consistency ( Yamada , 2011 ) . For machine translation , early works mainly contributes to employ TM for statistical machine translation ( SMT ) systems ( Simard and Isabelle , 2009 ; Utiyama et al . , 2011 ; Liu et al . , 2012 . Recently , as neural machine translation ( NMT ) model ( Sutskever et al . , 2014 ; Vaswani et al . , 2017 ) has achieved impressive performance in many * Corresponding author . translation tasks , there is also an emerging interest ( Gu et al . , 2018 ) in retrieval - augmented NMT model .
Furthermore , as reported in Table 6 , we observe that the increment in the multi - aspect setting is much larger than that in the single - aspect setting when compare Exp with other strategies . To sum up , the strategy of context augmentation in our COM - MRC is effective .
As a reference , T0 ( Sanh et al . , 2022 ) models andFlan - T5 ( Chung et al . , 2022 ) are all based on the original T5 model by Raffel et al . ( 2019 ) . The pretraining corpus is the C4 corpus ( Raffel et al . , 2019 ) of 800 GB of texts based on CommonCrawl . They encode the corpus with a cased vocabulary of 32k BPE tokens .
During inference , we obtain the relation ( s ) of a given entity pair by thresholding the predicted probabilities , following most previous work .
Our extensive experiments on a wide range of ED datasets demonstrate its effectiveness .
Self - Attention The SANs compute the attention of each pair of elements in parallel . It first converts the input into three matrices Q , K , V , representing queries , keys , and values , respectively :
The main contributions of this paper are as follows : ( i ) we introduce the task of Multimodal QE ( MQE ) for MT as an attempt to improve QE by using external sources of information , namely images ; ( ii ) we propose several ways of incorporating visual information in neural - based and featurebased QE architectures ; and ( iii ) we achieve the state - of - the - art performance for such architectures in document and sentence - level QE .
where n - best(f , s ) consists of the top n ( we use n = 100 ) predictions from the speech recognition model found with beam search , f ( x|s ) is the score the speech model assigns the candidate output sequence x. We select the best λ on the dev set out of [ 0.05 , 0.1 , ... , 0.95 , 1.0 ] , with different λs selected for the " clean " and " other " portions of the data .
Our method has the advantage of being more robust and better aware of its own competency , while extra - estimation is able to capture the supervised knowledge of the labeled data . One interesting question is whether they can be complementary .
One limitation of the zero - shot QE is its inability to perform when the language direction changes . In the scenario where we performed zero - shot learning from De - En to other language pairs , results degraded considerably from the bilingual result . Similarly , the performance is rather poor when we test on De - En for the multilingual zero - shot experiment as the direction of all the other pairs used for training is different . This is in line with results reported by Ranasinghe et al . ( 2020b ) for sentence level .
Electric is closely related to the ELECTRA pretraining method . ELECTRA also trains a binary classifier ( the " discriminator " ) to distinguish data tokens from noise tokens produced by a " generator " network . However , ELECTRA 's classifier is simply a sigmoid layer on top of the transformer : it models the probability of a token being negative ( i.e. , as replaced by a noise sample ) as σ(E(x ) t ) where σ denotes the sigmoid function . Electric on the other hand models this probability as
GLUE . CoLA ( Warstadt et al . , 2019 ) and SST-2 ( Socher et al . , 2013 ) are single - sentence tasks ; MRPC ( Dolan and Brockett , 2005 ) and QQP are paraphrase detection tasks ; MNLI ( Williams et al . , 2018 ) , QNLI ( Rajpurkar et al . , 2016 ) , andRTE ( Dagan andGlickman , 2005 ; Haim et al . , 2006 ; Giampiccolo et al . , 2007Giampiccolo et al . , , 2008Bentivogli et al . , 2009 ) are inference tasks ; STS - B ( Cer et al . , 2017 ) is a sentence similarity task . We report the accuracy for SST-2 , MNLI , QNLI , RTE , and STS - B , and the Matthews correlation coefficient for CoLA . Both the accuracy and the F-1 score are included for MRPC and QQP .
A For every submission : A1 . Did you describe the limitations of your work ?
Experiment Settings . We use BioBERT as the PLM on SciDocs , and BERT - base - uncased as the PLM on Amazon and Twitter . The embedding dimension of u w is 768 ( the same as e w ) ; the number of negative samples b = 5 . In ensemble ranking , the length of the general / local ranking list M = 100 ; the hyperparameter ρ in Eq . ( 6 ) is set as 0.1 ; the number of iterations T = 4 ; after each iteration , we increase the size of S i by N = 3 . We use the top-10 ranked terms in each topic for final evaluation ( i.e. , |S i | = 10 in Eqs . ( 8 ) - ( 11 ) ) . Experiments are run on Intel Xeon E5 - 2680 v2 @ 2.80GHz and one NVIDIA GeForce GTX 1080 .
For the CoNLL dataset , we also test the performance using PPRforNED entity candidates ( Pershina et al . , 2015 ) . We report the in - KB accuracy for the CoNLL dataset and the micro F1 score ( averaged per mention ) for the other datasets . Further details of the datasets are provided in Appendix C. Furthermore , we optionally fine - tune the model by maximizing the log likelihood of the ED predictions ( ŷ ED ) using the training set of the CoNLL dataset with the KB+YAGO candidates . We mask 90 % of the mentions and fix the entity token embeddings ( B and B * ) and the bias ( The model is trained for two epochs using AdamW. Additional details are provided in Appendix B. Our global models consistently perform better than the local model , demonstrating the effectiveness of using global contextual information even if local contextual information is captured using expressive BERT model . Moreover , the confidenceorder model performs better than the natural - order model on most datasets . An analysis investigating why the confidence - order model outperforms the natural - order model is provided in the next section .
We thank the first author 's dissertation committee members , Drs . Mausam and Samar Husain , as well as the Cornell C.Psyd members for their insightful comments and suggestions on this work . We thank Rupesh Pandey 's logistical assistance in gathering the human judgment data for this work . We are also grateful for the thorough feedback provided by the anonymous reviewers of EMNLP 2022 , ACL ARR 2021 , and COMCO 2021 . Finally , the last two authors also thank extramural funding from the Department of Science and Technology of India through the Cognitive Science Research Initiative ( project no . DST / CSRI / 2018 / 263 ) .
We collected responses from MTurk workers using the same user interfaces as in Experiment 2 . Simply put , we asked the workers to select a class which was relevant to a given word cloud and checked if the majority vote agreed with the weights in W.
The models also use the text embedding at each layer in different ways . In MHGRN , it is used to calculate the relevance of each path when creating the new embedding for each node . In QA - GNN , a pseudo - node initialised with this embedding is added to the graph , allowing it to participate in message passing with the other nodes .
In Table 1 we see a sudden drop in correlations for N RESLN . Although this method considers vector norms and residuals , incorporating LN # 1 in the encoder seems to have deteriorated the accuracy for token attribution analysis . To determine whether this deterioration of correlation in aggregated attributions is also present in individual single layers , we compare the HTA maps as a baseline with the attribution matrices extracted from different analysis methods . Figure 4 shows the correlation of HTA attribution maps with the maps obtained by N RES , N RESLN , and N ENC methods . The results indicate that N RESLN exhibits a significantly lower association .
1 Our code will be available at https : / / github.com / SupstarZh / WhitenedCSE . Meanwhile , in ( d ) , the positive samples after SGW ( red ) obtain higher diversity than the original bert features ( green ) . Using these diverse positive samples for contrastive learning , the proposed WhitenedCSE achieves better alignment .
k•q(xt|x \t ) ( n−k)•p θ ( xt|x noised \t ) + k•q(xt|x \t ) if t ∈ R − log ( n−k)•p θ ( xt|x noised \t ) ( n−k)•p θ ( xt|x noised \t ) + k•q(xt|x \t ) otherwise
To see the impact of correctly - paired inputs and labels in the demonstrations - which we call the ground truth input - label mapping - we compare the following three methods . 4 No demonstrations is a typical zero - shot method that does not use any labeled data . A prediction is made via argmax y∈C P ( y|x ) , where x is the test input and C is a small discrete set of possible labels .
WMT'18 QE Task 4 data : This dataset was created for the document - level track . It contains a sample of products from the Amazon Reviews Dataset ( McAuley et al . , 2015 ) taken from the Sports & Outdoors category . ' Documents ' consist of the English product title and its description , its French machinetranslation and a numerical score to predict , namely the MQM score ( Multidimensional Quality Metrics ) ( Lommel et al . , 2014 ) . This score is computed by annotating and weighting each word - level translation error according to its severity ( minor , major and critical ):
B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be . Not applicable . Left blank . D1 . Did you report the full text of instructions given to participants , including e.g. , screenshots , disclaimers of any risks to participants or annotators , etc . ? Not applicable . Left blank .
Although we are not aware of any systematic studies dedicated to the effect of formalism on probing results , the evidence of such effects is scattered across the related work : for example , the aforementioned results in Tenney et al . ( 2019a ) show a difference in layer utilization between constituents - and dependency - based syntactic probes and semantic role and proto - role probes . It is not clear whether this effect is due to the differences in the underlying datasets and task architecture or the formalism per se .
The BiRNN model uses an encoder - decoder architecture : it takes on its input both the source sentence and its translation which are encoded separately by two independent bi - directional Recurrent Neural Networks ( RNNs ) . The two resulting sentence representations are then concatenated as a weighted sum of their word vectors , generated by an attention mechanism . For sentence - level predictions , the weighted representation of the two input sentences is passed through a dense layer with sigmoid activation to generate the quality estimates . For document - level predictions , the final representation of a document is generated by a second attention mechanism , as the weighted sum of the weighted sentence - level representations of all the sentences within the document . The resulting document - level representation is then passed through a dense layer with sigmoid activation to generate the quality estimates .
Conventional entity linking models are trained and evaluated on the same KB , which is typically Wikipedia , or derived from Wikipedia ( Bunescu and Paşca , 2006 ; . This limited scope allows models to use other sources of information to improve linking , including alias tables , frequency statistics , and rich metadata .
The two tables show the downstream performance aligned with the model size of paraphrasers . We found it that large models ( davinci ) models only with a reasonable prompt show the advantages over the other smaller models that have much less parameters .
-DOCSTART- FIND : Human - in - the - Loop Debugging Deep Text Classifiers
able . Unseen languages can be added post - hoc , with no measurable drop in performance on XNLI . By pre - training the model in a modular fashion , we thus mitigate negative interference of idiosyncratic information , while simultaneously preparing the model to be extendable to unseen languages .
evolution pattern will satisfy the Equation 20 . Notice that our relation patterns defined are unconcerned with some specific entities , but focusing on the general rules among relations inside the universal entities .
Step 2a : Data preparation for ranking To create training data for the ranker , we use a simple heuristic to calculate scores between 0 and 1 for all N - grams of a sentence by dividing the number of aspect tokens within an N - gram by its length N : # aspect tokens N ∈ [ 0 , 1 ] . Our analysis reveals that 96 % ( 783 of 814 ) of all aspects in the preliminary annotation dataset only contain one to four tokens . We thus decide to ignore all candidates with more than four tokens . No other limitations or filtering mechanisms are applied .
To explain a prediction of a CNN text classifier , we propagate an activation value of the output node back to the word embedding matrix . After that , the relevance score of an input word equals the sum of relevance scores each dimension of its word vector receives . However , in this paper , we want to analyze the hidden features rather than the output , so we start back propagating from the hidden features instead to capture patterns of input words which highly activate the features .
Unlike the interface in Figure 3 , for each word cloud , we asked the participants to select the relevant class from three options ( Biosbias : surgeon , nurse , it could be either / Waseem : abusive , nonabusive , it could be either ) . The feature will be disabled if the majority vote does not select the class suggested by the weight matrix W. To ensure that the participants do not use their biases while answering our questions , we firmly mentioned in the instructions that gender - related terms should not be used as an indicator for one or the other class .
We only had one human evaluator . Further demographic and geographic characteristics are not relevant to the experiment , given the current state of research , and would unnecessarily reveal personal information of the evaluator .
Furthermore , considering the prompt tuning method does predictions by decoding label tokens , we need to check whether skill neurons depend on the label words used . If so , it indicates that the skill neurons do not encode the skills for handling tasks but encode the skills for selectively decoding some words . We rule out this possibility by finding that if we use different random words as label words , the Table 3 : Accuracies ( % ) on various tasks of top skill neurons found with random prompts and untuned hard prompts , compared to random guess and random model . We also report standard deviations over 5 random trials .
Given the social focus of our stories , we use the social commonsense knowledge graph ATOMIC ( Sap et al . , 2019 ) . 4 For each story , we first match possible ATOMIC events to sentences by selecting events that share noun chunks and verb phrases with sentences ( e.g. , " getting married " " Per - sonX gets married " ; Figure 1 ) . We then search the matched sentences ' surrounding sentences for commonsense inferences ( e.g. , " be very happy " " happy " ; Figure 1 ) . We describe this algorithm in further detail in Appendix B.2 . In our analyses , the measure quantifies the number of story sentences with commonsense tuple matches in the two preceding and following sentences .
Informed by our comprehensive evaluation , we augment the multilingual XNLI dataset ( Conneau et al . , 2018 ) with highlight - based explanations by extracting highlights for the English part of XNLI and projecting along word alignments to other languages . We perform a plausibility evaluation with the resulting dataset , which we dub e - XNLI , and perform a human evaluation on a subset of the dataset to validate its adequacy .
We provide curriculum samples , and additional details for quantitative and qualitative experiments . For quantitative , we provide details about parameter ranges for model selection . For qualitative experiments , we provide results on topic modeling , show counts for attended courses from our attention module , comparative word cloud analysis with TF - IDF , and attention weights for the best baseline competitor .
Answer generator . P is an input summary representation , formed by concatenating p 1 , . . . , p N . The answer generator takesP and outputs the final answer autoregressively . Specifically , it outputs the sequence probability for y as follows : P ( y|x , P ) = T j=1 p ( y j |y < j , x , P ) .
Results confirm that adding structure to the candidate string representations via [ SEP ] tokens leads to more accurate models compared to generating strings by concatenation ( Table 3 ) . Using attributeseparators instead of [ SEP ] tokens leads to an absolute gain of over 5 % and handling unseen attributes via attribute - OOV further increases the accuracy to 56.2 % , a 7.1 % increase over the [ SEP ] baseline . These results show that the attributeseparators capture meaningful information about attributes , even when only a small number of attributes from the training data ( 15 ) are observed during inference . Shuffling attribute - value pairs before converting them to a string representation using attributeseparators also independently provides an absolute gain of 3.5 % over the model which uses attribute - separators without shuffling . Overall , models that combine attribute - shuffling and attribute - OOV are the most accurate with an accuracy of 61.6 % , which represents a 12 % absolute gain over the best baseline model .
B Details of Fine - tuning on CoNLL Dataset
Contextualized Representations for Entity Linking Models in this work are based on BERT . While many studies have tried to explain the effectiveness of BERT for NLP tasks ( Rogers et al . , 2020 ) , the work by Tenney et al . ( 2019 ) is most relevant as they use probing tasks to show that BERT encodes knowledge of entities . This has also been shown empirically by many works that use BERT and other contextualized models for entity linking and disambiguation ( Broscheit , 2019;Shahbazi et al . , 2019;Yamada et al . , 2020;Févry et al . , 2020;Poerner et al . , 2020 ) .
where τ is a temperature parameter . In our implementation , we use a relatively small batch size of 128 , resulting in more frequent parameter updates than if a large batch size were used . Items enqueued early on can thus become outdated with a large queue , so we scale down the queue size to K = 32 , 000 to prevent the queue from becoming stale .
In this case , the adversarial response x ′ i+1 is either a randomly sampled utterance from a random conversation in the dataset or a random response from the following : ( y i+1 , x i+2 , ... , x n , y n ) from D. ( See Appendix A for example . ) 1 https : / / github.com / baber-sos / Explaining-Dialogue-Evaluation-Metrics-using-Adversarial-Behavioral-Analysis
( 4 ) . DA - training using prompt - tuning ( MLM ( Prompt ) ) ( Lester et al . , 2021 ) adds a sequence of prompt tokens to the end of the original sequence . In DA - training , RoBERTa ( the LM ) is fixed and only the prompt tokens are trained . In end - task fine - tuning , both LM and the trained prompt are trainable . We initialize 100 tokens and set the learning rate of the prompt token to 0.3 in DA - training , following the setting in Lester et al . ( 2021 ) .
Our research 's potential drawback is that it adds to the training burden . To tackle this problem , we introduce two techniques to reduce training costs . We greatly minimize the number of parameters that should be trained as well as the training time without sacrificing performance . Notably , our method does not introduce additional overhead for inference . Therefore , we can achieve a large performance improvement while maintaining the original fast decoding speed .
Candidate generation Since the focus of our experiments is on re - ranking , we use a fixed candidate generation model for all experiments that combines the architecture of Wu et al . ( 2020 ) ( Section 3 ) with [ SEP]-separation to generate candidate strings . This model also has no knowledge of the test KB and is trained only once on the CoNLL - Wikidata dataset . It achieves a recall@32 of 91.25 when evaluated on the unseen TAC - KBP 2010 data .
Effect of Positional kernel 's dimension As stated previously in Equation ( 22 ferent d p with the BLEU score . Figure 3 shows the result of ablation experiment on the validation dataset , i.e. the newstest2013 dataset . Notably , d p = 96 has a detrimental effect on the performance of PosNet - Embed and d p = 128 with the best performance . Thereby , we conduct the experiments on the PosNet - Embed in Table 1 with
We conduct an additional experiment to analyze the behavior of baseline models on single - time granularity datasets . We partition MULTITQ by time granularity , ensuring that there is only single granularity of time in each divided dataset ( Day , Month , and Year ) . At the setting of single - day , since the temporal granularity of KG coincides with that of the dataset , our model degenerates to CronKGQA .
Argument Aspect Detection Early work by Fujii and Ishikawa ( 2006 ) focuses mainly on Japanese and restricts aspects to noun - and verb - phrases , extracted via hand - crafted rules . Boltužić and Šnajder ( 2017 ) extract noun - phrases and aggregate them into concepts to analyze the microstructure of claims . Misra et al . ( 2015 ) introduce facets as low level issues , used to support or attack an argumentation . In that , facets are conceptually similar to aspects , but not explicitly phrased and instead seen as abstract concepts that define clusters of semantically similar text - spans of summaries . Bilu et al . ( 2019 ) define commonplace arguments that are valid in several situations for specified actions ( e.g. " ban " ) and topics ( e.g. " smoking " ) . These actions are similar to aspects , but limited in number and manually defined . Gemechu and Reed ( 2019 ) detect , amongst others , concepts and aspects in arguments with models trained on expert annotations . However , in their definition , aspects have to point to a target concept mentioned in the argument . In our definition , aspects refer to a general topic which is not necessarily part of the sentence and our annotation scheme is applicable by non - experts .
Another recent advance in ICL , namely Calibrate Before Use ( CBU ) , involves calibrating the output likelihood of the word tokens that correspond to the labels ( Zhao et al . , 2021 ) . We conduct the same set of experiments with CBU applied and report all three metrics . As shown in Figure 6 , the calibration technique reduces the label sensitivity while generally improving the ICL performance on both GPT - J and GPT - NeoX. Applying CBU can be an effective way to reduce label sensitivity while not sacrificing the performance . Figure 6 : The effect of applying Calibrate Before Use ( CBU ) ( Zhao et al . , 2021 ) . Label sensitivity decreases but the ground - truth label accuracy improves , making CBU ideal for sensitivity reduction . This trend is more apparent in the larger GPT - variant , GPT - NeoX ( 20B ) .
This section motivates the importance of queryawareness for document understanding , then compares and contrasts with existing approaches .
Based on the ranking of triples , ValCAT performs perturbations in a sequential manner , where each step a vulnerable position in the original text is replaced by or inserted with a set of adversarial spans generated by the encoder - decoder language model . The variable - length feature of the adversarial spans renders the language model enough space to produce more contextually appropriate candi - dates to improve fluency . Meanwhile , our variablelength method expands the perturbation units , for it supports multi - word transformations while is compatible with traditional one - to - one transformations .
Our baseline is adopted from the parsing model of Kitaev and Klein ( 2018 ) and . Given a sentence X = { x 1 , ... , x n } , its corresponding constituency parse tree T is composed by a set of labeled spans
Tab . 1 shows the results on En - De , inputting - level cross - lingual PE ( + InXL PE ) and head - level crosslingual PE ( + HeadXL PE ) outperform Transformer BIG by 0.30 and 0.36 BLEU points , and combining these two strategies 2 achieves a 0.69 BLEU point increase . For Ja - En , Zh - En , and En - Zh ( Tab . 2 ) , we observe a similar phenomenon , demonstrating that XL PE on SANs do improve the translation performance for several language pairs . It is worth noting that our approach introduces nearly no additional parameters ( +0.01 M over 282.55 M ) .
To evaluate the plausibility of attribution methods , we measure agreement with human rationales , following Atanasova et al . ( 2020 ) . This evaluation measures how much the attribution scores overlap with human annotations by calculating Mean Average Precision ( MAP ) across a dataset . For each instance in the dataset , Average Precision ( AP ) is calculated by comparing attribution scores ω ( i ) with gold rationales , w ( i ) , where ω ( i ) stands for the attribution scores calculated for the dataset instance x ( i ) and w ( i ) stands for the sequence of binary labels indicating whether the token is annotated as the rationale . For a dataset X = { x ( i ) |i ∈ [ 1 , M ] } , the MAP score is defined as :
Given : Input sequence x , number of negative samples k , noise distribution q , modelp θ . 1 . Pick k unique random positions R = { r 1 , ... , r k } where each r i is 1 ≤ r i ≤ n.
Data collection processes for AER datasets vary in terms of recording conditions , emotional elicitation scheme , and annotation procedure , etc . This work was tested on two typical datasets : IEMO - CAP and MSP - Podcast . The two datasets are both publicly available and differ in various aspects :
Task 2 : Constrained Sentence Generation ( CG ) Generating texts is a direct manifestation of a model 's belief . However , evaluating generated texts is notoriously difficult in NLP , especially without references . Therefore , we design a keywordto - sentence task to make the probing more controllable , which is similar to COMMONGEN ( Lin et al . , 2020 ) . Given a triple < s , r , o > , models need to generate sentences grounded in ( negative ) knowledge , i.e. , add negation cues ( e.g. , not , unable ) in the sentence if necessary , e.g. , Write a short and factual sentence according to commonsense based on the keywords : ( Examples for in - context learning ) Keywords : lion , located at , ocean Sentence : lions do n't live in the ocean .
• topic : The topic of the argument .
A second type of approach begins with various methods for word sense induction , then measures change in terms of the relative prevalence of a term 's different senses ( Frermann and Lapata , 2016 ; Hu et al . , 2019 ; Arefyev and Zhikov , 2020 ; Arefyev and Bykov , 2021 ) . In some cases , authors simply cluster contextual representations for each term , and measure differences in the distributions of clusters between two time periods , rather than dealing with explicit word senses ( Giulianelli et al . , 2020 ; Martinc et al . , 2020b ; Montariol et al . , 2021 ) .
Step 2c : Creating the annotation data For each of the four topics that are part of the preliminary annotation dataset , we use the in - topic model to predict aspects of 629 randomly chosen , unseen arguments from the UKP - Corpus . For the other four topics of the UKP - Corpus , we choose the best cross - topic model to predict aspects for the same amount of samples . To keep a recall of at least 80 % , we choose the ten and fifteen highest - ranked aspect candidates for samples as predicted by the in - topic and cross - topic model , respectively . We remove aspect candidates that include punctuation , begin or end with stopwords , or contain digits .
BERT is a Transformer ( Vaswani et al . , 2017 ) encoder pre - trained by jointly optimizing two unsupervised objectives : masked language model and next sentence prediction . It uses WordPiece ( WP , Wu et al . ( 2016 ) ) subword tokens along with positional embeddings as input , and gradually constructs sentence representations by applying tokenlevel self - attention pooling over a stack of layers L. The result of BERT encoding is a layer - wise representation of the input wordpiece tokens with higher layers representing higher - level abstractions over the input sequence . Thanks to the joint pre - training objective , BERT can encode words and sentences in a unified fashion : the encoding of a sentence or a sentence pair is stored in a special token [ CLS ] .
Typically , models for candidate generation are less complex ( and hence , less precise ) than those used in the following ( re - ranking ) stage since they handle all entities in KB .
Were you able to reproduce the results of the paper ?
However , we also notice that concatenating the context does not help improve the performance of H2H+CXT based on MBart-50M2 M and M2M100 in the setting of En - De and En - Ja , and the setting of En - Es and En - Fr , respectively . We hypothesize that the decrease of BLEU score comes from the noise brought by the context .
Our primary experiments focus on the first two research questions and study the accuracy of the model that uses the re - ranking architecture from Section 3 with the three core components introduced in Section 4 viz . attribute - separators to generate string representations of candidates , along with attribute - OOV and attribute - shuffle for regularization . We compare this against two baselines without these components that use the same architecture and use concatenation and [ SEP]separation instead of attribute - separators . As a reminder , all models are trained as well as validated on CoNLL - Wikidata and evaluated on the completely unseen TAC - KBP 2010 test set .
Lastly , some works build on large LMs ( LLMs ) via special fine - tuning or inference techniques . Chain - of - thought prompting prompts LLMs to generate intermediates steps before reaching the final answer . Cobbe et al . ( 2021 ) fine - tunes a model as a verifier and applies the verifier to rank outputs in the decoding phase . are using a majority vote among outputs to select the best answer . Lewkowycz et al . ( 2022 ) fine - tunes an LLM by a large collection of math - specific datasets combining existing tech - Table 4 : Demonstrations of generated solutions comparing planning - LM and chain - of - thought . Question 1 shows the intermediate step of chain - of - thought has wrong reasoning but still reaches the final answer . Question 2 shows that planning - LM results in a better reasoning strategy since the calculation process is simple and more concrete .
ing amounts of parallel data , we observe that 600k per language is our sweet spot considering the trade - off between resource and performance . Going up to 2 M helps on XNLI , but less significantly compared to the gain going from 250k to 600k . On MLQA , surprisingly , 250k slightly outperforms the other two for translate - train .
Le chronomètre A601X dispose calendrier cumulative - split . gold MQM score 0.167 BiRNN -0.248 BiRNN+Vis - embed - mult2 -0.002 Table 3 : Example of performance of sentence - level multimodal QE . Compared to the baseline prediction ( BiRNN ) , the prediction from the best multimodal model ( BiRNN+Vis - embed - mult2 ) is closer to the gold MQM score . This could be because the word stopwatch is correctly translated as chronomètre in French , and the additional visual feature confirms it . This could lead to an increase in the predicted score to reward the correct part , despite the poor translation ( extracted from the Amazon Reviews Dataset of McAuley et al . , 2015 ) .
The intended use of our code is for academic research . We consider probing publicly available PLMs , which are made available for research as well as end use cases , to be within the intended use of PLMs .
3 . We conduct extensive experiments on three benchmark datasets . The results show that ASoul consistently improves the OOD detection performance , and it obtains new SOTA results .
When implementing AFLite , we follow Sakaguchi et al . ( 2020 ) . We use a smaller training set size of m = 5620 , but keep the remaining hyperparameters unchanged , such that the ensemble consists of n = 64 logistic regression models , the filtering cutoff , k = 500 , and the filtering threshold τ = 0.75 .
Similarly , the statistical model ( Table 4 ) shows that both main factor of DaV - B ( β = 0.0106 , p < 1e-15 ) and its interaction with paraphrase ratio ( β = 0.0078 , p < 1e-15 ) are positive and significant , indicating that as P increases DaV - B has significantly more improvement than the reference model ( Ada - A ) but other paraphrases do not show such a pattern as the main factors are all insignificant and interactions are inconsistent .
-DOCSTART- Multilingual BERT Post - Pretraining Alignment
Next , we find that , in most situations , the performance of H2H can be further boosted by concatenating the constructed context from the table . Taking H2H+CXT based on M2M-100 as an example , comparing with H2H , H2H+CXT obtains 2.1 , 0.6 , and 1.6 points of improvement in En - Zh , En - De , and En - Ja settings , respectively . In terms of H2H+CXT based on MBart-50M2 M , the concatenation of context also boosts the BLEU score for translating schema from En to Zh and Es by 1.5 and 1.2 . The observations demonstrate the benefits of making good use of the constructed context .
Data . We use two data sources : the Penn Treebank ( Marcus et al . , 1993 ) for English constituency parsing and Statistical Parsing of Morphologically Rich Languages ( SPMRL ) 2013 / 2014 shared tasks ( Seddah et al . , 2013 ( Seddah et al . , , 2014 for 8 languages : Basque , French , German , Hebrew , Hungarian , Korean , Polish , and Swedish . We provide the dataset statistics in Table 5 . We perform similar preprocessing as Kitaev and Klein ( 2020 ) , which we explain in App . F. For evaluation , the EVALB Perl script 9 is used to calculate FMeasure of the parse tree . We use only one GPU node for the reported inference times . Further experimental details can be found in App . G .
Position embedding represents the position of the token in a word sequence . A word and an entity appearing at the i - th position in the sequence are represented as D i and E i , respectively . If an entity mention contains multiple words , its position embedding is computed by averaging the embeddings of the corresponding positions ( see Figure 2 ) . Following Devlin et al . ( 2019 ) , we tokenize the document text using the BERT 's wordpiece tokenizer , and insert [ CLS ] and [ SEP ] tokens as the first and last words , respectively .
• MRPC : Microsoft Research Paraphrase Corpus ( Dolan and Brockett , 2005 ) . The task is to predict whether two sentences are semantically equivalent or not . The dataset contains 3.7k train examples from online news sources .
Although we view this work as an important step towards better understanding and evaluation of coherence in summaries , we acknowledge there is much more to do here . In this work , we only collect annotations and analyze coherence errors in summaries of English language books and movie screenplays . Our proposed taxonomy may not cover errors made by text summarization models for other languages and our trained models and analysis are Englishspecific .
The first scheme , which we call attribute - OOV , prevents models from overtly relying on individual [ K i ] tokens and generalize to attributes that are not seen during training . Analogous to how out - of - vocabulary tokens are commonly handled ( Dyer et al . , 2015 , inter alia ) , every [ K i ] token is stochastically replaced with the [ SEP ] token during training with probability p drop . This encourages the model to encode semantics of the attributes in not only the [ K i ] tokens , but also in the [ SEP ] token , which is used when unseen attributes are encountered during inference .
Figure 2 : The screen of the developed grading system for a participant to annotate an answer in Study 1 : 1 gives annotation instruction to a participant ; 2 displays the same answer that the participant scored in the previous screen ; and 3 allows a participant to remove to all existing annotations and start the annotation from scratch again , and the participant is informed that annotation is not mandatory if she believes that there is nothing contributing / hurting the quality of the answer .
Our probing methodology builds upon the edge and layer probing framework . The encoding produced by a frozen BERT model can be seen as a layer - wise snapshot that reflects how the model has constructed the high - level abstractions . Tenney et al . ( 2019b ) introduce the edge probing task design : a simple classifier is tasked with predicting a linguistic property given a pair of spans encoded using a frozen pre - trained model . Tenney et al . ( 2019a ) use edge probing to analyse the layer utilization of a pre - trained BERT model via scalar mixing weights learned during training . We revisit this framework in Section 3 .
The first table shows some representative unnecessary state changes that EDH tasks require for " task success ' in evaluation . For example , in our common sense , it is not necessary that we leave the coffee machine on to successfully make coffee ( indeed , it is better to turn it off after use ) . However , since EDH evaluation requires that the agent exactly follows state changes done in the demonstration , the agent will have to leave coffee machine turned on for a particular validation task , if this was done in its corresponding demonstration .
Most evaluation metrics used in previous paraphrase generation research are not designed for the task itself , but adopted from other evaluation tasks , such as machine translation ( MT ) and summarization . However , paraphrase evaluation is inherently different from the evaluation of most other tasks , because a good paraphrase typically obeys two criteria ( Gleitman and Gleitman , 1970;Chen and Dolan , 2011;Bhagat and Hovy , 2013 ): semantic similarity ( Sim ) and lexical divergence ( Div ) . Sim means that the paraphrase maintains similar semantics to the input sentence , whereas Div requires that the paraphrase possesses lexical or syntactic differences from the input . In contrast , tasks like machine translation have no requirement for Div . It is therefore uncertain whether the metrics borrowed from other tasks perform well in paraphrase evaluation .
After the war ended , his mind was full of food and drink , and he was ready for a quick trip to the bar . He had been to the bar on many occasions . Only in there that he could fully relax and forget about all those mess things ... 0.16 0.14
where L sup is the supervised loss , L rl is the reinforcement learning loss , L att is a novel attention consistency loss and L td is a loss to guide the attention score distributions by tree constraints . λ rl , λ att andλ td are hyper - parameters .
Bill Text As mentioned above , legislators introduce bills to propose laws or amend existing ones in order to further their agenda . We acquire IDs , titles , and introduction dates of bills using the API of propublica.org , a non - profit organisation that collects and provides access to congressional documents . We further collect summaries of the bill 's content , which the API provides for around 95 % of all cases . For bills where no summary is available , we use the full - body texts instead . As we create our data set to study active and passive cosponsorship , we discard all bills for which no cosponsorship links were recorded . Overall , our data set contains information on over 50 , 000 bills .
Knowledge probing works such as LAMA ( Petroni et al . , 2019 ) aim to answer the following question : can models ( e.g. BERT ) which are pretrained on generic text corpora with a language modeling objective be used as knowledge bases ? In our case , the model has been explicitly trained with the link prediction objective , and a knowledge probing experiment would be akin to checking train set performance of link prediction ( which is discussed in § 4.8 ) . Furthermore , we do not claim that KGT5 is as general purpose as large LMs , or that it contains generic world knowledge . Hence we do not perform knowledge probing experiments on datasets such as T - Rex or Google - RE ( Petroni et al . , 2019 ( Das et al . , 2021b ) 73.1 70.4
Since our submission to the KILT leaderboard for the Wizard of Wikipedia , a new system called Hindsight [ Paranjape et al . , 2021 ] achieved even better results on the generation metrics on that particular task . The new system of SEAL has also achieved top results for some metrics on the Natural Questions and TriviaQA benchmarks .
In this paper , we explored multilingual , word - level QE with transformers . We introduced a new architecture based on transformers to perform wordlevel QE . The implementation of the architecture , which is based on Hugging Face ( Wolf et al . , 2020 ) , has been integrated into the TransQuest framework ( Ranasinghe et al . , 2020b ) which won the WMT 2020 QE task ) on sentencelevel direct assessment ( Ranasinghe et al . , 2020a ) 2 .
Besides NMT models , we also trained a phrasebased statistical machine translation ( PB - SMT ) schema translation model with Moses 3 ( Koehn et al . , 2007 ) , with the same data split .
First , we ablate the influence of query expansion and the feedback documents on lexical retrieval . We retrieve only using the query and remove the feedback documents from the retrieval and evaluation , i.e. we use the residual collection , even though the feedback documents are not used . From the first section of Table 4 we can observe a large performance drop . This shows that BM25 - QE is successfully able to exploit the feedback documents and retrieve more relevant documents .
MASKING We propose a novel data - centric debiasing alternative based on token masking . Instead of removing spurious artifacts altogether , we reserve a special token in the vocabulary of the model that we use as replacement for spurious artifacts . We then fine - tune the model on the masked data . Intuitively , this way we encourage the model to blend all artifacts to a single contextualized representation that will never appear during testing , also avoiding to redistribute the informativeness of spurious lexical items to surrounding tokens . As for REMOVAL , we experiment with S ¬I and S I masking variants .
The CNNs for the Amazon Products dataset also behaved in a similar way ( Figure 6 -bottom ) , except that disabling rank C features slightly undermined , not increased , performance . This implies that even the rank C features contain a certain amount of useful knowledge for this classifier . 4
In this section , we conduct experiments on our proposed schema translation dataset to evaluate the effectiveness of our approach . Furthermore , we ablate different ways of context modeling in our approach to understand their contributions . At last , we conduct a qualitative analysis and show example cases and their predicting results .
Our question generation method uses a set of unannotated sentences from which the questions will be generated . We compare three selection methods . First , we consider a scenario where the application developer does not manually collect any sentence , but simply gives the name ( or topic ) of the target domain . In our case , the topics are " Physics " , " Biology " and " Chemistry " since these are the main domains in SciQ. A simple information retrieval strategy is then applied to automatically mine sentences from Wikipedia . We first compute a list of Wikipedia categories by recursively visiting all subcategories starting from the target topic names . The maximum recursion number is limited to 4 . We then extract the summary ( head paragraph of each Wikipedia article ) for each of the articles matching the previously extracted categories and subcategories . We only keep articles with more than 800 average visitors per day for the last ten days ( on April 27 , 2021 ) , resulting in 12 656 pages .
The candidate with the highest score is chosen as the correct entity , i.e.
Manual Paraphrase . Next , we ask human annotators to paraphrase the templated utterances to better match the real - world natural language distribution . We design an interface that dynamically displays a multimodal scene that features either a still image ( static dialog phase ) or a user egocentric video ( active dialog phase ) . When clicking on a specific turn of a dialog , the corresponding visual input is shown in the display panel to help annotators navigate through the entire dialog flow . We ask the annotators to pay attention to detailed and sophisticated spatial - temporal relations of objects and encourage writing interesting shopping experiences . The paraphrases are collected from more than 20 different linguistic experts for diverse language patterns / usages .
We propose approaches to Quality Estimation ( QE ) for Machine Translation that explore both text and visual modalities for Multimodal QE . We compare various multimodality integration and fusion strategies . For both sentence - level and document - level predictions , we show that state - of - the - art neural and feature - based QE frameworks obtain better results when using the additional modality .
We start with examining the relation between media ideology and their stances . We first study do media tend to quote people of the same or opposite ideologies ? To answer this question , we count the average number of political figures quoted as source entities in each article . Interestingly , media from both sides are more likely to quote Republican politicians , as seen in Fig . 4 . This is consistent with recent study on U.S. TV media ( Hong et al . ,9 2021 ) , where the authors show that Republicans receive more screen time as well as get longer TV interviews time than Democrats . 10 Additionally , left - leaning outlets use more quotes than their right counterparts , which also aligns with previous observations ( Welch et al . , 1998 ) .
Given a multimodal input , we employ a sequenceto - sequence model BART ( Lewis et al . , 2020 ) to generate the output index sequence y .
Candidate Re - ranking The candidate reranking approach uses a BERT - based crossattention encoder to jointly encode a mention and its context along with each candidate from E ( Logeswaran et al . , 2019 ) . Specifically , the mention m is concatenated with its context on the left ( c l ) , its context on the right ( c r ) , and a single candidate entity e ∈ E. An [ SEP ] token , which is used in BERT to separate inputs from different segments , is used here to separate the mention in context , from the candidate . This concatenated string is encoded using BERT 3 to obtain , h m , e a representation for this mention / candidate pair ( from the [ CLS ] token ) . Given a candidate list E of size K generated in the previous stage , K scores are generated for each mention , which are subsequently scored using a dot - product with a learned weight vector ( w ) . Thus ,
Question Representation Unlike standard DKT , which treats questions as IDs or simple handcrafted features , we represent questions fully in text ( e.g. " she eats " in Figure 1 ) . This is a key contribution of our work , required by our eventual goal of generating questions in text , and allows the model to leverage similarity across linguistic features . We thus represent a question q as a sequence of words , with prefix and suffix tokens :
-DOCSTART- The SOFC - Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain
To mitigate the risk of performance overestimation associated with annotation artifacts , Zellers et al . ( 2019 ) advocate adversarial dataset construction , such that benchmarks will co - evolve with language models . This may be difficult to scale in knowledge - intensive domains , as expert validation of adversarially generated benchmarks is typically required . Additionally , in high - stakes domains such as medicine , information - rich inferences should be preferred over correct but trivial inferences that time - constrained expert annotators may be rationally incentivized to produce , because entropy - reducing inferences are more useful for downstream tasks .
• A positive data point is a text sequence x from the data and position in the sequence t.
We include the pseudocode algorithm of the proposed decoding method in Algorithm 1 . Note that we can use the nonzero operation to find and merge adjacent non - null entries as it returns the entries sorted in lexicographic order . This ensures that the order of entries seen in consecutive order if they correspond to the same hyper - relational fact .
A softmax classifier is added on top of the final hidden state of the [ CLS ] token ; ( 2 ) on MLQA , we concatenate the question with the context , and add a [ SEP ] token in between . We add two linear layers on top of mBERT followed by softmax over the context tokens to predict answer start and end positions , respectively . We conduct experiments in two settings : 1 . Zeroshot cross - lingual transfer , where training data is available in English but not in target languages . 2 . Translate - train , where the English training set is ( machine ) translated to all the target languages . For the latter setting , we perform data augmentation with code - switched inputs , when training on languages other than English . For example , a Spanish question q es and context c es pair can be augmented to two question - context pairs ( q es , c en ) and ( q en , c es ) with code - switching , resulting in 2x training data 2 . The same goes for XNLI with premises and hypotheses . The code - switching is always between English , and a target language . During training , we ensure the two augmented pairs appear in the same batch .
Position emb . ( Lewis et al . , 2020 ) to generate referent entity titles of target mentions in an autoregressive manner . Barba et al . ( 2022 ) formulated ED as a text extraction problem ; they fed the document and candidate entity titles to BART and Longformer ( Beltagy et al . , 2020 ) and disambiguated a mention in the document by extracting the referent entity title of the mention . However , unlike our model , these models addressed the task based only on local contextual information .
The dataset contains 130k train examples ,
Argument Quality We introduce a novel method to evaluate generated arguments based on the argument quality detection approach proposed by Gretz et al . ( 2020b ) . They create an argument quality dataset that contains around 30,000 arguments over 71 topics . For each argument , annotators were asked whether or not they would recommend a friend to use the displayed argument in a speech . The quality scores for each argument result from a weighted average ( WA ) or MACE Probability function of all annotations and range between 0 ( lowest quality ) and 1.0 ( highest quality ) . We use the WA - score as label , the same model ( BERT BASE ) and hyperparameters as given in the original paper , and reproduce the reported correlations of .52 ( Pearson ) and .48 ( Spearman ) on the test dataset ( averaged over five different seeds ) . The model predicts an average argument quality of .71 for the Arg - CTRL REDDIT arguments , .75 for the training arguments of the UKP - Corpus , and even .76 for the Arg - CTRL CC arguments . It shows that our model is able to produce arguments that are generally on a similar quality level as arguments made by humans . The difference of 5 points between the Arg - CTRL CC and the Arg - CTRL REDDIT , however , shows that training on REDDIT data leads to a lower quality of generated arguments . Exemplary Human : Nuclear energy produces waste that stays radioactive for thousands of years and pollutes the environment . Arg - CTRLCC : ( nuclear energy PRO waste . ) The only thing that 's going to be left over is the spent fuel rods , which are not very radioactive and can be reprocessed [ ... ] Arg - CTRLCC : ( nuclear energy PRO radioactive . ) The radioactivity of the waste is not a problem , because it will decay away in about 300 years or so [ ... ] Arg - CTRLCC : ( nuclear energy PRO environment . ) The fact that the radioactive waste is not only stored in a safe manner , but also protected from any potential threat to human health or the environment for thousands of years [ ... ] Human : School uniforms are expensive and affect the pupil 's individuality . Arg - CTRLCC : ( school uniforms PRO expensive . ) The cost of the uniform is not that much more than a regular shirt and shorts [ ... ] Arg - CTRLCC : ( school uniforms PRO individuality . ) The uniform is a symbol of unity and identity , which helps to foster an environment where students can feel comfortable about expressing their own individual style without being judged [ ... ] for three topics , we show the generated arguments with the highest and lowest argument quality in Table 4 ( see App . E for the full table ) .
Next , we project the secondary features into primary ones to obtain additional orthogonal features f o :
B5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ? Section 3 and Appendix A.3 B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be . Section 3 and Appendix A.3
B5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ? Section3 and Appendix D B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be . Section3 and Appendix D.1
We measure inter - annotator agreement by computing Cohens kappa ( K ) and observe substantial to high agreement across all metrics . Table 3 shares the averaged ratings from both evaluators . For each metric , we highlight in bold the best performing model ( s ) and mark with an asterisk the model ( s ) where the difference from the best is at least 5 % . We further plot ( in Appendix A ) the scores by each metric in Figure 10 , the variation of each metric across models in Figure 11 , and the distribution of scores for each metric in Figure 12 .
All our experiments are run parallelly on 4 NVIDIA Tesla V100 GPUs ; for smaller K values ( e.g. , K = 100 ) , most experiments require less than 1 GPU hour , while a setting with a larger K value ( e.g. , K = 1000 ) may require 2 GPU hours .
How can we identify when a model is unsure of its prediction ? The self - consistency method provides a solution . In sampling diverse reasoning paths and answers , self - consistency is found to be highly correlated with accuracy , suggesting that it could provide an uncertainty estimate and confer abilities for the model to " know when it does n't know " . Thus , we begin the VE framework by using the consistency method to sample n diverse reasoning paths for a prediction task . The highly consistent predictions are left as - is . When consistency is lower than ⌈n / 2⌉ , i.e. the majority can not agree on the same answer , we label it as " uncertain " .
Experiment sentence detection . Table 5 shows our results on the detection of experimentdescribing sentences . The neural models with bytepair encoding embeddings or BERT clearly outperform the SVM and logistic regression models . Within the neural models , BERT and SciBERT add the most value , both when using their embeddings as another input to the BiLSTM and when finetuning the original BERT models . Note that even the general - domain BERT is strong enough to cope with non - standard domains . Nevertheless , models based on SciBERT outperform BERT - based models , indicating that in - domain information is indeed beneficial . For performance reasons , we use BERT - base in our experiments , but for the sake of completeness , we also run BERT - large for the task of detecting experiment sentences . Because it did not outperform BERT - base in our cross - validation based development setting , we did not further experiment with BERT - large . However , we found that it resulted in the best F1 - score achieved on our test set . In general , SciBERT - based models provide very good performance and seem most robust across dev and test sets . Overall , achieving F1 - scores around 67.0 - 68.6 , such a retrieval model may already be useful in production . However , there certainly is room for improvement . Entity mention extraction . Table 6 provides our results on entity mention detection and typing .
Knowledge Tracing ( KT ) seeks to model a student 's knowledge state from their answer history in order to help individualize exercise sequences ( Corbett and Anderson , 1995 ) . This draws inspiration from traditional education curriculum practices , such as distributed spacing of vocabulary ( Bloom and Shuell , 1981 ) and mixed review in mathematics ( Rohrer , 2009 ) . To address simplifying assumptions in earlier KT approaches , such as discrete knowledge representations , Piech et al . ( 2015 ) introduced Deep Knowledge Tracing ( DKT ) , which uses RNNs to enable more complex knowledge representations for students . Recently , SAINT+ ( Shin et al . , 2020 ) showed state - of - the - art performance on the popular EdNet KT task using a Transformer model to capture temporal information across activities , motivating our use of Transformer LMs .
Implementation . We adopt a pretrained transformer encoder - decoder ( sequence - to - sequence ) WS - BART ( Lewis et al . , 2020a ) as the backbone of the generator in our proposed method MB - RPG . To improve the efficiency of reinforcement learning , we model the reinforced data valuator model M ψ as a pretrained BERT followed by two fullyconnected trainable layers as the head for policy output . BERT serves as a feature extractor and is kept fixed during policy learning . We present other details for our method in appendix .
Class - conditional language models . Classconditional language models ( CC - LMs ) , as the Conditional Transformer Language ( CTRL ) model ( Keskar et al . , 2019 ) , train or fine - tune the weights θ of a single neural model directly for controllable generation , by appending a control code in the beginning of a training sequence . The control code indicates the constraint to verify and is related to a class containing texts that satisfy the constraint .
Amazon product data contains customer reviews from 24 product categories ( He and McAuley , 2016 ) . Our goal is to predict the product category based on the content of the review . The 24 classes are split into 10 , 5 , 9 for training , validation and test respectively .
To facilitate the research study , we construct the first parallel dataset for schema translation written in six different languages . It consists of 3,158 tables with 11,979 headers written in six differ - ent languages , including English , Chinese , French , German , Spanish , and Japanese .
( 2 ) He said this could not be confirmed , but was quoted by Reuters . Hide Caption : A woman reacts after her car was blown up near an Islamist group rally in Dhaka .
To evaluate the computation complexity of different positional encoding methods , we manually con- Furthermore , we conduct an analysis of the GPU memory consumption of the Transformer base model with different methods on the WMT 2014 En - De and En - Fr tasks . This is done to determine their relative space complexity . The results of this analysis can be found in Appendix A.2 , which demonstrate that all methods exhibit comparable space complexity , except for RPE , which consumes more GPU memory on the WMT 2014 En - Fr task .
Few - shot learning for natural language understanding ( NLU ) has been significantly advanced by pretrained language models ( PLMs ; Brown et al . , 2020 ; Schick and Schütze , 2021a , b ) . With the goal of learning a new task with very few ( usually less than a hundred ) samples , few - shot learning benefits from the prior knowledge stored in PLMs . Various few - shot methods based on PLMs and prompting have been proposed ( Liu et al . , 2021b ; Menon et al . , 2021 ; Gao et al . , 2020 ) .
We conduct ablation studies on CAST to analyze the contributions of our predefined entity types and structural relationships for context modeling . First , we evaluate the variant of CAST without entity types . Next , we evaluate the performance of CAST , without structural relations . Finally , we erase all kinds of relations in CAST which is identical to H2H+CXT . We report the performance of models based on M2M-100 in the setting of En - De and En - Fr in Table 6 .
In this section , we present extensive empirical evaluation results on comparing our method with its various counterparts on four commonly used paraphrase generation datasets 1 .
Our analyses were sped up using multiprocessing and fuzzy regex . To do so , we split the subcorpus across multiple pieces . These runs take about 3 days across 40 CPU Cores , 60 GB of RAM and less than 600 GB hard disk space . We report the mean and standard deviation for the number of tokenizations a word has across the portion of the Pile corpus considered . These are also reported as a function of word length and its frequency of occurrence in the corpus .
For these experiments , models from Section 5.4 are further trained with increasing amounts of data from the TAC - KBP 2010 training set . A sample of 200 documents is held out from the training data as a validation set . The models are trained with the exact same configuration as the base models , except with a smaller constant learning rate of 2 × 10 −6 to not overfit on the small amounts of data . Unsurprisingly , the accuracy of all models increases as the amount of TAC training data in- Crucially , the model with only attribute separators is the most accurate model across the spectrum . Moreover , the difference between this model and the baseline model sharply increases as the amount of schema - aware data decreases ( e.g. when using 13 annotated documents , i.e. 1 % of the training data , we get a 9 % boost in accuracy over the model that does not see any schema - aware data ) . These trends show that our models are not only useful in settings without any data from the target KB , but also in settings where limited data is available .
In this section , we study the effect of cube - pruning and identify directions for future research . Further analysis is shown in Appendix F .
While vanilla distillation and meta distillation employ a two - stage training approach , online distillation and LGTM employ a one - stage joint training strategy for the teacher and student models . The key difference is whether to involve fine - tuning the teacher network on target task . In this study , we investigate the impact of the teacher network 's state on the student network .
Confidence - order Model Figure 3 shows an example of the inference performed by our confidence - order model fine - tuned on the CoNLL dataset . The document is obtained from the test set of the CoNLL dataset . As shown in the figure , the model starts with unambiguous player names to recognize the topic of the document , and subsequently resolves the mentions that are challenging to resolve . Notably , the model correctly resolves the mention " Nigel Walker " to the corresponding former rugby player instead of a football player , and the mention " Matthew Burke " to the correct former Australian rugby player born in 1973 instead of the former Australian rugby player born in 1964 . This is accomplished by resolving other contextual mentions , including their colleague players , in advance . These two mentions are denoted in red in the figure . Note that our local model fails to resolve both mentions , and our natural - order model fails to resolve " Matthew Burke . "